Skipping 5BK8, dataset/human_proteins/human_protein_ph_h2_files/5BK8/5BK8_ph_vec.npy already exists
Skipping 5R85, dataset/human_proteins/human_protein_ph_h2_files/5R85/5R85_ph_vec.npy already exists
Skipping 5R86, dataset/human_proteins/human_protein_ph_h2_files/5R86/5R86_ph_vec.npy already exists
Skipping 5R87, dataset/human_proteins/human_protein_ph_h2_files/5R87/5R87_ph_vec.npy already exists
Skipping 5R88, dataset/human_proteins/human_protein_ph_h2_files/5R88/5R88_ph_vec.npy already exists
2024-03-06 17:43:34.991864: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:43:35.035393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:43:36.412701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R89 ph vector generated, counter: 1
2024-03-06 17:43:40.717665: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:43:40.763902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:43:41.667517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8A ph vector generated, counter: 2
2024-03-06 17:43:44.807357: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:43:44.854948: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:43:45.946021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8B ph vector generated, counter: 3
2024-03-06 17:43:49.461768: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:43:49.506495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:43:50.538454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8C ph vector generated, counter: 4
2024-03-06 17:43:53.932216: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:43:53.974708: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:43:55.085518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8D ph vector generated, counter: 5
2024-03-06 17:43:58.407743: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:43:58.450266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:43:59.392982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8E ph vector generated, counter: 6
2024-03-06 17:44:02.932633: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:02.975737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:03.978290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8F ph vector generated, counter: 7
2024-03-06 17:44:07.719302: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:07.769268: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:08.884538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8G ph vector generated, counter: 8
2024-03-06 17:44:12.858362: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:12.900958: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:14.147142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8H ph vector generated, counter: 9
2024-03-06 17:44:17.891496: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:17.935443: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:19.051031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8I ph vector generated, counter: 10
2024-03-06 17:44:23.076804: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:23.120109: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:24.313066: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8J ph vector generated, counter: 11
2024-03-06 17:44:28.415772: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:28.458873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:29.612034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8K ph vector generated, counter: 12
2024-03-06 17:44:34.001577: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:34.045014: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:35.002569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8L ph vector generated, counter: 13
2024-03-06 17:44:38.722867: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:38.770542: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:40.237545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8M ph vector generated, counter: 14
2024-03-06 17:44:44.434368: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:44.477598: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:45.619672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8N ph vector generated, counter: 15
2024-03-06 17:44:49.047701: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:49.090229: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:50.277643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8O ph vector generated, counter: 16
2024-03-06 17:44:53.823708: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:53.865841: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:54.971573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8P ph vector generated, counter: 17
2024-03-06 17:44:58.701054: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:44:58.743993: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:44:59.878522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
5R8Q ph vector generated, counter: 18
2024-03-06 17:45:03.845576: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:45:03.887621: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:45:04.975899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6AKR ph vector generated, counter: 19
2024-03-06 17:45:08.867038: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:45:08.909022: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:45:09.848244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6CDY ph vector generated, counter: 20
2024-03-06 17:45:13.770358: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:45:13.812748: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:45:14.896859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6E7I ph vector generated, counter: 21
2024-03-06 17:45:19.000024: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:45:19.042566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:45:20.008452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6EOY ph vector generated, counter: 22
2024-03-06 17:45:23.465853: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:45:23.510708: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:45:24.510629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.307955 ), (0., 1.3189503), (0., 1.3238097), (0., 1.3257611),
       (0., 1.3257818), (0., 1.3263974), (0., 1.3265537), (0., 1.3269373),
       (0., 1.3272262), (0., 1.3274013), (0., 1.3275719), (0., 1.3276249),
       (0., 1.328183 ), (0., 1.328269 ), (0., 1.3282781), (0., 1.3283678),
       (0., 1.3283845), (0., 1.3284484), (0., 1.328694 ), (0., 1.328844 ),
       (0., 1.329222 ), (0., 1.3292495), (0., 1.3294393), (0., 1.329453 ),
       (0., 1.3295664), (0., 1.3295729), (0., 1.3297523), (0., 1.3298051),
       (0., 1.3300568), (0., 1.3301392), (0., 1.3301702), (0., 1.3301854),
       (0., 1.3302509), (0., 1.3302666), (0., 1.3305202), (0., 1.3307736),
       (0., 1.3308927), (0., 1.3310231), (0., 1.3311864), (0., 1.3312265),
       (0., 1.3313212), (0., 1.3313217), (0., 1.3314525), (0., 1.3315381),
       (0., 1.3315861), (0., 1.3316209), (0., 1.3317398), (0., 1.3318316),
       (0., 1.3318549), (0., 1.3321515), (0., 1.33221  ), (0., 1.3323194),
       (0., 1.3323975), (0., 1.3324033), (0., 1.3324072), (0., 1.3324467),
       (0., 1.3324629), (0., 1.33261  ), (0., 1.3326346), (0., 1.3327217),
       (0., 1.3327329), (0., 1.3327466), (0., 1.3329197), (0., 1.3329384),
       (0., 1.332995 ), (0., 1.3330553), (0., 1.3331853), (0., 1.3332403),
       (0., 1.3333628), (0., 1.3333874), (0., 1.3333892), (0., 1.3334259),
       (0., 1.3334812), (0., 1.333509 ), (0., 1.3335947), (0., 1.3336058),
       (0., 1.3338511), (0., 1.3339406), (0., 1.3339919), (0., 1.3340386),
       (0., 1.3341016), (0., 1.3341581), (0., 1.3342177), (0., 1.3345261),
       (0., 1.334594 ), (0., 1.3346175), (0., 1.3347738), (0., 1.3348686),
       (0., 1.3349414), (0., 1.3350024), (0., 1.3350375), (0., 1.3352699),
       (0., 1.3353534), (0., 1.3354108), (0., 1.3354663), (0., 1.3356923),
       (0., 1.3357141), (0., 1.3357335), (0., 1.3357683), (0., 1.3358157),
       (0., 1.3359742), (0., 1.3361553), (0., 1.3362074), (0., 1.3367594),
       (0., 1.3367647), (0., 1.3368934), (0., 1.3371452), (0., 1.3371917),
       (0., 1.3371919), (0., 1.3372906), (0., 1.3374043), (0., 1.3374233),
       (0., 1.3377876), (0., 1.3379501), (0., 1.33853  ), (0., 1.3389659),
       (0., 1.3390063), (0., 1.3394184), (0., 1.3394902), (0., 1.3395212),
       (0., 1.3405024), (0., 1.3409774), (0., 1.3416193), (0., 1.3418808),
       (0., 1.3426266), (0., 1.3429496), (0., 1.4461056), (0., 1.4485923),
       (0., 1.449387 ), (0., 1.4511925), (0., 1.4512901), (0., 1.4514987),
       (0., 1.4515803), (0., 1.4516019), (0., 1.4520357), (0., 1.4537221),
       (0., 1.453914 ), (0., 1.4539671), (0., 1.4541724), (0., 1.4541807),
       (0., 1.4544814), (0., 1.454752 ), (0., 1.45478  ), (0., 1.454804 ),
       (0., 1.4550521), (0., 1.4550999), (0., 1.4552593), (0., 1.4553273),
       (0., 1.4554619), (0., 1.4555289), (0., 1.4556624), (0., 1.4557365),
       (0., 1.4558986), (0., 1.456029 ), (0., 1.4560851), (0., 1.4561527),
       (0., 1.4562374), (0., 1.4563333), (0., 1.456422 ), (0., 1.4566077),
       (0., 1.4566861), (0., 1.4566985), (0., 1.4567624), (0., 1.4568074),
       (0., 1.4568574), (0., 1.4568975), (0., 1.457163 ), (0., 1.4572011),
       (0., 1.4572655), (0., 1.4575119), (0., 1.4575737), (0., 1.4575752),
       (0., 1.4576154), (0., 1.4578195), (0., 1.4578252), (0., 1.457919 ),
       (0., 1.4580009), (0., 1.4580057), (0., 1.4580282), (0., 1.4580313),
       (0., 1.458192 ), (0., 1.4581943), (0., 1.4581951), (0., 1.4583244),
       (0., 1.4585143), (0., 1.4585626), (0., 1.458655 ), (0., 1.4586579),
       (0., 1.4587364), (0., 1.458812 ), (0., 1.459029 ), (0., 1.4591061),
       (0., 1.4592233), (0., 1.4592414), (0., 1.4593166), (0., 1.4593757),
       (0., 1.4594339), (0., 1.4595163), (0., 1.4596841), (0., 1.4596968),
       (0., 1.4597682), (0., 1.4600406), (0., 1.4601603), (0., 1.4601908),
       (0., 1.4602157), (0., 1.4603952), (0., 1.4604146), (0., 1.4604838),
       (0., 1.4605875), (0., 1.4606401), (0., 1.4607916), (0., 1.4607937),
       (0., 1.4608865), (0., 1.4608896), (0., 1.4608978), (0., 1.4609407),
       (0., 1.4609561), (0., 1.4610302), (0., 1.4610633), (0., 1.4611073),
       (0., 1.4612173), (0., 1.4612695), (0., 1.461404 ), (0., 1.4614465),
       (0., 1.4616052), (0., 1.4616396), (0., 1.4616549), (0., 1.4616617),
       (0., 1.4619122), (0., 1.4620107), (0., 1.4622694), (0., 1.4622743),
       (0., 1.4626989), (0., 1.4628639), (0., 1.4631325), (0., 1.4636537),
       (0., 1.4640839), (0., 1.46409  ), (0., 1.4643304), (0., 1.4647434),
       (0., 1.4651752), (0., 1.4658829), (0., 1.4660822), (0., 1.4661136),
       (0., 1.4661328), (0., 1.4661702), (0., 1.4671363), (0., 1.4678313),
       (0., 1.4690229), (0., 1.4712542), (0., 1.4714156), (0., 1.4721373),
       (0., 1.4774374), (0., 1.4995078), (0., 1.5124686), (0., 1.5138121),
       (0., 1.5153224), (0., 1.5155836), (0., 1.5162278), (0., 1.5167242),
       (0., 1.5168717), (0., 1.5173793), (0., 1.5174288), (0., 1.5178542),
       (0., 1.5180233), (0., 1.5182191), (0., 1.5182308), (0., 1.5183043),
       (0., 1.5184258), (0., 1.518427 ), (0., 1.5185157), (0., 1.5187532),
       (0., 1.5189058), (0., 1.5189797), (0., 1.5190654), (0., 1.5193167),
       (0., 1.5195011), (0., 1.5195658), (0., 1.5196364), (0., 1.5198267),
       (0., 1.5201336), (0., 1.5203558), (0., 1.5204116), (0., 1.5204227),
       (0., 1.5204426), (0., 1.5204494), (0., 1.5205115), (0., 1.5205493),
       (0., 1.520556 ), (0., 1.520602 ), (0., 1.5207554), (0., 1.5207586),
       (0., 1.5208179), (0., 1.5209751), (0., 1.5214369), (0., 1.5215672),
       (0., 1.5216326), (0., 1.5216732), (0., 1.5217098), (0., 1.5217562),
       (0., 1.5218654), (0., 1.5218891), (0., 1.5218903), (0., 1.5219598),
       (0., 1.5221177), (0., 1.5221189), (0., 1.5221207), (0., 1.5221794),
       (0., 1.522297 ), (0., 1.5223631), (0., 1.5224106), (0., 1.5224324),
       (0., 1.5227442), (0., 1.5228218), (0., 1.522844 ), (0., 1.5229293),
       (0., 1.5229579), (0., 1.523028 ), (0., 1.5230659), (0., 1.5232357),
       (0., 1.5233704), (0., 1.5233763), (0., 1.5233837), (0., 1.5234152),
       (0., 1.5234727), (0., 1.5235345), (0., 1.523584 ), (0., 1.5236304),
       (0., 1.5236537), (0., 1.5236765), (0., 1.5237265), (0., 1.5238315),
       (0., 1.5239505), (0., 1.5239755), (0., 1.5240194), (0., 1.5240341),
       (0., 1.5240506), (0., 1.5241348), (0., 1.5241597), (0., 1.5242498),
       (0., 1.5242839), (0., 1.5243374), (0., 1.5245905), (0., 1.5246615),
       (0., 1.52468  ), (0., 1.5247966), (0., 1.5250475), (0., 1.5250673),
       (0., 1.5252764), (0., 1.5253437), (0., 1.5255113), (0., 1.52554  ),
       (0., 1.5255551), (0., 1.5256863), (0., 1.5257744), (0., 1.5259898),
       (0., 1.5261391), (0., 1.5262121), (0., 1.5262916), (0., 1.5263113),
       (0., 1.526389 ), (0., 1.5264199), (0., 1.5264397), (0., 1.5266042),
       (0., 1.5267148), (0., 1.5267277), (0., 1.5267694), (0., 1.5268002),
       (0., 1.5268447), (0., 1.5270191), (0., 1.5271443), (0., 1.5275195),
       (0., 1.5276116), (0., 1.5276419), (0., 1.5279102), (0., 1.5281553),
       (0., 1.5283991), (0., 1.5285671), (0., 1.5295835), (0., 1.5296788)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.237768 , 8.68115  ), (7.2088847, 7.767881 ),
       (7.0821114, 7.689977 ), (7.0281296, 7.2872996),
       (6.845793 , 6.8848457), (6.770406 , 7.1751456),
       (6.3735695, 7.0555525), (6.3396225, 6.96902  ),
       (6.332012 , 6.9939337), (6.106029 , 7.7937465),
       (6.0509715, 7.0998   ), (5.926788 , 7.075691 ),
       (5.779731 , 6.630762 ), (5.7720118, 6.067058 ),
       (5.6948786, 5.908947 ), (5.6277003, 6.1450086),
       (5.617442 , 8.764479 ), (5.5642934, 8.301624 ),
       (5.4678936, 8.368657 ), (5.4038415, 7.2492304),
       (5.299012 , 7.2978897), (4.449637 , 7.839249 ),
       (4.321457 , 8.008842 ), (4.0138283, 4.106974 ),
       (3.9932618, 4.267374 ), (3.9637394, 4.704086 ),
       (3.9311745, 4.306539 ), (3.6100907, 8.714261 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.524675 , 10.076065 ), (9.203794 ,  9.258495 ),
       (9.177969 ,  9.242981 ), (9.160996 , 10.591092 ),
       (7.7977977,  7.9221654), (6.7910557,  6.8913193)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.307955026626587), (0.0, 1.3189502954483032), (0.0, 1.3238097429275513), (0.0, 1.325761079788208), (0.0, 1.3257818222045898), (0.0, 1.32639741897583), (0.0, 1.3265537023544312), (0.0, 1.3269373178482056), (0.0, 1.327226161956787), (0.0, 1.3274012804031372), (0.0, 1.3275718688964844), (0.0, 1.3276249170303345), (0.0, 1.3281830549240112), (0.0, 1.3282690048217773), (0.0, 1.3282780647277832), (0.0, 1.328367829322815), (0.0, 1.328384518623352), (0.0, 1.3284484148025513), (0.0, 1.3286939859390259), (0.0, 1.3288439512252808), (0.0, 1.3292219638824463), (0.0, 1.3292495012283325), (0.0, 1.3294392824172974), (0.0, 1.3294529914855957), (0.0, 1.3295663595199585), (0.0, 1.3295729160308838), (0.0, 1.3297523260116577), (0.0, 1.3298051357269287), (0.0, 1.3300567865371704), (0.0, 1.33013916015625), (0.0, 1.3301701545715332), (0.0, 1.3301854133605957), (0.0, 1.330250859260559), (0.0, 1.3302665948867798), (0.0, 1.3305201530456543), (0.0, 1.3307735919952393), (0.0, 1.3308926820755005), (0.0, 1.331023097038269), (0.0, 1.3311864137649536), (0.0, 1.3312264680862427), (0.0, 1.3313212394714355), (0.0, 1.3313217163085938), (0.0, 1.331452488899231), (0.0, 1.3315380811691284), (0.0, 1.3315861225128174), (0.0, 1.3316209316253662), (0.0, 1.3317397832870483), (0.0, 1.3318315744400024), (0.0, 1.3318549394607544), (0.0, 1.3321515321731567), (0.0, 1.3322099447250366), (0.0, 1.3323193788528442), (0.0, 1.3323974609375), (0.0, 1.332403302192688), (0.0, 1.3324072360992432), (0.0, 1.3324466943740845), (0.0, 1.3324629068374634), (0.0, 1.332610011100769), (0.0, 1.3326345682144165), (0.0, 1.3327217102050781), (0.0, 1.332732915878296), (0.0, 1.3327466249465942), (0.0, 1.332919716835022), (0.0, 1.3329384326934814), (0.0, 1.332995057106018), (0.0, 1.3330552577972412), (0.0, 1.3331853151321411), (0.0, 1.333240270614624), (0.0, 1.3333628177642822), (0.0, 1.3333873748779297), (0.0, 1.333389163017273), (0.0, 1.3334258794784546), (0.0, 1.3334811925888062), (0.0, 1.3335089683532715), (0.0, 1.3335946798324585), (0.0, 1.3336057662963867), (0.0, 1.3338510990142822), (0.0, 1.3339406251907349), (0.0, 1.3339918851852417), (0.0, 1.3340386152267456), (0.0, 1.3341015577316284), (0.0, 1.3341580629348755), (0.0, 1.3342176675796509), (0.0, 1.3345260620117188), (0.0, 1.3345940113067627), (0.0, 1.3346174955368042), (0.0, 1.3347737789154053), (0.0, 1.3348685503005981), (0.0, 1.3349413871765137), (0.0, 1.3350024223327637), (0.0, 1.3350374698638916), (0.0, 1.3352699279785156), (0.0, 1.3353533744812012), (0.0, 1.3354108333587646), (0.0, 1.3354662656784058), (0.0, 1.335692286491394), (0.0, 1.3357141017913818), (0.0, 1.3357335329055786), (0.0, 1.3357683420181274), (0.0, 1.335815668106079), (0.0, 1.3359742164611816), (0.0, 1.3361552953720093), (0.0, 1.336207389831543), (0.0, 1.3367594480514526), (0.0, 1.3367646932601929), (0.0, 1.3368934392929077), (0.0, 1.337145209312439), (0.0, 1.3371917009353638), (0.0, 1.3371919393539429), (0.0, 1.337290644645691), (0.0, 1.3374042510986328), (0.0, 1.337423324584961), (0.0, 1.3377876281738281), (0.0, 1.3379501104354858), (0.0, 1.3385299444198608), (0.0, 1.338965892791748), (0.0, 1.3390063047409058), (0.0, 1.3394184112548828), (0.0, 1.3394901752471924), (0.0, 1.3395211696624756), (0.0, 1.340502381324768), (0.0, 1.340977430343628), (0.0, 1.3416192531585693), (0.0, 1.3418807983398438), (0.0, 1.3426265716552734), (0.0, 1.342949628829956), (0.0, 1.4461055994033813), (0.0, 1.4485923051834106), (0.0, 1.4493869543075562), (0.0, 1.4511924982070923), (0.0, 1.4512901306152344), (0.0, 1.4514987468719482), (0.0, 1.451580286026001), (0.0, 1.4516018629074097), (0.0, 1.452035665512085), (0.0, 1.4537221193313599), (0.0, 1.4539140462875366), (0.0, 1.4539670944213867), (0.0, 1.4541723728179932), (0.0, 1.4541807174682617), (0.0, 1.4544813632965088), (0.0, 1.454751968383789), (0.0, 1.4547799825668335), (0.0, 1.4548039436340332), (0.0, 1.455052137374878), (0.0, 1.4550999402999878), (0.0, 1.4552593231201172), (0.0, 1.4553272724151611), (0.0, 1.455461859703064), (0.0, 1.4555288553237915), (0.0, 1.4556623697280884), (0.0, 1.455736517906189), (0.0, 1.455898642539978), (0.0, 1.4560290575027466), (0.0, 1.4560850858688354), (0.0, 1.4561526775360107), (0.0, 1.4562374353408813), (0.0, 1.4563332796096802), (0.0, 1.456421971321106), (0.0, 1.456607699394226), (0.0, 1.4566861391067505), (0.0, 1.4566985368728638), (0.0, 1.456762433052063), (0.0, 1.4568073749542236), (0.0, 1.456857442855835), (0.0, 1.456897497177124), (0.0, 1.4571629762649536), (0.0, 1.4572011232376099), (0.0, 1.4572654962539673), (0.0, 1.4575119018554688), (0.0, 1.457573652267456), (0.0, 1.4575752019882202), (0.0, 1.4576153755187988), (0.0, 1.4578194618225098), (0.0, 1.4578251838684082), (0.0, 1.4579190015792847), (0.0, 1.458000898361206), (0.0, 1.458005666732788), (0.0, 1.4580281972885132), (0.0, 1.4580312967300415), (0.0, 1.458191990852356), (0.0, 1.4581942558288574), (0.0, 1.4581950902938843), (0.0, 1.4583244323730469), (0.0, 1.4585143327713013), (0.0, 1.4585626125335693), (0.0, 1.4586549997329712), (0.0, 1.4586578607559204), (0.0, 1.4587364196777344), (0.0, 1.4588119983673096), (0.0, 1.459028959274292), (0.0, 1.4591060876846313), (0.0, 1.4592232704162598), (0.0, 1.4592413902282715), (0.0, 1.459316611289978), (0.0, 1.4593757390975952), (0.0, 1.459433913230896), (0.0, 1.4595162868499756), (0.0, 1.459684133529663), (0.0, 1.4596967697143555), (0.0, 1.4597681760787964), (0.0, 1.46004056930542), (0.0, 1.460160255432129), (0.0, 1.460190773010254), (0.0, 1.46021568775177), (0.0, 1.4603952169418335), (0.0, 1.4604146480560303), (0.0, 1.4604837894439697), (0.0, 1.460587501525879), (0.0, 1.4606400728225708), (0.0, 1.4607915878295898), (0.0, 1.4607937335968018), (0.0, 1.4608864784240723), (0.0, 1.4608895778656006), (0.0, 1.4608978033065796), (0.0, 1.4609407186508179), (0.0, 1.46095609664917), (0.0, 1.4610302448272705), (0.0, 1.461063265800476), (0.0, 1.4611072540283203), (0.0, 1.4612172842025757), (0.0, 1.461269497871399), (0.0, 1.4614039659500122), (0.0, 1.4614465236663818), (0.0, 1.461605191230774), (0.0, 1.461639642715454), (0.0, 1.4616549015045166), (0.0, 1.461661696434021), (0.0, 1.4619121551513672), (0.0, 1.4620107412338257), (0.0, 1.4622694253921509), (0.0, 1.4622743129730225), (0.0, 1.4626989364624023), (0.0, 1.4628639221191406), (0.0, 1.4631325006484985), (0.0, 1.4636536836624146), (0.0, 1.4640839099884033), (0.0, 1.4640899896621704), (0.0, 1.4643304347991943), (0.0, 1.4647433757781982), (0.0, 1.4651751518249512), (0.0, 1.4658828973770142), (0.0, 1.466082215309143), (0.0, 1.466113567352295), (0.0, 1.4661327600479126), (0.0, 1.4661701917648315), (0.0, 1.467136263847351), (0.0, 1.4678312540054321), (0.0, 1.4690228700637817), (0.0, 1.4712542295455933), (0.0, 1.471415638923645), (0.0, 1.4721373319625854), (0.0, 1.4774373769760132), (0.0, 1.4995077848434448), (0.0, 1.5124685764312744), (0.0, 1.5138120651245117), (0.0, 1.5153224468231201), (0.0, 1.5155836343765259), (0.0, 1.5162278413772583), (0.0, 1.5167242288589478), (0.0, 1.516871690750122), (0.0, 1.5173792839050293), (0.0, 1.5174287557601929), (0.0, 1.5178542137145996), (0.0, 1.5180232524871826), (0.0, 1.5182191133499146), (0.0, 1.5182307958602905), (0.0, 1.5183043479919434), (0.0, 1.5184258222579956), (0.0, 1.5184270143508911), (0.0, 1.518515706062317), (0.0, 1.518753170967102), (0.0, 1.518905758857727), (0.0, 1.5189796686172485), (0.0, 1.5190653800964355), (0.0, 1.5193166732788086), (0.0, 1.5195010900497437), (0.0, 1.5195658206939697), (0.0, 1.5196363925933838), (0.0, 1.5198266506195068), (0.0, 1.5201336145401), (0.0, 1.5203558206558228), (0.0, 1.5204116106033325), (0.0, 1.5204226970672607), (0.0, 1.5204426050186157), (0.0, 1.5204493999481201), (0.0, 1.520511507987976), (0.0, 1.5205492973327637), (0.0, 1.5205559730529785), (0.0, 1.5206019878387451), (0.0, 1.520755410194397), (0.0, 1.5207586288452148), (0.0, 1.5208178758621216), (0.0, 1.520975112915039), (0.0, 1.5214369297027588), (0.0, 1.5215672254562378), (0.0, 1.5216325521469116), (0.0, 1.5216732025146484), (0.0, 1.5217097997665405), (0.0, 1.5217561721801758), (0.0, 1.5218653678894043), (0.0, 1.521889090538025), (0.0, 1.5218902826309204), (0.0, 1.5219597816467285), (0.0, 1.5221177339553833), (0.0, 1.5221189260482788), (0.0, 1.522120714187622), (0.0, 1.522179365158081), (0.0, 1.5222970247268677), (0.0, 1.5223630666732788), (0.0, 1.5224106311798096), (0.0, 1.5224324464797974), (0.0, 1.5227441787719727), (0.0, 1.5228217840194702), (0.0, 1.5228439569473267), (0.0, 1.522929310798645), (0.0, 1.5229579210281372), (0.0, 1.523028016090393), (0.0, 1.5230659246444702), (0.0, 1.5232356786727905), (0.0, 1.523370385169983), (0.0, 1.5233763456344604), (0.0, 1.5233837366104126), (0.0, 1.523415207862854), (0.0, 1.5234726667404175), (0.0, 1.5235345363616943), (0.0, 1.523584008216858), (0.0, 1.5236303806304932), (0.0, 1.5236537456512451), (0.0, 1.5236765146255493), (0.0, 1.523726463317871), (0.0, 1.5238314867019653), (0.0, 1.523950457572937), (0.0, 1.5239754915237427), (0.0, 1.5240193605422974), (0.0, 1.5240341424942017), (0.0, 1.5240505933761597), (0.0, 1.5241347551345825), (0.0, 1.5241596698760986), (0.0, 1.524249792098999), (0.0, 1.5242838859558105), (0.0, 1.5243374109268188), (0.0, 1.5245904922485352), (0.0, 1.5246615409851074), (0.0, 1.5246800184249878), (0.0, 1.5247966051101685), (0.0, 1.5250475406646729), (0.0, 1.5250673294067383), (0.0, 1.5252764225006104), (0.0, 1.525343656539917), (0.0, 1.5255112648010254), (0.0, 1.5255399942398071), (0.0, 1.52555513381958), (0.0, 1.525686264038086), (0.0, 1.525774359703064), (0.0, 1.5259897708892822), (0.0, 1.5261391401290894), (0.0, 1.5262120962142944), (0.0, 1.5262916088104248), (0.0, 1.5263112783432007), (0.0, 1.5263890027999878), (0.0, 1.5264198780059814), (0.0, 1.5264396667480469), (0.0, 1.526604175567627), (0.0, 1.52671480178833), (0.0, 1.5267276763916016), (0.0, 1.5267693996429443), (0.0, 1.5268001556396484), (0.0, 1.5268447399139404), (0.0, 1.5270191431045532), (0.0, 1.5271443128585815), (0.0, 1.5275194644927979), (0.0, 1.5276116132736206), (0.0, 1.5276418924331665), (0.0, 1.5279102325439453), (0.0, 1.5281553268432617), (0.0, 1.528399109840393), (0.0, 1.5285670757293701), (0.0, 1.52958345413208), (0.0, 1.5296788215637207)], [(8.237768173217773, 8.681150436401367), (7.2088847160339355, 7.767880916595459), (7.082111358642578, 7.689977169036865), (7.028129577636719, 7.287299633026123), (6.845792770385742, 6.884845733642578), (6.7704057693481445, 7.175145626068115), (6.373569488525391, 7.0555524826049805), (6.339622497558594, 6.969019889831543), (6.332012176513672, 6.99393367767334), (6.106029033660889, 7.793746471405029), (6.050971508026123, 7.099800109863281), (5.926787853240967, 7.075691223144531), (5.779730796813965, 6.630762100219727), (5.772011756896973, 6.067058086395264), (5.694878578186035, 5.908946990966797), (5.627700328826904, 6.145008563995361), (5.6174421310424805, 8.76447868347168), (5.564293384552002, 8.301624298095703), (5.467893600463867, 8.368657112121582), (5.403841495513916, 7.24923038482666), (5.299012184143066, 7.297889709472656), (4.449636936187744, 7.839249134063721), (4.3214569091796875, 8.008842468261719), (4.013828277587891, 4.106974124908447), (3.9932618141174316, 4.267374038696289), (3.9637393951416016, 4.704085826873779), (3.9311745166778564, 4.306539058685303), (3.610090732574463, 8.714261054992676)], [(9.524675369262695, 10.076065063476562), (9.2037935256958, 9.258495330810547), (9.177968978881836, 9.24298095703125), (9.160996437072754, 10.591092109680176), (7.797797679901123, 7.922165393829346), (6.791055679321289, 6.891319274902344)]]
[array([[0.        , 1.30795503],
       [0.        , 1.3189503 ],
       [0.        , 1.32380974],
       [0.        , 1.32576108],
       [0.        , 1.32578182],
       [0.        , 1.32639742],
       [0.        , 1.3265537 ],
       [0.        , 1.32693732],
       [0.        , 1.32722616],
       [0.        , 1.32740128],
       [0.        , 1.32757187],
       [0.        , 1.32762492],
       [0.        , 1.32818305],
       [0.        , 1.328269  ],
       [0.        , 1.32827806],
       [0.        , 1.32836783],
       [0.        , 1.32838452],
       [0.        , 1.32844841],
       [0.        , 1.32869399],
       [0.        , 1.32884395],
       [0.        , 1.32922196],
       [0.        , 1.3292495 ],
       [0.        , 1.32943928],
       [0.        , 1.32945299],
       [0.        , 1.32956636],
       [0.        , 1.32957292],
       [0.        , 1.32975233],
       [0.        , 1.32980514],
       [0.        , 1.33005679],
       [0.        , 1.33013916],
       [0.        , 1.33017015],
       [0.        , 1.33018541],
       [0.        , 1.33025086],
       [0.        , 1.33026659],
       [0.        , 1.33052015],
       [0.        , 1.33077359],
       [0.        , 1.33089268],
       [0.        , 1.3310231 ],
       [0.        , 1.33118641],
       [0.        , 1.33122647],
       [0.        , 1.33132124],
       [0.        , 1.33132172],
       [0.        , 1.33145249],
       [0.        , 1.33153808],
       [0.        , 1.33158612],
       [0.        , 1.33162093],
       [0.        , 1.33173978],
       [0.        , 1.33183157],
       [0.        , 1.33185494],
       [0.        , 1.33215153],
       [0.        , 1.33220994],
       [0.        , 1.33231938],
       [0.        , 1.33239746],
       [0.        , 1.3324033 ],
       [0.        , 1.33240724],
       [0.        , 1.33244669],
       [0.        , 1.33246291],
       [0.        , 1.33261001],
       [0.        , 1.33263457],
       [0.        , 1.33272171],
       [0.        , 1.33273292],
       [0.        , 1.33274662],
       [0.        , 1.33291972],
       [0.        , 1.33293843],
       [0.        , 1.33299506],
       [0.        , 1.33305526],
       [0.        , 1.33318532],
       [0.        , 1.33324027],
       [0.        , 1.33336282],
       [0.        , 1.33338737],
       [0.        , 1.33338916],
       [0.        , 1.33342588],
       [0.        , 1.33348119],
       [0.        , 1.33350897],
       [0.        , 1.33359468],
       [0.        , 1.33360577],
       [0.        , 1.3338511 ],
       [0.        , 1.33394063],
       [0.        , 1.33399189],
       [0.        , 1.33403862],
       [0.        , 1.33410156],
       [0.        , 1.33415806],
       [0.        , 1.33421767],
       [0.        , 1.33452606],
       [0.        , 1.33459401],
       [0.        , 1.3346175 ],
       [0.        , 1.33477378],
       [0.        , 1.33486855],
       [0.        , 1.33494139],
       [0.        , 1.33500242],
       [0.        , 1.33503747],
       [0.        , 1.33526993],
       [0.        , 1.33535337],
       [0.        , 1.33541083],
       [0.        , 1.33546627],
       [0.        , 1.33569229],
       [0.        , 1.3357141 ],
       [0.        , 1.33573353],
       [0.        , 1.33576834],
       [0.        , 1.33581567],
       [0.        , 1.33597422],
       [0.        , 1.3361553 ],
       [0.        , 1.33620739],
       [0.        , 1.33675945],
       [0.        , 1.33676469],
       [0.        , 1.33689344],
       [0.        , 1.33714521],
       [0.        , 1.3371917 ],
       [0.        , 1.33719194],
       [0.        , 1.33729064],
       [0.        , 1.33740425],
       [0.        , 1.33742332],
       [0.        , 1.33778763],
       [0.        , 1.33795011],
       [0.        , 1.33852994],
       [0.        , 1.33896589],
       [0.        , 1.3390063 ],
       [0.        , 1.33941841],
       [0.        , 1.33949018],
       [0.        , 1.33952117],
       [0.        , 1.34050238],
       [0.        , 1.34097743],
       [0.        , 1.34161925],
       [0.        , 1.3418808 ],
       [0.        , 1.34262657],
       [0.        , 1.34294963],
       [0.        , 1.4461056 ],
       [0.        , 1.44859231],
       [0.        , 1.44938695],
       [0.        , 1.4511925 ],
       [0.        , 1.45129013],
       [0.        , 1.45149875],
       [0.        , 1.45158029],
       [0.        , 1.45160186],
       [0.        , 1.45203567],
       [0.        , 1.45372212],
       [0.        , 1.45391405],
       [0.        , 1.45396709],
       [0.        , 1.45417237],
       [0.        , 1.45418072],
       [0.        , 1.45448136],
       [0.        , 1.45475197],
       [0.        , 1.45477998],
       [0.        , 1.45480394],
       [0.        , 1.45505214],
       [0.        , 1.45509994],
       [0.        , 1.45525932],
       [0.        , 1.45532727],
       [0.        , 1.45546186],
       [0.        , 1.45552886],
       [0.        , 1.45566237],
       [0.        , 1.45573652],
       [0.        , 1.45589864],
       [0.        , 1.45602906],
       [0.        , 1.45608509],
       [0.        , 1.45615268],
       [0.        , 1.45623744],
       [0.        , 1.45633328],
       [0.        , 1.45642197],
       [0.        , 1.4566077 ],
       [0.        , 1.45668614],
       [0.        , 1.45669854],
       [0.        , 1.45676243],
       [0.        , 1.45680737],
       [0.        , 1.45685744],
       [0.        , 1.4568975 ],
       [0.        , 1.45716298],
       [0.        , 1.45720112],
       [0.        , 1.4572655 ],
       [0.        , 1.4575119 ],
       [0.        , 1.45757365],
       [0.        , 1.4575752 ],
       [0.        , 1.45761538],
       [0.        , 1.45781946],
       [0.        , 1.45782518],
       [0.        , 1.457919  ],
       [0.        , 1.4580009 ],
       [0.        , 1.45800567],
       [0.        , 1.4580282 ],
       [0.        , 1.4580313 ],
       [0.        , 1.45819199],
       [0.        , 1.45819426],
       [0.        , 1.45819509],
       [0.        , 1.45832443],
       [0.        , 1.45851433],
       [0.        , 1.45856261],
       [0.        , 1.458655  ],
       [0.        , 1.45865786],
       [0.        , 1.45873642],
       [0.        , 1.458812  ],
       [0.        , 1.45902896],
       [0.        , 1.45910609],
       [0.        , 1.45922327],
       [0.        , 1.45924139],
       [0.        , 1.45931661],
       [0.        , 1.45937574],
       [0.        , 1.45943391],
       [0.        , 1.45951629],
       [0.        , 1.45968413],
       [0.        , 1.45969677],
       [0.        , 1.45976818],
       [0.        , 1.46004057],
       [0.        , 1.46016026],
       [0.        , 1.46019077],
       [0.        , 1.46021569],
       [0.        , 1.46039522],
       [0.        , 1.46041465],
       [0.        , 1.46048379],
       [0.        , 1.4605875 ],
       [0.        , 1.46064007],
       [0.        , 1.46079159],
       [0.        , 1.46079373],
       [0.        , 1.46088648],
       [0.        , 1.46088958],
       [0.        , 1.4608978 ],
       [0.        , 1.46094072],
       [0.        , 1.4609561 ],
       [0.        , 1.46103024],
       [0.        , 1.46106327],
       [0.        , 1.46110725],
       [0.        , 1.46121728],
       [0.        , 1.4612695 ],
       [0.        , 1.46140397],
       [0.        , 1.46144652],
       [0.        , 1.46160519],
       [0.        , 1.46163964],
       [0.        , 1.4616549 ],
       [0.        , 1.4616617 ],
       [0.        , 1.46191216],
       [0.        , 1.46201074],
       [0.        , 1.46226943],
       [0.        , 1.46227431],
       [0.        , 1.46269894],
       [0.        , 1.46286392],
       [0.        , 1.4631325 ],
       [0.        , 1.46365368],
       [0.        , 1.46408391],
       [0.        , 1.46408999],
       [0.        , 1.46433043],
       [0.        , 1.46474338],
       [0.        , 1.46517515],
       [0.        , 1.4658829 ],
       [0.        , 1.46608222],
       [0.        , 1.46611357],
       [0.        , 1.46613276],
       [0.        , 1.46617019],
       [0.        , 1.46713626],
       [0.        , 1.46783125],
       [0.        , 1.46902287],
       [0.        , 1.47125423],
       [0.        , 1.47141564],
       [0.        , 1.47213733],
       [0.        , 1.47743738],
       [0.        , 1.49950778],
       [0.        , 1.51246858],
       [0.        , 1.51381207],
       [0.        , 1.51532245],
       [0.        , 1.51558363],
       [0.        , 1.51622784],
       [0.        , 1.51672423],
       [0.        , 1.51687169],
       [0.        , 1.51737928],
       [0.        , 1.51742876],
       [0.        , 1.51785421],
       [0.        , 1.51802325],
       [0.        , 1.51821911],
       [0.        , 1.5182308 ],
       [0.        , 1.51830435],
       [0.        , 1.51842582],
       [0.        , 1.51842701],
       [0.        , 1.51851571],
       [0.        , 1.51875317],
       [0.        , 1.51890576],
       [0.        , 1.51897967],
       [0.        , 1.51906538],
       [0.        , 1.51931667],
       [0.        , 1.51950109],
       [0.        , 1.51956582],
       [0.        , 1.51963639],
       [0.        , 1.51982665],
       [0.        , 1.52013361],
       [0.        , 1.52035582],
       [0.        , 1.52041161],
       [0.        , 1.5204227 ],
       [0.        , 1.52044261],
       [0.        , 1.5204494 ],
       [0.        , 1.52051151],
       [0.        , 1.5205493 ],
       [0.        , 1.52055597],
       [0.        , 1.52060199],
       [0.        , 1.52075541],
       [0.        , 1.52075863],
       [0.        , 1.52081788],
       [0.        , 1.52097511],
       [0.        , 1.52143693],
       [0.        , 1.52156723],
       [0.        , 1.52163255],
       [0.        , 1.5216732 ],
       [0.        , 1.5217098 ],
       [0.        , 1.52175617],
       [0.        , 1.52186537],
       [0.        , 1.52188909],
       [0.        , 1.52189028],
       [0.        , 1.52195978],
       [0.        , 1.52211773],
       [0.        , 1.52211893],
       [0.        , 1.52212071],
       [0.        , 1.52217937],
       [0.        , 1.52229702],
       [0.        , 1.52236307],
       [0.        , 1.52241063],
       [0.        , 1.52243245],
       [0.        , 1.52274418],
       [0.        , 1.52282178],
       [0.        , 1.52284396],
       [0.        , 1.52292931],
       [0.        , 1.52295792],
       [0.        , 1.52302802],
       [0.        , 1.52306592],
       [0.        , 1.52323568],
       [0.        , 1.52337039],
       [0.        , 1.52337635],
       [0.        , 1.52338374],
       [0.        , 1.52341521],
       [0.        , 1.52347267],
       [0.        , 1.52353454],
       [0.        , 1.52358401],
       [0.        , 1.52363038],
       [0.        , 1.52365375],
       [0.        , 1.52367651],
       [0.        , 1.52372646],
       [0.        , 1.52383149],
       [0.        , 1.52395046],
       [0.        , 1.52397549],
       [0.        , 1.52401936],
       [0.        , 1.52403414],
       [0.        , 1.52405059],
       [0.        , 1.52413476],
       [0.        , 1.52415967],
       [0.        , 1.52424979],
       [0.        , 1.52428389],
       [0.        , 1.52433741],
       [0.        , 1.52459049],
       [0.        , 1.52466154],
       [0.        , 1.52468002],
       [0.        , 1.52479661],
       [0.        , 1.52504754],
       [0.        , 1.52506733],
       [0.        , 1.52527642],
       [0.        , 1.52534366],
       [0.        , 1.52551126],
       [0.        , 1.52553999],
       [0.        , 1.52555513],
       [0.        , 1.52568626],
       [0.        , 1.52577436],
       [0.        , 1.52598977],
       [0.        , 1.52613914],
       [0.        , 1.5262121 ],
       [0.        , 1.52629161],
       [0.        , 1.52631128],
       [0.        , 1.526389  ],
       [0.        , 1.52641988],
       [0.        , 1.52643967],
       [0.        , 1.52660418],
       [0.        , 1.5267148 ],
       [0.        , 1.52672768],
       [0.        , 1.5267694 ],
       [0.        , 1.52680016],
       [0.        , 1.52684474],
       [0.        , 1.52701914],
       [0.        , 1.52714431],
       [0.        , 1.52751946],
       [0.        , 1.52761161],
       [0.        , 1.52764189],
       [0.        , 1.52791023],
       [0.        , 1.52815533],
       [0.        , 1.52839911],
       [0.        , 1.52856708],
       [0.        , 1.52958345],
       [0.        , 1.52967882]]), array([[8.23776817, 8.68115044],
       [7.20888472, 7.76788092],
       [7.08211136, 7.68997717],
       [7.02812958, 7.28729963],
       [6.84579277, 6.88484573],
       [6.77040577, 7.17514563],
       [6.37356949, 7.05555248],
       [6.3396225 , 6.96901989],
       [6.33201218, 6.99393368],
       [6.10602903, 7.79374647],
       [6.05097151, 7.09980011],
       [5.92678785, 7.07569122],
       [5.7797308 , 6.6307621 ],
       [5.77201176, 6.06705809],
       [5.69487858, 5.90894699],
       [5.62770033, 6.14500856],
       [5.61744213, 8.76447868],
       [5.56429338, 8.3016243 ],
       [5.4678936 , 8.36865711],
       [5.4038415 , 7.24923038],
       [5.29901218, 7.29788971],
       [4.44963694, 7.83924913],
       [4.32145691, 8.00884247],
       [4.01382828, 4.10697412],
       [3.99326181, 4.26737404],
       [3.9637394 , 4.70408583],
       [3.93117452, 4.30653906],
       [3.61009073, 8.71426105]]), array([[ 9.52467537, 10.07606506],
       [ 9.20379353,  9.25849533],
       [ 9.17796898,  9.24298096],
       [ 9.16099644, 10.59109211],
       [ 7.79779768,  7.92216539],
       [ 6.79105568,  6.89131927]])]2024-03-06 17:45:29.595326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6G0D ph vector generated, counter: 23
2024-03-06 17:45:34.031711: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:45:34.075444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:45:35.259082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.2444307), (0., 1.3052814), (0., 1.324593 ), (0., 1.3250788),
       (0., 1.3265803), (0., 1.3268569), (0., 1.3270044), (0., 1.3278288),
       (0., 1.3281844), (0., 1.3282077), (0., 1.328273 ), (0., 1.3284367),
       (0., 1.3284369), (0., 1.3290173), (0., 1.3291339), (0., 1.3293661),
       (0., 1.3297546), (0., 1.3297781), (0., 1.3298415), (0., 1.3298686),
       (0., 1.3298954), (0., 1.3299757), (0., 1.3302274), (0., 1.3302801),
       (0., 1.3302828), (0., 1.3305274), (0., 1.3305972), (0., 1.3306098),
       (0., 1.330745 ), (0., 1.3308343), (0., 1.3308437), (0., 1.3308451),
       (0., 1.3309208), (0., 1.3311538), (0., 1.3315859), (0., 1.3316538),
       (0., 1.3319569), (0., 1.3319584), (0., 1.3322235), (0., 1.3323061),
       (0., 1.3324659), (0., 1.3325541), (0., 1.332596 ), (0., 1.3326229),
       (0., 1.3327787), (0., 1.3327906), (0., 1.3328003), (0., 1.3329109),
       (0., 1.3329372), (0., 1.3331579), (0., 1.3333275), (0., 1.3334254),
       (0., 1.3334317), (0., 1.3334688), (0., 1.3335366), (0., 1.3335434),
       (0., 1.3336432), (0., 1.333712 ), (0., 1.333814 ), (0., 1.3338872),
       (0., 1.3339834), (0., 1.3339844), (0., 1.3340135), (0., 1.3341892),
       (0., 1.3342063), (0., 1.3342541), (0., 1.3342557), (0., 1.3342657),
       (0., 1.3343302), (0., 1.3343579), (0., 1.334392 ), (0., 1.3344456),
       (0., 1.3344506), (0., 1.3345907), (0., 1.3346467), (0., 1.3347032),
       (0., 1.334712 ), (0., 1.334828 ), (0., 1.3349639), (0., 1.3350011),
       (0., 1.3350163), (0., 1.3350177), (0., 1.3350323), (0., 1.3350431),
       (0., 1.3350948), (0., 1.3351421), (0., 1.3351741), (0., 1.3352095),
       (0., 1.3353331), (0., 1.3353333), (0., 1.3354181), (0., 1.3354323),
       (0., 1.3354348), (0., 1.335442 ), (0., 1.3354695), (0., 1.3355204),
       (0., 1.3355461), (0., 1.3356049), (0., 1.3356317), (0., 1.3356538),
       (0., 1.3356584), (0., 1.3358072), (0., 1.3358655), (0., 1.3359636),
       (0., 1.3360177), (0., 1.3360537), (0., 1.3361164), (0., 1.3361686),
       (0., 1.3362411), (0., 1.3362987), (0., 1.3363214), (0., 1.3363935),
       (0., 1.3363957), (0., 1.3364289), (0., 1.3365391), (0., 1.3365651),
       (0., 1.3370379), (0., 1.3371056), (0., 1.3373221), (0., 1.3376038),
       (0., 1.3376971), (0., 1.3381602), (0., 1.3384843), (0., 1.3389385),
       (0., 1.3430377), (0., 1.4427139), (0., 1.4509785), (0., 1.4510245),
       (0., 1.451524 ), (0., 1.451983 ), (0., 1.4524354), (0., 1.452599 ),
       (0., 1.4526381), (0., 1.4536239), (0., 1.4537703), (0., 1.4539098),
       (0., 1.4541711), (0., 1.4542238), (0., 1.4543039), (0., 1.4543334),
       (0., 1.4546561), (0., 1.454735 ), (0., 1.4549141), (0., 1.455355 ),
       (0., 1.4553604), (0., 1.4555627), (0., 1.4556628), (0., 1.4558419),
       (0., 1.4562962), (0., 1.4563215), (0., 1.4564319), (0., 1.4565303),
       (0., 1.4565614), (0., 1.4566067), (0., 1.4566782), (0., 1.4567156),
       (0., 1.4569005), (0., 1.4569055), (0., 1.4572107), (0., 1.4572126),
       (0., 1.4572356), (0., 1.4574715), (0., 1.457575 ), (0., 1.4576285),
       (0., 1.457629 ), (0., 1.45764  ), (0., 1.4578304), (0., 1.4578416),
       (0., 1.4578578), (0., 1.4578708), (0., 1.457883 ), (0., 1.4579725),
       (0., 1.4580532), (0., 1.4580542), (0., 1.4580803), (0., 1.4581075),
       (0., 1.4581424), (0., 1.4581438), (0., 1.4581724), (0., 1.4583701),
       (0., 1.4584149), (0., 1.4584343), (0., 1.4585735), (0., 1.4585948),
       (0., 1.4586197), (0., 1.458658 ), (0., 1.4586681), (0., 1.4586794),
       (0., 1.4587479), (0., 1.4588126), (0., 1.4588534), (0., 1.4588842),
       (0., 1.4589155), (0., 1.458997 ), (0., 1.4590038), (0., 1.4591727),
       (0., 1.4592005), (0., 1.4592242), (0., 1.4592315), (0., 1.459232 ),
       (0., 1.4592497), (0., 1.4594048), (0., 1.4594315), (0., 1.4594647),
       (0., 1.4595513), (0., 1.4597147), (0., 1.4598259), (0., 1.4599358),
       (0., 1.459946 ), (0., 1.4600888), (0., 1.4601072), (0., 1.4601161),
       (0., 1.4602182), (0., 1.4602277), (0., 1.4603004), (0., 1.4603766),
       (0., 1.4603882), (0., 1.4604183), (0., 1.4604949), (0., 1.4605583),
       (0., 1.4605745), (0., 1.4605831), (0., 1.4605869), (0., 1.4606323),
       (0., 1.4607106), (0., 1.4608241), (0., 1.4609164), (0., 1.4609292),
       (0., 1.4609891), (0., 1.4611375), (0., 1.4611579), (0., 1.461262 ),
       (0., 1.4614117), (0., 1.4615291), (0., 1.4615891), (0., 1.4616535),
       (0., 1.4618523), (0., 1.4619575), (0., 1.4621916), (0., 1.4629191),
       (0., 1.4637606), (0., 1.4647075), (0., 1.4658958), (0., 1.4659959),
       (0., 1.4665881), (0., 1.4666319), (0., 1.4675983), (0., 1.4691403),
       (0., 1.4708499), (0., 1.4722424), (0., 1.4722501), (0., 1.4917982),
       (0., 1.5095595), (0., 1.5126784), (0., 1.514885 ), (0., 1.5153016),
       (0., 1.5153257), (0., 1.5169079), (0., 1.5171146), (0., 1.5172101),
       (0., 1.5175259), (0., 1.5186456), (0., 1.5186844), (0., 1.5187224),
       (0., 1.5193919), (0., 1.5197121), (0., 1.5198717), (0., 1.5201075),
       (0., 1.5201659), (0., 1.5205972), (0., 1.5206021), (0., 1.5206221),
       (0., 1.5207163), (0., 1.5207795), (0., 1.5208037), (0., 1.5208251),
       (0., 1.520831 ), (0., 1.5209591), (0., 1.5210602), (0., 1.5210795),
       (0., 1.5211809), (0., 1.5211859), (0., 1.5212417), (0., 1.5212506),
       (0., 1.5213336), (0., 1.521355 ), (0., 1.5214136), (0., 1.5215949),
       (0., 1.521601 ), (0., 1.5216707), (0., 1.5217034), (0., 1.5217035),
       (0., 1.5217539), (0., 1.5217743), (0., 1.5217861), (0., 1.5218432),
       (0., 1.5218477), (0., 1.5221732), (0., 1.5222088), (0., 1.5223333),
       (0., 1.5224035), (0., 1.5224568), (0., 1.5224676), (0., 1.5224866),
       (0., 1.5225042), (0., 1.5225607), (0., 1.5226107), (0., 1.5226529),
       (0., 1.5226643), (0., 1.5226905), (0., 1.5226955), (0., 1.5227096),
       (0., 1.5227222), (0., 1.5227473), (0., 1.5227714), (0., 1.522798 ),
       (0., 1.522918 ), (0., 1.5229396), (0., 1.5229801), (0., 1.5230026),
       (0., 1.5230151), (0., 1.5231397), (0., 1.5232155), (0., 1.5232564),
       (0., 1.5232807), (0., 1.5232882), (0., 1.5233114), (0., 1.5233234),
       (0., 1.5233546), (0., 1.5234222), (0., 1.5235059), (0., 1.5235429),
       (0., 1.523546 ), (0., 1.5235962), (0., 1.5236549), (0., 1.5236826),
       (0., 1.5237434), (0., 1.5237868), (0., 1.5238075), (0., 1.5238372),
       (0., 1.523852 ), (0., 1.52386  ), (0., 1.5239153), (0., 1.5239788),
       (0., 1.5239873), (0., 1.5240064), (0., 1.5240378), (0., 1.5241469),
       (0., 1.5241636), (0., 1.52417  ), (0., 1.5241942), (0., 1.5242767),
       (0., 1.5243485), (0., 1.5243903), (0., 1.5244392), (0., 1.5245284),
       (0., 1.5245676), (0., 1.5246145), (0., 1.5246303), (0., 1.5246841),
       (0., 1.5247382), (0., 1.5247602), (0., 1.5249163), (0., 1.5249356),
       (0., 1.5249944), (0., 1.5250748), (0., 1.5251557), (0., 1.5252365),
       (0., 1.5253109), (0., 1.5253578), (0., 1.5253638), (0., 1.5254401),
       (0., 1.5260574), (0., 1.5262992), (0., 1.5266227), (0., 1.5266471),
       (0., 1.5283544), (0., 1.5285848), (0., 1.5291288), (0., 1.605582 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.521368 , 8.563037 ), (7.899753 , 8.358857 ),
       (6.9684534, 6.9874444), (6.9671254, 7.537044 ),
       (6.809028 , 7.131197 ), (6.7384086, 7.2755947),
       (6.6053214, 7.1803336), (6.417499 , 7.012328 ),
       (6.404249 , 6.9919333), (6.2461348, 7.066649 ),
       (6.1468997, 7.8081737), (6.098751 , 6.117316 ),
       (6.068239 , 6.5168357), (5.9243984, 7.265967 ),
       (5.762787 , 5.8193183), (5.54757  , 6.106661 ),
       (5.4884562, 8.438722 ), (5.456042 , 5.560817 ),
       (5.257273 , 8.229495 ), (5.2265286, 8.516248 ),
       (5.019242 , 6.752376 ), (4.9282594, 7.0876465),
       (4.448166 , 7.8047924), (4.4196362, 7.8988028),
       (4.355413 , 4.376289 ), (4.017441 , 4.2814136),
       (3.9534488, 4.242351 ), (3.927703 , 4.681614 ),
       (3.926736 , 4.2834573), (3.6914237, 8.498681 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.681378 , 10.23434  ), (9.391522 ,  9.518212 ),
       (9.1487255, 10.63854  ), (8.643533 ,  8.7365055),
       (8.259705 ,  8.393829 ), (7.5461283,  7.5728908),
       (6.0781345,  6.097634 )],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.244430661201477), (0.0, 1.305281400680542), (0.0, 1.3245929479599), (0.0, 1.3250788450241089), (0.0, 1.326580286026001), (0.0, 1.3268568515777588), (0.0, 1.3270044326782227), (0.0, 1.3278287649154663), (0.0, 1.3281843662261963), (0.0, 1.3282077312469482), (0.0, 1.328273057937622), (0.0, 1.3284367322921753), (0.0, 1.3284368515014648), (0.0, 1.3290172815322876), (0.0, 1.3291338682174683), (0.0, 1.3293660879135132), (0.0, 1.3297545909881592), (0.0, 1.3297780752182007), (0.0, 1.3298414945602417), (0.0, 1.3298685550689697), (0.0, 1.3298953771591187), (0.0, 1.3299757242202759), (0.0, 1.3302273750305176), (0.0, 1.330280065536499), (0.0, 1.3302828073501587), (0.0, 1.330527424812317), (0.0, 1.330597162246704), (0.0, 1.3306097984313965), (0.0, 1.330744981765747), (0.0, 1.3308342695236206), (0.0, 1.3308436870574951), (0.0, 1.3308451175689697), (0.0, 1.3309208154678345), (0.0, 1.3311537504196167), (0.0, 1.3315858840942383), (0.0, 1.3316538333892822), (0.0, 1.3319568634033203), (0.0, 1.3319584131240845), (0.0, 1.3322235345840454), (0.0, 1.332306146621704), (0.0, 1.3324658870697021), (0.0, 1.3325541019439697), (0.0, 1.332595944404602), (0.0, 1.3326228857040405), (0.0, 1.3327786922454834), (0.0, 1.3327906131744385), (0.0, 1.332800269126892), (0.0, 1.3329108953475952), (0.0, 1.332937240600586), (0.0, 1.3331578969955444), (0.0, 1.3333275318145752), (0.0, 1.3334254026412964), (0.0, 1.3334317207336426), (0.0, 1.3334687948226929), (0.0, 1.3335366249084473), (0.0, 1.3335434198379517), (0.0, 1.3336431980133057), (0.0, 1.3337119817733765), (0.0, 1.333814024925232), (0.0, 1.3338872194290161), (0.0, 1.3339834213256836), (0.0, 1.333984375), (0.0, 1.3340134620666504), (0.0, 1.3341891765594482), (0.0, 1.3342063426971436), (0.0, 1.3342541456222534), (0.0, 1.3342556953430176), (0.0, 1.3342657089233398), (0.0, 1.3343302011489868), (0.0, 1.3343578577041626), (0.0, 1.3343919515609741), (0.0, 1.334445595741272), (0.0, 1.334450602531433), (0.0, 1.3345906734466553), (0.0, 1.3346467018127441), (0.0, 1.3347032070159912), (0.0, 1.334712028503418), (0.0, 1.3348280191421509), (0.0, 1.3349639177322388), (0.0, 1.3350011110305786), (0.0, 1.3350162506103516), (0.0, 1.3350176811218262), (0.0, 1.335032343864441), (0.0, 1.3350430727005005), (0.0, 1.3350948095321655), (0.0, 1.3351421356201172), (0.0, 1.3351740837097168), (0.0, 1.3352094888687134), (0.0, 1.3353331089019775), (0.0, 1.3353333473205566), (0.0, 1.3354181051254272), (0.0, 1.3354322910308838), (0.0, 1.3354347944259644), (0.0, 1.3354419469833374), (0.0, 1.3354694843292236), (0.0, 1.3355203866958618), (0.0, 1.3355461359024048), (0.0, 1.3356049060821533), (0.0, 1.3356317281723022), (0.0, 1.3356537818908691), (0.0, 1.3356584310531616), (0.0, 1.335807204246521), (0.0, 1.3358654975891113), (0.0, 1.3359636068344116), (0.0, 1.3360177278518677), (0.0, 1.336053729057312), (0.0, 1.3361164331436157), (0.0, 1.336168646812439), (0.0, 1.3362411260604858), (0.0, 1.3362987041473389), (0.0, 1.3363213539123535), (0.0, 1.3363934755325317), (0.0, 1.3363957405090332), (0.0, 1.3364288806915283), (0.0, 1.3365391492843628), (0.0, 1.3365651369094849), (0.0, 1.3370379209518433), (0.0, 1.337105631828308), (0.0, 1.3373221158981323), (0.0, 1.3376038074493408), (0.0, 1.337697148323059), (0.0, 1.3381601572036743), (0.0, 1.338484287261963), (0.0, 1.3389384746551514), (0.0, 1.343037724494934), (0.0, 1.4427138566970825), (0.0, 1.4509785175323486), (0.0, 1.4510245323181152), (0.0, 1.451524019241333), (0.0, 1.4519829750061035), (0.0, 1.4524353742599487), (0.0, 1.452599048614502), (0.0, 1.4526381492614746), (0.0, 1.45362389087677), (0.0, 1.4537702798843384), (0.0, 1.4539097547531128), (0.0, 1.454171061515808), (0.0, 1.4542237520217896), (0.0, 1.4543038606643677), (0.0, 1.4543334245681763), (0.0, 1.4546561241149902), (0.0, 1.4547350406646729), (0.0, 1.4549140930175781), (0.0, 1.4553550481796265), (0.0, 1.4553604125976562), (0.0, 1.455562710762024), (0.0, 1.4556628465652466), (0.0, 1.4558418989181519), (0.0, 1.4562962055206299), (0.0, 1.4563214778900146), (0.0, 1.4564318656921387), (0.0, 1.4565303325653076), (0.0, 1.4565614461898804), (0.0, 1.4566067457199097), (0.0, 1.4566781520843506), (0.0, 1.4567155838012695), (0.0, 1.4569004774093628), (0.0, 1.456905484199524), (0.0, 1.457210659980774), (0.0, 1.4572125673294067), (0.0, 1.45723557472229), (0.0, 1.457471489906311), (0.0, 1.4575749635696411), (0.0, 1.4576284885406494), (0.0, 1.4576289653778076), (0.0, 1.4576400518417358), (0.0, 1.4578304290771484), (0.0, 1.4578416347503662), (0.0, 1.4578578472137451), (0.0, 1.4578708410263062), (0.0, 1.4578830003738403), (0.0, 1.457972526550293), (0.0, 1.4580532312393188), (0.0, 1.4580541849136353), (0.0, 1.4580802917480469), (0.0, 1.4581074714660645), (0.0, 1.4581423997879028), (0.0, 1.4581438302993774), (0.0, 1.4581724405288696), (0.0, 1.4583700895309448), (0.0, 1.458414912223816), (0.0, 1.4584343433380127), (0.0, 1.4585734605789185), (0.0, 1.458594799041748), (0.0, 1.4586197137832642), (0.0, 1.45865797996521), (0.0, 1.4586681127548218), (0.0, 1.458679437637329), (0.0, 1.4587478637695312), (0.0, 1.4588125944137573), (0.0, 1.4588533639907837), (0.0, 1.4588842391967773), (0.0, 1.4589154720306396), (0.0, 1.4589970111846924), (0.0, 1.4590038061141968), (0.0, 1.4591727256774902), (0.0, 1.4592005014419556), (0.0, 1.4592242240905762), (0.0, 1.4592314958572388), (0.0, 1.459231972694397), (0.0, 1.45924973487854), (0.0, 1.4594048261642456), (0.0, 1.459431529045105), (0.0, 1.4594646692276), (0.0, 1.4595513343811035), (0.0, 1.459714651107788), (0.0, 1.459825873374939), (0.0, 1.4599357843399048), (0.0, 1.4599460363388062), (0.0, 1.460088849067688), (0.0, 1.4601072072982788), (0.0, 1.4601161479949951), (0.0, 1.4602181911468506), (0.0, 1.4602277278900146), (0.0, 1.4603004455566406), (0.0, 1.4603766202926636), (0.0, 1.46038818359375), (0.0, 1.4604183435440063), (0.0, 1.460494875907898), (0.0, 1.460558295249939), (0.0, 1.4605745077133179), (0.0, 1.4605830907821655), (0.0, 1.4605869054794312), (0.0, 1.46063232421875), (0.0, 1.4607106447219849), (0.0, 1.4608241319656372), (0.0, 1.4609163999557495), (0.0, 1.4609291553497314), (0.0, 1.4609891176223755), (0.0, 1.4611375331878662), (0.0, 1.4611579179763794), (0.0, 1.4612619876861572), (0.0, 1.461411714553833), (0.0, 1.4615291357040405), (0.0, 1.4615890979766846), (0.0, 1.461653470993042), (0.0, 1.4618523120880127), (0.0, 1.4619574546813965), (0.0, 1.4621915817260742), (0.0, 1.4629191160202026), (0.0, 1.4637606143951416), (0.0, 1.4647074937820435), (0.0, 1.4658957719802856), (0.0, 1.4659959077835083), (0.0, 1.4665881395339966), (0.0, 1.4666318893432617), (0.0, 1.46759831905365), (0.0, 1.4691402912139893), (0.0, 1.470849871635437), (0.0, 1.4722423553466797), (0.0, 1.4722501039505005), (0.0, 1.4917981624603271), (0.0, 1.5095595121383667), (0.0, 1.5126783847808838), (0.0, 1.5148849487304688), (0.0, 1.5153015851974487), (0.0, 1.515325665473938), (0.0, 1.5169079303741455), (0.0, 1.5171146392822266), (0.0, 1.5172101259231567), (0.0, 1.5175259113311768), (0.0, 1.5186456441879272), (0.0, 1.5186843872070312), (0.0, 1.518722414970398), (0.0, 1.5193918943405151), (0.0, 1.5197120904922485), (0.0, 1.519871711730957), (0.0, 1.5201075077056885), (0.0, 1.5201659202575684), (0.0, 1.520597219467163), (0.0, 1.5206021070480347), (0.0, 1.5206221342086792), (0.0, 1.5207163095474243), (0.0, 1.5207794904708862), (0.0, 1.520803689956665), (0.0, 1.5208251476287842), (0.0, 1.5208309888839722), (0.0, 1.5209591388702393), (0.0, 1.5210602283477783), (0.0, 1.5210795402526855), (0.0, 1.5211808681488037), (0.0, 1.5211858749389648), (0.0, 1.5212416648864746), (0.0, 1.521250605583191), (0.0, 1.5213335752487183), (0.0, 1.5213550329208374), (0.0, 1.5214135646820068), (0.0, 1.5215948820114136), (0.0, 1.5216009616851807), (0.0, 1.5216706991195679), (0.0, 1.5217033624649048), (0.0, 1.5217034816741943), (0.0, 1.5217539072036743), (0.0, 1.5217742919921875), (0.0, 1.521786093711853), (0.0, 1.5218431949615479), (0.0, 1.5218477249145508), (0.0, 1.5221731662750244), (0.0, 1.5222088098526), (0.0, 1.5223332643508911), (0.0, 1.5224034786224365), (0.0, 1.5224567651748657), (0.0, 1.5224676132202148), (0.0, 1.5224865674972534), (0.0, 1.522504210472107), (0.0, 1.522560715675354), (0.0, 1.5226106643676758), (0.0, 1.5226528644561768), (0.0, 1.5226643085479736), (0.0, 1.5226905345916748), (0.0, 1.522695541381836), (0.0, 1.522709608078003), (0.0, 1.5227222442626953), (0.0, 1.522747278213501), (0.0, 1.5227713584899902), (0.0, 1.52279794216156), (0.0, 1.5229179859161377), (0.0, 1.5229395627975464), (0.0, 1.5229800939559937), (0.0, 1.5230026245117188), (0.0, 1.5230151414871216), (0.0, 1.5231397151947021), (0.0, 1.5232155323028564), (0.0, 1.5232564210891724), (0.0, 1.5232807397842407), (0.0, 1.5232882499694824), (0.0, 1.5233113765716553), (0.0, 1.5233234167099), (0.0, 1.5233546495437622), (0.0, 1.5234222412109375), (0.0, 1.5235059261322021), (0.0, 1.523542881011963), (0.0, 1.5235459804534912), (0.0, 1.523596167564392), (0.0, 1.5236549377441406), (0.0, 1.5236825942993164), (0.0, 1.5237433910369873), (0.0, 1.5237867832183838), (0.0, 1.5238075256347656), (0.0, 1.5238372087478638), (0.0, 1.523851990699768), (0.0, 1.523859977722168), (0.0, 1.5239152908325195), (0.0, 1.52397882938385), (0.0, 1.5239872932434082), (0.0, 1.5240063667297363), (0.0, 1.5240378379821777), (0.0, 1.5241469144821167), (0.0, 1.5241636037826538), (0.0, 1.5241700410842896), (0.0, 1.5241942405700684), (0.0, 1.5242767333984375), (0.0, 1.524348497390747), (0.0, 1.5243903398513794), (0.0, 1.5244392156600952), (0.0, 1.5245283842086792), (0.0, 1.5245676040649414), (0.0, 1.5246144533157349), (0.0, 1.5246303081512451), (0.0, 1.5246840715408325), (0.0, 1.5247381925582886), (0.0, 1.5247602462768555), (0.0, 1.5249162912368774), (0.0, 1.5249356031417847), (0.0, 1.5249943733215332), (0.0, 1.52507483959198), (0.0, 1.5251556634902954), (0.0, 1.5252364873886108), (0.0, 1.5253108739852905), (0.0, 1.5253578424453735), (0.0, 1.525363802909851), (0.0, 1.5254400968551636), (0.0, 1.5260573625564575), (0.0, 1.526299238204956), (0.0, 1.5266226530075073), (0.0, 1.5266470909118652), (0.0, 1.5283544063568115), (0.0, 1.5285848379135132), (0.0, 1.5291287899017334), (0.0, 1.6055819988250732)], [(8.521368026733398, 8.563036918640137), (7.899753093719482, 8.358857154846191), (6.968453407287598, 6.9874444007873535), (6.967125415802002, 7.537044048309326), (6.809028148651123, 7.131196975708008), (6.73840856552124, 7.275594711303711), (6.605321407318115, 7.180333614349365), (6.41749906539917, 7.012328147888184), (6.40424919128418, 6.991933345794678), (6.2461347579956055, 7.066648960113525), (6.146899700164795, 7.808173656463623), (6.098751068115234, 6.117315769195557), (6.068239212036133, 6.516835689544678), (5.924398422241211, 7.265966892242432), (5.762786865234375, 5.8193182945251465), (5.54757022857666, 6.106660842895508), (5.4884562492370605, 8.438721656799316), (5.4560418128967285, 5.560816764831543), (5.257273197174072, 8.22949504852295), (5.226528644561768, 8.516247749328613), (5.019241809844971, 6.752376079559326), (4.928259372711182, 7.087646484375), (4.4481658935546875, 7.804792404174805), (4.419636249542236, 7.898802757263184), (4.35541296005249, 4.376288890838623), (4.0174407958984375, 4.281413555145264), (3.95344877243042, 4.242351055145264), (3.9277029037475586, 4.681613922119141), (3.9267361164093018, 4.283457279205322), (3.6914236545562744, 8.49868106842041)], [(9.681378364562988, 10.234339714050293), (9.391522407531738, 9.51821231842041), (9.148725509643555, 10.638540267944336), (8.643532752990723, 8.736505508422852), (8.25970458984375, 8.393829345703125), (7.546128273010254, 7.572890758514404), (6.078134536743164, 6.0976338386535645)]]
[array([[0.        , 1.24443066],
       [0.        , 1.3052814 ],
       [0.        , 1.32459295],
       [0.        , 1.32507885],
       [0.        , 1.32658029],
       [0.        , 1.32685685],
       [0.        , 1.32700443],
       [0.        , 1.32782876],
       [0.        , 1.32818437],
       [0.        , 1.32820773],
       [0.        , 1.32827306],
       [0.        , 1.32843673],
       [0.        , 1.32843685],
       [0.        , 1.32901728],
       [0.        , 1.32913387],
       [0.        , 1.32936609],
       [0.        , 1.32975459],
       [0.        , 1.32977808],
       [0.        , 1.32984149],
       [0.        , 1.32986856],
       [0.        , 1.32989538],
       [0.        , 1.32997572],
       [0.        , 1.33022738],
       [0.        , 1.33028007],
       [0.        , 1.33028281],
       [0.        , 1.33052742],
       [0.        , 1.33059716],
       [0.        , 1.3306098 ],
       [0.        , 1.33074498],
       [0.        , 1.33083427],
       [0.        , 1.33084369],
       [0.        , 1.33084512],
       [0.        , 1.33092082],
       [0.        , 1.33115375],
       [0.        , 1.33158588],
       [0.        , 1.33165383],
       [0.        , 1.33195686],
       [0.        , 1.33195841],
       [0.        , 1.33222353],
       [0.        , 1.33230615],
       [0.        , 1.33246589],
       [0.        , 1.3325541 ],
       [0.        , 1.33259594],
       [0.        , 1.33262289],
       [0.        , 1.33277869],
       [0.        , 1.33279061],
       [0.        , 1.33280027],
       [0.        , 1.3329109 ],
       [0.        , 1.33293724],
       [0.        , 1.3331579 ],
       [0.        , 1.33332753],
       [0.        , 1.3334254 ],
       [0.        , 1.33343172],
       [0.        , 1.33346879],
       [0.        , 1.33353662],
       [0.        , 1.33354342],
       [0.        , 1.3336432 ],
       [0.        , 1.33371198],
       [0.        , 1.33381402],
       [0.        , 1.33388722],
       [0.        , 1.33398342],
       [0.        , 1.33398438],
       [0.        , 1.33401346],
       [0.        , 1.33418918],
       [0.        , 1.33420634],
       [0.        , 1.33425415],
       [0.        , 1.3342557 ],
       [0.        , 1.33426571],
       [0.        , 1.3343302 ],
       [0.        , 1.33435786],
       [0.        , 1.33439195],
       [0.        , 1.3344456 ],
       [0.        , 1.3344506 ],
       [0.        , 1.33459067],
       [0.        , 1.3346467 ],
       [0.        , 1.33470321],
       [0.        , 1.33471203],
       [0.        , 1.33482802],
       [0.        , 1.33496392],
       [0.        , 1.33500111],
       [0.        , 1.33501625],
       [0.        , 1.33501768],
       [0.        , 1.33503234],
       [0.        , 1.33504307],
       [0.        , 1.33509481],
       [0.        , 1.33514214],
       [0.        , 1.33517408],
       [0.        , 1.33520949],
       [0.        , 1.33533311],
       [0.        , 1.33533335],
       [0.        , 1.33541811],
       [0.        , 1.33543229],
       [0.        , 1.33543479],
       [0.        , 1.33544195],
       [0.        , 1.33546948],
       [0.        , 1.33552039],
       [0.        , 1.33554614],
       [0.        , 1.33560491],
       [0.        , 1.33563173],
       [0.        , 1.33565378],
       [0.        , 1.33565843],
       [0.        , 1.3358072 ],
       [0.        , 1.3358655 ],
       [0.        , 1.33596361],
       [0.        , 1.33601773],
       [0.        , 1.33605373],
       [0.        , 1.33611643],
       [0.        , 1.33616865],
       [0.        , 1.33624113],
       [0.        , 1.3362987 ],
       [0.        , 1.33632135],
       [0.        , 1.33639348],
       [0.        , 1.33639574],
       [0.        , 1.33642888],
       [0.        , 1.33653915],
       [0.        , 1.33656514],
       [0.        , 1.33703792],
       [0.        , 1.33710563],
       [0.        , 1.33732212],
       [0.        , 1.33760381],
       [0.        , 1.33769715],
       [0.        , 1.33816016],
       [0.        , 1.33848429],
       [0.        , 1.33893847],
       [0.        , 1.34303772],
       [0.        , 1.44271386],
       [0.        , 1.45097852],
       [0.        , 1.45102453],
       [0.        , 1.45152402],
       [0.        , 1.45198298],
       [0.        , 1.45243537],
       [0.        , 1.45259905],
       [0.        , 1.45263815],
       [0.        , 1.45362389],
       [0.        , 1.45377028],
       [0.        , 1.45390975],
       [0.        , 1.45417106],
       [0.        , 1.45422375],
       [0.        , 1.45430386],
       [0.        , 1.45433342],
       [0.        , 1.45465612],
       [0.        , 1.45473504],
       [0.        , 1.45491409],
       [0.        , 1.45535505],
       [0.        , 1.45536041],
       [0.        , 1.45556271],
       [0.        , 1.45566285],
       [0.        , 1.4558419 ],
       [0.        , 1.45629621],
       [0.        , 1.45632148],
       [0.        , 1.45643187],
       [0.        , 1.45653033],
       [0.        , 1.45656145],
       [0.        , 1.45660675],
       [0.        , 1.45667815],
       [0.        , 1.45671558],
       [0.        , 1.45690048],
       [0.        , 1.45690548],
       [0.        , 1.45721066],
       [0.        , 1.45721257],
       [0.        , 1.45723557],
       [0.        , 1.45747149],
       [0.        , 1.45757496],
       [0.        , 1.45762849],
       [0.        , 1.45762897],
       [0.        , 1.45764005],
       [0.        , 1.45783043],
       [0.        , 1.45784163],
       [0.        , 1.45785785],
       [0.        , 1.45787084],
       [0.        , 1.457883  ],
       [0.        , 1.45797253],
       [0.        , 1.45805323],
       [0.        , 1.45805418],
       [0.        , 1.45808029],
       [0.        , 1.45810747],
       [0.        , 1.4581424 ],
       [0.        , 1.45814383],
       [0.        , 1.45817244],
       [0.        , 1.45837009],
       [0.        , 1.45841491],
       [0.        , 1.45843434],
       [0.        , 1.45857346],
       [0.        , 1.4585948 ],
       [0.        , 1.45861971],
       [0.        , 1.45865798],
       [0.        , 1.45866811],
       [0.        , 1.45867944],
       [0.        , 1.45874786],
       [0.        , 1.45881259],
       [0.        , 1.45885336],
       [0.        , 1.45888424],
       [0.        , 1.45891547],
       [0.        , 1.45899701],
       [0.        , 1.45900381],
       [0.        , 1.45917273],
       [0.        , 1.4592005 ],
       [0.        , 1.45922422],
       [0.        , 1.4592315 ],
       [0.        , 1.45923197],
       [0.        , 1.45924973],
       [0.        , 1.45940483],
       [0.        , 1.45943153],
       [0.        , 1.45946467],
       [0.        , 1.45955133],
       [0.        , 1.45971465],
       [0.        , 1.45982587],
       [0.        , 1.45993578],
       [0.        , 1.45994604],
       [0.        , 1.46008885],
       [0.        , 1.46010721],
       [0.        , 1.46011615],
       [0.        , 1.46021819],
       [0.        , 1.46022773],
       [0.        , 1.46030045],
       [0.        , 1.46037662],
       [0.        , 1.46038818],
       [0.        , 1.46041834],
       [0.        , 1.46049488],
       [0.        , 1.4605583 ],
       [0.        , 1.46057451],
       [0.        , 1.46058309],
       [0.        , 1.46058691],
       [0.        , 1.46063232],
       [0.        , 1.46071064],
       [0.        , 1.46082413],
       [0.        , 1.4609164 ],
       [0.        , 1.46092916],
       [0.        , 1.46098912],
       [0.        , 1.46113753],
       [0.        , 1.46115792],
       [0.        , 1.46126199],
       [0.        , 1.46141171],
       [0.        , 1.46152914],
       [0.        , 1.4615891 ],
       [0.        , 1.46165347],
       [0.        , 1.46185231],
       [0.        , 1.46195745],
       [0.        , 1.46219158],
       [0.        , 1.46291912],
       [0.        , 1.46376061],
       [0.        , 1.46470749],
       [0.        , 1.46589577],
       [0.        , 1.46599591],
       [0.        , 1.46658814],
       [0.        , 1.46663189],
       [0.        , 1.46759832],
       [0.        , 1.46914029],
       [0.        , 1.47084987],
       [0.        , 1.47224236],
       [0.        , 1.4722501 ],
       [0.        , 1.49179816],
       [0.        , 1.50955951],
       [0.        , 1.51267838],
       [0.        , 1.51488495],
       [0.        , 1.51530159],
       [0.        , 1.51532567],
       [0.        , 1.51690793],
       [0.        , 1.51711464],
       [0.        , 1.51721013],
       [0.        , 1.51752591],
       [0.        , 1.51864564],
       [0.        , 1.51868439],
       [0.        , 1.51872241],
       [0.        , 1.51939189],
       [0.        , 1.51971209],
       [0.        , 1.51987171],
       [0.        , 1.52010751],
       [0.        , 1.52016592],
       [0.        , 1.52059722],
       [0.        , 1.52060211],
       [0.        , 1.52062213],
       [0.        , 1.52071631],
       [0.        , 1.52077949],
       [0.        , 1.52080369],
       [0.        , 1.52082515],
       [0.        , 1.52083099],
       [0.        , 1.52095914],
       [0.        , 1.52106023],
       [0.        , 1.52107954],
       [0.        , 1.52118087],
       [0.        , 1.52118587],
       [0.        , 1.52124166],
       [0.        , 1.52125061],
       [0.        , 1.52133358],
       [0.        , 1.52135503],
       [0.        , 1.52141356],
       [0.        , 1.52159488],
       [0.        , 1.52160096],
       [0.        , 1.5216707 ],
       [0.        , 1.52170336],
       [0.        , 1.52170348],
       [0.        , 1.52175391],
       [0.        , 1.52177429],
       [0.        , 1.52178609],
       [0.        , 1.52184319],
       [0.        , 1.52184772],
       [0.        , 1.52217317],
       [0.        , 1.52220881],
       [0.        , 1.52233326],
       [0.        , 1.52240348],
       [0.        , 1.52245677],
       [0.        , 1.52246761],
       [0.        , 1.52248657],
       [0.        , 1.52250421],
       [0.        , 1.52256072],
       [0.        , 1.52261066],
       [0.        , 1.52265286],
       [0.        , 1.52266431],
       [0.        , 1.52269053],
       [0.        , 1.52269554],
       [0.        , 1.52270961],
       [0.        , 1.52272224],
       [0.        , 1.52274728],
       [0.        , 1.52277136],
       [0.        , 1.52279794],
       [0.        , 1.52291799],
       [0.        , 1.52293956],
       [0.        , 1.52298009],
       [0.        , 1.52300262],
       [0.        , 1.52301514],
       [0.        , 1.52313972],
       [0.        , 1.52321553],
       [0.        , 1.52325642],
       [0.        , 1.52328074],
       [0.        , 1.52328825],
       [0.        , 1.52331138],
       [0.        , 1.52332342],
       [0.        , 1.52335465],
       [0.        , 1.52342224],
       [0.        , 1.52350593],
       [0.        , 1.52354288],
       [0.        , 1.52354598],
       [0.        , 1.52359617],
       [0.        , 1.52365494],
       [0.        , 1.52368259],
       [0.        , 1.52374339],
       [0.        , 1.52378678],
       [0.        , 1.52380753],
       [0.        , 1.52383721],
       [0.        , 1.52385199],
       [0.        , 1.52385998],
       [0.        , 1.52391529],
       [0.        , 1.52397883],
       [0.        , 1.52398729],
       [0.        , 1.52400637],
       [0.        , 1.52403784],
       [0.        , 1.52414691],
       [0.        , 1.5241636 ],
       [0.        , 1.52417004],
       [0.        , 1.52419424],
       [0.        , 1.52427673],
       [0.        , 1.5243485 ],
       [0.        , 1.52439034],
       [0.        , 1.52443922],
       [0.        , 1.52452838],
       [0.        , 1.5245676 ],
       [0.        , 1.52461445],
       [0.        , 1.52463031],
       [0.        , 1.52468407],
       [0.        , 1.52473819],
       [0.        , 1.52476025],
       [0.        , 1.52491629],
       [0.        , 1.5249356 ],
       [0.        , 1.52499437],
       [0.        , 1.52507484],
       [0.        , 1.52515566],
       [0.        , 1.52523649],
       [0.        , 1.52531087],
       [0.        , 1.52535784],
       [0.        , 1.5253638 ],
       [0.        , 1.5254401 ],
       [0.        , 1.52605736],
       [0.        , 1.52629924],
       [0.        , 1.52662265],
       [0.        , 1.52664709],
       [0.        , 1.52835441],
       [0.        , 1.52858484],
       [0.        , 1.52912879],
       [0.        , 1.605582  ]]), array([[8.52136803, 8.56303692],
       [7.89975309, 8.35885715],
       [6.96845341, 6.9874444 ],
       [6.96712542, 7.53704405],
       [6.80902815, 7.13119698],
       [6.73840857, 7.27559471],
       [6.60532141, 7.18033361],
       [6.41749907, 7.01232815],
       [6.40424919, 6.99193335],
       [6.24613476, 7.06664896],
       [6.1468997 , 7.80817366],
       [6.09875107, 6.11731577],
       [6.06823921, 6.51683569],
       [5.92439842, 7.26596689],
       [5.76278687, 5.81931829],
       [5.54757023, 6.10666084],
       [5.48845625, 8.43872166],
       [5.45604181, 5.56081676],
       [5.2572732 , 8.22949505],
       [5.22652864, 8.51624775],
       [5.01924181, 6.75237608],
       [4.92825937, 7.08764648],
       [4.44816589, 7.8047924 ],
       [4.41963625, 7.89880276],
       [4.35541296, 4.37628889],
       [4.0174408 , 4.28141356],
       [3.95344877, 4.24235106],
       [3.9277029 , 4.68161392],
       [3.92673612, 4.28345728],
       [3.69142365, 8.49868107]]), array([[ 9.68137836, 10.23433971],
       [ 9.39152241,  9.51821232],
       [ 9.14872551, 10.63854027],
       [ 8.64353275,  8.73650551],
       [ 8.25970459,  8.39382935],
       [ 7.54612827,  7.57289076],
       [ 6.07813454,  6.09763384]])]2024-03-06 17:45:39.765200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6G0E ph vector generated, counter: 24
2024-03-06 17:45:44.127629: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:45:44.172381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:45:45.485190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.318541 ), (0., 1.3230015), (0., 1.3238674), (0., 1.325728 ),
       (0., 1.3259215), (0., 1.3260734), (0., 1.3262457), (0., 1.3263856),
       (0., 1.327124 ), (0., 1.3271251), (0., 1.327793 ), (0., 1.3278981),
       (0., 1.3280231), (0., 1.3280627), (0., 1.3281063), (0., 1.3281897),
       (0., 1.3283148), (0., 1.3284686), (0., 1.328909 ), (0., 1.3290617),
       (0., 1.329372 ), (0., 1.329398 ), (0., 1.329431 ), (0., 1.329457 ),
       (0., 1.32947  ), (0., 1.329539 ), (0., 1.3297204), (0., 1.329842 ),
       (0., 1.3300235), (0., 1.3301355), (0., 1.3302758), (0., 1.3306509),
       (0., 1.3306834), (0., 1.3307678), (0., 1.3308437), (0., 1.3311119),
       (0., 1.3311299), (0., 1.3312117), (0., 1.3314449), (0., 1.3314868),
       (0., 1.3315659), (0., 1.331869 ), (0., 1.331947 ), (0., 1.3320053),
       (0., 1.3320205), (0., 1.332323 ), (0., 1.3324431), (0., 1.3325359),
       (0., 1.3325363), (0., 1.3325876), (0., 1.3327951), (0., 1.3328197),
       (0., 1.3329389), (0., 1.3329517), (0., 1.3329861), (0., 1.333118 ),
       (0., 1.3332204), (0., 1.3332458), (0., 1.3333604), (0., 1.3333834),
       (0., 1.333721 ), (0., 1.3339038), (0., 1.3339076), (0., 1.3340756),
       (0., 1.3343652), (0., 1.3344865), (0., 1.3345127), (0., 1.3345276),
       (0., 1.3345473), (0., 1.334873 ), (0., 1.3351275), (0., 1.335151 ),
       (0., 1.335258 ), (0., 1.335292 ), (0., 1.3355043), (0., 1.335684 ),
       (0., 1.3358346), (0., 1.3363547), (0., 1.3364341), (0., 1.3364896),
       (0., 1.3365705), (0., 1.3365787), (0., 1.3366169), (0., 1.3367463),
       (0., 1.3369623), (0., 1.3370373), (0., 1.3371449), (0., 1.3371886),
       (0., 1.3372172), (0., 1.3372453), (0., 1.3373244), (0., 1.3373967),
       (0., 1.3375237), (0., 1.3377006), (0., 1.33781  ), (0., 1.337874 ),
       (0., 1.3378747), (0., 1.3380046), (0., 1.3380872), (0., 1.3381324),
       (0., 1.3386643), (0., 1.3387501), (0., 1.3391602), (0., 1.339171 ),
       (0., 1.3392098), (0., 1.3392496), (0., 1.3392805), (0., 1.3397741),
       (0., 1.3400131), (0., 1.3400459), (0., 1.3403653), (0., 1.3404411),
       (0., 1.3404971), (0., 1.3406001), (0., 1.3406479), (0., 1.3412251),
       (0., 1.3418795), (0., 1.3418922), (0., 1.3419712), (0., 1.3425257),
       (0., 1.3425772), (0., 1.3436619), (0., 1.3440313), (0., 1.3445874),
       (0., 1.3459007), (0., 1.4077407), (0., 1.4406723), (0., 1.4495932),
       (0., 1.4516817), (0., 1.4519539), (0., 1.4525206), (0., 1.4525452),
       (0., 1.4527838), (0., 1.4528012), (0., 1.452941 ), (0., 1.4531304),
       (0., 1.4532781), (0., 1.4535936), (0., 1.4542849), (0., 1.4544173),
       (0., 1.4546145), (0., 1.4549011), (0., 1.455029 ), (0., 1.4552759),
       (0., 1.4553949), (0., 1.4556445), (0., 1.4557036), (0., 1.4558882),
       (0., 1.4559369), (0., 1.4559401), (0., 1.455949 ), (0., 1.4561259),
       (0., 1.4561366), (0., 1.4563602), (0., 1.4565032), (0., 1.4565392),
       (0., 1.4565716), (0., 1.456828 ), (0., 1.456914 ), (0., 1.4570317),
       (0., 1.4573436), (0., 1.4574467), (0., 1.4574507), (0., 1.457663 ),
       (0., 1.4578109), (0., 1.4578373), (0., 1.4578815), (0., 1.4580866),
       (0., 1.4583875), (0., 1.4583964), (0., 1.458467 ), (0., 1.4585215),
       (0., 1.4585792), (0., 1.4586877), (0., 1.458726 ), (0., 1.4588712),
       (0., 1.458898 ), (0., 1.458929 ), (0., 1.4589679), (0., 1.4590645),
       (0., 1.459078 ), (0., 1.4593081), (0., 1.4593425), (0., 1.4593714),
       (0., 1.4594083), (0., 1.4597101), (0., 1.4597554), (0., 1.4597957),
       (0., 1.4598399), (0., 1.459921 ), (0., 1.4601002), (0., 1.4601488),
       (0., 1.4602185), (0., 1.4604502), (0., 1.4605123), (0., 1.4606991),
       (0., 1.4607894), (0., 1.4608521), (0., 1.4610481), (0., 1.4610904),
       (0., 1.4611905), (0., 1.4613167), (0., 1.4613302), (0., 1.4616187),
       (0., 1.4617168), (0., 1.461773 ), (0., 1.4617836), (0., 1.4617909),
       (0., 1.4618648), (0., 1.4620185), (0., 1.462019 ), (0., 1.4620539),
       (0., 1.4620587), (0., 1.4620876), (0., 1.4621171), (0., 1.4622318),
       (0., 1.4622319), (0., 1.4624053), (0., 1.4625331), (0., 1.4626582),
       (0., 1.4630104), (0., 1.4632882), (0., 1.4636434), (0., 1.463741 ),
       (0., 1.4637487), (0., 1.4637566), (0., 1.4639313), (0., 1.4642335),
       (0., 1.4643177), (0., 1.4644611), (0., 1.4644655), (0., 1.4645276),
       (0., 1.464594 ), (0., 1.4647145), (0., 1.4647549), (0., 1.4648246),
       (0., 1.465146 ), (0., 1.4652015), (0., 1.4654826), (0., 1.4656684),
       (0., 1.4657329), (0., 1.4662975), (0., 1.466572 ), (0., 1.4668404),
       (0., 1.4670694), (0., 1.4673742), (0., 1.4686971), (0., 1.4692559),
       (0., 1.4693244), (0., 1.4702302), (0., 1.4708104), (0., 1.47396  ),
       (0., 1.4740303), (0., 1.5082321), (0., 1.5088967), (0., 1.5130782),
       (0., 1.5138867), (0., 1.514335 ), (0., 1.5144354), (0., 1.5148184),
       (0., 1.5154428), (0., 1.515467 ), (0., 1.5159054), (0., 1.5159345),
       (0., 1.516191 ), (0., 1.5166618), (0., 1.5167001), (0., 1.5167456),
       (0., 1.5168138), (0., 1.516831 ), (0., 1.5171013), (0., 1.5171156),
       (0., 1.5173671), (0., 1.5175034), (0., 1.5176717), (0., 1.517758 ),
       (0., 1.5178189), (0., 1.5179968), (0., 1.5180492), (0., 1.518107 ),
       (0., 1.5182952), (0., 1.5184511), (0., 1.5184656), (0., 1.5184919),
       (0., 1.5187453), (0., 1.519208 ), (0., 1.5192833), (0., 1.5196332),
       (0., 1.5196718), (0., 1.5197381), (0., 1.5198265), (0., 1.5201527),
       (0., 1.5202155), (0., 1.5203367), (0., 1.520377 ), (0., 1.520422 ),
       (0., 1.5205415), (0., 1.5205485), (0., 1.5207558), (0., 1.5209385),
       (0., 1.5209874), (0., 1.5210491), (0., 1.5210674), (0., 1.5212003),
       (0., 1.5212919), (0., 1.521401 ), (0., 1.5214486), (0., 1.5215584),
       (0., 1.5217103), (0., 1.5217651), (0., 1.5217683), (0., 1.5220423),
       (0., 1.5220437), (0., 1.522165 ), (0., 1.5223223), (0., 1.5224653),
       (0., 1.5224855), (0., 1.5226039), (0., 1.5226312), (0., 1.522678 ),
       (0., 1.5228863), (0., 1.5229305), (0., 1.5230772), (0., 1.5231444),
       (0., 1.5231736), (0., 1.5232344), (0., 1.5232451), (0., 1.523268 ),
       (0., 1.5233418), (0., 1.523402 ), (0., 1.5235167), (0., 1.5236853),
       (0., 1.5238008), (0., 1.5239332), (0., 1.5240169), (0., 1.5241858),
       (0., 1.5241878), (0., 1.5243505), (0., 1.5244428), (0., 1.5244428),
       (0., 1.5244609), (0., 1.52448  ), (0., 1.5245295), (0., 1.5246207),
       (0., 1.5247005), (0., 1.5247736), (0., 1.5249441), (0., 1.5252882),
       (0., 1.5253267), (0., 1.5255259), (0., 1.5257276), (0., 1.5258167),
       (0., 1.5260321), (0., 1.5260621), (0., 1.5262994), (0., 1.5265198),
       (0., 1.5266886), (0., 1.5267456), (0., 1.5269479), (0., 1.5270996),
       (0., 1.5272768), (0., 1.5273247), (0., 1.5273257), (0., 1.5277454),
       (0., 1.5277758), (0., 1.5278661), (0., 1.5280092), (0., 1.5285401),
       (0., 1.5286273), (0., 1.528689 ), (0., 1.5287309), (0., 1.5288903),
       (0., 1.529998 ), (0., 1.5300565), (0., 1.5305232), (0., 1.5310904),
       (0., 1.5311171), (0., 1.5322157), (0., 1.5337251), (0., 1.5451686)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.385194 , 8.545871 ), (7.660478 , 8.258452 ),
       (7.018295 , 7.122623 ), (6.986468 , 7.3603005),
       (6.974494 , 6.983719 ), (6.948615 , 7.2438035),
       (6.879772 , 7.4564214), (6.4616184, 7.0640054),
       (6.3962755, 7.043123 ), (6.192946 , 7.6621933),
       (6.1240463, 6.96867  ), (6.027879 , 7.1457624),
       (5.9752984, 6.7234483), (5.970968 , 7.098398 ),
       (5.8311887, 6.1682715), (5.8058743, 5.841333 ),
       (5.7054524, 6.2025003), (5.6992483, 8.1116295),
       (5.6632905, 5.9477086), (5.6085258, 5.7207093),
       (5.3748326, 8.438769 ), (5.3421   , 7.276237 ),
       (5.1496134, 7.2562103), (4.452667 , 7.893497 ),
       (4.364139 , 4.36983  ), (4.2970753, 7.950195 ),
       (4.150943 , 4.2925386), (3.97011  , 4.1984415),
       (3.9461696, 4.0509577), (3.9400399, 4.3264794),
       (3.9247284, 8.592203 ), (3.920657 , 4.7369714),
       (3.6543899, 8.679194 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.294883 ,  9.877703 ), (9.266363 ,  9.3143015),
       (9.154213 , 10.438318 ), (9.060761 ,  9.078367 ),
       (8.549322 ,  8.563404 ), (8.164368 ,  8.209746 ),
       (7.1358576,  7.175077 ), (6.825454 ,  6.8524747)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3185410499572754), (0.0, 1.323001503944397), (0.0, 1.3238674402236938), (0.0, 1.3257280588150024), (0.0, 1.3259215354919434), (0.0, 1.326073408126831), (0.0, 1.326245665550232), (0.0, 1.3263856172561646), (0.0, 1.327123999595642), (0.0, 1.327125072479248), (0.0, 1.327793002128601), (0.0, 1.3278981447219849), (0.0, 1.328023076057434), (0.0, 1.328062653541565), (0.0, 1.3281062841415405), (0.0, 1.328189730644226), (0.0, 1.3283147811889648), (0.0, 1.3284685611724854), (0.0, 1.3289090394973755), (0.0, 1.32906174659729), (0.0, 1.3293720483779907), (0.0, 1.3293980360031128), (0.0, 1.3294310569763184), (0.0, 1.3294570446014404), (0.0, 1.3294700384140015), (0.0, 1.3295389413833618), (0.0, 1.329720377922058), (0.0, 1.3298419713974), (0.0, 1.3300235271453857), (0.0, 1.330135464668274), (0.0, 1.3302757740020752), (0.0, 1.3306509256362915), (0.0, 1.3306833505630493), (0.0, 1.3307677507400513), (0.0, 1.3308436870574951), (0.0, 1.3311119079589844), (0.0, 1.3311299085617065), (0.0, 1.3312116861343384), (0.0, 1.3314448595046997), (0.0, 1.3314868211746216), (0.0, 1.3315658569335938), (0.0, 1.3318690061569214), (0.0, 1.3319469690322876), (0.0, 1.332005262374878), (0.0, 1.3320205211639404), (0.0, 1.3323229551315308), (0.0, 1.332443118095398), (0.0, 1.3325358629226685), (0.0, 1.3325363397598267), (0.0, 1.3325875997543335), (0.0, 1.3327951431274414), (0.0, 1.3328197002410889), (0.0, 1.3329389095306396), (0.0, 1.3329516649246216), (0.0, 1.3329861164093018), (0.0, 1.333117961883545), (0.0, 1.333220362663269), (0.0, 1.3332457542419434), (0.0, 1.3333604335784912), (0.0, 1.3333834409713745), (0.0, 1.3337210416793823), (0.0, 1.3339037895202637), (0.0, 1.3339076042175293), (0.0, 1.3340755701065063), (0.0, 1.3343652486801147), (0.0, 1.334486484527588), (0.0, 1.334512710571289), (0.0, 1.334527611732483), (0.0, 1.3345472812652588), (0.0, 1.3348729610443115), (0.0, 1.3351274728775024), (0.0, 1.335150957107544), (0.0, 1.3352580070495605), (0.0, 1.3352919816970825), (0.0, 1.3355042934417725), (0.0, 1.3356839418411255), (0.0, 1.3358346223831177), (0.0, 1.3363547325134277), (0.0, 1.3364341259002686), (0.0, 1.3364895582199097), (0.0, 1.3365705013275146), (0.0, 1.3365787267684937), (0.0, 1.33661687374115), (0.0, 1.336746335029602), (0.0, 1.336962342262268), (0.0, 1.3370373249053955), (0.0, 1.3371448516845703), (0.0, 1.3371886014938354), (0.0, 1.3372172117233276), (0.0, 1.3372453451156616), (0.0, 1.3373243808746338), (0.0, 1.3373967409133911), (0.0, 1.3375236988067627), (0.0, 1.337700605392456), (0.0, 1.3378100395202637), (0.0, 1.3378740549087524), (0.0, 1.3378746509552002), (0.0, 1.3380045890808105), (0.0, 1.3380872011184692), (0.0, 1.338132381439209), (0.0, 1.3386642932891846), (0.0, 1.3387501239776611), (0.0, 1.3391602039337158), (0.0, 1.339171051979065), (0.0, 1.339209794998169), (0.0, 1.339249610900879), (0.0, 1.3392804861068726), (0.0, 1.3397741317749023), (0.0, 1.3400131464004517), (0.0, 1.3400459289550781), (0.0, 1.3403652906417847), (0.0, 1.340441107749939), (0.0, 1.3404971361160278), (0.0, 1.3406001329421997), (0.0, 1.3406479358673096), (0.0, 1.3412251472473145), (0.0, 1.3418794870376587), (0.0, 1.3418922424316406), (0.0, 1.3419711589813232), (0.0, 1.3425257205963135), (0.0, 1.3425772190093994), (0.0, 1.343661904335022), (0.0, 1.3440313339233398), (0.0, 1.3445874452590942), (0.0, 1.3459006547927856), (0.0, 1.4077407121658325), (0.0, 1.4406722784042358), (0.0, 1.449593186378479), (0.0, 1.4516817331314087), (0.0, 1.4519538879394531), (0.0, 1.4525206089019775), (0.0, 1.452545166015625), (0.0, 1.4527838230133057), (0.0, 1.45280122756958), (0.0, 1.4529409408569336), (0.0, 1.4531303644180298), (0.0, 1.4532780647277832), (0.0, 1.4535936117172241), (0.0, 1.454284906387329), (0.0, 1.45441734790802), (0.0, 1.454614520072937), (0.0, 1.454901099205017), (0.0, 1.455029010772705), (0.0, 1.4552758932113647), (0.0, 1.4553948640823364), (0.0, 1.4556444883346558), (0.0, 1.455703616142273), (0.0, 1.4558881521224976), (0.0, 1.4559369087219238), (0.0, 1.4559401273727417), (0.0, 1.4559489488601685), (0.0, 1.4561258554458618), (0.0, 1.4561365842819214), (0.0, 1.4563602209091187), (0.0, 1.45650315284729), (0.0, 1.4565391540527344), (0.0, 1.4565715789794922), (0.0, 1.456827998161316), (0.0, 1.456913948059082), (0.0, 1.4570317268371582), (0.0, 1.457343578338623), (0.0, 1.4574466943740845), (0.0, 1.4574507474899292), (0.0, 1.4576630592346191), (0.0, 1.457810878753662), (0.0, 1.4578373432159424), (0.0, 1.4578814506530762), (0.0, 1.458086609840393), (0.0, 1.4583874940872192), (0.0, 1.4583964347839355), (0.0, 1.4584670066833496), (0.0, 1.4585214853286743), (0.0, 1.458579182624817), (0.0, 1.458687663078308), (0.0, 1.4587260484695435), (0.0, 1.4588712453842163), (0.0, 1.4588979482650757), (0.0, 1.4589289426803589), (0.0, 1.458967924118042), (0.0, 1.4590644836425781), (0.0, 1.4590779542922974), (0.0, 1.45930814743042), (0.0, 1.4593424797058105), (0.0, 1.4593714475631714), (0.0, 1.4594082832336426), (0.0, 1.4597101211547852), (0.0, 1.4597554206848145), (0.0, 1.4597957134246826), (0.0, 1.459839940071106), (0.0, 1.4599210023880005), (0.0, 1.4601001739501953), (0.0, 1.460148811340332), (0.0, 1.4602185487747192), (0.0, 1.4604501724243164), (0.0, 1.4605122804641724), (0.0, 1.4606990814208984), (0.0, 1.460789442062378), (0.0, 1.4608521461486816), (0.0, 1.4610481262207031), (0.0, 1.4610904455184937), (0.0, 1.4611904621124268), (0.0, 1.461316704750061), (0.0, 1.4613301753997803), (0.0, 1.4616186618804932), (0.0, 1.4617167711257935), (0.0, 1.4617730379104614), (0.0, 1.4617836475372314), (0.0, 1.461790919303894), (0.0, 1.4618648290634155), (0.0, 1.4620184898376465), (0.0, 1.4620189666748047), (0.0, 1.462053894996643), (0.0, 1.462058663368225), (0.0, 1.462087631225586), (0.0, 1.462117075920105), (0.0, 1.4622317552566528), (0.0, 1.4622318744659424), (0.0, 1.4624053239822388), (0.0, 1.4625331163406372), (0.0, 1.462658166885376), (0.0, 1.4630104303359985), (0.0, 1.4632881879806519), (0.0, 1.4636434316635132), (0.0, 1.4637409448623657), (0.0, 1.4637486934661865), (0.0, 1.4637565612792969), (0.0, 1.4639313220977783), (0.0, 1.4642335176467896), (0.0, 1.4643176794052124), (0.0, 1.464461088180542), (0.0, 1.4644654989242554), (0.0, 1.4645276069641113), (0.0, 1.4645940065383911), (0.0, 1.464714527130127), (0.0, 1.4647549390792847), (0.0, 1.4648245573043823), (0.0, 1.4651459455490112), (0.0, 1.465201497077942), (0.0, 1.4654825925827026), (0.0, 1.4656684398651123), (0.0, 1.4657329320907593), (0.0, 1.4662975072860718), (0.0, 1.4665720462799072), (0.0, 1.466840386390686), (0.0, 1.467069387435913), (0.0, 1.4673742055892944), (0.0, 1.4686970710754395), (0.0, 1.4692559242248535), (0.0, 1.4693243503570557), (0.0, 1.470230221748352), (0.0, 1.4708104133605957), (0.0, 1.473960041999817), (0.0, 1.4740302562713623), (0.0, 1.5082321166992188), (0.0, 1.5088967084884644), (0.0, 1.513078212738037), (0.0, 1.5138866901397705), (0.0, 1.514335036277771), (0.0, 1.5144354104995728), (0.0, 1.5148184299468994), (0.0, 1.5154428482055664), (0.0, 1.5154670476913452), (0.0, 1.5159053802490234), (0.0, 1.5159344673156738), (0.0, 1.516191005706787), (0.0, 1.5166617631912231), (0.0, 1.5167001485824585), (0.0, 1.5167455673217773), (0.0, 1.5168137550354004), (0.0, 1.5168310403823853), (0.0, 1.5171012878417969), (0.0, 1.517115592956543), (0.0, 1.5173671245574951), (0.0, 1.5175033807754517), (0.0, 1.5176717042922974), (0.0, 1.5177580118179321), (0.0, 1.5178189277648926), (0.0, 1.5179967880249023), (0.0, 1.5180492401123047), (0.0, 1.5181070566177368), (0.0, 1.518295168876648), (0.0, 1.5184510946273804), (0.0, 1.5184656381607056), (0.0, 1.5184918642044067), (0.0, 1.5187453031539917), (0.0, 1.5192079544067383), (0.0, 1.5192832946777344), (0.0, 1.519633173942566), (0.0, 1.5196717977523804), (0.0, 1.5197380781173706), (0.0, 1.5198265314102173), (0.0, 1.5201526880264282), (0.0, 1.5202155113220215), (0.0, 1.5203367471694946), (0.0, 1.5203770399093628), (0.0, 1.5204219818115234), (0.0, 1.5205415487289429), (0.0, 1.5205484628677368), (0.0, 1.5207557678222656), (0.0, 1.520938515663147), (0.0, 1.5209873914718628), (0.0, 1.52104914188385), (0.0, 1.5210673809051514), (0.0, 1.5212002992630005), (0.0, 1.5212918519973755), (0.0, 1.521401047706604), (0.0, 1.5214486122131348), (0.0, 1.521558403968811), (0.0, 1.5217102766036987), (0.0, 1.521765112876892), (0.0, 1.52176833152771), (0.0, 1.5220422744750977), (0.0, 1.5220437049865723), (0.0, 1.5221649408340454), (0.0, 1.5223222970962524), (0.0, 1.5224653482437134), (0.0, 1.5224854946136475), (0.0, 1.5226038694381714), (0.0, 1.5226311683654785), (0.0, 1.522678017616272), (0.0, 1.5228862762451172), (0.0, 1.5229305028915405), (0.0, 1.5230772495269775), (0.0, 1.5231443643569946), (0.0, 1.5231735706329346), (0.0, 1.5232343673706055), (0.0, 1.523245096206665), (0.0, 1.5232679843902588), (0.0, 1.5233417749404907), (0.0, 1.5234019756317139), (0.0, 1.5235166549682617), (0.0, 1.523685336112976), (0.0, 1.5238008499145508), (0.0, 1.5239331722259521), (0.0, 1.5240168571472168), (0.0, 1.5241857767105103), (0.0, 1.5241878032684326), (0.0, 1.5243505239486694), (0.0, 1.5244427919387817), (0.0, 1.5244427919387817), (0.0, 1.5244609117507935), (0.0, 1.5244799852371216), (0.0, 1.5245294570922852), (0.0, 1.5246206521987915), (0.0, 1.5247005224227905), (0.0, 1.5247735977172852), (0.0, 1.5249440670013428), (0.0, 1.5252882242202759), (0.0, 1.5253267288208008), (0.0, 1.5255259275436401), (0.0, 1.52572762966156), (0.0, 1.5258166790008545), (0.0, 1.5260320901870728), (0.0, 1.5260621309280396), (0.0, 1.5262993574142456), (0.0, 1.526519775390625), (0.0, 1.526688575744629), (0.0, 1.5267455577850342), (0.0, 1.5269478559494019), (0.0, 1.527099609375), (0.0, 1.5272767543792725), (0.0, 1.5273246765136719), (0.0, 1.5273257493972778), (0.0, 1.5277453660964966), (0.0, 1.527775764465332), (0.0, 1.5278661251068115), (0.0, 1.5280091762542725), (0.0, 1.5285401344299316), (0.0, 1.5286272764205933), (0.0, 1.5286890268325806), (0.0, 1.528730869293213), (0.0, 1.5288902521133423), (0.0, 1.5299979448318481), (0.0, 1.5300564765930176), (0.0, 1.5305231809616089), (0.0, 1.5310903787612915), (0.0, 1.5311170816421509), (0.0, 1.5322157144546509), (0.0, 1.5337251424789429), (0.0, 1.5451686382293701)], [(8.385193824768066, 8.545870780944824), (7.660478115081787, 8.258452415466309), (7.018294811248779, 7.122622966766357), (6.9864678382873535, 7.360300540924072), (6.974493980407715, 6.9837188720703125), (6.948615074157715, 7.24380350112915), (6.879772186279297, 7.456421375274658), (6.461618423461914, 7.064005374908447), (6.396275520324707, 7.0431227684021), (6.192945957183838, 7.662193298339844), (6.124046325683594, 6.968669891357422), (6.027879238128662, 7.1457624435424805), (5.9752984046936035, 6.723448276519775), (5.970967769622803, 7.098398208618164), (5.831188678741455, 6.168271541595459), (5.805874347686768, 5.841332912445068), (5.7054524421691895, 6.202500343322754), (5.699248313903809, 8.111629486083984), (5.663290500640869, 5.947708606719971), (5.60852575302124, 5.720709323883057), (5.374832630157471, 8.438769340515137), (5.342100143432617, 7.2762370109558105), (5.149613380432129, 7.2562103271484375), (4.452667236328125, 7.893496990203857), (4.364139080047607, 4.369830131530762), (4.297075271606445, 7.950194835662842), (4.150942802429199, 4.292538642883301), (3.9701099395751953, 4.198441505432129), (3.94616961479187, 4.050957679748535), (3.940039873123169, 4.326479434967041), (3.9247283935546875, 8.592203140258789), (3.92065691947937, 4.736971378326416), (3.6543898582458496, 8.679194450378418)], [(9.294882774353027, 9.877702713012695), (9.266363143920898, 9.314301490783691), (9.154212951660156, 10.438318252563477), (9.060761451721191, 9.078367233276367), (8.549322128295898, 8.563404083251953), (8.16436767578125, 8.209746360778809), (7.135857582092285, 7.175076961517334), (6.825454235076904, 6.852474689483643)]]
[array([[0.        , 1.31854105],
       [0.        , 1.3230015 ],
       [0.        , 1.32386744],
       [0.        , 1.32572806],
       [0.        , 1.32592154],
       [0.        , 1.32607341],
       [0.        , 1.32624567],
       [0.        , 1.32638562],
       [0.        , 1.327124  ],
       [0.        , 1.32712507],
       [0.        , 1.327793  ],
       [0.        , 1.32789814],
       [0.        , 1.32802308],
       [0.        , 1.32806265],
       [0.        , 1.32810628],
       [0.        , 1.32818973],
       [0.        , 1.32831478],
       [0.        , 1.32846856],
       [0.        , 1.32890904],
       [0.        , 1.32906175],
       [0.        , 1.32937205],
       [0.        , 1.32939804],
       [0.        , 1.32943106],
       [0.        , 1.32945704],
       [0.        , 1.32947004],
       [0.        , 1.32953894],
       [0.        , 1.32972038],
       [0.        , 1.32984197],
       [0.        , 1.33002353],
       [0.        , 1.33013546],
       [0.        , 1.33027577],
       [0.        , 1.33065093],
       [0.        , 1.33068335],
       [0.        , 1.33076775],
       [0.        , 1.33084369],
       [0.        , 1.33111191],
       [0.        , 1.33112991],
       [0.        , 1.33121169],
       [0.        , 1.33144486],
       [0.        , 1.33148682],
       [0.        , 1.33156586],
       [0.        , 1.33186901],
       [0.        , 1.33194697],
       [0.        , 1.33200526],
       [0.        , 1.33202052],
       [0.        , 1.33232296],
       [0.        , 1.33244312],
       [0.        , 1.33253586],
       [0.        , 1.33253634],
       [0.        , 1.3325876 ],
       [0.        , 1.33279514],
       [0.        , 1.3328197 ],
       [0.        , 1.33293891],
       [0.        , 1.33295166],
       [0.        , 1.33298612],
       [0.        , 1.33311796],
       [0.        , 1.33322036],
       [0.        , 1.33324575],
       [0.        , 1.33336043],
       [0.        , 1.33338344],
       [0.        , 1.33372104],
       [0.        , 1.33390379],
       [0.        , 1.3339076 ],
       [0.        , 1.33407557],
       [0.        , 1.33436525],
       [0.        , 1.33448648],
       [0.        , 1.33451271],
       [0.        , 1.33452761],
       [0.        , 1.33454728],
       [0.        , 1.33487296],
       [0.        , 1.33512747],
       [0.        , 1.33515096],
       [0.        , 1.33525801],
       [0.        , 1.33529198],
       [0.        , 1.33550429],
       [0.        , 1.33568394],
       [0.        , 1.33583462],
       [0.        , 1.33635473],
       [0.        , 1.33643413],
       [0.        , 1.33648956],
       [0.        , 1.3365705 ],
       [0.        , 1.33657873],
       [0.        , 1.33661687],
       [0.        , 1.33674634],
       [0.        , 1.33696234],
       [0.        , 1.33703732],
       [0.        , 1.33714485],
       [0.        , 1.3371886 ],
       [0.        , 1.33721721],
       [0.        , 1.33724535],
       [0.        , 1.33732438],
       [0.        , 1.33739674],
       [0.        , 1.3375237 ],
       [0.        , 1.33770061],
       [0.        , 1.33781004],
       [0.        , 1.33787405],
       [0.        , 1.33787465],
       [0.        , 1.33800459],
       [0.        , 1.3380872 ],
       [0.        , 1.33813238],
       [0.        , 1.33866429],
       [0.        , 1.33875012],
       [0.        , 1.3391602 ],
       [0.        , 1.33917105],
       [0.        , 1.33920979],
       [0.        , 1.33924961],
       [0.        , 1.33928049],
       [0.        , 1.33977413],
       [0.        , 1.34001315],
       [0.        , 1.34004593],
       [0.        , 1.34036529],
       [0.        , 1.34044111],
       [0.        , 1.34049714],
       [0.        , 1.34060013],
       [0.        , 1.34064794],
       [0.        , 1.34122515],
       [0.        , 1.34187949],
       [0.        , 1.34189224],
       [0.        , 1.34197116],
       [0.        , 1.34252572],
       [0.        , 1.34257722],
       [0.        , 1.3436619 ],
       [0.        , 1.34403133],
       [0.        , 1.34458745],
       [0.        , 1.34590065],
       [0.        , 1.40774071],
       [0.        , 1.44067228],
       [0.        , 1.44959319],
       [0.        , 1.45168173],
       [0.        , 1.45195389],
       [0.        , 1.45252061],
       [0.        , 1.45254517],
       [0.        , 1.45278382],
       [0.        , 1.45280123],
       [0.        , 1.45294094],
       [0.        , 1.45313036],
       [0.        , 1.45327806],
       [0.        , 1.45359361],
       [0.        , 1.45428491],
       [0.        , 1.45441735],
       [0.        , 1.45461452],
       [0.        , 1.4549011 ],
       [0.        , 1.45502901],
       [0.        , 1.45527589],
       [0.        , 1.45539486],
       [0.        , 1.45564449],
       [0.        , 1.45570362],
       [0.        , 1.45588815],
       [0.        , 1.45593691],
       [0.        , 1.45594013],
       [0.        , 1.45594895],
       [0.        , 1.45612586],
       [0.        , 1.45613658],
       [0.        , 1.45636022],
       [0.        , 1.45650315],
       [0.        , 1.45653915],
       [0.        , 1.45657158],
       [0.        , 1.456828  ],
       [0.        , 1.45691395],
       [0.        , 1.45703173],
       [0.        , 1.45734358],
       [0.        , 1.45744669],
       [0.        , 1.45745075],
       [0.        , 1.45766306],
       [0.        , 1.45781088],
       [0.        , 1.45783734],
       [0.        , 1.45788145],
       [0.        , 1.45808661],
       [0.        , 1.45838749],
       [0.        , 1.45839643],
       [0.        , 1.45846701],
       [0.        , 1.45852149],
       [0.        , 1.45857918],
       [0.        , 1.45868766],
       [0.        , 1.45872605],
       [0.        , 1.45887125],
       [0.        , 1.45889795],
       [0.        , 1.45892894],
       [0.        , 1.45896792],
       [0.        , 1.45906448],
       [0.        , 1.45907795],
       [0.        , 1.45930815],
       [0.        , 1.45934248],
       [0.        , 1.45937145],
       [0.        , 1.45940828],
       [0.        , 1.45971012],
       [0.        , 1.45975542],
       [0.        , 1.45979571],
       [0.        , 1.45983994],
       [0.        , 1.459921  ],
       [0.        , 1.46010017],
       [0.        , 1.46014881],
       [0.        , 1.46021855],
       [0.        , 1.46045017],
       [0.        , 1.46051228],
       [0.        , 1.46069908],
       [0.        , 1.46078944],
       [0.        , 1.46085215],
       [0.        , 1.46104813],
       [0.        , 1.46109045],
       [0.        , 1.46119046],
       [0.        , 1.4613167 ],
       [0.        , 1.46133018],
       [0.        , 1.46161866],
       [0.        , 1.46171677],
       [0.        , 1.46177304],
       [0.        , 1.46178365],
       [0.        , 1.46179092],
       [0.        , 1.46186483],
       [0.        , 1.46201849],
       [0.        , 1.46201897],
       [0.        , 1.46205389],
       [0.        , 1.46205866],
       [0.        , 1.46208763],
       [0.        , 1.46211708],
       [0.        , 1.46223176],
       [0.        , 1.46223187],
       [0.        , 1.46240532],
       [0.        , 1.46253312],
       [0.        , 1.46265817],
       [0.        , 1.46301043],
       [0.        , 1.46328819],
       [0.        , 1.46364343],
       [0.        , 1.46374094],
       [0.        , 1.46374869],
       [0.        , 1.46375656],
       [0.        , 1.46393132],
       [0.        , 1.46423352],
       [0.        , 1.46431768],
       [0.        , 1.46446109],
       [0.        , 1.4644655 ],
       [0.        , 1.46452761],
       [0.        , 1.46459401],
       [0.        , 1.46471453],
       [0.        , 1.46475494],
       [0.        , 1.46482456],
       [0.        , 1.46514595],
       [0.        , 1.4652015 ],
       [0.        , 1.46548259],
       [0.        , 1.46566844],
       [0.        , 1.46573293],
       [0.        , 1.46629751],
       [0.        , 1.46657205],
       [0.        , 1.46684039],
       [0.        , 1.46706939],
       [0.        , 1.46737421],
       [0.        , 1.46869707],
       [0.        , 1.46925592],
       [0.        , 1.46932435],
       [0.        , 1.47023022],
       [0.        , 1.47081041],
       [0.        , 1.47396004],
       [0.        , 1.47403026],
       [0.        , 1.50823212],
       [0.        , 1.50889671],
       [0.        , 1.51307821],
       [0.        , 1.51388669],
       [0.        , 1.51433504],
       [0.        , 1.51443541],
       [0.        , 1.51481843],
       [0.        , 1.51544285],
       [0.        , 1.51546705],
       [0.        , 1.51590538],
       [0.        , 1.51593447],
       [0.        , 1.51619101],
       [0.        , 1.51666176],
       [0.        , 1.51670015],
       [0.        , 1.51674557],
       [0.        , 1.51681376],
       [0.        , 1.51683104],
       [0.        , 1.51710129],
       [0.        , 1.51711559],
       [0.        , 1.51736712],
       [0.        , 1.51750338],
       [0.        , 1.5176717 ],
       [0.        , 1.51775801],
       [0.        , 1.51781893],
       [0.        , 1.51799679],
       [0.        , 1.51804924],
       [0.        , 1.51810706],
       [0.        , 1.51829517],
       [0.        , 1.51845109],
       [0.        , 1.51846564],
       [0.        , 1.51849186],
       [0.        , 1.5187453 ],
       [0.        , 1.51920795],
       [0.        , 1.51928329],
       [0.        , 1.51963317],
       [0.        , 1.5196718 ],
       [0.        , 1.51973808],
       [0.        , 1.51982653],
       [0.        , 1.52015269],
       [0.        , 1.52021551],
       [0.        , 1.52033675],
       [0.        , 1.52037704],
       [0.        , 1.52042198],
       [0.        , 1.52054155],
       [0.        , 1.52054846],
       [0.        , 1.52075577],
       [0.        , 1.52093852],
       [0.        , 1.52098739],
       [0.        , 1.52104914],
       [0.        , 1.52106738],
       [0.        , 1.5212003 ],
       [0.        , 1.52129185],
       [0.        , 1.52140105],
       [0.        , 1.52144861],
       [0.        , 1.5215584 ],
       [0.        , 1.52171028],
       [0.        , 1.52176511],
       [0.        , 1.52176833],
       [0.        , 1.52204227],
       [0.        , 1.5220437 ],
       [0.        , 1.52216494],
       [0.        , 1.5223223 ],
       [0.        , 1.52246535],
       [0.        , 1.52248549],
       [0.        , 1.52260387],
       [0.        , 1.52263117],
       [0.        , 1.52267802],
       [0.        , 1.52288628],
       [0.        , 1.5229305 ],
       [0.        , 1.52307725],
       [0.        , 1.52314436],
       [0.        , 1.52317357],
       [0.        , 1.52323437],
       [0.        , 1.5232451 ],
       [0.        , 1.52326798],
       [0.        , 1.52334177],
       [0.        , 1.52340198],
       [0.        , 1.52351665],
       [0.        , 1.52368534],
       [0.        , 1.52380085],
       [0.        , 1.52393317],
       [0.        , 1.52401686],
       [0.        , 1.52418578],
       [0.        , 1.5241878 ],
       [0.        , 1.52435052],
       [0.        , 1.52444279],
       [0.        , 1.52444279],
       [0.        , 1.52446091],
       [0.        , 1.52447999],
       [0.        , 1.52452946],
       [0.        , 1.52462065],
       [0.        , 1.52470052],
       [0.        , 1.5247736 ],
       [0.        , 1.52494407],
       [0.        , 1.52528822],
       [0.        , 1.52532673],
       [0.        , 1.52552593],
       [0.        , 1.52572763],
       [0.        , 1.52581668],
       [0.        , 1.52603209],
       [0.        , 1.52606213],
       [0.        , 1.52629936],
       [0.        , 1.52651978],
       [0.        , 1.52668858],
       [0.        , 1.52674556],
       [0.        , 1.52694786],
       [0.        , 1.52709961],
       [0.        , 1.52727675],
       [0.        , 1.52732468],
       [0.        , 1.52732575],
       [0.        , 1.52774537],
       [0.        , 1.52777576],
       [0.        , 1.52786613],
       [0.        , 1.52800918],
       [0.        , 1.52854013],
       [0.        , 1.52862728],
       [0.        , 1.52868903],
       [0.        , 1.52873087],
       [0.        , 1.52889025],
       [0.        , 1.52999794],
       [0.        , 1.53005648],
       [0.        , 1.53052318],
       [0.        , 1.53109038],
       [0.        , 1.53111708],
       [0.        , 1.53221571],
       [0.        , 1.53372514],
       [0.        , 1.54516864]]), array([[8.38519382, 8.54587078],
       [7.66047812, 8.25845242],
       [7.01829481, 7.12262297],
       [6.98646784, 7.36030054],
       [6.97449398, 6.98371887],
       [6.94861507, 7.2438035 ],
       [6.87977219, 7.45642138],
       [6.46161842, 7.06400537],
       [6.39627552, 7.04312277],
       [6.19294596, 7.6621933 ],
       [6.12404633, 6.96866989],
       [6.02787924, 7.14576244],
       [5.9752984 , 6.72344828],
       [5.97096777, 7.09839821],
       [5.83118868, 6.16827154],
       [5.80587435, 5.84133291],
       [5.70545244, 6.20250034],
       [5.69924831, 8.11162949],
       [5.6632905 , 5.94770861],
       [5.60852575, 5.72070932],
       [5.37483263, 8.43876934],
       [5.34210014, 7.27623701],
       [5.14961338, 7.25621033],
       [4.45266724, 7.89349699],
       [4.36413908, 4.36983013],
       [4.29707527, 7.95019484],
       [4.1509428 , 4.29253864],
       [3.97010994, 4.19844151],
       [3.94616961, 4.05095768],
       [3.94003987, 4.32647943],
       [3.92472839, 8.59220314],
       [3.92065692, 4.73697138],
       [3.65438986, 8.67919445]]), array([[ 9.29488277,  9.87770271],
       [ 9.26636314,  9.31430149],
       [ 9.15421295, 10.43831825],
       [ 9.06076145,  9.07836723],
       [ 8.54932213,  8.56340408],
       [ 8.16436768,  8.20974636],
       [ 7.13585758,  7.17507696],
       [ 6.82545424,  6.85247469]])]2024-03-06 17:45:50.497116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6G0F ph vector generated, counter: 25
2024-03-06 17:45:54.974924: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:45:55.016437: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:45:56.063665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3125441), (0., 1.3193022), (0., 1.3202376), (0., 1.3219   ),
       (0., 1.322714 ), (0., 1.3231076), (0., 1.3234264), (0., 1.3256932),
       (0., 1.3258332), (0., 1.3266685), (0., 1.3267508), (0., 1.3269261),
       (0., 1.32726  ), (0., 1.3273568), (0., 1.3274713), (0., 1.3278621),
       (0., 1.328099 ), (0., 1.3283938), (0., 1.3284563), (0., 1.3284761),
       (0., 1.328912 ), (0., 1.3290756), (0., 1.3291053), (0., 1.3293073),
       (0., 1.3293111), (0., 1.329423 ), (0., 1.3297498), (0., 1.3297561),
       (0., 1.3299037), (0., 1.3299607), (0., 1.3300674), (0., 1.330191 ),
       (0., 1.3301945), (0., 1.3302672), (0., 1.3303803), (0., 1.3306066),
       (0., 1.330634 ), (0., 1.3306438), (0., 1.3306547), (0., 1.3308684),
       (0., 1.331041 ), (0., 1.3310635), (0., 1.3311405), (0., 1.3311684),
       (0., 1.3314224), (0., 1.3315414), (0., 1.331774 ), (0., 1.3319823),
       (0., 1.3320063), (0., 1.3320477), (0., 1.3320692), (0., 1.332245 ),
       (0., 1.3322905), (0., 1.3323226), (0., 1.332331 ), (0., 1.3323509),
       (0., 1.3325039), (0., 1.3325294), (0., 1.3326817), (0., 1.3326902),
       (0., 1.3327358), (0., 1.3328027), (0., 1.3330581), (0., 1.3331267),
       (0., 1.3332099), (0., 1.3332783), (0., 1.3332835), (0., 1.3334883),
       (0., 1.333514 ), (0., 1.3335923), (0., 1.3336459), (0., 1.3340279),
       (0., 1.3340313), (0., 1.334114 ), (0., 1.3341564), (0., 1.3341628),
       (0., 1.3342143), (0., 1.334249 ), (0., 1.3343955), (0., 1.3346117),
       (0., 1.3346452), (0., 1.3346606), (0., 1.3348273), (0., 1.3351303),
       (0., 1.3351624), (0., 1.3354685), (0., 1.3357986), (0., 1.3358146),
       (0., 1.3361198), (0., 1.3361406), (0., 1.3361694), (0., 1.3364022),
       (0., 1.3364309), (0., 1.3365709), (0., 1.336743 ), (0., 1.336785 ),
       (0., 1.3368665), (0., 1.337343 ), (0., 1.3375064), (0., 1.3375552),
       (0., 1.337579 ), (0., 1.3377749), (0., 1.3378841), (0., 1.3379159),
       (0., 1.3379207), (0., 1.3384882), (0., 1.3388617), (0., 1.3389758),
       (0., 1.3395765), (0., 1.3399286), (0., 1.3400853), (0., 1.3401335),
       (0., 1.340675 ), (0., 1.3414141), (0., 1.3415222), (0., 1.3419331),
       (0., 1.3420625), (0., 1.3421687), (0., 1.3427223), (0., 1.3428764),
       (0., 1.3428993), (0., 1.3429474), (0., 1.3453968), (0., 1.345602 ),
       (0., 1.3680147), (0., 1.4425489), (0., 1.4487953), (0., 1.449938 ),
       (0., 1.4503906), (0., 1.450666 ), (0., 1.4511855), (0., 1.4514986),
       (0., 1.452409 ), (0., 1.4524099), (0., 1.4524177), (0., 1.4524493),
       (0., 1.4524894), (0., 1.452498 ), (0., 1.4528494), (0., 1.4530507),
       (0., 1.4533755), (0., 1.4537209), (0., 1.4538709), (0., 1.4543257),
       (0., 1.4545822), (0., 1.454801 ), (0., 1.4548755), (0., 1.4550283),
       (0., 1.4551169), (0., 1.4553565), (0., 1.4556003), (0., 1.455733 ),
       (0., 1.4558234), (0., 1.455845 ), (0., 1.4558823), (0., 1.4561336),
       (0., 1.456151 ), (0., 1.456292 ), (0., 1.4564488), (0., 1.4566706),
       (0., 1.4568963), (0., 1.4570942), (0., 1.4572188), (0., 1.4572875),
       (0., 1.4574275), (0., 1.4575157), (0., 1.4575174), (0., 1.4575888),
       (0., 1.4576786), (0., 1.4578557), (0., 1.4578758), (0., 1.4580472),
       (0., 1.4580638), (0., 1.4583402), (0., 1.4586676), (0., 1.4586822),
       (0., 1.458895 ), (0., 1.459097 ), (0., 1.4592229), (0., 1.4594033),
       (0., 1.4594146), (0., 1.4595789), (0., 1.459619 ), (0., 1.4596311),
       (0., 1.4597024), (0., 1.4598919), (0., 1.460061 ), (0., 1.4603767),
       (0., 1.4604286), (0., 1.460529 ), (0., 1.4605417), (0., 1.4605657),
       (0., 1.4606674), (0., 1.4607205), (0., 1.4608291), (0., 1.46085  ),
       (0., 1.4611726), (0., 1.4612231), (0., 1.4613099), (0., 1.461364 ),
       (0., 1.4616593), (0., 1.4618721), (0., 1.4619037), (0., 1.4620191),
       (0., 1.4621015), (0., 1.4621264), (0., 1.462132 ), (0., 1.4621999),
       (0., 1.4622283), (0., 1.4623178), (0., 1.4623364), (0., 1.4624616),
       (0., 1.4624732), (0., 1.4624903), (0., 1.4625016), (0., 1.462502 ),
       (0., 1.4627627), (0., 1.4628474), (0., 1.4630764), (0., 1.4631748),
       (0., 1.4633414), (0., 1.4634491), (0., 1.4636617), (0., 1.463821 ),
       (0., 1.4639649), (0., 1.4641811), (0., 1.4643217), (0., 1.4646248),
       (0., 1.4647151), (0., 1.4647241), (0., 1.4647608), (0., 1.4648044),
       (0., 1.4650859), (0., 1.4652786), (0., 1.4654821), (0., 1.4656446),
       (0., 1.4659692), (0., 1.4661403), (0., 1.466985 ), (0., 1.4675637),
       (0., 1.4680525), (0., 1.4686624), (0., 1.4687065), (0., 1.4688148),
       (0., 1.4701382), (0., 1.4705893), (0., 1.4714874), (0., 1.4722074),
       (0., 1.4722226), (0., 1.4745437), (0., 1.4864817), (0., 1.4986329),
       (0., 1.5063982), (0., 1.5099581), (0., 1.5103809), (0., 1.5104396),
       (0., 1.5107727), (0., 1.5115355), (0., 1.5127053), (0., 1.5127668),
       (0., 1.5140653), (0., 1.5143772), (0., 1.5144305), (0., 1.5145828),
       (0., 1.5147898), (0., 1.5153102), (0., 1.5156311), (0., 1.5157   ),
       (0., 1.51624  ), (0., 1.5163369), (0., 1.5166287), (0., 1.5166867),
       (0., 1.5167032), (0., 1.5168165), (0., 1.5168965), (0., 1.5170319),
       (0., 1.5178326), (0., 1.518014 ), (0., 1.5181125), (0., 1.51818  ),
       (0., 1.5182492), (0., 1.518431 ), (0., 1.5184997), (0., 1.5186771),
       (0., 1.5187212), (0., 1.5187266), (0., 1.5189881), (0., 1.519016 ),
       (0., 1.5191516), (0., 1.5191816), (0., 1.5197649), (0., 1.5198686),
       (0., 1.5200984), (0., 1.5201726), (0., 1.5202445), (0., 1.5202845),
       (0., 1.5203156), (0., 1.5203513), (0., 1.5203674), (0., 1.5204535),
       (0., 1.5209045), (0., 1.5209957), (0., 1.5211316), (0., 1.5215064),
       (0., 1.5218027), (0., 1.521824 ), (0., 1.5218391), (0., 1.5218755),
       (0., 1.522239 ), (0., 1.5223445), (0., 1.5224527), (0., 1.5227333),
       (0., 1.5228579), (0., 1.5228599), (0., 1.5229071), (0., 1.5229094),
       (0., 1.5231417), (0., 1.523167 ), (0., 1.5231893), (0., 1.5232037),
       (0., 1.5234381), (0., 1.5234702), (0., 1.523539 ), (0., 1.5235779),
       (0., 1.5236427), (0., 1.5237547), (0., 1.5237767), (0., 1.5239388),
       (0., 1.5239605), (0., 1.5240291), (0., 1.52407  ), (0., 1.5243274),
       (0., 1.5244234), (0., 1.5245101), (0., 1.5246965), (0., 1.5247847),
       (0., 1.524957 ), (0., 1.5250033), (0., 1.5250919), (0., 1.5251677),
       (0., 1.5252113), (0., 1.5252196), (0., 1.5255262), (0., 1.5255475),
       (0., 1.5255542), (0., 1.5257099), (0., 1.5257235), (0., 1.5257251),
       (0., 1.5257425), (0., 1.5257934), (0., 1.5261594), (0., 1.5261748),
       (0., 1.5262052), (0., 1.5262944), (0., 1.5264045), (0., 1.5264896),
       (0., 1.5265244), (0., 1.5265836), (0., 1.5269443), (0., 1.526972 ),
       (0., 1.5270398), (0., 1.5272728), (0., 1.5273037), (0., 1.5277779),
       (0., 1.5280961), (0., 1.5281074), (0., 1.5285383), (0., 1.5287285),
       (0., 1.5295773), (0., 1.5302861), (0., 1.5303023), (0., 1.5305707),
       (0., 1.5307134), (0., 1.5308551), (0., 1.530892 ), (0., 1.5309873),
       (0., 1.5314536), (0., 1.5339708), (0., 1.5455421), (0., 1.5631356)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.569643 , 8.586546 ), (8.525722 , 8.683898 ),
       (7.0435243, 7.0617747), (6.8846507, 6.9417267),
       (6.8614593, 7.0877624), (6.69131  , 6.715255 ),
       (6.6602173, 7.4190726), (6.5826364, 6.992643 ),
       (6.5017204, 6.9513397), (6.49736  , 7.0002313),
       (6.2214103, 7.5500717), (6.207073 , 6.640653 ),
       (6.0439196, 6.897527 ), (6.039967 , 7.3530626),
       (5.9766045, 6.9836044), (5.879404 , 7.857719 ),
       (5.8212733, 6.2893043), (5.795064 , 6.43355  ),
       (5.7243533, 5.90037  ), (5.6514916, 8.507406 ),
       (5.640135 , 5.698148 ), (5.592682 , 6.3277855),
       (5.529249 , 7.602902 ), (5.513336 , 7.192134 ),
       (5.3416233, 8.687568 ), (4.8486004, 7.668286 ),
       (4.5002203, 7.6137977), (4.028902 , 4.397696 ),
       (3.998501 , 4.696974 ), (3.9522512, 3.9877152),
       (3.9474833, 4.0790176), (3.9195445, 4.211341 ),
       (3.7144778, 8.609584 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.555504 , 10.144299), (9.16325  ,  9.25708 ),
       (9.119277 , 10.556738), (8.915873 ,  9.00115 ),
       (8.107091 ,  8.171465), (7.890803 ,  8.187595),
       (7.1970944,  7.317608)], dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3125441074371338), (0.0, 1.3193022012710571), (0.0, 1.320237636566162), (0.0, 1.3219000101089478), (0.0, 1.3227139711380005), (0.0, 1.3231076002120972), (0.0, 1.323426365852356), (0.0, 1.3256932497024536), (0.0, 1.3258332014083862), (0.0, 1.3266685009002686), (0.0, 1.3267507553100586), (0.0, 1.3269261121749878), (0.0, 1.3272600173950195), (0.0, 1.3273568153381348), (0.0, 1.3274712562561035), (0.0, 1.3278621435165405), (0.0, 1.328099012374878), (0.0, 1.328393816947937), (0.0, 1.3284562826156616), (0.0, 1.328476071357727), (0.0, 1.3289120197296143), (0.0, 1.329075574874878), (0.0, 1.329105257987976), (0.0, 1.3293073177337646), (0.0, 1.3293111324310303), (0.0, 1.329422950744629), (0.0, 1.3297498226165771), (0.0, 1.3297561407089233), (0.0, 1.3299037218093872), (0.0, 1.3299607038497925), (0.0, 1.3300673961639404), (0.0, 1.3301910161972046), (0.0, 1.3301944732666016), (0.0, 1.3302671909332275), (0.0, 1.3303803205490112), (0.0, 1.3306065797805786), (0.0, 1.3306339979171753), (0.0, 1.3306437730789185), (0.0, 1.3306547403335571), (0.0, 1.3308683633804321), (0.0, 1.3310409784317017), (0.0, 1.3310635089874268), (0.0, 1.3311405181884766), (0.0, 1.3311684131622314), (0.0, 1.3314224481582642), (0.0, 1.3315414190292358), (0.0, 1.3317739963531494), (0.0, 1.3319822549819946), (0.0, 1.3320063352584839), (0.0, 1.332047700881958), (0.0, 1.3320691585540771), (0.0, 1.3322449922561646), (0.0, 1.332290530204773), (0.0, 1.332322597503662), (0.0, 1.3323309421539307), (0.0, 1.3323508501052856), (0.0, 1.3325039148330688), (0.0, 1.3325294256210327), (0.0, 1.332681655883789), (0.0, 1.3326902389526367), (0.0, 1.3327357769012451), (0.0, 1.332802653312683), (0.0, 1.3330581188201904), (0.0, 1.3331266641616821), (0.0, 1.3332098722457886), (0.0, 1.3332782983779907), (0.0, 1.333283543586731), (0.0, 1.3334883451461792), (0.0, 1.3335139751434326), (0.0, 1.3335922956466675), (0.0, 1.3336459398269653), (0.0, 1.334027886390686), (0.0, 1.334031343460083), (0.0, 1.3341139554977417), (0.0, 1.3341563940048218), (0.0, 1.3341628313064575), (0.0, 1.3342143297195435), (0.0, 1.3342490196228027), (0.0, 1.3343955278396606), (0.0, 1.3346116542816162), (0.0, 1.33464515209198), (0.0, 1.3346606492996216), (0.0, 1.3348273038864136), (0.0, 1.3351303339004517), (0.0, 1.3351624011993408), (0.0, 1.3354685306549072), (0.0, 1.3357986211776733), (0.0, 1.3358145952224731), (0.0, 1.3361197710037231), (0.0, 1.3361406326293945), (0.0, 1.3361693620681763), (0.0, 1.336402177810669), (0.0, 1.3364309072494507), (0.0, 1.3365708589553833), (0.0, 1.3367429971694946), (0.0, 1.3367849588394165), (0.0, 1.3368664979934692), (0.0, 1.3373429775238037), (0.0, 1.3375064134597778), (0.0, 1.337555170059204), (0.0, 1.3375790119171143), (0.0, 1.3377748727798462), (0.0, 1.3378840684890747), (0.0, 1.3379158973693848), (0.0, 1.3379206657409668), (0.0, 1.338488221168518), (0.0, 1.3388617038726807), (0.0, 1.3389757871627808), (0.0, 1.3395764827728271), (0.0, 1.3399286270141602), (0.0, 1.3400852680206299), (0.0, 1.340133547782898), (0.0, 1.3406749963760376), (0.0, 1.3414140939712524), (0.0, 1.341522216796875), (0.0, 1.3419331312179565), (0.0, 1.3420624732971191), (0.0, 1.3421686887741089), (0.0, 1.3427222967147827), (0.0, 1.3428764343261719), (0.0, 1.3428993225097656), (0.0, 1.3429473638534546), (0.0, 1.3453967571258545), (0.0, 1.345602035522461), (0.0, 1.3680146932601929), (0.0, 1.4425488710403442), (0.0, 1.4487953186035156), (0.0, 1.4499380588531494), (0.0, 1.4503905773162842), (0.0, 1.4506659507751465), (0.0, 1.4511854648590088), (0.0, 1.4514986276626587), (0.0, 1.452409029006958), (0.0, 1.4524098634719849), (0.0, 1.4524177312850952), (0.0, 1.4524493217468262), (0.0, 1.4524893760681152), (0.0, 1.452497959136963), (0.0, 1.4528493881225586), (0.0, 1.4530507326126099), (0.0, 1.4533754587173462), (0.0, 1.4537209272384644), (0.0, 1.4538708925247192), (0.0, 1.4543256759643555), (0.0, 1.4545822143554688), (0.0, 1.4548009634017944), (0.0, 1.4548754692077637), (0.0, 1.4550282955169678), (0.0, 1.455116868019104), (0.0, 1.455356478691101), (0.0, 1.4556002616882324), (0.0, 1.4557329416275024), (0.0, 1.4558234214782715), (0.0, 1.4558449983596802), (0.0, 1.4558823108673096), (0.0, 1.4561336040496826), (0.0, 1.456151008605957), (0.0, 1.4562920331954956), (0.0, 1.4564487934112549), (0.0, 1.4566706418991089), (0.0, 1.4568963050842285), (0.0, 1.4570941925048828), (0.0, 1.4572187662124634), (0.0, 1.4572875499725342), (0.0, 1.4574275016784668), (0.0, 1.4575157165527344), (0.0, 1.457517385482788), (0.0, 1.457588791847229), (0.0, 1.4576785564422607), (0.0, 1.4578557014465332), (0.0, 1.4578758478164673), (0.0, 1.4580471515655518), (0.0, 1.4580638408660889), (0.0, 1.4583401679992676), (0.0, 1.4586676359176636), (0.0, 1.4586821794509888), (0.0, 1.458894968032837), (0.0, 1.4590970277786255), (0.0, 1.4592229127883911), (0.0, 1.4594032764434814), (0.0, 1.4594146013259888), (0.0, 1.4595788717269897), (0.0, 1.4596190452575684), (0.0, 1.459631085395813), (0.0, 1.4597023725509644), (0.0, 1.45989191532135), (0.0, 1.460060954093933), (0.0, 1.4603767395019531), (0.0, 1.4604285955429077), (0.0, 1.4605289697647095), (0.0, 1.4605417251586914), (0.0, 1.4605656862258911), (0.0, 1.460667371749878), (0.0, 1.4607205390930176), (0.0, 1.4608291387557983), (0.0, 1.4608500003814697), (0.0, 1.4611725807189941), (0.0, 1.4612231254577637), (0.0, 1.4613099098205566), (0.0, 1.4613640308380127), (0.0, 1.46165931224823), (0.0, 1.4618721008300781), (0.0, 1.461903691291809), (0.0, 1.4620190858840942), (0.0, 1.4621014595031738), (0.0, 1.46212637424469), (0.0, 1.4621319770812988), (0.0, 1.4621999263763428), (0.0, 1.4622282981872559), (0.0, 1.4623178243637085), (0.0, 1.4623364210128784), (0.0, 1.4624615907669067), (0.0, 1.4624731540679932), (0.0, 1.4624903202056885), (0.0, 1.4625016450881958), (0.0, 1.4625020027160645), (0.0, 1.462762713432312), (0.0, 1.462847352027893), (0.0, 1.4630763530731201), (0.0, 1.463174819946289), (0.0, 1.4633413553237915), (0.0, 1.4634491205215454), (0.0, 1.4636616706848145), (0.0, 1.4638210535049438), (0.0, 1.4639649391174316), (0.0, 1.4641810655593872), (0.0, 1.4643217325210571), (0.0, 1.4646247625350952), (0.0, 1.4647151231765747), (0.0, 1.464724063873291), (0.0, 1.4647607803344727), (0.0, 1.4648044109344482), (0.0, 1.4650858640670776), (0.0, 1.4652786254882812), (0.0, 1.4654821157455444), (0.0, 1.4656445980072021), (0.0, 1.465969204902649), (0.0, 1.4661402702331543), (0.0, 1.4669849872589111), (0.0, 1.4675637483596802), (0.0, 1.4680525064468384), (0.0, 1.4686623811721802), (0.0, 1.468706488609314), (0.0, 1.4688148498535156), (0.0, 1.4701381921768188), (0.0, 1.470589280128479), (0.0, 1.4714874029159546), (0.0, 1.4722074270248413), (0.0, 1.4722225666046143), (0.0, 1.4745436906814575), (0.0, 1.4864816665649414), (0.0, 1.4986329078674316), (0.0, 1.5063982009887695), (0.0, 1.5099581480026245), (0.0, 1.5103808641433716), (0.0, 1.5104396343231201), (0.0, 1.510772705078125), (0.0, 1.5115355253219604), (0.0, 1.5127053260803223), (0.0, 1.5127668380737305), (0.0, 1.5140652656555176), (0.0, 1.514377236366272), (0.0, 1.5144305229187012), (0.0, 1.5145827531814575), (0.0, 1.5147898197174072), (0.0, 1.5153101682662964), (0.0, 1.515631079673767), (0.0, 1.5156999826431274), (0.0, 1.5162400007247925), (0.0, 1.5163369178771973), (0.0, 1.5166287422180176), (0.0, 1.5166866779327393), (0.0, 1.5167032480239868), (0.0, 1.51681649684906), (0.0, 1.5168964862823486), (0.0, 1.5170319080352783), (0.0, 1.517832636833191), (0.0, 1.5180139541625977), (0.0, 1.5181125402450562), (0.0, 1.518180012702942), (0.0, 1.5182491540908813), (0.0, 1.5184309482574463), (0.0, 1.518499732017517), (0.0, 1.5186771154403687), (0.0, 1.5187212228775024), (0.0, 1.5187265872955322), (0.0, 1.5189881324768066), (0.0, 1.5190160274505615), (0.0, 1.5191515684127808), (0.0, 1.5191816091537476), (0.0, 1.5197649002075195), (0.0, 1.5198686122894287), (0.0, 1.5200984477996826), (0.0, 1.5201725959777832), (0.0, 1.5202444791793823), (0.0, 1.5202845335006714), (0.0, 1.5203156471252441), (0.0, 1.5203512907028198), (0.0, 1.5203673839569092), (0.0, 1.5204534530639648), (0.0, 1.520904541015625), (0.0, 1.5209957361221313), (0.0, 1.5211316347122192), (0.0, 1.521506428718567), (0.0, 1.5218026638031006), (0.0, 1.5218240022659302), (0.0, 1.5218391418457031), (0.0, 1.5218755006790161), (0.0, 1.5222389698028564), (0.0, 1.5223444700241089), (0.0, 1.522452712059021), (0.0, 1.5227333307266235), (0.0, 1.522857904434204), (0.0, 1.5228599309921265), (0.0, 1.5229071378707886), (0.0, 1.52290940284729), (0.0, 1.5231417417526245), (0.0, 1.5231670141220093), (0.0, 1.5231893062591553), (0.0, 1.523203730583191), (0.0, 1.5234380960464478), (0.0, 1.523470163345337), (0.0, 1.5235389471054077), (0.0, 1.5235779285430908), (0.0, 1.523642659187317), (0.0, 1.5237547159194946), (0.0, 1.523776650428772), (0.0, 1.523938775062561), (0.0, 1.5239604711532593), (0.0, 1.5240291357040405), (0.0, 1.5240700244903564), (0.0, 1.5243273973464966), (0.0, 1.524423360824585), (0.0, 1.524510145187378), (0.0, 1.5246964693069458), (0.0, 1.5247846841812134), (0.0, 1.5249569416046143), (0.0, 1.5250033140182495), (0.0, 1.5250918865203857), (0.0, 1.52516770362854), (0.0, 1.5252113342285156), (0.0, 1.5252195596694946), (0.0, 1.5255261659622192), (0.0, 1.5255475044250488), (0.0, 1.5255541801452637), (0.0, 1.525709867477417), (0.0, 1.5257234573364258), (0.0, 1.5257251262664795), (0.0, 1.525742530822754), (0.0, 1.525793433189392), (0.0, 1.526159405708313), (0.0, 1.526174783706665), (0.0, 1.5262051820755005), (0.0, 1.5262943506240845), (0.0, 1.5264045000076294), (0.0, 1.5264896154403687), (0.0, 1.5265244245529175), (0.0, 1.5265835523605347), (0.0, 1.5269442796707153), (0.0, 1.5269720554351807), (0.0, 1.5270397663116455), (0.0, 1.5272728204727173), (0.0, 1.527303695678711), (0.0, 1.527777910232544), (0.0, 1.528096079826355), (0.0, 1.5281074047088623), (0.0, 1.5285383462905884), (0.0, 1.5287284851074219), (0.0, 1.5295772552490234), (0.0, 1.5302860736846924), (0.0, 1.5303022861480713), (0.0, 1.5305707454681396), (0.0, 1.530713438987732), (0.0, 1.5308550596237183), (0.0, 1.530892014503479), (0.0, 1.53098726272583), (0.0, 1.5314536094665527), (0.0, 1.533970832824707), (0.0, 1.5455421209335327), (0.0, 1.5631356239318848)], [(8.569643020629883, 8.586545944213867), (8.525721549987793, 8.683897972106934), (7.043524265289307, 7.061774730682373), (6.884650707244873, 6.9417266845703125), (6.861459255218506, 7.087762355804443), (6.691309928894043, 6.715254783630371), (6.66021728515625, 7.41907262802124), (6.58263635635376, 6.992642879486084), (6.501720428466797, 6.9513397216796875), (6.4973602294921875, 7.0002312660217285), (6.221410274505615, 7.550071716308594), (6.207073211669922, 6.640653133392334), (6.043919563293457, 6.89752721786499), (6.039967060089111, 7.353062629699707), (5.976604461669922, 6.983604431152344), (5.879404067993164, 7.8577189445495605), (5.821273326873779, 6.289304256439209), (5.7950639724731445, 6.433549880981445), (5.724353313446045, 5.900370121002197), (5.651491641998291, 8.507406234741211), (5.640134811401367, 5.698147773742676), (5.592681884765625, 6.327785491943359), (5.52924919128418, 7.602901935577393), (5.513336181640625, 7.192133903503418), (5.341623306274414, 8.687567710876465), (4.848600387573242, 7.668285846710205), (4.50022029876709, 7.613797664642334), (4.028902053833008, 4.397696018218994), (3.9985010623931885, 4.69697380065918), (3.9522511959075928, 3.987715244293213), (3.9474833011627197, 4.079017639160156), (3.9195444583892822, 4.21134090423584), (3.714477777481079, 8.609583854675293)], [(9.555503845214844, 10.144298553466797), (9.163249969482422, 9.257080078125), (9.119277000427246, 10.556737899780273), (8.915872573852539, 9.001150131225586), (8.107090950012207, 8.171464920043945), (7.89080286026001, 8.18759536743164), (7.197094440460205, 7.317607879638672)]]
[array([[0.        , 1.31254411],
       [0.        , 1.3193022 ],
       [0.        , 1.32023764],
       [0.        , 1.32190001],
       [0.        , 1.32271397],
       [0.        , 1.3231076 ],
       [0.        , 1.32342637],
       [0.        , 1.32569325],
       [0.        , 1.3258332 ],
       [0.        , 1.3266685 ],
       [0.        , 1.32675076],
       [0.        , 1.32692611],
       [0.        , 1.32726002],
       [0.        , 1.32735682],
       [0.        , 1.32747126],
       [0.        , 1.32786214],
       [0.        , 1.32809901],
       [0.        , 1.32839382],
       [0.        , 1.32845628],
       [0.        , 1.32847607],
       [0.        , 1.32891202],
       [0.        , 1.32907557],
       [0.        , 1.32910526],
       [0.        , 1.32930732],
       [0.        , 1.32931113],
       [0.        , 1.32942295],
       [0.        , 1.32974982],
       [0.        , 1.32975614],
       [0.        , 1.32990372],
       [0.        , 1.3299607 ],
       [0.        , 1.3300674 ],
       [0.        , 1.33019102],
       [0.        , 1.33019447],
       [0.        , 1.33026719],
       [0.        , 1.33038032],
       [0.        , 1.33060658],
       [0.        , 1.330634  ],
       [0.        , 1.33064377],
       [0.        , 1.33065474],
       [0.        , 1.33086836],
       [0.        , 1.33104098],
       [0.        , 1.33106351],
       [0.        , 1.33114052],
       [0.        , 1.33116841],
       [0.        , 1.33142245],
       [0.        , 1.33154142],
       [0.        , 1.331774  ],
       [0.        , 1.33198225],
       [0.        , 1.33200634],
       [0.        , 1.3320477 ],
       [0.        , 1.33206916],
       [0.        , 1.33224499],
       [0.        , 1.33229053],
       [0.        , 1.3323226 ],
       [0.        , 1.33233094],
       [0.        , 1.33235085],
       [0.        , 1.33250391],
       [0.        , 1.33252943],
       [0.        , 1.33268166],
       [0.        , 1.33269024],
       [0.        , 1.33273578],
       [0.        , 1.33280265],
       [0.        , 1.33305812],
       [0.        , 1.33312666],
       [0.        , 1.33320987],
       [0.        , 1.3332783 ],
       [0.        , 1.33328354],
       [0.        , 1.33348835],
       [0.        , 1.33351398],
       [0.        , 1.3335923 ],
       [0.        , 1.33364594],
       [0.        , 1.33402789],
       [0.        , 1.33403134],
       [0.        , 1.33411396],
       [0.        , 1.33415639],
       [0.        , 1.33416283],
       [0.        , 1.33421433],
       [0.        , 1.33424902],
       [0.        , 1.33439553],
       [0.        , 1.33461165],
       [0.        , 1.33464515],
       [0.        , 1.33466065],
       [0.        , 1.3348273 ],
       [0.        , 1.33513033],
       [0.        , 1.3351624 ],
       [0.        , 1.33546853],
       [0.        , 1.33579862],
       [0.        , 1.3358146 ],
       [0.        , 1.33611977],
       [0.        , 1.33614063],
       [0.        , 1.33616936],
       [0.        , 1.33640218],
       [0.        , 1.33643091],
       [0.        , 1.33657086],
       [0.        , 1.336743  ],
       [0.        , 1.33678496],
       [0.        , 1.3368665 ],
       [0.        , 1.33734298],
       [0.        , 1.33750641],
       [0.        , 1.33755517],
       [0.        , 1.33757901],
       [0.        , 1.33777487],
       [0.        , 1.33788407],
       [0.        , 1.3379159 ],
       [0.        , 1.33792067],
       [0.        , 1.33848822],
       [0.        , 1.3388617 ],
       [0.        , 1.33897579],
       [0.        , 1.33957648],
       [0.        , 1.33992863],
       [0.        , 1.34008527],
       [0.        , 1.34013355],
       [0.        , 1.340675  ],
       [0.        , 1.34141409],
       [0.        , 1.34152222],
       [0.        , 1.34193313],
       [0.        , 1.34206247],
       [0.        , 1.34216869],
       [0.        , 1.3427223 ],
       [0.        , 1.34287643],
       [0.        , 1.34289932],
       [0.        , 1.34294736],
       [0.        , 1.34539676],
       [0.        , 1.34560204],
       [0.        , 1.36801469],
       [0.        , 1.44254887],
       [0.        , 1.44879532],
       [0.        , 1.44993806],
       [0.        , 1.45039058],
       [0.        , 1.45066595],
       [0.        , 1.45118546],
       [0.        , 1.45149863],
       [0.        , 1.45240903],
       [0.        , 1.45240986],
       [0.        , 1.45241773],
       [0.        , 1.45244932],
       [0.        , 1.45248938],
       [0.        , 1.45249796],
       [0.        , 1.45284939],
       [0.        , 1.45305073],
       [0.        , 1.45337546],
       [0.        , 1.45372093],
       [0.        , 1.45387089],
       [0.        , 1.45432568],
       [0.        , 1.45458221],
       [0.        , 1.45480096],
       [0.        , 1.45487547],
       [0.        , 1.4550283 ],
       [0.        , 1.45511687],
       [0.        , 1.45535648],
       [0.        , 1.45560026],
       [0.        , 1.45573294],
       [0.        , 1.45582342],
       [0.        , 1.455845  ],
       [0.        , 1.45588231],
       [0.        , 1.4561336 ],
       [0.        , 1.45615101],
       [0.        , 1.45629203],
       [0.        , 1.45644879],
       [0.        , 1.45667064],
       [0.        , 1.45689631],
       [0.        , 1.45709419],
       [0.        , 1.45721877],
       [0.        , 1.45728755],
       [0.        , 1.4574275 ],
       [0.        , 1.45751572],
       [0.        , 1.45751739],
       [0.        , 1.45758879],
       [0.        , 1.45767856],
       [0.        , 1.4578557 ],
       [0.        , 1.45787585],
       [0.        , 1.45804715],
       [0.        , 1.45806384],
       [0.        , 1.45834017],
       [0.        , 1.45866764],
       [0.        , 1.45868218],
       [0.        , 1.45889497],
       [0.        , 1.45909703],
       [0.        , 1.45922291],
       [0.        , 1.45940328],
       [0.        , 1.4594146 ],
       [0.        , 1.45957887],
       [0.        , 1.45961905],
       [0.        , 1.45963109],
       [0.        , 1.45970237],
       [0.        , 1.45989192],
       [0.        , 1.46006095],
       [0.        , 1.46037674],
       [0.        , 1.4604286 ],
       [0.        , 1.46052897],
       [0.        , 1.46054173],
       [0.        , 1.46056569],
       [0.        , 1.46066737],
       [0.        , 1.46072054],
       [0.        , 1.46082914],
       [0.        , 1.46085   ],
       [0.        , 1.46117258],
       [0.        , 1.46122313],
       [0.        , 1.46130991],
       [0.        , 1.46136403],
       [0.        , 1.46165931],
       [0.        , 1.4618721 ],
       [0.        , 1.46190369],
       [0.        , 1.46201909],
       [0.        , 1.46210146],
       [0.        , 1.46212637],
       [0.        , 1.46213198],
       [0.        , 1.46219993],
       [0.        , 1.4622283 ],
       [0.        , 1.46231782],
       [0.        , 1.46233642],
       [0.        , 1.46246159],
       [0.        , 1.46247315],
       [0.        , 1.46249032],
       [0.        , 1.46250165],
       [0.        , 1.462502  ],
       [0.        , 1.46276271],
       [0.        , 1.46284735],
       [0.        , 1.46307635],
       [0.        , 1.46317482],
       [0.        , 1.46334136],
       [0.        , 1.46344912],
       [0.        , 1.46366167],
       [0.        , 1.46382105],
       [0.        , 1.46396494],
       [0.        , 1.46418107],
       [0.        , 1.46432173],
       [0.        , 1.46462476],
       [0.        , 1.46471512],
       [0.        , 1.46472406],
       [0.        , 1.46476078],
       [0.        , 1.46480441],
       [0.        , 1.46508586],
       [0.        , 1.46527863],
       [0.        , 1.46548212],
       [0.        , 1.4656446 ],
       [0.        , 1.4659692 ],
       [0.        , 1.46614027],
       [0.        , 1.46698499],
       [0.        , 1.46756375],
       [0.        , 1.46805251],
       [0.        , 1.46866238],
       [0.        , 1.46870649],
       [0.        , 1.46881485],
       [0.        , 1.47013819],
       [0.        , 1.47058928],
       [0.        , 1.4714874 ],
       [0.        , 1.47220743],
       [0.        , 1.47222257],
       [0.        , 1.47454369],
       [0.        , 1.48648167],
       [0.        , 1.49863291],
       [0.        , 1.5063982 ],
       [0.        , 1.50995815],
       [0.        , 1.51038086],
       [0.        , 1.51043963],
       [0.        , 1.51077271],
       [0.        , 1.51153553],
       [0.        , 1.51270533],
       [0.        , 1.51276684],
       [0.        , 1.51406527],
       [0.        , 1.51437724],
       [0.        , 1.51443052],
       [0.        , 1.51458275],
       [0.        , 1.51478982],
       [0.        , 1.51531017],
       [0.        , 1.51563108],
       [0.        , 1.51569998],
       [0.        , 1.51624   ],
       [0.        , 1.51633692],
       [0.        , 1.51662874],
       [0.        , 1.51668668],
       [0.        , 1.51670325],
       [0.        , 1.5168165 ],
       [0.        , 1.51689649],
       [0.        , 1.51703191],
       [0.        , 1.51783264],
       [0.        , 1.51801395],
       [0.        , 1.51811254],
       [0.        , 1.51818001],
       [0.        , 1.51824915],
       [0.        , 1.51843095],
       [0.        , 1.51849973],
       [0.        , 1.51867712],
       [0.        , 1.51872122],
       [0.        , 1.51872659],
       [0.        , 1.51898813],
       [0.        , 1.51901603],
       [0.        , 1.51915157],
       [0.        , 1.51918161],
       [0.        , 1.5197649 ],
       [0.        , 1.51986861],
       [0.        , 1.52009845],
       [0.        , 1.5201726 ],
       [0.        , 1.52024448],
       [0.        , 1.52028453],
       [0.        , 1.52031565],
       [0.        , 1.52035129],
       [0.        , 1.52036738],
       [0.        , 1.52045345],
       [0.        , 1.52090454],
       [0.        , 1.52099574],
       [0.        , 1.52113163],
       [0.        , 1.52150643],
       [0.        , 1.52180266],
       [0.        , 1.521824  ],
       [0.        , 1.52183914],
       [0.        , 1.5218755 ],
       [0.        , 1.52223897],
       [0.        , 1.52234447],
       [0.        , 1.52245271],
       [0.        , 1.52273333],
       [0.        , 1.5228579 ],
       [0.        , 1.52285993],
       [0.        , 1.52290714],
       [0.        , 1.5229094 ],
       [0.        , 1.52314174],
       [0.        , 1.52316701],
       [0.        , 1.52318931],
       [0.        , 1.52320373],
       [0.        , 1.5234381 ],
       [0.        , 1.52347016],
       [0.        , 1.52353895],
       [0.        , 1.52357793],
       [0.        , 1.52364266],
       [0.        , 1.52375472],
       [0.        , 1.52377665],
       [0.        , 1.52393878],
       [0.        , 1.52396047],
       [0.        , 1.52402914],
       [0.        , 1.52407002],
       [0.        , 1.5243274 ],
       [0.        , 1.52442336],
       [0.        , 1.52451015],
       [0.        , 1.52469647],
       [0.        , 1.52478468],
       [0.        , 1.52495694],
       [0.        , 1.52500331],
       [0.        , 1.52509189],
       [0.        , 1.5251677 ],
       [0.        , 1.52521133],
       [0.        , 1.52521956],
       [0.        , 1.52552617],
       [0.        , 1.5255475 ],
       [0.        , 1.52555418],
       [0.        , 1.52570987],
       [0.        , 1.52572346],
       [0.        , 1.52572513],
       [0.        , 1.52574253],
       [0.        , 1.52579343],
       [0.        , 1.52615941],
       [0.        , 1.52617478],
       [0.        , 1.52620518],
       [0.        , 1.52629435],
       [0.        , 1.5264045 ],
       [0.        , 1.52648962],
       [0.        , 1.52652442],
       [0.        , 1.52658355],
       [0.        , 1.52694428],
       [0.        , 1.52697206],
       [0.        , 1.52703977],
       [0.        , 1.52727282],
       [0.        , 1.5273037 ],
       [0.        , 1.52777791],
       [0.        , 1.52809608],
       [0.        , 1.5281074 ],
       [0.        , 1.52853835],
       [0.        , 1.52872849],
       [0.        , 1.52957726],
       [0.        , 1.53028607],
       [0.        , 1.53030229],
       [0.        , 1.53057075],
       [0.        , 1.53071344],
       [0.        , 1.53085506],
       [0.        , 1.53089201],
       [0.        , 1.53098726],
       [0.        , 1.53145361],
       [0.        , 1.53397083],
       [0.        , 1.54554212],
       [0.        , 1.56313562]]), array([[8.56964302, 8.58654594],
       [8.52572155, 8.68389797],
       [7.04352427, 7.06177473],
       [6.88465071, 6.94172668],
       [6.86145926, 7.08776236],
       [6.69130993, 6.71525478],
       [6.66021729, 7.41907263],
       [6.58263636, 6.99264288],
       [6.50172043, 6.95133972],
       [6.49736023, 7.00023127],
       [6.22141027, 7.55007172],
       [6.20707321, 6.64065313],
       [6.04391956, 6.89752722],
       [6.03996706, 7.35306263],
       [5.97660446, 6.98360443],
       [5.87940407, 7.85771894],
       [5.82127333, 6.28930426],
       [5.79506397, 6.43354988],
       [5.72435331, 5.90037012],
       [5.65149164, 8.50740623],
       [5.64013481, 5.69814777],
       [5.59268188, 6.32778549],
       [5.52924919, 7.60290194],
       [5.51333618, 7.1921339 ],
       [5.34162331, 8.68756771],
       [4.84860039, 7.66828585],
       [4.5002203 , 7.61379766],
       [4.02890205, 4.39769602],
       [3.99850106, 4.6969738 ],
       [3.9522512 , 3.98771524],
       [3.9474833 , 4.07901764],
       [3.91954446, 4.2113409 ],
       [3.71447778, 8.60958385]]), array([[ 9.55550385, 10.14429855],
       [ 9.16324997,  9.25708008],
       [ 9.119277  , 10.5567379 ],
       [ 8.91587257,  9.00115013],
       [ 8.10709095,  8.17146492],
       [ 7.89080286,  8.18759537],
       [ 7.19709444,  7.31760788]])]2024-03-06 17:46:00.789923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6G0G ph vector generated, counter: 26
2024-03-06 17:46:05.060698: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:05.102879: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:06.241083: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3115383), (0., 1.3212886), (0., 1.3216665), (0., 1.3240352),
       (0., 1.32426  ), (0., 1.3250107), (0., 1.3259616), (0., 1.3266408),
       (0., 1.326686 ), (0., 1.3268985), (0., 1.3272791), (0., 1.3274422),
       (0., 1.3279779), (0., 1.3287845), (0., 1.32879  ), (0., 1.3290431),
       (0., 1.3290851), (0., 1.3291285), (0., 1.329134 ), (0., 1.3293866),
       (0., 1.329617 ), (0., 1.3296406), (0., 1.3298019), (0., 1.3298075),
       (0., 1.3301109), (0., 1.3303043), (0., 1.3303967), (0., 1.330469 ),
       (0., 1.3305267), (0., 1.3305657), (0., 1.3305687), (0., 1.3305901),
       (0., 1.3307234), (0., 1.3308259), (0., 1.3308295), (0., 1.3309699),
       (0., 1.3309915), (0., 1.3310211), (0., 1.3310703), (0., 1.3312255),
       (0., 1.3312591), (0., 1.3314857), (0., 1.3315388), (0., 1.3315569),
       (0., 1.3317657), (0., 1.3320141), (0., 1.3321922), (0., 1.3323847),
       (0., 1.3324   ), (0., 1.3325418), (0., 1.3326917), (0., 1.3327615),
       (0., 1.3328215), (0., 1.3329076), (0., 1.3329307), (0., 1.3329928),
       (0., 1.3332313), (0., 1.3332548), (0., 1.3333018), (0., 1.3333621),
       (0., 1.3334569), (0., 1.3334796), (0., 1.3335453), (0., 1.3335629),
       (0., 1.3336315), (0., 1.3336785), (0., 1.3336889), (0., 1.3336982),
       (0., 1.3337005), (0., 1.3337569), (0., 1.3337576), (0., 1.3337673),
       (0., 1.333806 ), (0., 1.3341149), (0., 1.3343537), (0., 1.3343992),
       (0., 1.3344793), (0., 1.3345339), (0., 1.3346545), (0., 1.3346891),
       (0., 1.3347179), (0., 1.3348782), (0., 1.3349617), (0., 1.3349992),
       (0., 1.3350554), (0., 1.3350626), (0., 1.3350894), (0., 1.3351824),
       (0., 1.3353173), (0., 1.3353956), (0., 1.3354498), (0., 1.3354731),
       (0., 1.3356186), (0., 1.3356221), (0., 1.3357828), (0., 1.3358177),
       (0., 1.3359085), (0., 1.3359182), (0., 1.3361554), (0., 1.3362615),
       (0., 1.3363153), (0., 1.3364037), (0., 1.3364872), (0., 1.3365864),
       (0., 1.33663  ), (0., 1.3369187), (0., 1.3369454), (0., 1.3370514),
       (0., 1.3370588), (0., 1.3370876), (0., 1.3370881), (0., 1.3371204),
       (0., 1.3371472), (0., 1.3371965), (0., 1.3376032), (0., 1.3376402),
       (0., 1.3377265), (0., 1.3377547), (0., 1.3377578), (0., 1.3381476),
       (0., 1.3383386), (0., 1.3384957), (0., 1.3387419), (0., 1.3393943),
       (0., 1.3400121), (0., 1.3419156), (0., 1.4439719), (0., 1.4481608),
       (0., 1.4482892), (0., 1.4485911), (0., 1.4489534), (0., 1.450193 ),
       (0., 1.4511057), (0., 1.4511242), (0., 1.4515836), (0., 1.452306 ),
       (0., 1.4524713), (0., 1.4536117), (0., 1.453667 ), (0., 1.4536674),
       (0., 1.4537076), (0., 1.4537141), (0., 1.4543841), (0., 1.4551452),
       (0., 1.455262 ), (0., 1.455325 ), (0., 1.4553272), (0., 1.4554266),
       (0., 1.4554777), (0., 1.4555211), (0., 1.4555619), (0., 1.4555825),
       (0., 1.4556088), (0., 1.4557408), (0., 1.45575  ), (0., 1.4560055),
       (0., 1.456455 ), (0., 1.4564766), (0., 1.4565234), (0., 1.456725 ),
       (0., 1.4569304), (0., 1.4569757), (0., 1.4570049), (0., 1.4570184),
       (0., 1.4570473), (0., 1.4571009), (0., 1.4571217), (0., 1.4571623),
       (0., 1.4573504), (0., 1.4574645), (0., 1.4575422), (0., 1.4575483),
       (0., 1.4576013), (0., 1.4577385), (0., 1.4577655), (0., 1.4577733),
       (0., 1.4578143), (0., 1.4578216), (0., 1.4578229), (0., 1.4579364),
       (0., 1.4580299), (0., 1.4581559), (0., 1.4581641), (0., 1.4581906),
       (0., 1.4582785), (0., 1.4583715), (0., 1.458383 ), (0., 1.4584732),
       (0., 1.4585146), (0., 1.4585316), (0., 1.4586165), (0., 1.4587126),
       (0., 1.4588697), (0., 1.4589771), (0., 1.4589816), (0., 1.4591128),
       (0., 1.4593337), (0., 1.4594339), (0., 1.4595298), (0., 1.4597322),
       (0., 1.4598246), (0., 1.4600438), (0., 1.4600639), (0., 1.4601101),
       (0., 1.4602047), (0., 1.4602067), (0., 1.4604216), (0., 1.4604551),
       (0., 1.4605197), (0., 1.460659 ), (0., 1.460673 ), (0., 1.4609541),
       (0., 1.4609616), (0., 1.4609654), (0., 1.4610372), (0., 1.4611762),
       (0., 1.4613378), (0., 1.461412 ), (0., 1.4614452), (0., 1.4615284),
       (0., 1.4616015), (0., 1.4616588), (0., 1.4616756), (0., 1.4616932),
       (0., 1.4617864), (0., 1.4620001), (0., 1.46204  ), (0., 1.4621549),
       (0., 1.4623816), (0., 1.4624779), (0., 1.4626118), (0., 1.4628388),
       (0., 1.4628483), (0., 1.4633052), (0., 1.463321 ), (0., 1.4633603),
       (0., 1.4633908), (0., 1.4635079), (0., 1.4635273), (0., 1.4636258),
       (0., 1.4640511), (0., 1.4641292), (0., 1.4642416), (0., 1.464516 ),
       (0., 1.4653522), (0., 1.4656731), (0., 1.4662131), (0., 1.4669164),
       (0., 1.4681065), (0., 1.4707565), (0., 1.471164 ), (0., 1.474425 ),
       (0., 1.4745085), (0., 1.5137068), (0., 1.5144733), (0., 1.5151509),
       (0., 1.5157893), (0., 1.5158415), (0., 1.5160656), (0., 1.5161198),
       (0., 1.5163004), (0., 1.5163952), (0., 1.5166714), (0., 1.5166734),
       (0., 1.5167348), (0., 1.5167692), (0., 1.5168728), (0., 1.5179524),
       (0., 1.5179787), (0., 1.5184907), (0., 1.518731 ), (0., 1.5191095),
       (0., 1.5191733), (0., 1.5192797), (0., 1.5194685), (0., 1.5195991),
       (0., 1.519648 ), (0., 1.5197654), (0., 1.5199617), (0., 1.5199857),
       (0., 1.5199878), (0., 1.5204383), (0., 1.5205463), (0., 1.5206025),
       (0., 1.5206276), (0., 1.520842 ), (0., 1.5208465), (0., 1.520879 ),
       (0., 1.5209044), (0., 1.521211 ), (0., 1.5212399), (0., 1.5212718),
       (0., 1.5212913), (0., 1.5213041), (0., 1.5213155), (0., 1.5213428),
       (0., 1.5214381), (0., 1.521455 ), (0., 1.5215465), (0., 1.5219002),
       (0., 1.5219356), (0., 1.5219712), (0., 1.5219727), (0., 1.5219975),
       (0., 1.5220141), (0., 1.5220268), (0., 1.5220637), (0., 1.522138 ),
       (0., 1.522214 ), (0., 1.5224146), (0., 1.5225474), (0., 1.5226986),
       (0., 1.5227417), (0., 1.5227606), (0., 1.5229776), (0., 1.5229958),
       (0., 1.523126 ), (0., 1.5231401), (0., 1.5231493), (0., 1.5231721),
       (0., 1.5235091), (0., 1.5236012), (0., 1.5237359), (0., 1.52377  ),
       (0., 1.5237745), (0., 1.5238888), (0., 1.5239673), (0., 1.5240022),
       (0., 1.5240068), (0., 1.524088 ), (0., 1.5241442), (0., 1.5242077),
       (0., 1.5243425), (0., 1.5243928), (0., 1.5244101), (0., 1.5245212),
       (0., 1.5246172), (0., 1.5246413), (0., 1.5247039), (0., 1.5247452),
       (0., 1.5247455), (0., 1.52485  ), (0., 1.5249143), (0., 1.5253731),
       (0., 1.5254513), (0., 1.5255624), (0., 1.5255917), (0., 1.525785 ),
       (0., 1.5258557), (0., 1.5260131), (0., 1.5260168), (0., 1.5261561),
       (0., 1.5262119), (0., 1.5263956), (0., 1.5264776), (0., 1.5266252),
       (0., 1.5266405), (0., 1.5267115), (0., 1.5267906), (0., 1.5269095),
       (0., 1.5269126), (0., 1.5270476), (0., 1.5270483), (0., 1.5271522),
       (0., 1.5271968), (0., 1.5273604), (0., 1.5274318), (0., 1.5274415),
       (0., 1.5275089), (0., 1.5275396), (0., 1.5275525), (0., 1.5282391),
       (0., 1.5287358), (0., 1.5291235), (0., 1.529433 ), (0., 1.5297433),
       (0., 1.5298392), (0., 1.5305241), (0., 1.5310388), (0., 1.5321776)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.488397 , 8.67078  ), (8.383085 , 8.396487 ),
       (7.1207066, 7.573849 ), (7.0060635, 7.013388 ),
       (6.920721 , 7.032719 ), (6.8023257, 7.062948 ),
       (6.687697 , 7.028107 ), (6.670407 , 6.688924 ),
       (6.649856 , 7.383786 ), (6.5345488, 7.030677 ),
       (6.497192 , 6.9858065), (6.2865953, 7.4760675),
       (5.9661493, 7.00449  ), (5.962411 , 7.348054 ),
       (5.9600477, 6.3625665), (5.8185806, 7.763227 ),
       (5.779322 , 6.4669123), (5.7781672, 5.8645544),
       (5.777405 , 6.0231466), (5.761853 , 6.438195 ),
       (5.55539  , 7.180072 ), (5.423798 , 8.445027 ),
       (5.322458 , 7.620278 ), (5.2422595, 5.30553  ),
       (5.1819983, 8.66627  ), (4.7884445, 7.680219 ),
       (4.538039 , 7.60881  ), (3.9920928, 4.4279346),
       (3.9750266, 4.3093324), (3.9683938, 3.969749 ),
       (3.9665303, 4.6944046), (3.9186976, 4.0161495),
       (3.7279274, 8.667064 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.456401 , 10.085945 ), (9.144139 ,  9.214922 ),
       (9.126095 , 10.52283  ), (8.852588 ,  8.995812 ),
       (8.12194  ,  8.142331 ), (7.938454 ,  8.085927 ),
       (7.2497683,  7.3149776)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3115383386611938), (0.0, 1.3212885856628418), (0.0, 1.3216664791107178), (0.0, 1.3240351676940918), (0.0, 1.3242599964141846), (0.0, 1.3250106573104858), (0.0, 1.3259615898132324), (0.0, 1.3266408443450928), (0.0, 1.3266860246658325), (0.0, 1.326898455619812), (0.0, 1.3272790908813477), (0.0, 1.3274421691894531), (0.0, 1.3279778957366943), (0.0, 1.328784465789795), (0.0, 1.3287899494171143), (0.0, 1.3290431499481201), (0.0, 1.329085111618042), (0.0, 1.3291285037994385), (0.0, 1.3291339874267578), (0.0, 1.329386591911316), (0.0, 1.3296170234680176), (0.0, 1.3296406269073486), (0.0, 1.3298019170761108), (0.0, 1.3298075199127197), (0.0, 1.3301109075546265), (0.0, 1.3303042650222778), (0.0, 1.3303966522216797), (0.0, 1.330469012260437), (0.0, 1.3305267095565796), (0.0, 1.3305656909942627), (0.0, 1.3305686712265015), (0.0, 1.3305901288986206), (0.0, 1.3307234048843384), (0.0, 1.330825924873352), (0.0, 1.3308295011520386), (0.0, 1.3309699296951294), (0.0, 1.330991506576538), (0.0, 1.3310210704803467), (0.0, 1.3310703039169312), (0.0, 1.3312255144119263), (0.0, 1.3312591314315796), (0.0, 1.3314857482910156), (0.0, 1.3315387964248657), (0.0, 1.3315569162368774), (0.0, 1.3317656517028809), (0.0, 1.3320140838623047), (0.0, 1.3321921825408936), (0.0, 1.332384705543518), (0.0, 1.3323999643325806), (0.0, 1.332541823387146), (0.0, 1.3326916694641113), (0.0, 1.332761526107788), (0.0, 1.3328214883804321), (0.0, 1.3329075574874878), (0.0, 1.3329306840896606), (0.0, 1.3329927921295166), (0.0, 1.3332313299179077), (0.0, 1.3332548141479492), (0.0, 1.3333017826080322), (0.0, 1.333362102508545), (0.0, 1.3334568738937378), (0.0, 1.333479642868042), (0.0, 1.3335453271865845), (0.0, 1.3335628509521484), (0.0, 1.3336315155029297), (0.0, 1.3336784839630127), (0.0, 1.3336888551712036), (0.0, 1.3336981534957886), (0.0, 1.3337005376815796), (0.0, 1.333756923675537), (0.0, 1.3337576389312744), (0.0, 1.333767294883728), (0.0, 1.333806037902832), (0.0, 1.334114909172058), (0.0, 1.3343536853790283), (0.0, 1.3343992233276367), (0.0, 1.3344793319702148), (0.0, 1.334533929824829), (0.0, 1.334654450416565), (0.0, 1.3346891403198242), (0.0, 1.334717869758606), (0.0, 1.3348782062530518), (0.0, 1.3349616527557373), (0.0, 1.3349992036819458), (0.0, 1.3350553512573242), (0.0, 1.3350626230239868), (0.0, 1.3350894451141357), (0.0, 1.3351824283599854), (0.0, 1.3353172540664673), (0.0, 1.3353955745697021), (0.0, 1.3354498147964478), (0.0, 1.3354730606079102), (0.0, 1.3356186151504517), (0.0, 1.3356220722198486), (0.0, 1.335782766342163), (0.0, 1.3358176946640015), (0.0, 1.3359085321426392), (0.0, 1.3359181880950928), (0.0, 1.3361554145812988), (0.0, 1.336261510848999), (0.0, 1.3363152742385864), (0.0, 1.336403727531433), (0.0, 1.3364871740341187), (0.0, 1.336586356163025), (0.0, 1.3366299867630005), (0.0, 1.3369187116622925), (0.0, 1.3369454145431519), (0.0, 1.3370513916015625), (0.0, 1.3370587825775146), (0.0, 1.337087631225586), (0.0, 1.3370881080627441), (0.0, 1.3371204137802124), (0.0, 1.3371472358703613), (0.0, 1.3371964693069458), (0.0, 1.337603211402893), (0.0, 1.3376401662826538), (0.0, 1.3377264738082886), (0.0, 1.337754726409912), (0.0, 1.3377578258514404), (0.0, 1.3381476402282715), (0.0, 1.3383386135101318), (0.0, 1.3384957313537598), (0.0, 1.3387418985366821), (0.0, 1.3393943309783936), (0.0, 1.3400120735168457), (0.0, 1.3419156074523926), (0.0, 1.443971872329712), (0.0, 1.4481607675552368), (0.0, 1.448289155960083), (0.0, 1.4485911130905151), (0.0, 1.44895339012146), (0.0, 1.4501930475234985), (0.0, 1.4511057138442993), (0.0, 1.4511241912841797), (0.0, 1.4515836238861084), (0.0, 1.4523060321807861), (0.0, 1.4524712562561035), (0.0, 1.4536117315292358), (0.0, 1.4536670446395874), (0.0, 1.453667402267456), (0.0, 1.4537075757980347), (0.0, 1.45371413230896), (0.0, 1.4543840885162354), (0.0, 1.455145239830017), (0.0, 1.4552619457244873), (0.0, 1.4553250074386597), (0.0, 1.4553271532058716), (0.0, 1.455426573753357), (0.0, 1.4554777145385742), (0.0, 1.4555211067199707), (0.0, 1.455561876296997), (0.0, 1.4555824995040894), (0.0, 1.45560884475708), (0.0, 1.4557408094406128), (0.0, 1.4557499885559082), (0.0, 1.4560054540634155), (0.0, 1.4564549922943115), (0.0, 1.4564765691757202), (0.0, 1.4565234184265137), (0.0, 1.456725001335144), (0.0, 1.45693039894104), (0.0, 1.4569756984710693), (0.0, 1.4570049047470093), (0.0, 1.4570183753967285), (0.0, 1.4570473432540894), (0.0, 1.4571008682250977), (0.0, 1.457121729850769), (0.0, 1.4571622610092163), (0.0, 1.4573503732681274), (0.0, 1.4574644565582275), (0.0, 1.4575421810150146), (0.0, 1.4575482606887817), (0.0, 1.4576013088226318), (0.0, 1.4577385187149048), (0.0, 1.4577654600143433), (0.0, 1.4577733278274536), (0.0, 1.457814335823059), (0.0, 1.4578216075897217), (0.0, 1.4578229188919067), (0.0, 1.457936406135559), (0.0, 1.458029866218567), (0.0, 1.458155870437622), (0.0, 1.458164095878601), (0.0, 1.4581905603408813), (0.0, 1.4582785367965698), (0.0, 1.4583715200424194), (0.0, 1.4583829641342163), (0.0, 1.4584732055664062), (0.0, 1.4585145711898804), (0.0, 1.4585316181182861), (0.0, 1.4586164951324463), (0.0, 1.4587125778198242), (0.0, 1.4588696956634521), (0.0, 1.4589771032333374), (0.0, 1.4589816331863403), (0.0, 1.4591127634048462), (0.0, 1.4593336582183838), (0.0, 1.459433913230896), (0.0, 1.4595297574996948), (0.0, 1.459732174873352), (0.0, 1.459824562072754), (0.0, 1.4600437879562378), (0.0, 1.4600639343261719), (0.0, 1.460110068321228), (0.0, 1.4602047204971313), (0.0, 1.4602067470550537), (0.0, 1.4604215621948242), (0.0, 1.460455060005188), (0.0, 1.4605196714401245), (0.0, 1.4606590270996094), (0.0, 1.4606729745864868), (0.0, 1.4609540700912476), (0.0, 1.4609615802764893), (0.0, 1.4609653949737549), (0.0, 1.4610371589660645), (0.0, 1.4611761569976807), (0.0, 1.4613378047943115), (0.0, 1.461411952972412), (0.0, 1.4614452123641968), (0.0, 1.4615284204483032), (0.0, 1.4616014957427979), (0.0, 1.4616588354110718), (0.0, 1.4616756439208984), (0.0, 1.4616931676864624), (0.0, 1.4617863893508911), (0.0, 1.4620001316070557), (0.0, 1.4620399475097656), (0.0, 1.4621548652648926), (0.0, 1.4623816013336182), (0.0, 1.4624779224395752), (0.0, 1.4626117944717407), (0.0, 1.4628387689590454), (0.0, 1.4628483057022095), (0.0, 1.4633052349090576), (0.0, 1.4633209705352783), (0.0, 1.46336030960083), (0.0, 1.463390827178955), (0.0, 1.463507890701294), (0.0, 1.4635273218154907), (0.0, 1.4636257886886597), (0.0, 1.4640511274337769), (0.0, 1.4641292095184326), (0.0, 1.464241623878479), (0.0, 1.464516043663025), (0.0, 1.465352177619934), (0.0, 1.4656730890274048), (0.0, 1.4662131071090698), (0.0, 1.4669164419174194), (0.0, 1.4681065082550049), (0.0, 1.4707565307617188), (0.0, 1.4711639881134033), (0.0, 1.474424958229065), (0.0, 1.47450852394104), (0.0, 1.5137068033218384), (0.0, 1.51447331905365), (0.0, 1.5151509046554565), (0.0, 1.515789270401001), (0.0, 1.5158414840698242), (0.0, 1.5160655975341797), (0.0, 1.5161198377609253), (0.0, 1.5163004398345947), (0.0, 1.5163952112197876), (0.0, 1.5166714191436768), (0.0, 1.5166734457015991), (0.0, 1.5167348384857178), (0.0, 1.5167691707611084), (0.0, 1.516872763633728), (0.0, 1.5179524421691895), (0.0, 1.5179786682128906), (0.0, 1.5184906721115112), (0.0, 1.5187309980392456), (0.0, 1.5191094875335693), (0.0, 1.519173264503479), (0.0, 1.5192797183990479), (0.0, 1.5194685459136963), (0.0, 1.5195990800857544), (0.0, 1.5196479558944702), (0.0, 1.5197653770446777), (0.0, 1.5199617147445679), (0.0, 1.5199856758117676), (0.0, 1.5199878215789795), (0.0, 1.520438313484192), (0.0, 1.520546317100525), (0.0, 1.5206024646759033), (0.0, 1.5206276178359985), (0.0, 1.5208419561386108), (0.0, 1.5208464860916138), (0.0, 1.5208790302276611), (0.0, 1.5209044218063354), (0.0, 1.52121102809906), (0.0, 1.5212398767471313), (0.0, 1.521271824836731), (0.0, 1.5212912559509277), (0.0, 1.5213041305541992), (0.0, 1.5213154554367065), (0.0, 1.5213427543640137), (0.0, 1.5214381217956543), (0.0, 1.5214550495147705), (0.0, 1.521546483039856), (0.0, 1.5219001770019531), (0.0, 1.5219355821609497), (0.0, 1.5219712257385254), (0.0, 1.52197265625), (0.0, 1.5219974517822266), (0.0, 1.5220141410827637), (0.0, 1.522026777267456), (0.0, 1.5220637321472168), (0.0, 1.522137999534607), (0.0, 1.5222140550613403), (0.0, 1.5224145650863647), (0.0, 1.5225473642349243), (0.0, 1.5226986408233643), (0.0, 1.522741675376892), (0.0, 1.5227606296539307), (0.0, 1.522977590560913), (0.0, 1.5229958295822144), (0.0, 1.5231260061264038), (0.0, 1.5231400728225708), (0.0, 1.5231492519378662), (0.0, 1.52317214012146), (0.0, 1.52350914478302), (0.0, 1.5236011743545532), (0.0, 1.5237358808517456), (0.0, 1.5237699747085571), (0.0, 1.52377450466156), (0.0, 1.5238888263702393), (0.0, 1.5239672660827637), (0.0, 1.524002194404602), (0.0, 1.5240068435668945), (0.0, 1.5240880250930786), (0.0, 1.524144172668457), (0.0, 1.5242077112197876), (0.0, 1.5243425369262695), (0.0, 1.52439284324646), (0.0, 1.5244101285934448), (0.0, 1.5245212316513062), (0.0, 1.5246171951293945), (0.0, 1.5246412754058838), (0.0, 1.524703860282898), (0.0, 1.524745225906372), (0.0, 1.5247454643249512), (0.0, 1.5248500108718872), (0.0, 1.524914264678955), (0.0, 1.525373101234436), (0.0, 1.5254513025283813), (0.0, 1.5255624055862427), (0.0, 1.5255917310714722), (0.0, 1.525784969329834), (0.0, 1.5258556604385376), (0.0, 1.5260131359100342), (0.0, 1.5260168313980103), (0.0, 1.5261560678482056), (0.0, 1.5262118577957153), (0.0, 1.526395559310913), (0.0, 1.526477575302124), (0.0, 1.526625156402588), (0.0, 1.52664053440094), (0.0, 1.5267114639282227), (0.0, 1.5267906188964844), (0.0, 1.5269094705581665), (0.0, 1.5269125699996948), (0.0, 1.5270476341247559), (0.0, 1.5270483493804932), (0.0, 1.527152180671692), (0.0, 1.5271967649459839), (0.0, 1.527360439300537), (0.0, 1.527431845664978), (0.0, 1.5274415016174316), (0.0, 1.5275088548660278), (0.0, 1.527539610862732), (0.0, 1.5275524854660034), (0.0, 1.528239130973816), (0.0, 1.5287357568740845), (0.0, 1.5291235446929932), (0.0, 1.529433012008667), (0.0, 1.5297433137893677), (0.0, 1.5298391580581665), (0.0, 1.5305241346359253), (0.0, 1.531038761138916), (0.0, 1.5321775674819946)], [(8.488396644592285, 8.670780181884766), (8.383085250854492, 8.39648723602295), (7.120706558227539, 7.573849201202393), (7.006063461303711, 7.013388156890869), (6.920721054077148, 7.032719135284424), (6.80232572555542, 7.062948226928711), (6.687696933746338, 7.028107166290283), (6.670406818389893, 6.6889238357543945), (6.649856090545654, 7.383786201477051), (6.534548759460449, 7.03067684173584), (6.497191905975342, 6.985806465148926), (6.286595344543457, 7.476067543029785), (5.96614933013916, 7.004489898681641), (5.962410926818848, 7.348053932189941), (5.960047721862793, 6.3625664710998535), (5.818580627441406, 7.7632269859313965), (5.779322147369385, 6.466912269592285), (5.778167247772217, 5.864554405212402), (5.77740478515625, 6.023146629333496), (5.761853218078613, 6.43819522857666), (5.555389881134033, 7.180071830749512), (5.423798084259033, 8.445027351379395), (5.322457790374756, 7.6202778816223145), (5.242259502410889, 5.305530071258545), (5.181998252868652, 8.66627025604248), (4.788444519042969, 7.6802191734313965), (4.538039207458496, 7.608809947967529), (3.9920928478240967, 4.427934646606445), (3.9750266075134277, 4.309332370758057), (3.9683938026428223, 3.9697489738464355), (3.9665303230285645, 4.694404602050781), (3.9186975955963135, 4.016149520874023), (3.7279274463653564, 8.66706371307373)], [(9.456400871276855, 10.085945129394531), (9.144139289855957, 9.214921951293945), (9.126094818115234, 10.52283000946045), (8.852587699890137, 8.99581241607666), (8.121939659118652, 8.14233112335205), (7.9384541511535645, 8.08592700958252), (7.249768257141113, 7.314977645874023)]]
[array([[0.        , 1.31153834],
       [0.        , 1.32128859],
       [0.        , 1.32166648],
       [0.        , 1.32403517],
       [0.        , 1.32426   ],
       [0.        , 1.32501066],
       [0.        , 1.32596159],
       [0.        , 1.32664084],
       [0.        , 1.32668602],
       [0.        , 1.32689846],
       [0.        , 1.32727909],
       [0.        , 1.32744217],
       [0.        , 1.3279779 ],
       [0.        , 1.32878447],
       [0.        , 1.32878995],
       [0.        , 1.32904315],
       [0.        , 1.32908511],
       [0.        , 1.3291285 ],
       [0.        , 1.32913399],
       [0.        , 1.32938659],
       [0.        , 1.32961702],
       [0.        , 1.32964063],
       [0.        , 1.32980192],
       [0.        , 1.32980752],
       [0.        , 1.33011091],
       [0.        , 1.33030427],
       [0.        , 1.33039665],
       [0.        , 1.33046901],
       [0.        , 1.33052671],
       [0.        , 1.33056569],
       [0.        , 1.33056867],
       [0.        , 1.33059013],
       [0.        , 1.3307234 ],
       [0.        , 1.33082592],
       [0.        , 1.3308295 ],
       [0.        , 1.33096993],
       [0.        , 1.33099151],
       [0.        , 1.33102107],
       [0.        , 1.3310703 ],
       [0.        , 1.33122551],
       [0.        , 1.33125913],
       [0.        , 1.33148575],
       [0.        , 1.3315388 ],
       [0.        , 1.33155692],
       [0.        , 1.33176565],
       [0.        , 1.33201408],
       [0.        , 1.33219218],
       [0.        , 1.33238471],
       [0.        , 1.33239996],
       [0.        , 1.33254182],
       [0.        , 1.33269167],
       [0.        , 1.33276153],
       [0.        , 1.33282149],
       [0.        , 1.33290756],
       [0.        , 1.33293068],
       [0.        , 1.33299279],
       [0.        , 1.33323133],
       [0.        , 1.33325481],
       [0.        , 1.33330178],
       [0.        , 1.3333621 ],
       [0.        , 1.33345687],
       [0.        , 1.33347964],
       [0.        , 1.33354533],
       [0.        , 1.33356285],
       [0.        , 1.33363152],
       [0.        , 1.33367848],
       [0.        , 1.33368886],
       [0.        , 1.33369815],
       [0.        , 1.33370054],
       [0.        , 1.33375692],
       [0.        , 1.33375764],
       [0.        , 1.33376729],
       [0.        , 1.33380604],
       [0.        , 1.33411491],
       [0.        , 1.33435369],
       [0.        , 1.33439922],
       [0.        , 1.33447933],
       [0.        , 1.33453393],
       [0.        , 1.33465445],
       [0.        , 1.33468914],
       [0.        , 1.33471787],
       [0.        , 1.33487821],
       [0.        , 1.33496165],
       [0.        , 1.3349992 ],
       [0.        , 1.33505535],
       [0.        , 1.33506262],
       [0.        , 1.33508945],
       [0.        , 1.33518243],
       [0.        , 1.33531725],
       [0.        , 1.33539557],
       [0.        , 1.33544981],
       [0.        , 1.33547306],
       [0.        , 1.33561862],
       [0.        , 1.33562207],
       [0.        , 1.33578277],
       [0.        , 1.33581769],
       [0.        , 1.33590853],
       [0.        , 1.33591819],
       [0.        , 1.33615541],
       [0.        , 1.33626151],
       [0.        , 1.33631527],
       [0.        , 1.33640373],
       [0.        , 1.33648717],
       [0.        , 1.33658636],
       [0.        , 1.33662999],
       [0.        , 1.33691871],
       [0.        , 1.33694541],
       [0.        , 1.33705139],
       [0.        , 1.33705878],
       [0.        , 1.33708763],
       [0.        , 1.33708811],
       [0.        , 1.33712041],
       [0.        , 1.33714724],
       [0.        , 1.33719647],
       [0.        , 1.33760321],
       [0.        , 1.33764017],
       [0.        , 1.33772647],
       [0.        , 1.33775473],
       [0.        , 1.33775783],
       [0.        , 1.33814764],
       [0.        , 1.33833861],
       [0.        , 1.33849573],
       [0.        , 1.3387419 ],
       [0.        , 1.33939433],
       [0.        , 1.34001207],
       [0.        , 1.34191561],
       [0.        , 1.44397187],
       [0.        , 1.44816077],
       [0.        , 1.44828916],
       [0.        , 1.44859111],
       [0.        , 1.44895339],
       [0.        , 1.45019305],
       [0.        , 1.45110571],
       [0.        , 1.45112419],
       [0.        , 1.45158362],
       [0.        , 1.45230603],
       [0.        , 1.45247126],
       [0.        , 1.45361173],
       [0.        , 1.45366704],
       [0.        , 1.4536674 ],
       [0.        , 1.45370758],
       [0.        , 1.45371413],
       [0.        , 1.45438409],
       [0.        , 1.45514524],
       [0.        , 1.45526195],
       [0.        , 1.45532501],
       [0.        , 1.45532715],
       [0.        , 1.45542657],
       [0.        , 1.45547771],
       [0.        , 1.45552111],
       [0.        , 1.45556188],
       [0.        , 1.4555825 ],
       [0.        , 1.45560884],
       [0.        , 1.45574081],
       [0.        , 1.45574999],
       [0.        , 1.45600545],
       [0.        , 1.45645499],
       [0.        , 1.45647657],
       [0.        , 1.45652342],
       [0.        , 1.456725  ],
       [0.        , 1.4569304 ],
       [0.        , 1.4569757 ],
       [0.        , 1.4570049 ],
       [0.        , 1.45701838],
       [0.        , 1.45704734],
       [0.        , 1.45710087],
       [0.        , 1.45712173],
       [0.        , 1.45716226],
       [0.        , 1.45735037],
       [0.        , 1.45746446],
       [0.        , 1.45754218],
       [0.        , 1.45754826],
       [0.        , 1.45760131],
       [0.        , 1.45773852],
       [0.        , 1.45776546],
       [0.        , 1.45777333],
       [0.        , 1.45781434],
       [0.        , 1.45782161],
       [0.        , 1.45782292],
       [0.        , 1.45793641],
       [0.        , 1.45802987],
       [0.        , 1.45815587],
       [0.        , 1.4581641 ],
       [0.        , 1.45819056],
       [0.        , 1.45827854],
       [0.        , 1.45837152],
       [0.        , 1.45838296],
       [0.        , 1.45847321],
       [0.        , 1.45851457],
       [0.        , 1.45853162],
       [0.        , 1.4586165 ],
       [0.        , 1.45871258],
       [0.        , 1.4588697 ],
       [0.        , 1.4589771 ],
       [0.        , 1.45898163],
       [0.        , 1.45911276],
       [0.        , 1.45933366],
       [0.        , 1.45943391],
       [0.        , 1.45952976],
       [0.        , 1.45973217],
       [0.        , 1.45982456],
       [0.        , 1.46004379],
       [0.        , 1.46006393],
       [0.        , 1.46011007],
       [0.        , 1.46020472],
       [0.        , 1.46020675],
       [0.        , 1.46042156],
       [0.        , 1.46045506],
       [0.        , 1.46051967],
       [0.        , 1.46065903],
       [0.        , 1.46067297],
       [0.        , 1.46095407],
       [0.        , 1.46096158],
       [0.        , 1.46096539],
       [0.        , 1.46103716],
       [0.        , 1.46117616],
       [0.        , 1.4613378 ],
       [0.        , 1.46141195],
       [0.        , 1.46144521],
       [0.        , 1.46152842],
       [0.        , 1.4616015 ],
       [0.        , 1.46165884],
       [0.        , 1.46167564],
       [0.        , 1.46169317],
       [0.        , 1.46178639],
       [0.        , 1.46200013],
       [0.        , 1.46203995],
       [0.        , 1.46215487],
       [0.        , 1.4623816 ],
       [0.        , 1.46247792],
       [0.        , 1.46261179],
       [0.        , 1.46283877],
       [0.        , 1.46284831],
       [0.        , 1.46330523],
       [0.        , 1.46332097],
       [0.        , 1.46336031],
       [0.        , 1.46339083],
       [0.        , 1.46350789],
       [0.        , 1.46352732],
       [0.        , 1.46362579],
       [0.        , 1.46405113],
       [0.        , 1.46412921],
       [0.        , 1.46424162],
       [0.        , 1.46451604],
       [0.        , 1.46535218],
       [0.        , 1.46567309],
       [0.        , 1.46621311],
       [0.        , 1.46691644],
       [0.        , 1.46810651],
       [0.        , 1.47075653],
       [0.        , 1.47116399],
       [0.        , 1.47442496],
       [0.        , 1.47450852],
       [0.        , 1.5137068 ],
       [0.        , 1.51447332],
       [0.        , 1.5151509 ],
       [0.        , 1.51578927],
       [0.        , 1.51584148],
       [0.        , 1.5160656 ],
       [0.        , 1.51611984],
       [0.        , 1.51630044],
       [0.        , 1.51639521],
       [0.        , 1.51667142],
       [0.        , 1.51667345],
       [0.        , 1.51673484],
       [0.        , 1.51676917],
       [0.        , 1.51687276],
       [0.        , 1.51795244],
       [0.        , 1.51797867],
       [0.        , 1.51849067],
       [0.        , 1.518731  ],
       [0.        , 1.51910949],
       [0.        , 1.51917326],
       [0.        , 1.51927972],
       [0.        , 1.51946855],
       [0.        , 1.51959908],
       [0.        , 1.51964796],
       [0.        , 1.51976538],
       [0.        , 1.51996171],
       [0.        , 1.51998568],
       [0.        , 1.51998782],
       [0.        , 1.52043831],
       [0.        , 1.52054632],
       [0.        , 1.52060246],
       [0.        , 1.52062762],
       [0.        , 1.52084196],
       [0.        , 1.52084649],
       [0.        , 1.52087903],
       [0.        , 1.52090442],
       [0.        , 1.52121103],
       [0.        , 1.52123988],
       [0.        , 1.52127182],
       [0.        , 1.52129126],
       [0.        , 1.52130413],
       [0.        , 1.52131546],
       [0.        , 1.52134275],
       [0.        , 1.52143812],
       [0.        , 1.52145505],
       [0.        , 1.52154648],
       [0.        , 1.52190018],
       [0.        , 1.52193558],
       [0.        , 1.52197123],
       [0.        , 1.52197266],
       [0.        , 1.52199745],
       [0.        , 1.52201414],
       [0.        , 1.52202678],
       [0.        , 1.52206373],
       [0.        , 1.522138  ],
       [0.        , 1.52221406],
       [0.        , 1.52241457],
       [0.        , 1.52254736],
       [0.        , 1.52269864],
       [0.        , 1.52274168],
       [0.        , 1.52276063],
       [0.        , 1.52297759],
       [0.        , 1.52299583],
       [0.        , 1.52312601],
       [0.        , 1.52314007],
       [0.        , 1.52314925],
       [0.        , 1.52317214],
       [0.        , 1.52350914],
       [0.        , 1.52360117],
       [0.        , 1.52373588],
       [0.        , 1.52376997],
       [0.        , 1.5237745 ],
       [0.        , 1.52388883],
       [0.        , 1.52396727],
       [0.        , 1.52400219],
       [0.        , 1.52400684],
       [0.        , 1.52408803],
       [0.        , 1.52414417],
       [0.        , 1.52420771],
       [0.        , 1.52434254],
       [0.        , 1.52439284],
       [0.        , 1.52441013],
       [0.        , 1.52452123],
       [0.        , 1.5246172 ],
       [0.        , 1.52464128],
       [0.        , 1.52470386],
       [0.        , 1.52474523],
       [0.        , 1.52474546],
       [0.        , 1.52485001],
       [0.        , 1.52491426],
       [0.        , 1.5253731 ],
       [0.        , 1.5254513 ],
       [0.        , 1.52556241],
       [0.        , 1.52559173],
       [0.        , 1.52578497],
       [0.        , 1.52585566],
       [0.        , 1.52601314],
       [0.        , 1.52601683],
       [0.        , 1.52615607],
       [0.        , 1.52621186],
       [0.        , 1.52639556],
       [0.        , 1.52647758],
       [0.        , 1.52662516],
       [0.        , 1.52664053],
       [0.        , 1.52671146],
       [0.        , 1.52679062],
       [0.        , 1.52690947],
       [0.        , 1.52691257],
       [0.        , 1.52704763],
       [0.        , 1.52704835],
       [0.        , 1.52715218],
       [0.        , 1.52719676],
       [0.        , 1.52736044],
       [0.        , 1.52743185],
       [0.        , 1.5274415 ],
       [0.        , 1.52750885],
       [0.        , 1.52753961],
       [0.        , 1.52755249],
       [0.        , 1.52823913],
       [0.        , 1.52873576],
       [0.        , 1.52912354],
       [0.        , 1.52943301],
       [0.        , 1.52974331],
       [0.        , 1.52983916],
       [0.        , 1.53052413],
       [0.        , 1.53103876],
       [0.        , 1.53217757]]), array([[8.48839664, 8.67078018],
       [8.38308525, 8.39648724],
       [7.12070656, 7.5738492 ],
       [7.00606346, 7.01338816],
       [6.92072105, 7.03271914],
       [6.80232573, 7.06294823],
       [6.68769693, 7.02810717],
       [6.67040682, 6.68892384],
       [6.64985609, 7.3837862 ],
       [6.53454876, 7.03067684],
       [6.49719191, 6.98580647],
       [6.28659534, 7.47606754],
       [5.96614933, 7.0044899 ],
       [5.96241093, 7.34805393],
       [5.96004772, 6.36256647],
       [5.81858063, 7.76322699],
       [5.77932215, 6.46691227],
       [5.77816725, 5.86455441],
       [5.77740479, 6.02314663],
       [5.76185322, 6.43819523],
       [5.55538988, 7.18007183],
       [5.42379808, 8.44502735],
       [5.32245779, 7.62027788],
       [5.2422595 , 5.30553007],
       [5.18199825, 8.66627026],
       [4.78844452, 7.68021917],
       [4.53803921, 7.60880995],
       [3.99209285, 4.42793465],
       [3.97502661, 4.30933237],
       [3.9683938 , 3.96974897],
       [3.96653032, 4.6944046 ],
       [3.9186976 , 4.01614952],
       [3.72792745, 8.66706371]]), array([[ 9.45640087, 10.08594513],
       [ 9.14413929,  9.21492195],
       [ 9.12609482, 10.52283001],
       [ 8.8525877 ,  8.99581242],
       [ 8.12193966,  8.14233112],
       [ 7.93845415,  8.08592701],
       [ 7.24976826,  7.31497765]])]2024-03-06 17:46:10.581046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6G0H ph vector generated, counter: 27
2024-03-06 17:46:14.676858: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:14.745613: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:15.724506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6GWN ph vector generated, counter: 28
2024-03-06 17:46:19.026546: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:19.068834: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:20.008118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6GWP ph vector generated, counter: 29
2024-03-06 17:46:23.666614: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:23.717691: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:24.964064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6GWQ ph vector generated, counter: 30
2024-03-06 17:46:28.596006: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:28.639032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:29.602890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6H0W ph vector generated, counter: 31
2024-03-06 17:46:33.302656: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:33.345480: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:34.521142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6H0X ph vector generated, counter: 32
2024-03-06 17:46:38.260729: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:38.304311: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:39.801828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6H0Y ph vector generated, counter: 33
2024-03-06 17:46:43.648934: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:43.692892: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:44.873429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6H0Z ph vector generated, counter: 34
2024-03-06 17:46:48.539614: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:48.582367: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:49.670506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6H10 ph vector generated, counter: 35
2024-03-06 17:46:53.523729: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:53.565920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:54.787636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6H11 ph vector generated, counter: 36
2024-03-06 17:46:58.393675: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:46:58.436241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:46:59.803625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6HJ2 ph vector generated, counter: 37
2024-03-06 17:47:03.866687: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:03.928659: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:04.887553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6HKQ ph vector generated, counter: 38
2024-03-06 17:47:08.100164: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:08.143791: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:09.065384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6HN3 ph vector generated, counter: 39
2024-03-06 17:47:12.125946: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:12.168486: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:13.098600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6HR1 ph vector generated, counter: 40
2024-03-06 17:47:16.242821: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:16.285240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:17.351819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6HTF ph vector generated, counter: 41
2024-03-06 17:47:20.527486: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:20.570865: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:21.445527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6HW2 ph vector generated, counter: 42
2024-03-06 17:47:24.629950: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:24.672618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:25.561440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6I42 ph vector generated, counter: 43
2024-03-06 17:47:28.770601: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:28.816491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:29.712840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6I82 ph vector generated, counter: 44
2024-03-06 17:47:32.961298: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:33.004442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:34.084214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6I83 ph vector generated, counter: 45
2024-03-06 17:47:37.418419: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:37.461087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:38.464229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6I99 ph vector generated, counter: 46
2024-03-06 17:47:41.815983: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:41.858666: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:42.796470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6IC9 ph vector generated, counter: 47
2024-03-06 17:47:46.137127: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:46.179013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:47.223626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6ICI ph vector generated, counter: 48
2024-03-06 17:47:50.553910: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:50.596920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:51.791821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6IPU ph vector generated, counter: 49
2024-03-06 17:47:55.675205: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:47:55.717321: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:47:56.802504: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JBU ph vector generated, counter: 50
2024-03-06 17:48:00.213916: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:00.256403: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:01.414991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JD0 ph vector generated, counter: 51
2024-03-06 17:48:04.755421: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:04.797642: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:05.720298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JD8 ph vector generated, counter: 52
2024-03-06 17:48:08.984588: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:09.027173: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:10.231147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JEY ph vector generated, counter: 53
2024-03-06 17:48:14.158076: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:14.200785: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:15.198636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JEZ ph vector generated, counter: 54
2024-03-06 17:48:18.843862: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:18.889858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:20.107718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JGM ph vector generated, counter: 55
2024-03-06 17:48:23.644025: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:23.687596: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:24.575189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3050565), (0., 1.3213508), (0., 1.321582 ), (0., 1.3237575),
       (0., 1.3239276), (0., 1.3250132), (0., 1.3254752), (0., 1.3256161),
       (0., 1.325804 ), (0., 1.3267349), (0., 1.3267834), (0., 1.3271661),
       (0., 1.3274826), (0., 1.3275533), (0., 1.3275611), (0., 1.3278135),
       (0., 1.3280214), (0., 1.3280542), (0., 1.3284233), (0., 1.328555 ),
       (0., 1.3285605), (0., 1.3287668), (0., 1.3289479), (0., 1.328966 ),
       (0., 1.3289753), (0., 1.3290085), (0., 1.3290309), (0., 1.3291916),
       (0., 1.3292851), (0., 1.3294218), (0., 1.3295032), (0., 1.3296831),
       (0., 1.3296933), (0., 1.3297492), (0., 1.3297639), (0., 1.3298954),
       (0., 1.3299506), (0., 1.3299754), (0., 1.3299962), (0., 1.3303571),
       (0., 1.330634 ), (0., 1.3307294), (0., 1.330928 ), (0., 1.3310177),
       (0., 1.3311236), (0., 1.3311696), (0., 1.3313928), (0., 1.3314271),
       (0., 1.331481 ), (0., 1.3315341), (0., 1.3315723), (0., 1.3316956),
       (0., 1.3317538), (0., 1.3317543), (0., 1.3317828), (0., 1.3317894),
       (0., 1.3318053), (0., 1.3322788), (0., 1.3324177), (0., 1.3324381),
       (0., 1.3324457), (0., 1.3325667), (0., 1.3328632), (0., 1.3331271),
       (0., 1.3332067), (0., 1.3333193), (0., 1.3333391), (0., 1.333455 ),
       (0., 1.3337485), (0., 1.3338536), (0., 1.3338633), (0., 1.333978 ),
       (0., 1.3339967), (0., 1.3340667), (0., 1.3341876), (0., 1.334333 ),
       (0., 1.334662 ), (0., 1.3347286), (0., 1.3348938), (0., 1.3349833),
       (0., 1.3349888), (0., 1.3350012), (0., 1.3350575), (0., 1.3350785),
       (0., 1.33521  ), (0., 1.3352953), (0., 1.3354877), (0., 1.3354943),
       (0., 1.3356286), (0., 1.3358049), (0., 1.3358493), (0., 1.335908 ),
       (0., 1.3359637), (0., 1.3359997), (0., 1.3360285), (0., 1.336121 ),
       (0., 1.3362334), (0., 1.3363363), (0., 1.3366061), (0., 1.3367436),
       (0., 1.3368391), (0., 1.3370862), (0., 1.3372216), (0., 1.3379675),
       (0., 1.3380946), (0., 1.3381324), (0., 1.3381435), (0., 1.3383089),
       (0., 1.3384205), (0., 1.3390511), (0., 1.3391093), (0., 1.3392271),
       (0., 1.3395566), (0., 1.3413782), (0., 1.3418068), (0., 1.3424106),
       (0., 1.3425009), (0., 1.3427976), (0., 1.3429446), (0., 1.3430812),
       (0., 1.3458353), (0., 1.3464357), (0., 1.347962 ), (0., 1.4374511),
       (0., 1.446861 ), (0., 1.4473919), (0., 1.4510049), (0., 1.4510365),
       (0., 1.4510927), (0., 1.451094 ), (0., 1.4511379), (0., 1.4512348),
       (0., 1.4513155), (0., 1.4515463), (0., 1.451564 ), (0., 1.4517034),
       (0., 1.4517614), (0., 1.4520144), (0., 1.4528306), (0., 1.4535245),
       (0., 1.4539053), (0., 1.4540825), (0., 1.4541816), (0., 1.4541955),
       (0., 1.4546858), (0., 1.4546971), (0., 1.4548222), (0., 1.4550465),
       (0., 1.4550751), (0., 1.4553974), (0., 1.4554263), (0., 1.455541 ),
       (0., 1.4556837), (0., 1.4557593), (0., 1.4557949), (0., 1.4558119),
       (0., 1.4558924), (0., 1.4561764), (0., 1.456365 ), (0., 1.4563687),
       (0., 1.4563698), (0., 1.4564694), (0., 1.4566815), (0., 1.4566919),
       (0., 1.456754 ), (0., 1.4568403), (0., 1.4569488), (0., 1.4573848),
       (0., 1.4575064), (0., 1.4578853), (0., 1.4579118), (0., 1.4580088),
       (0., 1.4580723), (0., 1.4580857), (0., 1.4582444), (0., 1.4582494),
       (0., 1.458335 ), (0., 1.4583584), (0., 1.4583657), (0., 1.4585676),
       (0., 1.4585688), (0., 1.4586436), (0., 1.4587648), (0., 1.4590309),
       (0., 1.4590523), (0., 1.4590682), (0., 1.4590884), (0., 1.4591193),
       (0., 1.4591374), (0., 1.4591429), (0., 1.4592203), (0., 1.4592924),
       (0., 1.4593178), (0., 1.4596301), (0., 1.459647 ), (0., 1.4597104),
       (0., 1.4598333), (0., 1.4598838), (0., 1.4600592), (0., 1.4601228),
       (0., 1.4601977), (0., 1.4601996), (0., 1.4602104), (0., 1.4602666),
       (0., 1.460423 ), (0., 1.4604266), (0., 1.4604753), (0., 1.4605814),
       (0., 1.4607209), (0., 1.4607956), (0., 1.4610281), (0., 1.4611518),
       (0., 1.4611572), (0., 1.461182 ), (0., 1.4613187), (0., 1.4614131),
       (0., 1.4614365), (0., 1.4618543), (0., 1.4623783), (0., 1.4623907),
       (0., 1.4625762), (0., 1.462668 ), (0., 1.4626833), (0., 1.4630257),
       (0., 1.4630314), (0., 1.4631271), (0., 1.4634705), (0., 1.4636192),
       (0., 1.4636612), (0., 1.4637777), (0., 1.4642558), (0., 1.4644468),
       (0., 1.4646517), (0., 1.4665525), (0., 1.4668677), (0., 1.4673318),
       (0., 1.4675395), (0., 1.4681327), (0., 1.4685217), (0., 1.4692818),
       (0., 1.4694282), (0., 1.4700725), (0., 1.4704257), (0., 1.4706235),
       (0., 1.4715194), (0., 1.4742543), (0., 1.4758922), (0., 1.5048661),
       (0., 1.5073588), (0., 1.508699 ), (0., 1.5107499), (0., 1.5134482),
       (0., 1.5136337), (0., 1.5143026), (0., 1.5143483), (0., 1.5146481),
       (0., 1.5158497), (0., 1.5160028), (0., 1.516407 ), (0., 1.5165614),
       (0., 1.5169445), (0., 1.517604 ), (0., 1.5181214), (0., 1.5181401),
       (0., 1.5182261), (0., 1.5182571), (0., 1.5185375), (0., 1.5185525),
       (0., 1.5187579), (0., 1.5190011), (0., 1.5193502), (0., 1.5193831),
       (0., 1.5194029), (0., 1.5194284), (0., 1.5194902), (0., 1.5196894),
       (0., 1.5197482), (0., 1.5198362), (0., 1.5198603), (0., 1.519889 ),
       (0., 1.5200837), (0., 1.520132 ), (0., 1.5202343), (0., 1.5203339),
       (0., 1.5207865), (0., 1.5209408), (0., 1.5209767), (0., 1.5211418),
       (0., 1.5211453), (0., 1.5212854), (0., 1.5213851), (0., 1.5214026),
       (0., 1.5216078), (0., 1.5218945), (0., 1.5219396), (0., 1.5221304),
       (0., 1.5222287), (0., 1.5222594), (0., 1.5222921), (0., 1.5223905),
       (0., 1.522393 ), (0., 1.5223944), (0., 1.5224488), (0., 1.522456 ),
       (0., 1.5225112), (0., 1.5225825), (0., 1.5227911), (0., 1.5229509),
       (0., 1.5230553), (0., 1.5231075), (0., 1.5231816), (0., 1.5232499),
       (0., 1.5233934), (0., 1.5234162), (0., 1.5234244), (0., 1.5234774),
       (0., 1.5236686), (0., 1.5237575), (0., 1.5238448), (0., 1.5239457),
       (0., 1.5240592), (0., 1.5240604), (0., 1.5240632), (0., 1.5244426),
       (0., 1.5245944), (0., 1.5246253), (0., 1.5246253), (0., 1.5246893),
       (0., 1.5247071), (0., 1.5247573), (0., 1.5247903), (0., 1.5248481),
       (0., 1.5249428), (0., 1.5251608), (0., 1.5252959), (0., 1.5253527),
       (0., 1.5256374), (0., 1.5260183), (0., 1.5260653), (0., 1.5260712),
       (0., 1.5261788), (0., 1.5262291), (0., 1.5263511), (0., 1.5264542),
       (0., 1.5265067), (0., 1.5266675), (0., 1.5266817), (0., 1.5267256),
       (0., 1.5269829), (0., 1.5270666), (0., 1.5270879), (0., 1.5273588),
       (0., 1.5276029), (0., 1.527813 ), (0., 1.5280602), (0., 1.5280943),
       (0., 1.5282576), (0., 1.5284041), (0., 1.5288342), (0., 1.5289289),
       (0., 1.529069 ), (0., 1.5292244), (0., 1.5293905), (0., 1.5295258),
       (0., 1.5302398), (0., 1.5302688), (0., 1.5304558), (0., 1.5304945),
       (0., 1.5307522), (0., 1.5319778), (0., 1.5344962)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.109958 , 8.558997 ), (7.0516977, 7.0576773),
       (6.9868836, 7.280027 ), (6.786412 , 7.038916 ),
       (6.738849 , 7.035799 ), (6.7099667, 7.3115373),
       (6.4740973, 7.004995 ), (6.325068 , 6.9549522),
       (6.062588 , 7.8869953), (6.0277357, 6.9402723),
       (6.0232053, 7.299569 ), (5.8605056, 6.5370483),
       (5.590057 , 5.8507843), (5.493303 , 8.6661005),
       (5.3724785, 5.473032 ), (5.3354354, 8.315365 ),
       (5.150078 , 6.6814065), (5.1470838, 8.5477085),
       (4.990993 , 4.995892 ), (4.928501 , 7.1535244),
       (4.498813 , 7.9132843), (4.4256663, 4.5684223),
       (4.1460505, 7.973386 ), (3.9551399, 4.5974503),
       (3.9475787, 4.2276583), (3.9362247, 4.0147195),
       (3.7888653, 4.209006 ), (3.6375744, 8.591074 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.596484, 10.192491), (9.301732, 10.643867),
       (9.213191,  9.311786), (8.415722,  8.517905),
       (8.358347,  8.453434)], dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3050564527511597), (0.0, 1.3213508129119873), (0.0, 1.3215819597244263), (0.0, 1.323757529258728), (0.0, 1.323927640914917), (0.0, 1.3250131607055664), (0.0, 1.3254752159118652), (0.0, 1.3256161212921143), (0.0, 1.3258039951324463), (0.0, 1.3267349004745483), (0.0, 1.3267834186553955), (0.0, 1.3271660804748535), (0.0, 1.3274825811386108), (0.0, 1.3275532722473145), (0.0, 1.3275611400604248), (0.0, 1.3278135061264038), (0.0, 1.3280214071273804), (0.0, 1.3280541896820068), (0.0, 1.328423261642456), (0.0, 1.3285549879074097), (0.0, 1.328560471534729), (0.0, 1.3287668228149414), (0.0, 1.328947901725769), (0.0, 1.3289660215377808), (0.0, 1.3289753198623657), (0.0, 1.3290084600448608), (0.0, 1.3290308713912964), (0.0, 1.3291915655136108), (0.0, 1.3292851448059082), (0.0, 1.3294217586517334), (0.0, 1.3295031785964966), (0.0, 1.3296830654144287), (0.0, 1.32969331741333), (0.0, 1.3297492265701294), (0.0, 1.3297638893127441), (0.0, 1.3298953771591187), (0.0, 1.3299505710601807), (0.0, 1.3299753665924072), (0.0, 1.3299962282180786), (0.0, 1.3303570747375488), (0.0, 1.3306339979171753), (0.0, 1.330729365348816), (0.0, 1.3309279680252075), (0.0, 1.3310177326202393), (0.0, 1.3311235904693604), (0.0, 1.331169605255127), (0.0, 1.331392765045166), (0.0, 1.3314270973205566), (0.0, 1.3314809799194336), (0.0, 1.3315341472625732), (0.0, 1.3315722942352295), (0.0, 1.331695556640625), (0.0, 1.3317538499832153), (0.0, 1.3317543268203735), (0.0, 1.3317828178405762), (0.0, 1.3317893743515015), (0.0, 1.3318053483963013), (0.0, 1.332278847694397), (0.0, 1.3324177265167236), (0.0, 1.3324381113052368), (0.0, 1.332445740699768), (0.0, 1.332566738128662), (0.0, 1.332863211631775), (0.0, 1.3331271409988403), (0.0, 1.3332066535949707), (0.0, 1.3333193063735962), (0.0, 1.3333390951156616), (0.0, 1.333454966545105), (0.0, 1.333748459815979), (0.0, 1.3338536024093628), (0.0, 1.3338632583618164), (0.0, 1.3339780569076538), (0.0, 1.3339966535568237), (0.0, 1.3340667486190796), (0.0, 1.334187626838684), (0.0, 1.3343329429626465), (0.0, 1.3346619606018066), (0.0, 1.3347285985946655), (0.0, 1.334893822669983), (0.0, 1.3349833488464355), (0.0, 1.3349888324737549), (0.0, 1.3350012302398682), (0.0, 1.3350574970245361), (0.0, 1.335078477859497), (0.0, 1.3352099657058716), (0.0, 1.33529531955719), (0.0, 1.335487723350525), (0.0, 1.3354942798614502), (0.0, 1.335628628730774), (0.0, 1.3358049392700195), (0.0, 1.3358492851257324), (0.0, 1.335908055305481), (0.0, 1.3359637260437012), (0.0, 1.3359997272491455), (0.0, 1.3360284566879272), (0.0, 1.3361209630966187), (0.0, 1.336233377456665), (0.0, 1.3363362550735474), (0.0, 1.3366061449050903), (0.0, 1.3367435932159424), (0.0, 1.3368390798568726), (0.0, 1.3370862007141113), (0.0, 1.337221622467041), (0.0, 1.3379675149917603), (0.0, 1.3380945920944214), (0.0, 1.338132381439209), (0.0, 1.3381434679031372), (0.0, 1.3383089303970337), (0.0, 1.3384205102920532), (0.0, 1.3390511274337769), (0.0, 1.3391093015670776), (0.0, 1.3392270803451538), (0.0, 1.3395565748214722), (0.0, 1.3413782119750977), (0.0, 1.3418067693710327), (0.0, 1.3424105644226074), (0.0, 1.342500925064087), (0.0, 1.3427976369857788), (0.0, 1.342944622039795), (0.0, 1.3430812358856201), (0.0, 1.3458353281021118), (0.0, 1.3464356660842896), (0.0, 1.3479620218276978), (0.0, 1.4374511241912842), (0.0, 1.4468610286712646), (0.0, 1.4473918676376343), (0.0, 1.4510048627853394), (0.0, 1.4510364532470703), (0.0, 1.4510927200317383), (0.0, 1.4510940313339233), (0.0, 1.451137900352478), (0.0, 1.4512348175048828), (0.0, 1.4513155221939087), (0.0, 1.451546311378479), (0.0, 1.4515639543533325), (0.0, 1.451703429222107), (0.0, 1.4517613649368286), (0.0, 1.452014446258545), (0.0, 1.4528305530548096), (0.0, 1.4535244703292847), (0.0, 1.4539053440093994), (0.0, 1.4540824890136719), (0.0, 1.4541815519332886), (0.0, 1.454195499420166), (0.0, 1.4546858072280884), (0.0, 1.4546971321105957), (0.0, 1.4548221826553345), (0.0, 1.455046534538269), (0.0, 1.4550751447677612), (0.0, 1.455397367477417), (0.0, 1.4554263353347778), (0.0, 1.4555410146713257), (0.0, 1.455683708190918), (0.0, 1.4557592868804932), (0.0, 1.4557949304580688), (0.0, 1.455811858177185), (0.0, 1.4558924436569214), (0.0, 1.4561764001846313), (0.0, 1.4563649892807007), (0.0, 1.4563686847686768), (0.0, 1.4563697576522827), (0.0, 1.4564694166183472), (0.0, 1.456681489944458), (0.0, 1.456691861152649), (0.0, 1.4567539691925049), (0.0, 1.4568402767181396), (0.0, 1.4569487571716309), (0.0, 1.4573848247528076), (0.0, 1.4575064182281494), (0.0, 1.4578852653503418), (0.0, 1.4579118490219116), (0.0, 1.4580087661743164), (0.0, 1.458072304725647), (0.0, 1.4580856561660767), (0.0, 1.4582444429397583), (0.0, 1.4582494497299194), (0.0, 1.458335041999817), (0.0, 1.4583584070205688), (0.0, 1.4583656787872314), (0.0, 1.4585676193237305), (0.0, 1.458568811416626), (0.0, 1.4586435556411743), (0.0, 1.4587647914886475), (0.0, 1.4590308666229248), (0.0, 1.459052324295044), (0.0, 1.4590681791305542), (0.0, 1.4590884447097778), (0.0, 1.4591193199157715), (0.0, 1.4591374397277832), (0.0, 1.4591429233551025), (0.0, 1.459220290184021), (0.0, 1.4592924118041992), (0.0, 1.4593178033828735), (0.0, 1.4596301317214966), (0.0, 1.4596470594406128), (0.0, 1.4597103595733643), (0.0, 1.4598332643508911), (0.0, 1.4598838090896606), (0.0, 1.4600591659545898), (0.0, 1.46012282371521), (0.0, 1.4601976871490479), (0.0, 1.4601995944976807), (0.0, 1.4602104425430298), (0.0, 1.4602665901184082), (0.0, 1.4604229927062988), (0.0, 1.4604265689849854), (0.0, 1.4604753255844116), (0.0, 1.4605814218521118), (0.0, 1.4607208967208862), (0.0, 1.4607956409454346), (0.0, 1.4610280990600586), (0.0, 1.4611518383026123), (0.0, 1.461157202720642), (0.0, 1.4611819982528687), (0.0, 1.4613187313079834), (0.0, 1.4614131450653076), (0.0, 1.4614365100860596), (0.0, 1.461854338645935), (0.0, 1.4623782634735107), (0.0, 1.462390661239624), (0.0, 1.462576150894165), (0.0, 1.4626679420471191), (0.0, 1.4626833200454712), (0.0, 1.463025689125061), (0.0, 1.4630314111709595), (0.0, 1.4631271362304688), (0.0, 1.463470458984375), (0.0, 1.4636192321777344), (0.0, 1.4636611938476562), (0.0, 1.4637776613235474), (0.0, 1.4642558097839355), (0.0, 1.464446783065796), (0.0, 1.4646517038345337), (0.0, 1.466552495956421), (0.0, 1.4668676853179932), (0.0, 1.4673317670822144), (0.0, 1.4675395488739014), (0.0, 1.468132734298706), (0.0, 1.4685217142105103), (0.0, 1.469281792640686), (0.0, 1.4694281816482544), (0.0, 1.4700725078582764), (0.0, 1.4704257249832153), (0.0, 1.47062349319458), (0.0, 1.4715193510055542), (0.0, 1.4742542505264282), (0.0, 1.475892186164856), (0.0, 1.504866123199463), (0.0, 1.5073587894439697), (0.0, 1.5086990594863892), (0.0, 1.5107499361038208), (0.0, 1.5134482383728027), (0.0, 1.5136337280273438), (0.0, 1.5143026113510132), (0.0, 1.5143482685089111), (0.0, 1.5146480798721313), (0.0, 1.5158497095108032), (0.0, 1.5160027742385864), (0.0, 1.5164070129394531), (0.0, 1.5165613889694214), (0.0, 1.5169445276260376), (0.0, 1.5176039934158325), (0.0, 1.518121361732483), (0.0, 1.5181400775909424), (0.0, 1.518226146697998), (0.0, 1.5182571411132812), (0.0, 1.5185375213623047), (0.0, 1.518552541732788), (0.0, 1.518757939338684), (0.0, 1.5190011262893677), (0.0, 1.5193501710891724), (0.0, 1.5193830728530884), (0.0, 1.5194028615951538), (0.0, 1.5194283723831177), (0.0, 1.5194902420043945), (0.0, 1.5196894407272339), (0.0, 1.5197482109069824), (0.0, 1.519836187362671), (0.0, 1.5198602676391602), (0.0, 1.519888997077942), (0.0, 1.5200836658477783), (0.0, 1.5201319456100464), (0.0, 1.5202343463897705), (0.0, 1.5203338861465454), (0.0, 1.5207865238189697), (0.0, 1.5209407806396484), (0.0, 1.5209766626358032), (0.0, 1.521141767501831), (0.0, 1.5211453437805176), (0.0, 1.5212854146957397), (0.0, 1.5213850736618042), (0.0, 1.5214025974273682), (0.0, 1.521607756614685), (0.0, 1.5218944549560547), (0.0, 1.5219396352767944), (0.0, 1.5221303701400757), (0.0, 1.522228717803955), (0.0, 1.5222593545913696), (0.0, 1.522292137145996), (0.0, 1.5223904848098755), (0.0, 1.522392988204956), (0.0, 1.5223944187164307), (0.0, 1.5224487781524658), (0.0, 1.5224560499191284), (0.0, 1.5225112438201904), (0.0, 1.5225825309753418), (0.0, 1.5227911472320557), (0.0, 1.5229508876800537), (0.0, 1.5230553150177002), (0.0, 1.5231075286865234), (0.0, 1.5231815576553345), (0.0, 1.523249864578247), (0.0, 1.5233933925628662), (0.0, 1.5234161615371704), (0.0, 1.5234243869781494), (0.0, 1.5234774351119995), (0.0, 1.523668646812439), (0.0, 1.5237574577331543), (0.0, 1.523844838142395), (0.0, 1.523945689201355), (0.0, 1.5240591764450073), (0.0, 1.5240603685379028), (0.0, 1.524063229560852), (0.0, 1.5244425535202026), (0.0, 1.5245944261550903), (0.0, 1.524625301361084), (0.0, 1.524625301361084), (0.0, 1.5246893167495728), (0.0, 1.5247070789337158), (0.0, 1.5247572660446167), (0.0, 1.5247902870178223), (0.0, 1.5248481035232544), (0.0, 1.5249427556991577), (0.0, 1.525160789489746), (0.0, 1.5252958536148071), (0.0, 1.5253527164459229), (0.0, 1.5256373882293701), (0.0, 1.5260182619094849), (0.0, 1.5260653495788574), (0.0, 1.5260711908340454), (0.0, 1.5261788368225098), (0.0, 1.5262291431427002), (0.0, 1.5263510942459106), (0.0, 1.526454210281372), (0.0, 1.5265066623687744), (0.0, 1.5266674757003784), (0.0, 1.526681661605835), (0.0, 1.5267256498336792), (0.0, 1.5269829034805298), (0.0, 1.5270665884017944), (0.0, 1.527087926864624), (0.0, 1.5273587703704834), (0.0, 1.5276029109954834), (0.0, 1.5278129577636719), (0.0, 1.5280601978302002), (0.0, 1.5280942916870117), (0.0, 1.5282576084136963), (0.0, 1.5284041166305542), (0.0, 1.5288342237472534), (0.0, 1.5289288759231567), (0.0, 1.529068946838379), (0.0, 1.5292243957519531), (0.0, 1.5293904542922974), (0.0, 1.5295257568359375), (0.0, 1.5302398204803467), (0.0, 1.5302687883377075), (0.0, 1.5304558277130127), (0.0, 1.5304944515228271), (0.0, 1.530752182006836), (0.0, 1.5319777727127075), (0.0, 1.5344961881637573)], [(8.109957695007324, 8.55899715423584), (7.051697731018066, 7.057677268981934), (6.986883640289307, 7.280026912689209), (6.786411762237549, 7.038916110992432), (6.73884916305542, 7.035799026489258), (6.709966659545898, 7.311537265777588), (6.47409725189209, 7.004994869232178), (6.32506799697876, 6.954952239990234), (6.062588214874268, 7.886995315551758), (6.027735710144043, 6.940272331237793), (6.023205280303955, 7.299569129943848), (5.8605055809021, 6.53704833984375), (5.590056896209717, 5.8507843017578125), (5.493302822113037, 8.66610050201416), (5.372478485107422, 5.473031997680664), (5.335435390472412, 8.315364837646484), (5.150077819824219, 6.681406497955322), (5.147083759307861, 8.547708511352539), (4.990993022918701, 4.99589204788208), (4.928501129150391, 7.153524398803711), (4.498813152313232, 7.9132843017578125), (4.425666332244873, 4.568422317504883), (4.146050453186035, 7.973385810852051), (3.9551398754119873, 4.597450256347656), (3.9475786685943604, 4.227658271789551), (3.9362246990203857, 4.014719486236572), (3.788865327835083, 4.209005832672119), (3.6375744342803955, 8.591073989868164)], [(9.596484184265137, 10.192490577697754), (9.301732063293457, 10.643866539001465), (9.213191032409668, 9.311785697937012), (8.415721893310547, 8.517905235290527), (8.358346939086914, 8.453433990478516)]]
[array([[0.        , 1.30505645],
       [0.        , 1.32135081],
       [0.        , 1.32158196],
       [0.        , 1.32375753],
       [0.        , 1.32392764],
       [0.        , 1.32501316],
       [0.        , 1.32547522],
       [0.        , 1.32561612],
       [0.        , 1.325804  ],
       [0.        , 1.3267349 ],
       [0.        , 1.32678342],
       [0.        , 1.32716608],
       [0.        , 1.32748258],
       [0.        , 1.32755327],
       [0.        , 1.32756114],
       [0.        , 1.32781351],
       [0.        , 1.32802141],
       [0.        , 1.32805419],
       [0.        , 1.32842326],
       [0.        , 1.32855499],
       [0.        , 1.32856047],
       [0.        , 1.32876682],
       [0.        , 1.3289479 ],
       [0.        , 1.32896602],
       [0.        , 1.32897532],
       [0.        , 1.32900846],
       [0.        , 1.32903087],
       [0.        , 1.32919157],
       [0.        , 1.32928514],
       [0.        , 1.32942176],
       [0.        , 1.32950318],
       [0.        , 1.32968307],
       [0.        , 1.32969332],
       [0.        , 1.32974923],
       [0.        , 1.32976389],
       [0.        , 1.32989538],
       [0.        , 1.32995057],
       [0.        , 1.32997537],
       [0.        , 1.32999623],
       [0.        , 1.33035707],
       [0.        , 1.330634  ],
       [0.        , 1.33072937],
       [0.        , 1.33092797],
       [0.        , 1.33101773],
       [0.        , 1.33112359],
       [0.        , 1.33116961],
       [0.        , 1.33139277],
       [0.        , 1.3314271 ],
       [0.        , 1.33148098],
       [0.        , 1.33153415],
       [0.        , 1.33157229],
       [0.        , 1.33169556],
       [0.        , 1.33175385],
       [0.        , 1.33175433],
       [0.        , 1.33178282],
       [0.        , 1.33178937],
       [0.        , 1.33180535],
       [0.        , 1.33227885],
       [0.        , 1.33241773],
       [0.        , 1.33243811],
       [0.        , 1.33244574],
       [0.        , 1.33256674],
       [0.        , 1.33286321],
       [0.        , 1.33312714],
       [0.        , 1.33320665],
       [0.        , 1.33331931],
       [0.        , 1.3333391 ],
       [0.        , 1.33345497],
       [0.        , 1.33374846],
       [0.        , 1.3338536 ],
       [0.        , 1.33386326],
       [0.        , 1.33397806],
       [0.        , 1.33399665],
       [0.        , 1.33406675],
       [0.        , 1.33418763],
       [0.        , 1.33433294],
       [0.        , 1.33466196],
       [0.        , 1.3347286 ],
       [0.        , 1.33489382],
       [0.        , 1.33498335],
       [0.        , 1.33498883],
       [0.        , 1.33500123],
       [0.        , 1.3350575 ],
       [0.        , 1.33507848],
       [0.        , 1.33520997],
       [0.        , 1.33529532],
       [0.        , 1.33548772],
       [0.        , 1.33549428],
       [0.        , 1.33562863],
       [0.        , 1.33580494],
       [0.        , 1.33584929],
       [0.        , 1.33590806],
       [0.        , 1.33596373],
       [0.        , 1.33599973],
       [0.        , 1.33602846],
       [0.        , 1.33612096],
       [0.        , 1.33623338],
       [0.        , 1.33633626],
       [0.        , 1.33660614],
       [0.        , 1.33674359],
       [0.        , 1.33683908],
       [0.        , 1.3370862 ],
       [0.        , 1.33722162],
       [0.        , 1.33796751],
       [0.        , 1.33809459],
       [0.        , 1.33813238],
       [0.        , 1.33814347],
       [0.        , 1.33830893],
       [0.        , 1.33842051],
       [0.        , 1.33905113],
       [0.        , 1.3391093 ],
       [0.        , 1.33922708],
       [0.        , 1.33955657],
       [0.        , 1.34137821],
       [0.        , 1.34180677],
       [0.        , 1.34241056],
       [0.        , 1.34250093],
       [0.        , 1.34279764],
       [0.        , 1.34294462],
       [0.        , 1.34308124],
       [0.        , 1.34583533],
       [0.        , 1.34643567],
       [0.        , 1.34796202],
       [0.        , 1.43745112],
       [0.        , 1.44686103],
       [0.        , 1.44739187],
       [0.        , 1.45100486],
       [0.        , 1.45103645],
       [0.        , 1.45109272],
       [0.        , 1.45109403],
       [0.        , 1.4511379 ],
       [0.        , 1.45123482],
       [0.        , 1.45131552],
       [0.        , 1.45154631],
       [0.        , 1.45156395],
       [0.        , 1.45170343],
       [0.        , 1.45176136],
       [0.        , 1.45201445],
       [0.        , 1.45283055],
       [0.        , 1.45352447],
       [0.        , 1.45390534],
       [0.        , 1.45408249],
       [0.        , 1.45418155],
       [0.        , 1.4541955 ],
       [0.        , 1.45468581],
       [0.        , 1.45469713],
       [0.        , 1.45482218],
       [0.        , 1.45504653],
       [0.        , 1.45507514],
       [0.        , 1.45539737],
       [0.        , 1.45542634],
       [0.        , 1.45554101],
       [0.        , 1.45568371],
       [0.        , 1.45575929],
       [0.        , 1.45579493],
       [0.        , 1.45581186],
       [0.        , 1.45589244],
       [0.        , 1.4561764 ],
       [0.        , 1.45636499],
       [0.        , 1.45636868],
       [0.        , 1.45636976],
       [0.        , 1.45646942],
       [0.        , 1.45668149],
       [0.        , 1.45669186],
       [0.        , 1.45675397],
       [0.        , 1.45684028],
       [0.        , 1.45694876],
       [0.        , 1.45738482],
       [0.        , 1.45750642],
       [0.        , 1.45788527],
       [0.        , 1.45791185],
       [0.        , 1.45800877],
       [0.        , 1.4580723 ],
       [0.        , 1.45808566],
       [0.        , 1.45824444],
       [0.        , 1.45824945],
       [0.        , 1.45833504],
       [0.        , 1.45835841],
       [0.        , 1.45836568],
       [0.        , 1.45856762],
       [0.        , 1.45856881],
       [0.        , 1.45864356],
       [0.        , 1.45876479],
       [0.        , 1.45903087],
       [0.        , 1.45905232],
       [0.        , 1.45906818],
       [0.        , 1.45908844],
       [0.        , 1.45911932],
       [0.        , 1.45913744],
       [0.        , 1.45914292],
       [0.        , 1.45922029],
       [0.        , 1.45929241],
       [0.        , 1.4593178 ],
       [0.        , 1.45963013],
       [0.        , 1.45964706],
       [0.        , 1.45971036],
       [0.        , 1.45983326],
       [0.        , 1.45988381],
       [0.        , 1.46005917],
       [0.        , 1.46012282],
       [0.        , 1.46019769],
       [0.        , 1.46019959],
       [0.        , 1.46021044],
       [0.        , 1.46026659],
       [0.        , 1.46042299],
       [0.        , 1.46042657],
       [0.        , 1.46047533],
       [0.        , 1.46058142],
       [0.        , 1.4607209 ],
       [0.        , 1.46079564],
       [0.        , 1.4610281 ],
       [0.        , 1.46115184],
       [0.        , 1.4611572 ],
       [0.        , 1.461182  ],
       [0.        , 1.46131873],
       [0.        , 1.46141315],
       [0.        , 1.46143651],
       [0.        , 1.46185434],
       [0.        , 1.46237826],
       [0.        , 1.46239066],
       [0.        , 1.46257615],
       [0.        , 1.46266794],
       [0.        , 1.46268332],
       [0.        , 1.46302569],
       [0.        , 1.46303141],
       [0.        , 1.46312714],
       [0.        , 1.46347046],
       [0.        , 1.46361923],
       [0.        , 1.46366119],
       [0.        , 1.46377766],
       [0.        , 1.46425581],
       [0.        , 1.46444678],
       [0.        , 1.4646517 ],
       [0.        , 1.4665525 ],
       [0.        , 1.46686769],
       [0.        , 1.46733177],
       [0.        , 1.46753955],
       [0.        , 1.46813273],
       [0.        , 1.46852171],
       [0.        , 1.46928179],
       [0.        , 1.46942818],
       [0.        , 1.47007251],
       [0.        , 1.47042572],
       [0.        , 1.47062349],
       [0.        , 1.47151935],
       [0.        , 1.47425425],
       [0.        , 1.47589219],
       [0.        , 1.50486612],
       [0.        , 1.50735879],
       [0.        , 1.50869906],
       [0.        , 1.51074994],
       [0.        , 1.51344824],
       [0.        , 1.51363373],
       [0.        , 1.51430261],
       [0.        , 1.51434827],
       [0.        , 1.51464808],
       [0.        , 1.51584971],
       [0.        , 1.51600277],
       [0.        , 1.51640701],
       [0.        , 1.51656139],
       [0.        , 1.51694453],
       [0.        , 1.51760399],
       [0.        , 1.51812136],
       [0.        , 1.51814008],
       [0.        , 1.51822615],
       [0.        , 1.51825714],
       [0.        , 1.51853752],
       [0.        , 1.51855254],
       [0.        , 1.51875794],
       [0.        , 1.51900113],
       [0.        , 1.51935017],
       [0.        , 1.51938307],
       [0.        , 1.51940286],
       [0.        , 1.51942837],
       [0.        , 1.51949024],
       [0.        , 1.51968944],
       [0.        , 1.51974821],
       [0.        , 1.51983619],
       [0.        , 1.51986027],
       [0.        , 1.519889  ],
       [0.        , 1.52008367],
       [0.        , 1.52013195],
       [0.        , 1.52023435],
       [0.        , 1.52033389],
       [0.        , 1.52078652],
       [0.        , 1.52094078],
       [0.        , 1.52097666],
       [0.        , 1.52114177],
       [0.        , 1.52114534],
       [0.        , 1.52128541],
       [0.        , 1.52138507],
       [0.        , 1.5214026 ],
       [0.        , 1.52160776],
       [0.        , 1.52189445],
       [0.        , 1.52193964],
       [0.        , 1.52213037],
       [0.        , 1.52222872],
       [0.        , 1.52225935],
       [0.        , 1.52229214],
       [0.        , 1.52239048],
       [0.        , 1.52239299],
       [0.        , 1.52239442],
       [0.        , 1.52244878],
       [0.        , 1.52245605],
       [0.        , 1.52251124],
       [0.        , 1.52258253],
       [0.        , 1.52279115],
       [0.        , 1.52295089],
       [0.        , 1.52305532],
       [0.        , 1.52310753],
       [0.        , 1.52318156],
       [0.        , 1.52324986],
       [0.        , 1.52339339],
       [0.        , 1.52341616],
       [0.        , 1.52342439],
       [0.        , 1.52347744],
       [0.        , 1.52366865],
       [0.        , 1.52375746],
       [0.        , 1.52384484],
       [0.        , 1.52394569],
       [0.        , 1.52405918],
       [0.        , 1.52406037],
       [0.        , 1.52406323],
       [0.        , 1.52444255],
       [0.        , 1.52459443],
       [0.        , 1.5246253 ],
       [0.        , 1.5246253 ],
       [0.        , 1.52468932],
       [0.        , 1.52470708],
       [0.        , 1.52475727],
       [0.        , 1.52479029],
       [0.        , 1.5248481 ],
       [0.        , 1.52494276],
       [0.        , 1.52516079],
       [0.        , 1.52529585],
       [0.        , 1.52535272],
       [0.        , 1.52563739],
       [0.        , 1.52601826],
       [0.        , 1.52606535],
       [0.        , 1.52607119],
       [0.        , 1.52617884],
       [0.        , 1.52622914],
       [0.        , 1.52635109],
       [0.        , 1.52645421],
       [0.        , 1.52650666],
       [0.        , 1.52666748],
       [0.        , 1.52668166],
       [0.        , 1.52672565],
       [0.        , 1.5269829 ],
       [0.        , 1.52706659],
       [0.        , 1.52708793],
       [0.        , 1.52735877],
       [0.        , 1.52760291],
       [0.        , 1.52781296],
       [0.        , 1.5280602 ],
       [0.        , 1.52809429],
       [0.        , 1.52825761],
       [0.        , 1.52840412],
       [0.        , 1.52883422],
       [0.        , 1.52892888],
       [0.        , 1.52906895],
       [0.        , 1.5292244 ],
       [0.        , 1.52939045],
       [0.        , 1.52952576],
       [0.        , 1.53023982],
       [0.        , 1.53026879],
       [0.        , 1.53045583],
       [0.        , 1.53049445],
       [0.        , 1.53075218],
       [0.        , 1.53197777],
       [0.        , 1.53449619]]), array([[8.1099577 , 8.55899715],
       [7.05169773, 7.05767727],
       [6.98688364, 7.28002691],
       [6.78641176, 7.03891611],
       [6.73884916, 7.03579903],
       [6.70996666, 7.31153727],
       [6.47409725, 7.00499487],
       [6.325068  , 6.95495224],
       [6.06258821, 7.88699532],
       [6.02773571, 6.94027233],
       [6.02320528, 7.29956913],
       [5.86050558, 6.53704834],
       [5.5900569 , 5.8507843 ],
       [5.49330282, 8.6661005 ],
       [5.37247849, 5.473032  ],
       [5.33543539, 8.31536484],
       [5.15007782, 6.6814065 ],
       [5.14708376, 8.54770851],
       [4.99099302, 4.99589205],
       [4.92850113, 7.1535244 ],
       [4.49881315, 7.9132843 ],
       [4.42566633, 4.56842232],
       [4.14605045, 7.97338581],
       [3.95513988, 4.59745026],
       [3.94757867, 4.22765827],
       [3.9362247 , 4.01471949],
       [3.78886533, 4.20900583],
       [3.63757443, 8.59107399]]), array([[ 9.59648418, 10.19249058],
       [ 9.30173206, 10.64386654],
       [ 9.21319103,  9.3117857 ],
       [ 8.41572189,  8.51790524],
       [ 8.35834694,  8.45343399]])]2024-03-06 17:48:28.982708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6JI3 ph vector generated, counter: 56
2024-03-06 17:48:33.145444: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:33.188509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:34.076736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3232751), (0., 1.3235024), (0., 1.324022 ), (0., 1.3241503),
       (0., 1.3251116), (0., 1.325269 ), (0., 1.3254027), (0., 1.325433 ),
       (0., 1.3258328), (0., 1.3259794), (0., 1.3276719), (0., 1.3277466),
       (0., 1.3277849), (0., 1.3280691), (0., 1.3281387), (0., 1.3281597),
       (0., 1.3282914), (0., 1.3284746), (0., 1.328868 ), (0., 1.3294414),
       (0., 1.3295606), (0., 1.3296322), (0., 1.3296804), (0., 1.3298346),
       (0., 1.3300238), (0., 1.3300757), (0., 1.3300934), (0., 1.3302367),
       (0., 1.3302712), (0., 1.330328 ), (0., 1.3303509), (0., 1.3303815),
       (0., 1.330562 ), (0., 1.3307672), (0., 1.3307731), (0., 1.330852 ),
       (0., 1.3309678), (0., 1.3309922), (0., 1.3310244), (0., 1.3310981),
       (0., 1.3313116), (0., 1.3313361), (0., 1.3316041), (0., 1.3316872),
       (0., 1.3317618), (0., 1.3318267), (0., 1.3321397), (0., 1.3323444),
       (0., 1.3323872), (0., 1.33246  ), (0., 1.3326066), (0., 1.3327352),
       (0., 1.332841 ), (0., 1.3329585), (0., 1.3333011), (0., 1.3333188),
       (0., 1.3333904), (0., 1.3334891), (0., 1.3335153), (0., 1.3336767),
       (0., 1.3338032), (0., 1.3338331), (0., 1.3339243), (0., 1.3339697),
       (0., 1.3340892), (0., 1.3341094), (0., 1.3343399), (0., 1.3343668),
       (0., 1.3344911), (0., 1.3345919), (0., 1.3348839), (0., 1.3349868),
       (0., 1.3350291), (0., 1.3353277), (0., 1.335398 ), (0., 1.3354104),
       (0., 1.3355685), (0., 1.3355885), (0., 1.3359176), (0., 1.3359214),
       (0., 1.3359326), (0., 1.3359439), (0., 1.3359782), (0., 1.3360875),
       (0., 1.3361174), (0., 1.336196 ), (0., 1.3362601), (0., 1.3362895),
       (0., 1.336372 ), (0., 1.3363879), (0., 1.3363934), (0., 1.3368101),
       (0., 1.3368342), (0., 1.3368349), (0., 1.3368597), (0., 1.3370329),
       (0., 1.3370805), (0., 1.3371146), (0., 1.3374679), (0., 1.3375179),
       (0., 1.3376857), (0., 1.3378477), (0., 1.3380655), (0., 1.338098 ),
       (0., 1.338105 ), (0., 1.3381331), (0., 1.3381456), (0., 1.33866  ),
       (0., 1.3389027), (0., 1.3390803), (0., 1.3391147), (0., 1.3392651),
       (0., 1.3395504), (0., 1.339638 ), (0., 1.3396604), (0., 1.3397938),
       (0., 1.3401634), (0., 1.3401806), (0., 1.3420112), (0., 1.3420427),
       (0., 1.3434696), (0., 1.3513153), (0., 1.4438379), (0., 1.4453337),
       (0., 1.4463015), (0., 1.4496583), (0., 1.4503624), (0., 1.4505205),
       (0., 1.450913 ), (0., 1.4509791), (0., 1.4514394), (0., 1.4518448),
       (0., 1.4519694), (0., 1.452264 ), (0., 1.4525812), (0., 1.4527068),
       (0., 1.4535432), (0., 1.453657 ), (0., 1.4538207), (0., 1.45383  ),
       (0., 1.4539142), (0., 1.4539527), (0., 1.4542868), (0., 1.4542933),
       (0., 1.454392 ), (0., 1.4545282), (0., 1.454549 ), (0., 1.4546973),
       (0., 1.4548522), (0., 1.4549347), (0., 1.4550489), (0., 1.4552537),
       (0., 1.4557444), (0., 1.4557916), (0., 1.4560226), (0., 1.4562553),
       (0., 1.4563311), (0., 1.4564128), (0., 1.4567745), (0., 1.4569677),
       (0., 1.4571435), (0., 1.4571466), (0., 1.4573522), (0., 1.4576377),
       (0., 1.4577289), (0., 1.4580756), (0., 1.4582204), (0., 1.4582697),
       (0., 1.4583571), (0., 1.458367 ), (0., 1.4584019), (0., 1.4585428),
       (0., 1.4587034), (0., 1.458706 ), (0., 1.4587382), (0., 1.4590745),
       (0., 1.459096 ), (0., 1.4592649), (0., 1.459294 ), (0., 1.4595417),
       (0., 1.4595436), (0., 1.4597002), (0., 1.4597127), (0., 1.4598215),
       (0., 1.459937 ), (0., 1.4600981), (0., 1.4601145), (0., 1.4601511),
       (0., 1.460177 ), (0., 1.4602085), (0., 1.4602087), (0., 1.4602597),
       (0., 1.4604161), (0., 1.460599 ), (0., 1.4606491), (0., 1.4607267),
       (0., 1.4607755), (0., 1.4607974), (0., 1.4608065), (0., 1.4608117),
       (0., 1.4608139), (0., 1.4608784), (0., 1.4609115), (0., 1.4609613),
       (0., 1.461037 ), (0., 1.4610682), (0., 1.4610684), (0., 1.4613048),
       (0., 1.4614412), (0., 1.4616129), (0., 1.4617723), (0., 1.461843 ),
       (0., 1.4618583), (0., 1.4618651), (0., 1.4619354), (0., 1.461962 ),
       (0., 1.4622656), (0., 1.4622759), (0., 1.4624686), (0., 1.4627512),
       (0., 1.4628003), (0., 1.4629757), (0., 1.46305  ), (0., 1.4630919),
       (0., 1.463137 ), (0., 1.4632897), (0., 1.4633363), (0., 1.4636104),
       (0., 1.4636141), (0., 1.4641978), (0., 1.4642797), (0., 1.4645699),
       (0., 1.4647934), (0., 1.4650615), (0., 1.4650949), (0., 1.465532 ),
       (0., 1.4662422), (0., 1.4670272), (0., 1.4672031), (0., 1.4687847),
       (0., 1.4696214), (0., 1.4709224), (0., 1.4710008), (0., 1.471254 ),
       (0., 1.4717729), (0., 1.5042068), (0., 1.5071824), (0., 1.5130147),
       (0., 1.5137866), (0., 1.5147871), (0., 1.5152509), (0., 1.5159656),
       (0., 1.5163114), (0., 1.5166619), (0., 1.5166998), (0., 1.5171323),
       (0., 1.5172482), (0., 1.517423 ), (0., 1.51784  ), (0., 1.5179837),
       (0., 1.5180523), (0., 1.518151 ), (0., 1.5183744), (0., 1.5184808),
       (0., 1.5187622), (0., 1.5191203), (0., 1.5191362), (0., 1.5191903),
       (0., 1.5192112), (0., 1.5193937), (0., 1.519446 ), (0., 1.5194932),
       (0., 1.5196214), (0., 1.5196319), (0., 1.5198371), (0., 1.51985  ),
       (0., 1.5199608), (0., 1.5200242), (0., 1.5201118), (0., 1.5201157),
       (0., 1.5201247), (0., 1.5201391), (0., 1.5201575), (0., 1.5202495),
       (0., 1.520275 ), (0., 1.5203505), (0., 1.5204537), (0., 1.5204598),
       (0., 1.5205052), (0., 1.520514 ), (0., 1.520689 ), (0., 1.5207887),
       (0., 1.5209051), (0., 1.5209755), (0., 1.5210549), (0., 1.5210695),
       (0., 1.5210989), (0., 1.5211596), (0., 1.5211601), (0., 1.5213304),
       (0., 1.5214506), (0., 1.5214684), (0., 1.5214894), (0., 1.5215245),
       (0., 1.5215293), (0., 1.5215704), (0., 1.5215997), (0., 1.5216066),
       (0., 1.5216095), (0., 1.5218538), (0., 1.5220143), (0., 1.5220652),
       (0., 1.522164 ), (0., 1.5222043), (0., 1.5222844), (0., 1.5223076),
       (0., 1.5223424), (0., 1.5223596), (0., 1.522591 ), (0., 1.5225956),
       (0., 1.5226363), (0., 1.5228684), (0., 1.5231367), (0., 1.5231484),
       (0., 1.5231684), (0., 1.523179 ), (0., 1.5235656), (0., 1.5236503),
       (0., 1.5237063), (0., 1.5238003), (0., 1.5238032), (0., 1.5238626),
       (0., 1.5238686), (0., 1.5239143), (0., 1.5242231), (0., 1.524552 ),
       (0., 1.5246346), (0., 1.5246489), (0., 1.5247617), (0., 1.5248394),
       (0., 1.5248864), (0., 1.5248972), (0., 1.5249833), (0., 1.5250555),
       (0., 1.5250928), (0., 1.5255632), (0., 1.5257926), (0., 1.5258275),
       (0., 1.5261804), (0., 1.5263779), (0., 1.5266559), (0., 1.5267832),
       (0., 1.5268539), (0., 1.526914 ), (0., 1.5269209), (0., 1.5270073),
       (0., 1.5273558), (0., 1.528155 ), (0., 1.5281581), (0., 1.5285128),
       (0., 1.5288088), (0., 1.5292162), (0., 1.5295848), (0., 1.5304393),
       (0., 1.5307288), (0., 1.5310594), (0., 1.531182 ), (0., 1.5346652)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.5815325, 8.660518 ), (8.0743065, 8.549015 ),
       (7.0009212, 7.593982 ), (6.99036  , 6.9936247),
       (6.869919 , 7.5210333), (6.8135433, 7.0826526),
       (6.5037966, 6.998053 ), (6.437653 , 7.032239 ),
       (6.4030437, 7.0208087), (6.352536 , 6.386094 ),
       (6.257071 , 7.135776 ), (6.204144 , 7.778234 ),
       (6.014944 , 6.586416 ), (6.011446 , 7.008519 ),
       (5.5465193, 5.615578 ), (5.379268 , 8.26053  ),
       (5.2902184, 8.467211 ), (5.230829 , 5.2354   ),
       (5.150346 , 6.6880717), (5.067542 , 8.345012 ),
       (5.032298 , 7.073676 ), (4.6844883, 4.7981014),
       (4.514915 , 4.52187  ), (4.4163475, 7.8386087),
       (4.2575417, 7.854683 ), (3.9976068, 4.259976 ),
       (3.9598808, 4.781764 ), (3.9248686, 4.2312727),
       (3.6902435, 8.582222 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.552807 ,  9.581271 ), (9.410565 , 10.058672 ),
       (9.253618 , 10.47441  ), (8.348209 ,  8.501333 ),
       (8.342956 ,  8.351145 ), (7.6179366,  7.7209897),
       (6.695088 ,  6.7547994)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.323275089263916), (0.0, 1.3235024213790894), (0.0, 1.3240220546722412), (0.0, 1.3241503238677979), (0.0, 1.3251116275787354), (0.0, 1.3252689838409424), (0.0, 1.3254027366638184), (0.0, 1.3254330158233643), (0.0, 1.3258328437805176), (0.0, 1.3259793519973755), (0.0, 1.3276718854904175), (0.0, 1.3277466297149658), (0.0, 1.3277848958969116), (0.0, 1.3280690908432007), (0.0, 1.3281387090682983), (0.0, 1.3281596899032593), (0.0, 1.328291416168213), (0.0, 1.3284746408462524), (0.0, 1.32886803150177), (0.0, 1.3294414281845093), (0.0, 1.32956063747406), (0.0, 1.3296321630477905), (0.0, 1.3296804428100586), (0.0, 1.3298345804214478), (0.0, 1.3300237655639648), (0.0, 1.330075740814209), (0.0, 1.3300933837890625), (0.0, 1.3302366733551025), (0.0, 1.3302712440490723), (0.0, 1.3303279876708984), (0.0, 1.3303508758544922), (0.0, 1.3303815126419067), (0.0, 1.3305619955062866), (0.0, 1.3307671546936035), (0.0, 1.330773115158081), (0.0, 1.3308520317077637), (0.0, 1.3309677839279175), (0.0, 1.3309922218322754), (0.0, 1.331024408340454), (0.0, 1.3310980796813965), (0.0, 1.331311583518982), (0.0, 1.3313361406326294), (0.0, 1.3316041231155396), (0.0, 1.3316872119903564), (0.0, 1.3317618370056152), (0.0, 1.3318266868591309), (0.0, 1.3321397304534912), (0.0, 1.33234441280365), (0.0, 1.3323872089385986), (0.0, 1.3324600458145142), (0.0, 1.332606554031372), (0.0, 1.3327351808547974), (0.0, 1.3328410387039185), (0.0, 1.332958459854126), (0.0, 1.333301067352295), (0.0, 1.333318829536438), (0.0, 1.3333903551101685), (0.0, 1.3334890604019165), (0.0, 1.3335152864456177), (0.0, 1.3336766958236694), (0.0, 1.3338031768798828), (0.0, 1.33383309841156), (0.0, 1.3339242935180664), (0.0, 1.3339697122573853), (0.0, 1.3340891599655151), (0.0, 1.3341094255447388), (0.0, 1.3343398571014404), (0.0, 1.334366798400879), (0.0, 1.3344911336898804), (0.0, 1.3345918655395508), (0.0, 1.3348839282989502), (0.0, 1.3349868059158325), (0.0, 1.335029125213623), (0.0, 1.3353277444839478), (0.0, 1.3353979587554932), (0.0, 1.3354103565216064), (0.0, 1.3355685472488403), (0.0, 1.3355884552001953), (0.0, 1.335917592048645), (0.0, 1.3359214067459106), (0.0, 1.3359326124191284), (0.0, 1.3359439373016357), (0.0, 1.3359781503677368), (0.0, 1.3360874652862549), (0.0, 1.3361173868179321), (0.0, 1.336195945739746), (0.0, 1.3362600803375244), (0.0, 1.3362895250320435), (0.0, 1.3363720178604126), (0.0, 1.3363878726959229), (0.0, 1.3363933563232422), (0.0, 1.3368101119995117), (0.0, 1.336834192276001), (0.0, 1.3368349075317383), (0.0, 1.3368597030639648), (0.0, 1.3370329141616821), (0.0, 1.337080478668213), (0.0, 1.3371145725250244), (0.0, 1.337467908859253), (0.0, 1.3375178575515747), (0.0, 1.3376857042312622), (0.0, 1.3378477096557617), (0.0, 1.338065505027771), (0.0, 1.3380980491638184), (0.0, 1.3381049633026123), (0.0, 1.3381330966949463), (0.0, 1.3381456136703491), (0.0, 1.3386600017547607), (0.0, 1.3389027118682861), (0.0, 1.3390803337097168), (0.0, 1.3391146659851074), (0.0, 1.3392651081085205), (0.0, 1.3395503759384155), (0.0, 1.3396379947662354), (0.0, 1.339660406112671), (0.0, 1.3397938013076782), (0.0, 1.3401633501052856), (0.0, 1.3401806354522705), (0.0, 1.3420112133026123), (0.0, 1.3420426845550537), (0.0, 1.3434696197509766), (0.0, 1.3513152599334717), (0.0, 1.4438378810882568), (0.0, 1.44533371925354), (0.0, 1.4463014602661133), (0.0, 1.4496582746505737), (0.0, 1.4503624439239502), (0.0, 1.4505205154418945), (0.0, 1.4509129524230957), (0.0, 1.4509791135787964), (0.0, 1.451439380645752), (0.0, 1.4518448114395142), (0.0, 1.4519693851470947), (0.0, 1.4522639513015747), (0.0, 1.4525811672210693), (0.0, 1.4527068138122559), (0.0, 1.4535431861877441), (0.0, 1.4536570310592651), (0.0, 1.4538207054138184), (0.0, 1.4538300037384033), (0.0, 1.4539141654968262), (0.0, 1.453952670097351), (0.0, 1.454286813735962), (0.0, 1.4542932510375977), (0.0, 1.4543919563293457), (0.0, 1.4545282125473022), (0.0, 1.454548954963684), (0.0, 1.4546972513198853), (0.0, 1.4548522233963013), (0.0, 1.4549347162246704), (0.0, 1.45504891872406), (0.0, 1.4552537202835083), (0.0, 1.4557443857192993), (0.0, 1.4557915925979614), (0.0, 1.4560226202011108), (0.0, 1.456255316734314), (0.0, 1.4563311338424683), (0.0, 1.4564127922058105), (0.0, 1.4567744731903076), (0.0, 1.4569677114486694), (0.0, 1.4571435451507568), (0.0, 1.4571466445922852), (0.0, 1.4573521614074707), (0.0, 1.4576376676559448), (0.0, 1.4577288627624512), (0.0, 1.4580756425857544), (0.0, 1.458220362663269), (0.0, 1.458269715309143), (0.0, 1.4583570957183838), (0.0, 1.4583669900894165), (0.0, 1.4584019184112549), (0.0, 1.458542823791504), (0.0, 1.4587033987045288), (0.0, 1.458706021308899), (0.0, 1.4587382078170776), (0.0, 1.4590744972229004), (0.0, 1.4590959548950195), (0.0, 1.459264874458313), (0.0, 1.4592939615249634), (0.0, 1.45954167842865), (0.0, 1.4595435857772827), (0.0, 1.4597002267837524), (0.0, 1.4597127437591553), (0.0, 1.4598214626312256), (0.0, 1.4599369764328003), (0.0, 1.460098147392273), (0.0, 1.4601144790649414), (0.0, 1.4601510763168335), (0.0, 1.460176944732666), (0.0, 1.460208535194397), (0.0, 1.4602086544036865), (0.0, 1.4602596759796143), (0.0, 1.4604160785675049), (0.0, 1.4605989456176758), (0.0, 1.4606491327285767), (0.0, 1.4607267379760742), (0.0, 1.4607754945755005), (0.0, 1.4607974290847778), (0.0, 1.4608064889907837), (0.0, 1.460811734199524), (0.0, 1.4608138799667358), (0.0, 1.4608783721923828), (0.0, 1.460911512374878), (0.0, 1.4609613418579102), (0.0, 1.461037039756775), (0.0, 1.4610681533813477), (0.0, 1.4610683917999268), (0.0, 1.461304783821106), (0.0, 1.461441159248352), (0.0, 1.4616129398345947), (0.0, 1.4617723226547241), (0.0, 1.4618430137634277), (0.0, 1.4618582725524902), (0.0, 1.4618650674819946), (0.0, 1.4619354009628296), (0.0, 1.4619619846343994), (0.0, 1.4622656106948853), (0.0, 1.4622758626937866), (0.0, 1.4624686241149902), (0.0, 1.4627511501312256), (0.0, 1.4628002643585205), (0.0, 1.4629757404327393), (0.0, 1.4630500078201294), (0.0, 1.4630918502807617), (0.0, 1.4631370306015015), (0.0, 1.463289737701416), (0.0, 1.4633363485336304), (0.0, 1.4636104106903076), (0.0, 1.4636141061782837), (0.0, 1.4641977548599243), (0.0, 1.4642796516418457), (0.0, 1.4645699262619019), (0.0, 1.4647934436798096), (0.0, 1.4650615453720093), (0.0, 1.4650949239730835), (0.0, 1.4655319452285767), (0.0, 1.4662421941757202), (0.0, 1.467027187347412), (0.0, 1.467203140258789), (0.0, 1.4687846899032593), (0.0, 1.4696214199066162), (0.0, 1.4709223508834839), (0.0, 1.4710007905960083), (0.0, 1.4712539911270142), (0.0, 1.4717729091644287), (0.0, 1.5042067766189575), (0.0, 1.5071823596954346), (0.0, 1.5130146741867065), (0.0, 1.5137865543365479), (0.0, 1.5147870779037476), (0.0, 1.5152509212493896), (0.0, 1.5159655809402466), (0.0, 1.5163114070892334), (0.0, 1.5166618824005127), (0.0, 1.5166997909545898), (0.0, 1.51713228225708), (0.0, 1.5172481536865234), (0.0, 1.5174230337142944), (0.0, 1.517840027809143), (0.0, 1.5179836750030518), (0.0, 1.518052339553833), (0.0, 1.518151044845581), (0.0, 1.5183744430541992), (0.0, 1.5184807777404785), (0.0, 1.518762230873108), (0.0, 1.5191203355789185), (0.0, 1.5191361904144287), (0.0, 1.5191903114318848), (0.0, 1.5192111730575562), (0.0, 1.5193936824798584), (0.0, 1.5194460153579712), (0.0, 1.5194932222366333), (0.0, 1.5196213722229004), (0.0, 1.5196318626403809), (0.0, 1.5198371410369873), (0.0, 1.5198500156402588), (0.0, 1.5199607610702515), (0.0, 1.5200241804122925), (0.0, 1.5201117992401123), (0.0, 1.5201157331466675), (0.0, 1.5201246738433838), (0.0, 1.5201390981674194), (0.0, 1.5201574563980103), (0.0, 1.5202494859695435), (0.0, 1.5202749967575073), (0.0, 1.520350456237793), (0.0, 1.520453691482544), (0.0, 1.520459771156311), (0.0, 1.5205051898956299), (0.0, 1.5205140113830566), (0.0, 1.5206890106201172), (0.0, 1.5207886695861816), (0.0, 1.5209051370620728), (0.0, 1.5209754705429077), (0.0, 1.5210548639297485), (0.0, 1.5210695266723633), (0.0, 1.5210988521575928), (0.0, 1.5211596488952637), (0.0, 1.5211601257324219), (0.0, 1.5213303565979004), (0.0, 1.5214506387710571), (0.0, 1.5214684009552002), (0.0, 1.5214893817901611), (0.0, 1.5215245485305786), (0.0, 1.5215293169021606), (0.0, 1.5215704441070557), (0.0, 1.5215996503829956), (0.0, 1.5216065645217896), (0.0, 1.5216095447540283), (0.0, 1.5218538045883179), (0.0, 1.5220142602920532), (0.0, 1.5220651626586914), (0.0, 1.522163987159729), (0.0, 1.5222042798995972), (0.0, 1.5222843885421753), (0.0, 1.5223076343536377), (0.0, 1.5223424434661865), (0.0, 1.5223596096038818), (0.0, 1.5225909948349), (0.0, 1.5225956439971924), (0.0, 1.5226362943649292), (0.0, 1.5228683948516846), (0.0, 1.5231367349624634), (0.0, 1.5231484174728394), (0.0, 1.5231684446334839), (0.0, 1.523179054260254), (0.0, 1.523565649986267), (0.0, 1.5236502885818481), (0.0, 1.523706316947937), (0.0, 1.523800253868103), (0.0, 1.5238032341003418), (0.0, 1.523862600326538), (0.0, 1.5238685607910156), (0.0, 1.5239143371582031), (0.0, 1.5242230892181396), (0.0, 1.5245519876480103), (0.0, 1.524634599685669), (0.0, 1.524648904800415), (0.0, 1.52476167678833), (0.0, 1.5248394012451172), (0.0, 1.5248863697052002), (0.0, 1.5248972177505493), (0.0, 1.524983286857605), (0.0, 1.5250555276870728), (0.0, 1.5250928401947021), (0.0, 1.5255632400512695), (0.0, 1.5257925987243652), (0.0, 1.5258275270462036), (0.0, 1.526180386543274), (0.0, 1.5263779163360596), (0.0, 1.526655912399292), (0.0, 1.5267832279205322), (0.0, 1.5268539190292358), (0.0, 1.5269140005111694), (0.0, 1.5269209146499634), (0.0, 1.5270073413848877), (0.0, 1.5273557901382446), (0.0, 1.528154969215393), (0.0, 1.5281580686569214), (0.0, 1.5285128355026245), (0.0, 1.528808832168579), (0.0, 1.5292161703109741), (0.0, 1.5295847654342651), (0.0, 1.5304392576217651), (0.0, 1.530728816986084), (0.0, 1.5310593843460083), (0.0, 1.531182050704956), (0.0, 1.5346652269363403)], [(8.58153247833252, 8.660517692565918), (8.07430648803711, 8.549015045166016), (7.000921249389648, 7.593982219696045), (6.990359783172607, 6.993624687194824), (6.8699188232421875, 7.52103328704834), (6.813543319702148, 7.082652568817139), (6.503796577453613, 6.998053073883057), (6.437653064727783, 7.032238960266113), (6.403043746948242, 7.020808696746826), (6.352536201477051, 6.386094093322754), (6.257071018218994, 7.135776042938232), (6.20414400100708, 7.778234004974365), (6.014944076538086, 6.586415767669678), (6.011445999145508, 7.008519172668457), (5.5465192794799805, 5.6155781745910645), (5.379268169403076, 8.260530471801758), (5.290218353271484, 8.46721076965332), (5.230828762054443, 5.235400199890137), (5.150345802307129, 6.6880717277526855), (5.06754207611084, 8.345011711120605), (5.0322980880737305, 7.073676109313965), (4.684488296508789, 4.798101425170898), (4.5149149894714355, 4.521870136260986), (4.416347503662109, 7.838608741760254), (4.257541656494141, 7.854682922363281), (3.9976067543029785, 4.259975910186768), (3.959880828857422, 4.781764030456543), (3.924868583679199, 4.2312726974487305), (3.6902434825897217, 8.582221984863281)], [(9.552806854248047, 9.581271171569824), (9.410565376281738, 10.058671951293945), (9.253618240356445, 10.474410057067871), (8.348209381103516, 8.501333236694336), (8.342955589294434, 8.351144790649414), (7.617936611175537, 7.72098970413208), (6.695087909698486, 6.7547993659973145)]]
[array([[0.        , 1.32327509],
       [0.        , 1.32350242],
       [0.        , 1.32402205],
       [0.        , 1.32415032],
       [0.        , 1.32511163],
       [0.        , 1.32526898],
       [0.        , 1.32540274],
       [0.        , 1.32543302],
       [0.        , 1.32583284],
       [0.        , 1.32597935],
       [0.        , 1.32767189],
       [0.        , 1.32774663],
       [0.        , 1.3277849 ],
       [0.        , 1.32806909],
       [0.        , 1.32813871],
       [0.        , 1.32815969],
       [0.        , 1.32829142],
       [0.        , 1.32847464],
       [0.        , 1.32886803],
       [0.        , 1.32944143],
       [0.        , 1.32956064],
       [0.        , 1.32963216],
       [0.        , 1.32968044],
       [0.        , 1.32983458],
       [0.        , 1.33002377],
       [0.        , 1.33007574],
       [0.        , 1.33009338],
       [0.        , 1.33023667],
       [0.        , 1.33027124],
       [0.        , 1.33032799],
       [0.        , 1.33035088],
       [0.        , 1.33038151],
       [0.        , 1.330562  ],
       [0.        , 1.33076715],
       [0.        , 1.33077312],
       [0.        , 1.33085203],
       [0.        , 1.33096778],
       [0.        , 1.33099222],
       [0.        , 1.33102441],
       [0.        , 1.33109808],
       [0.        , 1.33131158],
       [0.        , 1.33133614],
       [0.        , 1.33160412],
       [0.        , 1.33168721],
       [0.        , 1.33176184],
       [0.        , 1.33182669],
       [0.        , 1.33213973],
       [0.        , 1.33234441],
       [0.        , 1.33238721],
       [0.        , 1.33246005],
       [0.        , 1.33260655],
       [0.        , 1.33273518],
       [0.        , 1.33284104],
       [0.        , 1.33295846],
       [0.        , 1.33330107],
       [0.        , 1.33331883],
       [0.        , 1.33339036],
       [0.        , 1.33348906],
       [0.        , 1.33351529],
       [0.        , 1.3336767 ],
       [0.        , 1.33380318],
       [0.        , 1.3338331 ],
       [0.        , 1.33392429],
       [0.        , 1.33396971],
       [0.        , 1.33408916],
       [0.        , 1.33410943],
       [0.        , 1.33433986],
       [0.        , 1.3343668 ],
       [0.        , 1.33449113],
       [0.        , 1.33459187],
       [0.        , 1.33488393],
       [0.        , 1.33498681],
       [0.        , 1.33502913],
       [0.        , 1.33532774],
       [0.        , 1.33539796],
       [0.        , 1.33541036],
       [0.        , 1.33556855],
       [0.        , 1.33558846],
       [0.        , 1.33591759],
       [0.        , 1.33592141],
       [0.        , 1.33593261],
       [0.        , 1.33594394],
       [0.        , 1.33597815],
       [0.        , 1.33608747],
       [0.        , 1.33611739],
       [0.        , 1.33619595],
       [0.        , 1.33626008],
       [0.        , 1.33628953],
       [0.        , 1.33637202],
       [0.        , 1.33638787],
       [0.        , 1.33639336],
       [0.        , 1.33681011],
       [0.        , 1.33683419],
       [0.        , 1.33683491],
       [0.        , 1.3368597 ],
       [0.        , 1.33703291],
       [0.        , 1.33708048],
       [0.        , 1.33711457],
       [0.        , 1.33746791],
       [0.        , 1.33751786],
       [0.        , 1.3376857 ],
       [0.        , 1.33784771],
       [0.        , 1.33806551],
       [0.        , 1.33809805],
       [0.        , 1.33810496],
       [0.        , 1.3381331 ],
       [0.        , 1.33814561],
       [0.        , 1.33866   ],
       [0.        , 1.33890271],
       [0.        , 1.33908033],
       [0.        , 1.33911467],
       [0.        , 1.33926511],
       [0.        , 1.33955038],
       [0.        , 1.33963799],
       [0.        , 1.33966041],
       [0.        , 1.3397938 ],
       [0.        , 1.34016335],
       [0.        , 1.34018064],
       [0.        , 1.34201121],
       [0.        , 1.34204268],
       [0.        , 1.34346962],
       [0.        , 1.35131526],
       [0.        , 1.44383788],
       [0.        , 1.44533372],
       [0.        , 1.44630146],
       [0.        , 1.44965827],
       [0.        , 1.45036244],
       [0.        , 1.45052052],
       [0.        , 1.45091295],
       [0.        , 1.45097911],
       [0.        , 1.45143938],
       [0.        , 1.45184481],
       [0.        , 1.45196939],
       [0.        , 1.45226395],
       [0.        , 1.45258117],
       [0.        , 1.45270681],
       [0.        , 1.45354319],
       [0.        , 1.45365703],
       [0.        , 1.45382071],
       [0.        , 1.45383   ],
       [0.        , 1.45391417],
       [0.        , 1.45395267],
       [0.        , 1.45428681],
       [0.        , 1.45429325],
       [0.        , 1.45439196],
       [0.        , 1.45452821],
       [0.        , 1.45454895],
       [0.        , 1.45469725],
       [0.        , 1.45485222],
       [0.        , 1.45493472],
       [0.        , 1.45504892],
       [0.        , 1.45525372],
       [0.        , 1.45574439],
       [0.        , 1.45579159],
       [0.        , 1.45602262],
       [0.        , 1.45625532],
       [0.        , 1.45633113],
       [0.        , 1.45641279],
       [0.        , 1.45677447],
       [0.        , 1.45696771],
       [0.        , 1.45714355],
       [0.        , 1.45714664],
       [0.        , 1.45735216],
       [0.        , 1.45763767],
       [0.        , 1.45772886],
       [0.        , 1.45807564],
       [0.        , 1.45822036],
       [0.        , 1.45826972],
       [0.        , 1.4583571 ],
       [0.        , 1.45836699],
       [0.        , 1.45840192],
       [0.        , 1.45854282],
       [0.        , 1.4587034 ],
       [0.        , 1.45870602],
       [0.        , 1.45873821],
       [0.        , 1.4590745 ],
       [0.        , 1.45909595],
       [0.        , 1.45926487],
       [0.        , 1.45929396],
       [0.        , 1.45954168],
       [0.        , 1.45954359],
       [0.        , 1.45970023],
       [0.        , 1.45971274],
       [0.        , 1.45982146],
       [0.        , 1.45993698],
       [0.        , 1.46009815],
       [0.        , 1.46011448],
       [0.        , 1.46015108],
       [0.        , 1.46017694],
       [0.        , 1.46020854],
       [0.        , 1.46020865],
       [0.        , 1.46025968],
       [0.        , 1.46041608],
       [0.        , 1.46059895],
       [0.        , 1.46064913],
       [0.        , 1.46072674],
       [0.        , 1.46077549],
       [0.        , 1.46079743],
       [0.        , 1.46080649],
       [0.        , 1.46081173],
       [0.        , 1.46081388],
       [0.        , 1.46087837],
       [0.        , 1.46091151],
       [0.        , 1.46096134],
       [0.        , 1.46103704],
       [0.        , 1.46106815],
       [0.        , 1.46106839],
       [0.        , 1.46130478],
       [0.        , 1.46144116],
       [0.        , 1.46161294],
       [0.        , 1.46177232],
       [0.        , 1.46184301],
       [0.        , 1.46185827],
       [0.        , 1.46186507],
       [0.        , 1.4619354 ],
       [0.        , 1.46196198],
       [0.        , 1.46226561],
       [0.        , 1.46227586],
       [0.        , 1.46246862],
       [0.        , 1.46275115],
       [0.        , 1.46280026],
       [0.        , 1.46297574],
       [0.        , 1.46305001],
       [0.        , 1.46309185],
       [0.        , 1.46313703],
       [0.        , 1.46328974],
       [0.        , 1.46333635],
       [0.        , 1.46361041],
       [0.        , 1.46361411],
       [0.        , 1.46419775],
       [0.        , 1.46427965],
       [0.        , 1.46456993],
       [0.        , 1.46479344],
       [0.        , 1.46506155],
       [0.        , 1.46509492],
       [0.        , 1.46553195],
       [0.        , 1.46624219],
       [0.        , 1.46702719],
       [0.        , 1.46720314],
       [0.        , 1.46878469],
       [0.        , 1.46962142],
       [0.        , 1.47092235],
       [0.        , 1.47100079],
       [0.        , 1.47125399],
       [0.        , 1.47177291],
       [0.        , 1.50420678],
       [0.        , 1.50718236],
       [0.        , 1.51301467],
       [0.        , 1.51378655],
       [0.        , 1.51478708],
       [0.        , 1.51525092],
       [0.        , 1.51596558],
       [0.        , 1.51631141],
       [0.        , 1.51666188],
       [0.        , 1.51669979],
       [0.        , 1.51713228],
       [0.        , 1.51724815],
       [0.        , 1.51742303],
       [0.        , 1.51784003],
       [0.        , 1.51798368],
       [0.        , 1.51805234],
       [0.        , 1.51815104],
       [0.        , 1.51837444],
       [0.        , 1.51848078],
       [0.        , 1.51876223],
       [0.        , 1.51912034],
       [0.        , 1.51913619],
       [0.        , 1.51919031],
       [0.        , 1.51921117],
       [0.        , 1.51939368],
       [0.        , 1.51944602],
       [0.        , 1.51949322],
       [0.        , 1.51962137],
       [0.        , 1.51963186],
       [0.        , 1.51983714],
       [0.        , 1.51985002],
       [0.        , 1.51996076],
       [0.        , 1.52002418],
       [0.        , 1.5201118 ],
       [0.        , 1.52011573],
       [0.        , 1.52012467],
       [0.        , 1.5201391 ],
       [0.        , 1.52015746],
       [0.        , 1.52024949],
       [0.        , 1.520275  ],
       [0.        , 1.52035046],
       [0.        , 1.52045369],
       [0.        , 1.52045977],
       [0.        , 1.52050519],
       [0.        , 1.52051401],
       [0.        , 1.52068901],
       [0.        , 1.52078867],
       [0.        , 1.52090514],
       [0.        , 1.52097547],
       [0.        , 1.52105486],
       [0.        , 1.52106953],
       [0.        , 1.52109885],
       [0.        , 1.52115965],
       [0.        , 1.52116013],
       [0.        , 1.52133036],
       [0.        , 1.52145064],
       [0.        , 1.5214684 ],
       [0.        , 1.52148938],
       [0.        , 1.52152455],
       [0.        , 1.52152932],
       [0.        , 1.52157044],
       [0.        , 1.52159965],
       [0.        , 1.52160656],
       [0.        , 1.52160954],
       [0.        , 1.5218538 ],
       [0.        , 1.52201426],
       [0.        , 1.52206516],
       [0.        , 1.52216399],
       [0.        , 1.52220428],
       [0.        , 1.52228439],
       [0.        , 1.52230763],
       [0.        , 1.52234244],
       [0.        , 1.52235961],
       [0.        , 1.52259099],
       [0.        , 1.52259564],
       [0.        , 1.52263629],
       [0.        , 1.52286839],
       [0.        , 1.52313673],
       [0.        , 1.52314842],
       [0.        , 1.52316844],
       [0.        , 1.52317905],
       [0.        , 1.52356565],
       [0.        , 1.52365029],
       [0.        , 1.52370632],
       [0.        , 1.52380025],
       [0.        , 1.52380323],
       [0.        , 1.5238626 ],
       [0.        , 1.52386856],
       [0.        , 1.52391434],
       [0.        , 1.52422309],
       [0.        , 1.52455199],
       [0.        , 1.5246346 ],
       [0.        , 1.5246489 ],
       [0.        , 1.52476168],
       [0.        , 1.5248394 ],
       [0.        , 1.52488637],
       [0.        , 1.52489722],
       [0.        , 1.52498329],
       [0.        , 1.52505553],
       [0.        , 1.52509284],
       [0.        , 1.52556324],
       [0.        , 1.5257926 ],
       [0.        , 1.52582753],
       [0.        , 1.52618039],
       [0.        , 1.52637792],
       [0.        , 1.52665591],
       [0.        , 1.52678323],
       [0.        , 1.52685392],
       [0.        , 1.526914  ],
       [0.        , 1.52692091],
       [0.        , 1.52700734],
       [0.        , 1.52735579],
       [0.        , 1.52815497],
       [0.        , 1.52815807],
       [0.        , 1.52851284],
       [0.        , 1.52880883],
       [0.        , 1.52921617],
       [0.        , 1.52958477],
       [0.        , 1.53043926],
       [0.        , 1.53072882],
       [0.        , 1.53105938],
       [0.        , 1.53118205],
       [0.        , 1.53466523]]), array([[8.58153248, 8.66051769],
       [8.07430649, 8.54901505],
       [7.00092125, 7.59398222],
       [6.99035978, 6.99362469],
       [6.86991882, 7.52103329],
       [6.81354332, 7.08265257],
       [6.50379658, 6.99805307],
       [6.43765306, 7.03223896],
       [6.40304375, 7.0208087 ],
       [6.3525362 , 6.38609409],
       [6.25707102, 7.13577604],
       [6.204144  , 7.778234  ],
       [6.01494408, 6.58641577],
       [6.011446  , 7.00851917],
       [5.54651928, 5.61557817],
       [5.37926817, 8.26053047],
       [5.29021835, 8.46721077],
       [5.23082876, 5.2354002 ],
       [5.1503458 , 6.68807173],
       [5.06754208, 8.34501171],
       [5.03229809, 7.07367611],
       [4.6844883 , 4.79810143],
       [4.51491499, 4.52187014],
       [4.4163475 , 7.83860874],
       [4.25754166, 7.85468292],
       [3.99760675, 4.25997591],
       [3.95988083, 4.78176403],
       [3.92486858, 4.2312727 ],
       [3.69024348, 8.58222198]]), array([[ 9.55280685,  9.58127117],
       [ 9.41056538, 10.05867195],
       [ 9.25361824, 10.47441006],
       [ 8.34820938,  8.50133324],
       [ 8.34295559,  8.35114479],
       [ 7.61793661,  7.7209897 ],
       [ 6.69508791,  6.75479937]])]2024-03-06 17:48:38.066071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6JI4 ph vector generated, counter: 57
2024-03-06 17:48:41.834963: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:41.877605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:42.943398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3090765), (0., 1.3091439), (0., 1.3098372), (0., 1.3131895),
       (0., 1.3142579), (0., 1.3150465), (0., 1.315937 ), (0., 1.3163627),
       (0., 1.3169218), (0., 1.3177135), (0., 1.3180244), (0., 1.3180282),
       (0., 1.3186753), (0., 1.3191972), (0., 1.3196295), (0., 1.3197463),
       (0., 1.3202059), (0., 1.3209705), (0., 1.321113 ), (0., 1.3218814),
       (0., 1.3220099), (0., 1.3220525), (0., 1.3221549), (0., 1.3228809),
       (0., 1.3230821), (0., 1.3231474), (0., 1.3231574), (0., 1.3236384),
       (0., 1.3237879), (0., 1.3239168), (0., 1.3246169), (0., 1.32477  ),
       (0., 1.3248931), (0., 1.3251982), (0., 1.3253441), (0., 1.3254168),
       (0., 1.3254476), (0., 1.3258411), (0., 1.3258964), (0., 1.3260425),
       (0., 1.3263589), (0., 1.3264312), (0., 1.3264701), (0., 1.3267145),
       (0., 1.3269321), (0., 1.3281908), (0., 1.3284494), (0., 1.3290619),
       (0., 1.3291065), (0., 1.3292897), (0., 1.3294277), (0., 1.329586 ),
       (0., 1.3298959), (0., 1.3299155), (0., 1.3301605), (0., 1.330192 ),
       (0., 1.3302606), (0., 1.3305956), (0., 1.3306338), (0., 1.330805 ),
       (0., 1.3310539), (0., 1.3311722), (0., 1.3317068), (0., 1.3319709),
       (0., 1.3319868), (0., 1.3320107), (0., 1.3322393), (0., 1.3323948),
       (0., 1.332395 ), (0., 1.3324114), (0., 1.3324169), (0., 1.3327256),
       (0., 1.3328042), (0., 1.3328363), (0., 1.3329796), (0., 1.3333452),
       (0., 1.3334361), (0., 1.3335316), (0., 1.3336056), (0., 1.3336332),
       (0., 1.3336501), (0., 1.3339995), (0., 1.3342365), (0., 1.3345218),
       (0., 1.3346268), (0., 1.3347104), (0., 1.3353934), (0., 1.3356369),
       (0., 1.3361207), (0., 1.3363034), (0., 1.3363115), (0., 1.3365414),
       (0., 1.3367902), (0., 1.3372082), (0., 1.337545 ), (0., 1.3376781),
       (0., 1.337933 ), (0., 1.338897 ), (0., 1.3389598), (0., 1.3391073),
       (0., 1.3392155), (0., 1.3393562), (0., 1.3394293), (0., 1.3404288),
       (0., 1.3412455), (0., 1.3412504), (0., 1.3412529), (0., 1.3416059),
       (0., 1.3434321), (0., 1.343761 ), (0., 1.3442147), (0., 1.3445576),
       (0., 1.3455163), (0., 1.3455434), (0., 1.3475423), (0., 1.3481227),
       (0., 1.3488252), (0., 1.3513006), (0., 1.3536041), (0., 1.3566078),
       (0., 1.3655003), (0., 1.3690479), (0., 1.417221 ), (0., 1.4237905),
       (0., 1.4247721), (0., 1.4300928), (0., 1.4321526), (0., 1.4349198),
       (0., 1.4369129), (0., 1.4372231), (0., 1.4390205), (0., 1.4406533),
       (0., 1.4416233), (0., 1.4417816), (0., 1.4423974), (0., 1.4424502),
       (0., 1.4426273), (0., 1.4434452), (0., 1.443979 ), (0., 1.4440073),
       (0., 1.4445893), (0., 1.4450804), (0., 1.446564 ), (0., 1.4466212),
       (0., 1.4466549), (0., 1.4469203), (0., 1.4469285), (0., 1.4470212),
       (0., 1.4471139), (0., 1.447416 ), (0., 1.4475838), (0., 1.4478602),
       (0., 1.4479723), (0., 1.4480487), (0., 1.448298 ), (0., 1.4498649),
       (0., 1.4503251), (0., 1.4510771), (0., 1.4511861), (0., 1.4513981),
       (0., 1.4515059), (0., 1.4515116), (0., 1.4525721), (0., 1.4531806),
       (0., 1.4535226), (0., 1.4539944), (0., 1.4547012), (0., 1.4547766),
       (0., 1.4550704), (0., 1.4559081), (0., 1.4560373), (0., 1.4567105),
       (0., 1.4567357), (0., 1.4567504), (0., 1.4569236), (0., 1.4573443),
       (0., 1.4576408), (0., 1.4579031), (0., 1.458251 ), (0., 1.4585173),
       (0., 1.4594859), (0., 1.4596267), (0., 1.4597663), (0., 1.4599409),
       (0., 1.4600537), (0., 1.4601692), (0., 1.4602475), (0., 1.4610962),
       (0., 1.4615341), (0., 1.4619774), (0., 1.4620092), (0., 1.4620695),
       (0., 1.4624206), (0., 1.4627117), (0., 1.4637573), (0., 1.4637617),
       (0., 1.4639338), (0., 1.4641416), (0., 1.465144 ), (0., 1.465335 ),
       (0., 1.4666828), (0., 1.4669808), (0., 1.4670017), (0., 1.467192 ),
       (0., 1.4673643), (0., 1.4674703), (0., 1.4675069), (0., 1.4685115),
       (0., 1.4686487), (0., 1.4693146), (0., 1.4696851), (0., 1.4697603),
       (0., 1.4700849), (0., 1.4702408), (0., 1.4702606), (0., 1.4703499),
       (0., 1.4704893), (0., 1.4707615), (0., 1.471362 ), (0., 1.4722784),
       (0., 1.4724014), (0., 1.4740264), (0., 1.4761983), (0., 1.4763434),
       (0., 1.4772296), (0., 1.4776042), (0., 1.4779973), (0., 1.4782305),
       (0., 1.4783005), (0., 1.4789474), (0., 1.4799792), (0., 1.4802922),
       (0., 1.4807757), (0., 1.482917 ), (0., 1.4838513), (0., 1.484871 ),
       (0., 1.48563  ), (0., 1.4863456), (0., 1.4873962), (0., 1.4875019),
       (0., 1.4876739), (0., 1.4880794), (0., 1.4886023), (0., 1.4926698),
       (0., 1.4928484), (0., 1.4936647), (0., 1.4943732), (0., 1.4947481),
       (0., 1.4956689), (0., 1.4979537), (0., 1.4995534), (0., 1.5003307),
       (0., 1.5024813), (0., 1.5057449), (0., 1.5074704), (0., 1.5075941),
       (0., 1.5084307), (0., 1.5090121), (0., 1.5092182), (0., 1.5095015),
       (0., 1.5097597), (0., 1.511003 ), (0., 1.5110865), (0., 1.5120012),
       (0., 1.512084 ), (0., 1.5129795), (0., 1.5131173), (0., 1.5134372),
       (0., 1.5148048), (0., 1.5152662), (0., 1.5158067), (0., 1.5168747),
       (0., 1.5170141), (0., 1.5174623), (0., 1.517549 ), (0., 1.5176736),
       (0., 1.5179193), (0., 1.5185238), (0., 1.5188389), (0., 1.5188853),
       (0., 1.5191886), (0., 1.5192252), (0., 1.5195765), (0., 1.519824 ),
       (0., 1.5206773), (0., 1.5210712), (0., 1.5213171), (0., 1.5223284),
       (0., 1.5224868), (0., 1.5231973), (0., 1.523879 ), (0., 1.5239022),
       (0., 1.5241901), (0., 1.5242425), (0., 1.5242884), (0., 1.5247854),
       (0., 1.5250744), (0., 1.5256282), (0., 1.5258453), (0., 1.5267544),
       (0., 1.5269945), (0., 1.5277178), (0., 1.5286177), (0., 1.5288846),
       (0., 1.5289539), (0., 1.5293877), (0., 1.529867 ), (0., 1.5300269),
       (0., 1.5305176), (0., 1.5309426), (0., 1.5310277), (0., 1.5311435),
       (0., 1.5312432), (0., 1.5318573), (0., 1.5318949), (0., 1.5323086),
       (0., 1.5326328), (0., 1.5328138), (0., 1.532828 ), (0., 1.5329615),
       (0., 1.5337914), (0., 1.5350897), (0., 1.53541  ), (0., 1.5356469),
       (0., 1.5371938), (0., 1.5383013), (0., 1.5387248), (0., 1.5392171),
       (0., 1.5414388), (0., 1.5414665), (0., 1.5420383), (0., 1.5423623),
       (0., 1.5425582), (0., 1.5427488), (0., 1.5430242), (0., 1.5430366),
       (0., 1.5434413), (0., 1.5434449), (0., 1.5439081), (0., 1.5446934),
       (0., 1.5448806), (0., 1.5460446), (0., 1.5462825), (0., 1.5467426),
       (0., 1.5474954), (0., 1.5484343), (0., 1.5496161), (0., 1.5507034),
       (0., 1.5515455), (0., 1.5520128), (0., 1.5520419), (0., 1.5530057),
       (0., 1.553134 ), (0., 1.5532634), (0., 1.5544373), (0., 1.5551604),
       (0., 1.5555485), (0., 1.5558361), (0., 1.5562421), (0., 1.5575502),
       (0., 1.558708 ), (0., 1.5595397), (0., 1.5597091), (0., 1.5608761),
       (0., 1.561946 ), (0., 1.5659649), (0., 1.5682857), (0., 1.5688316)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.45788  , 8.479177 ), (7.9420114, 8.401724 ),
       (7.0232015, 7.0444016), (6.9882574, 7.551711 ),
       (6.8784122, 7.648692 ), (6.678106 , 7.010565 ),
       (6.665951 , 7.0592732), (6.497678 , 7.0692143),
       (6.431339 , 7.0247455), (6.2011247, 7.125885 ),
       (6.1277285, 7.7986345), (6.096276 , 7.144584 ),
       (6.0067563, 6.655061 ), (5.5547075, 8.667019 ),
       (5.4262295, 5.5623426), (5.338258 , 8.289953 ),
       (5.2637763, 6.905348 ), (5.2200236, 8.479659 ),
       (4.887046 , 7.333765 ), (4.792075 , 4.8923626),
       (4.509993 , 7.820492 ), (4.225268 , 7.9638367),
       (4.023769 , 4.204403 ), (3.886473 , 4.207421 ),
       (3.8408272, 4.7302403), (3.659519 , 8.682925 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.630218, 10.271889 ), (9.322252,  9.4152355),
       (9.185721, 10.513509 ), (9.055527,  9.059474 ),
       (8.384913,  8.504466 ), (7.676763,  7.7905455)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3090765476226807), (0.0, 1.3091439008712769), (0.0, 1.3098372220993042), (0.0, 1.3131895065307617), (0.0, 1.3142578601837158), (0.0, 1.3150465488433838), (0.0, 1.3159370422363281), (0.0, 1.316362738609314), (0.0, 1.3169218301773071), (0.0, 1.3177134990692139), (0.0, 1.3180243968963623), (0.0, 1.318028211593628), (0.0, 1.3186752796173096), (0.0, 1.319197177886963), (0.0, 1.3196295499801636), (0.0, 1.3197462558746338), (0.0, 1.3202059268951416), (0.0, 1.3209705352783203), (0.0, 1.3211129903793335), (0.0, 1.3218814134597778), (0.0, 1.3220099210739136), (0.0, 1.3220524787902832), (0.0, 1.3221548795700073), (0.0, 1.3228808641433716), (0.0, 1.3230820894241333), (0.0, 1.3231474161148071), (0.0, 1.3231574296951294), (0.0, 1.3236384391784668), (0.0, 1.3237879276275635), (0.0, 1.3239167928695679), (0.0, 1.3246169090270996), (0.0, 1.3247699737548828), (0.0, 1.3248931169509888), (0.0, 1.3251981735229492), (0.0, 1.3253440856933594), (0.0, 1.3254168033599854), (0.0, 1.3254475593566895), (0.0, 1.3258410692214966), (0.0, 1.3258963823318481), (0.0, 1.3260425329208374), (0.0, 1.3263589143753052), (0.0, 1.326431155204773), (0.0, 1.326470136642456), (0.0, 1.3267145156860352), (0.0, 1.3269320726394653), (0.0, 1.328190803527832), (0.0, 1.3284493684768677), (0.0, 1.3290618658065796), (0.0, 1.3291064500808716), (0.0, 1.3292896747589111), (0.0, 1.329427719116211), (0.0, 1.3295860290527344), (0.0, 1.3298958539962769), (0.0, 1.3299155235290527), (0.0, 1.3301604986190796), (0.0, 1.330191969871521), (0.0, 1.3302606344223022), (0.0, 1.33059561252594), (0.0, 1.3306337594985962), (0.0, 1.3308049440383911), (0.0, 1.3310538530349731), (0.0, 1.331172227859497), (0.0, 1.3317067623138428), (0.0, 1.3319709300994873), (0.0, 1.3319867849349976), (0.0, 1.3320107460021973), (0.0, 1.3322392702102661), (0.0, 1.3323948383331299), (0.0, 1.3323949575424194), (0.0, 1.3324114084243774), (0.0, 1.3324168920516968), (0.0, 1.3327256441116333), (0.0, 1.3328042030334473), (0.0, 1.3328362703323364), (0.0, 1.3329795598983765), (0.0, 1.3333451747894287), (0.0, 1.333436131477356), (0.0, 1.3335316181182861), (0.0, 1.3336056470870972), (0.0, 1.3336331844329834), (0.0, 1.3336501121520996), (0.0, 1.333999514579773), (0.0, 1.3342365026474), (0.0, 1.334521770477295), (0.0, 1.3346267938613892), (0.0, 1.3347103595733643), (0.0, 1.3353934288024902), (0.0, 1.335636854171753), (0.0, 1.3361207246780396), (0.0, 1.3363033533096313), (0.0, 1.3363114595413208), (0.0, 1.3365414142608643), (0.0, 1.3367902040481567), (0.0, 1.3372081518173218), (0.0, 1.3375450372695923), (0.0, 1.337678074836731), (0.0, 1.3379329442977905), (0.0, 1.3388969898223877), (0.0, 1.338959813117981), (0.0, 1.3391072750091553), (0.0, 1.3392155170440674), (0.0, 1.3393561840057373), (0.0, 1.339429259300232), (0.0, 1.3404288291931152), (0.0, 1.3412455320358276), (0.0, 1.3412504196166992), (0.0, 1.3412529230117798), (0.0, 1.3416059017181396), (0.0, 1.343432068824768), (0.0, 1.3437609672546387), (0.0, 1.344214677810669), (0.0, 1.3445576429367065), (0.0, 1.345516324043274), (0.0, 1.345543384552002), (0.0, 1.3475422859191895), (0.0, 1.3481227159500122), (0.0, 1.348825216293335), (0.0, 1.351300597190857), (0.0, 1.3536040782928467), (0.0, 1.3566077947616577), (0.0, 1.3655003309249878), (0.0, 1.3690478801727295), (0.0, 1.417220950126648), (0.0, 1.423790454864502), (0.0, 1.4247721433639526), (0.0, 1.4300928115844727), (0.0, 1.4321526288986206), (0.0, 1.434919834136963), (0.0, 1.4369128942489624), (0.0, 1.4372230768203735), (0.0, 1.4390205144882202), (0.0, 1.4406533241271973), (0.0, 1.441623330116272), (0.0, 1.4417816400527954), (0.0, 1.4423973560333252), (0.0, 1.4424501657485962), (0.0, 1.4426273107528687), (0.0, 1.4434452056884766), (0.0, 1.443979024887085), (0.0, 1.4440072774887085), (0.0, 1.4445892572402954), (0.0, 1.4450803995132446), (0.0, 1.446563959121704), (0.0, 1.4466211795806885), (0.0, 1.4466549158096313), (0.0, 1.4469202756881714), (0.0, 1.4469285011291504), (0.0, 1.447021245956421), (0.0, 1.4471138715744019), (0.0, 1.4474159479141235), (0.0, 1.447583794593811), (0.0, 1.4478602409362793), (0.0, 1.447972297668457), (0.0, 1.448048710823059), (0.0, 1.4482979774475098), (0.0, 1.4498648643493652), (0.0, 1.4503251314163208), (0.0, 1.4510771036148071), (0.0, 1.4511860609054565), (0.0, 1.4513981342315674), (0.0, 1.4515058994293213), (0.0, 1.4515116214752197), (0.0, 1.4525721073150635), (0.0, 1.4531805515289307), (0.0, 1.4535225629806519), (0.0, 1.4539943933486938), (0.0, 1.4547011852264404), (0.0, 1.454776644706726), (0.0, 1.4550703763961792), (0.0, 1.4559080600738525), (0.0, 1.4560372829437256), (0.0, 1.4567104578018188), (0.0, 1.4567357301712036), (0.0, 1.4567503929138184), (0.0, 1.4569236040115356), (0.0, 1.4573442935943604), (0.0, 1.4576407670974731), (0.0, 1.4579031467437744), (0.0, 1.4582509994506836), (0.0, 1.45851731300354), (0.0, 1.4594858884811401), (0.0, 1.4596266746520996), (0.0, 1.4597662687301636), (0.0, 1.4599409103393555), (0.0, 1.4600536823272705), (0.0, 1.4601691961288452), (0.0, 1.46024751663208), (0.0, 1.461096167564392), (0.0, 1.4615341424942017), (0.0, 1.4619773626327515), (0.0, 1.4620091915130615), (0.0, 1.4620695114135742), (0.0, 1.4624205827713013), (0.0, 1.4627116918563843), (0.0, 1.4637572765350342), (0.0, 1.4637616872787476), (0.0, 1.4639338254928589), (0.0, 1.464141607284546), (0.0, 1.4651440382003784), (0.0, 1.4653350114822388), (0.0, 1.4666827917099), (0.0, 1.4669808149337769), (0.0, 1.4670016765594482), (0.0, 1.4671920537948608), (0.0, 1.4673643112182617), (0.0, 1.4674702882766724), (0.0, 1.4675068855285645), (0.0, 1.4685114622116089), (0.0, 1.4686486721038818), (0.0, 1.4693145751953125), (0.0, 1.4696850776672363), (0.0, 1.4697602987289429), (0.0, 1.4700849056243896), (0.0, 1.470240831375122), (0.0, 1.4702606201171875), (0.0, 1.470349907875061), (0.0, 1.470489263534546), (0.0, 1.4707615375518799), (0.0, 1.4713619947433472), (0.0, 1.472278356552124), (0.0, 1.4724013805389404), (0.0, 1.4740264415740967), (0.0, 1.4761983156204224), (0.0, 1.4763433933258057), (0.0, 1.4772295951843262), (0.0, 1.4776041507720947), (0.0, 1.4779973030090332), (0.0, 1.4782304763793945), (0.0, 1.4783004522323608), (0.0, 1.478947401046753), (0.0, 1.479979157447815), (0.0, 1.4802922010421753), (0.0, 1.4807757139205933), (0.0, 1.4829169511795044), (0.0, 1.4838513135910034), (0.0, 1.4848710298538208), (0.0, 1.4856300354003906), (0.0, 1.486345648765564), (0.0, 1.487396240234375), (0.0, 1.487501859664917), (0.0, 1.4876738786697388), (0.0, 1.4880794286727905), (0.0, 1.4886022806167603), (0.0, 1.4926698207855225), (0.0, 1.4928483963012695), (0.0, 1.4936647415161133), (0.0, 1.4943732023239136), (0.0, 1.4947481155395508), (0.0, 1.495668888092041), (0.0, 1.4979536533355713), (0.0, 1.4995534420013428), (0.0, 1.5003306865692139), (0.0, 1.5024813413619995), (0.0, 1.5057449340820312), (0.0, 1.5074703693389893), (0.0, 1.507594108581543), (0.0, 1.5084307193756104), (0.0, 1.5090121030807495), (0.0, 1.5092182159423828), (0.0, 1.5095014572143555), (0.0, 1.5097596645355225), (0.0, 1.511003017425537), (0.0, 1.5110864639282227), (0.0, 1.5120011568069458), (0.0, 1.5120840072631836), (0.0, 1.512979507446289), (0.0, 1.5131173133850098), (0.0, 1.5134371519088745), (0.0, 1.5148048400878906), (0.0, 1.5152661800384521), (0.0, 1.5158066749572754), (0.0, 1.5168746709823608), (0.0, 1.5170141458511353), (0.0, 1.5174622535705566), (0.0, 1.5175490379333496), (0.0, 1.5176736116409302), (0.0, 1.5179193019866943), (0.0, 1.5185238122940063), (0.0, 1.518838882446289), (0.0, 1.5188852548599243), (0.0, 1.519188642501831), (0.0, 1.5192252397537231), (0.0, 1.5195765495300293), (0.0, 1.5198240280151367), (0.0, 1.5206773281097412), (0.0, 1.521071195602417), (0.0, 1.5213171243667603), (0.0, 1.5223283767700195), (0.0, 1.5224868059158325), (0.0, 1.5231972932815552), (0.0, 1.523879051208496), (0.0, 1.523902177810669), (0.0, 1.524190068244934), (0.0, 1.5242425203323364), (0.0, 1.5242884159088135), (0.0, 1.5247853994369507), (0.0, 1.5250743627548218), (0.0, 1.5256282091140747), (0.0, 1.5258452892303467), (0.0, 1.526754379272461), (0.0, 1.5269944667816162), (0.0, 1.5277178287506104), (0.0, 1.5286177396774292), (0.0, 1.5288846492767334), (0.0, 1.5289539098739624), (0.0, 1.5293877124786377), (0.0, 1.5298670530319214), (0.0, 1.530026912689209), (0.0, 1.530517578125), (0.0, 1.5309425592422485), (0.0, 1.5310276746749878), (0.0, 1.5311435461044312), (0.0, 1.5312432050704956), (0.0, 1.5318572521209717), (0.0, 1.5318949222564697), (0.0, 1.532308578491211), (0.0, 1.532632827758789), (0.0, 1.5328137874603271), (0.0, 1.5328279733657837), (0.0, 1.5329614877700806), (0.0, 1.533791422843933), (0.0, 1.5350897312164307), (0.0, 1.5354100465774536), (0.0, 1.535646915435791), (0.0, 1.537193775177002), (0.0, 1.5383013486862183), (0.0, 1.5387247800827026), (0.0, 1.5392171144485474), (0.0, 1.5414388179779053), (0.0, 1.541466474533081), (0.0, 1.5420383214950562), (0.0, 1.5423623323440552), (0.0, 1.542558193206787), (0.0, 1.5427488088607788), (0.0, 1.5430241823196411), (0.0, 1.5430365800857544), (0.0, 1.5434412956237793), (0.0, 1.5434448719024658), (0.0, 1.5439081192016602), (0.0, 1.5446933507919312), (0.0, 1.5448806285858154), (0.0, 1.5460445880889893), (0.0, 1.5462825298309326), (0.0, 1.546742558479309), (0.0, 1.5474953651428223), (0.0, 1.5484342575073242), (0.0, 1.5496160984039307), (0.0, 1.5507034063339233), (0.0, 1.55154550075531), (0.0, 1.5520128011703491), (0.0, 1.5520418882369995), (0.0, 1.5530056953430176), (0.0, 1.5531339645385742), (0.0, 1.5532634258270264), (0.0, 1.554437279701233), (0.0, 1.555160403251648), (0.0, 1.5555485486984253), (0.0, 1.5558360815048218), (0.0, 1.5562421083450317), (0.0, 1.5575501918792725), (0.0, 1.5587079524993896), (0.0, 1.5595396757125854), (0.0, 1.559709072113037), (0.0, 1.5608761310577393), (0.0, 1.5619460344314575), (0.0, 1.565964937210083), (0.0, 1.5682857036590576), (0.0, 1.5688315629959106)], [(8.457880020141602, 8.479177474975586), (7.94201135635376, 8.401723861694336), (7.0232014656066895, 7.0444016456604), (6.98825740814209, 7.551711082458496), (6.878412246704102, 7.6486921310424805), (6.67810583114624, 7.010564804077148), (6.665950775146484, 7.0592732429504395), (6.497677803039551, 7.069214344024658), (6.431338787078857, 7.024745464324951), (6.201124668121338, 7.125885009765625), (6.127728462219238, 7.7986345291137695), (6.096275806427002, 7.1445841789245605), (6.00675630569458, 6.655060768127441), (5.5547075271606445, 8.66701889038086), (5.426229476928711, 5.562342643737793), (5.338257789611816, 8.289953231811523), (5.2637763023376465, 6.90534782409668), (5.2200236320495605, 8.479659080505371), (4.887045860290527, 7.333765029907227), (4.792075157165527, 4.892362594604492), (4.509993076324463, 7.820491790771484), (4.2252678871154785, 7.963836669921875), (4.023768901824951, 4.204402923583984), (3.8864729404449463, 4.207420825958252), (3.840827226638794, 4.730240345001221), (3.6595189571380615, 8.6829252243042)], [(9.630217552185059, 10.271888732910156), (9.32225227355957, 9.41523551940918), (9.185721397399902, 10.513508796691895), (9.055526733398438, 9.059473991394043), (8.384913444519043, 8.50446605682373), (7.67676305770874, 7.790545463562012)]]
[array([[0.        , 1.30907655],
       [0.        , 1.3091439 ],
       [0.        , 1.30983722],
       [0.        , 1.31318951],
       [0.        , 1.31425786],
       [0.        , 1.31504655],
       [0.        , 1.31593704],
       [0.        , 1.31636274],
       [0.        , 1.31692183],
       [0.        , 1.3177135 ],
       [0.        , 1.3180244 ],
       [0.        , 1.31802821],
       [0.        , 1.31867528],
       [0.        , 1.31919718],
       [0.        , 1.31962955],
       [0.        , 1.31974626],
       [0.        , 1.32020593],
       [0.        , 1.32097054],
       [0.        , 1.32111299],
       [0.        , 1.32188141],
       [0.        , 1.32200992],
       [0.        , 1.32205248],
       [0.        , 1.32215488],
       [0.        , 1.32288086],
       [0.        , 1.32308209],
       [0.        , 1.32314742],
       [0.        , 1.32315743],
       [0.        , 1.32363844],
       [0.        , 1.32378793],
       [0.        , 1.32391679],
       [0.        , 1.32461691],
       [0.        , 1.32476997],
       [0.        , 1.32489312],
       [0.        , 1.32519817],
       [0.        , 1.32534409],
       [0.        , 1.3254168 ],
       [0.        , 1.32544756],
       [0.        , 1.32584107],
       [0.        , 1.32589638],
       [0.        , 1.32604253],
       [0.        , 1.32635891],
       [0.        , 1.32643116],
       [0.        , 1.32647014],
       [0.        , 1.32671452],
       [0.        , 1.32693207],
       [0.        , 1.3281908 ],
       [0.        , 1.32844937],
       [0.        , 1.32906187],
       [0.        , 1.32910645],
       [0.        , 1.32928967],
       [0.        , 1.32942772],
       [0.        , 1.32958603],
       [0.        , 1.32989585],
       [0.        , 1.32991552],
       [0.        , 1.3301605 ],
       [0.        , 1.33019197],
       [0.        , 1.33026063],
       [0.        , 1.33059561],
       [0.        , 1.33063376],
       [0.        , 1.33080494],
       [0.        , 1.33105385],
       [0.        , 1.33117223],
       [0.        , 1.33170676],
       [0.        , 1.33197093],
       [0.        , 1.33198678],
       [0.        , 1.33201075],
       [0.        , 1.33223927],
       [0.        , 1.33239484],
       [0.        , 1.33239496],
       [0.        , 1.33241141],
       [0.        , 1.33241689],
       [0.        , 1.33272564],
       [0.        , 1.3328042 ],
       [0.        , 1.33283627],
       [0.        , 1.33297956],
       [0.        , 1.33334517],
       [0.        , 1.33343613],
       [0.        , 1.33353162],
       [0.        , 1.33360565],
       [0.        , 1.33363318],
       [0.        , 1.33365011],
       [0.        , 1.33399951],
       [0.        , 1.3342365 ],
       [0.        , 1.33452177],
       [0.        , 1.33462679],
       [0.        , 1.33471036],
       [0.        , 1.33539343],
       [0.        , 1.33563685],
       [0.        , 1.33612072],
       [0.        , 1.33630335],
       [0.        , 1.33631146],
       [0.        , 1.33654141],
       [0.        , 1.3367902 ],
       [0.        , 1.33720815],
       [0.        , 1.33754504],
       [0.        , 1.33767807],
       [0.        , 1.33793294],
       [0.        , 1.33889699],
       [0.        , 1.33895981],
       [0.        , 1.33910728],
       [0.        , 1.33921552],
       [0.        , 1.33935618],
       [0.        , 1.33942926],
       [0.        , 1.34042883],
       [0.        , 1.34124553],
       [0.        , 1.34125042],
       [0.        , 1.34125292],
       [0.        , 1.3416059 ],
       [0.        , 1.34343207],
       [0.        , 1.34376097],
       [0.        , 1.34421468],
       [0.        , 1.34455764],
       [0.        , 1.34551632],
       [0.        , 1.34554338],
       [0.        , 1.34754229],
       [0.        , 1.34812272],
       [0.        , 1.34882522],
       [0.        , 1.3513006 ],
       [0.        , 1.35360408],
       [0.        , 1.35660779],
       [0.        , 1.36550033],
       [0.        , 1.36904788],
       [0.        , 1.41722095],
       [0.        , 1.42379045],
       [0.        , 1.42477214],
       [0.        , 1.43009281],
       [0.        , 1.43215263],
       [0.        , 1.43491983],
       [0.        , 1.43691289],
       [0.        , 1.43722308],
       [0.        , 1.43902051],
       [0.        , 1.44065332],
       [0.        , 1.44162333],
       [0.        , 1.44178164],
       [0.        , 1.44239736],
       [0.        , 1.44245017],
       [0.        , 1.44262731],
       [0.        , 1.44344521],
       [0.        , 1.44397902],
       [0.        , 1.44400728],
       [0.        , 1.44458926],
       [0.        , 1.4450804 ],
       [0.        , 1.44656396],
       [0.        , 1.44662118],
       [0.        , 1.44665492],
       [0.        , 1.44692028],
       [0.        , 1.4469285 ],
       [0.        , 1.44702125],
       [0.        , 1.44711387],
       [0.        , 1.44741595],
       [0.        , 1.44758379],
       [0.        , 1.44786024],
       [0.        , 1.4479723 ],
       [0.        , 1.44804871],
       [0.        , 1.44829798],
       [0.        , 1.44986486],
       [0.        , 1.45032513],
       [0.        , 1.4510771 ],
       [0.        , 1.45118606],
       [0.        , 1.45139813],
       [0.        , 1.4515059 ],
       [0.        , 1.45151162],
       [0.        , 1.45257211],
       [0.        , 1.45318055],
       [0.        , 1.45352256],
       [0.        , 1.45399439],
       [0.        , 1.45470119],
       [0.        , 1.45477664],
       [0.        , 1.45507038],
       [0.        , 1.45590806],
       [0.        , 1.45603728],
       [0.        , 1.45671046],
       [0.        , 1.45673573],
       [0.        , 1.45675039],
       [0.        , 1.4569236 ],
       [0.        , 1.45734429],
       [0.        , 1.45764077],
       [0.        , 1.45790315],
       [0.        , 1.458251  ],
       [0.        , 1.45851731],
       [0.        , 1.45948589],
       [0.        , 1.45962667],
       [0.        , 1.45976627],
       [0.        , 1.45994091],
       [0.        , 1.46005368],
       [0.        , 1.4601692 ],
       [0.        , 1.46024752],
       [0.        , 1.46109617],
       [0.        , 1.46153414],
       [0.        , 1.46197736],
       [0.        , 1.46200919],
       [0.        , 1.46206951],
       [0.        , 1.46242058],
       [0.        , 1.46271169],
       [0.        , 1.46375728],
       [0.        , 1.46376169],
       [0.        , 1.46393383],
       [0.        , 1.46414161],
       [0.        , 1.46514404],
       [0.        , 1.46533501],
       [0.        , 1.46668279],
       [0.        , 1.46698081],
       [0.        , 1.46700168],
       [0.        , 1.46719205],
       [0.        , 1.46736431],
       [0.        , 1.46747029],
       [0.        , 1.46750689],
       [0.        , 1.46851146],
       [0.        , 1.46864867],
       [0.        , 1.46931458],
       [0.        , 1.46968508],
       [0.        , 1.4697603 ],
       [0.        , 1.47008491],
       [0.        , 1.47024083],
       [0.        , 1.47026062],
       [0.        , 1.47034991],
       [0.        , 1.47048926],
       [0.        , 1.47076154],
       [0.        , 1.47136199],
       [0.        , 1.47227836],
       [0.        , 1.47240138],
       [0.        , 1.47402644],
       [0.        , 1.47619832],
       [0.        , 1.47634339],
       [0.        , 1.4772296 ],
       [0.        , 1.47760415],
       [0.        , 1.4779973 ],
       [0.        , 1.47823048],
       [0.        , 1.47830045],
       [0.        , 1.4789474 ],
       [0.        , 1.47997916],
       [0.        , 1.4802922 ],
       [0.        , 1.48077571],
       [0.        , 1.48291695],
       [0.        , 1.48385131],
       [0.        , 1.48487103],
       [0.        , 1.48563004],
       [0.        , 1.48634565],
       [0.        , 1.48739624],
       [0.        , 1.48750186],
       [0.        , 1.48767388],
       [0.        , 1.48807943],
       [0.        , 1.48860228],
       [0.        , 1.49266982],
       [0.        , 1.4928484 ],
       [0.        , 1.49366474],
       [0.        , 1.4943732 ],
       [0.        , 1.49474812],
       [0.        , 1.49566889],
       [0.        , 1.49795365],
       [0.        , 1.49955344],
       [0.        , 1.50033069],
       [0.        , 1.50248134],
       [0.        , 1.50574493],
       [0.        , 1.50747037],
       [0.        , 1.50759411],
       [0.        , 1.50843072],
       [0.        , 1.5090121 ],
       [0.        , 1.50921822],
       [0.        , 1.50950146],
       [0.        , 1.50975966],
       [0.        , 1.51100302],
       [0.        , 1.51108646],
       [0.        , 1.51200116],
       [0.        , 1.51208401],
       [0.        , 1.51297951],
       [0.        , 1.51311731],
       [0.        , 1.51343715],
       [0.        , 1.51480484],
       [0.        , 1.51526618],
       [0.        , 1.51580667],
       [0.        , 1.51687467],
       [0.        , 1.51701415],
       [0.        , 1.51746225],
       [0.        , 1.51754904],
       [0.        , 1.51767361],
       [0.        , 1.5179193 ],
       [0.        , 1.51852381],
       [0.        , 1.51883888],
       [0.        , 1.51888525],
       [0.        , 1.51918864],
       [0.        , 1.51922524],
       [0.        , 1.51957655],
       [0.        , 1.51982403],
       [0.        , 1.52067733],
       [0.        , 1.5210712 ],
       [0.        , 1.52131712],
       [0.        , 1.52232838],
       [0.        , 1.52248681],
       [0.        , 1.52319729],
       [0.        , 1.52387905],
       [0.        , 1.52390218],
       [0.        , 1.52419007],
       [0.        , 1.52424252],
       [0.        , 1.52428842],
       [0.        , 1.5247854 ],
       [0.        , 1.52507436],
       [0.        , 1.52562821],
       [0.        , 1.52584529],
       [0.        , 1.52675438],
       [0.        , 1.52699447],
       [0.        , 1.52771783],
       [0.        , 1.52861774],
       [0.        , 1.52888465],
       [0.        , 1.52895391],
       [0.        , 1.52938771],
       [0.        , 1.52986705],
       [0.        , 1.53002691],
       [0.        , 1.53051758],
       [0.        , 1.53094256],
       [0.        , 1.53102767],
       [0.        , 1.53114355],
       [0.        , 1.53124321],
       [0.        , 1.53185725],
       [0.        , 1.53189492],
       [0.        , 1.53230858],
       [0.        , 1.53263283],
       [0.        , 1.53281379],
       [0.        , 1.53282797],
       [0.        , 1.53296149],
       [0.        , 1.53379142],
       [0.        , 1.53508973],
       [0.        , 1.53541005],
       [0.        , 1.53564692],
       [0.        , 1.53719378],
       [0.        , 1.53830135],
       [0.        , 1.53872478],
       [0.        , 1.53921711],
       [0.        , 1.54143882],
       [0.        , 1.54146647],
       [0.        , 1.54203832],
       [0.        , 1.54236233],
       [0.        , 1.54255819],
       [0.        , 1.54274881],
       [0.        , 1.54302418],
       [0.        , 1.54303658],
       [0.        , 1.5434413 ],
       [0.        , 1.54344487],
       [0.        , 1.54390812],
       [0.        , 1.54469335],
       [0.        , 1.54488063],
       [0.        , 1.54604459],
       [0.        , 1.54628253],
       [0.        , 1.54674256],
       [0.        , 1.54749537],
       [0.        , 1.54843426],
       [0.        , 1.5496161 ],
       [0.        , 1.55070341],
       [0.        , 1.5515455 ],
       [0.        , 1.5520128 ],
       [0.        , 1.55204189],
       [0.        , 1.5530057 ],
       [0.        , 1.55313396],
       [0.        , 1.55326343],
       [0.        , 1.55443728],
       [0.        , 1.5551604 ],
       [0.        , 1.55554855],
       [0.        , 1.55583608],
       [0.        , 1.55624211],
       [0.        , 1.55755019],
       [0.        , 1.55870795],
       [0.        , 1.55953968],
       [0.        , 1.55970907],
       [0.        , 1.56087613],
       [0.        , 1.56194603],
       [0.        , 1.56596494],
       [0.        , 1.5682857 ],
       [0.        , 1.56883156]]), array([[8.45788002, 8.47917747],
       [7.94201136, 8.40172386],
       [7.02320147, 7.04440165],
       [6.98825741, 7.55171108],
       [6.87841225, 7.64869213],
       [6.67810583, 7.0105648 ],
       [6.66595078, 7.05927324],
       [6.4976778 , 7.06921434],
       [6.43133879, 7.02474546],
       [6.20112467, 7.12588501],
       [6.12772846, 7.79863453],
       [6.09627581, 7.14458418],
       [6.00675631, 6.65506077],
       [5.55470753, 8.66701889],
       [5.42622948, 5.56234264],
       [5.33825779, 8.28995323],
       [5.2637763 , 6.90534782],
       [5.22002363, 8.47965908],
       [4.88704586, 7.33376503],
       [4.79207516, 4.89236259],
       [4.50999308, 7.82049179],
       [4.22526789, 7.96383667],
       [4.0237689 , 4.20440292],
       [3.88647294, 4.20742083],
       [3.84082723, 4.73024035],
       [3.65951896, 8.68292522]]), array([[ 9.63021755, 10.27188873],
       [ 9.32225227,  9.41523552],
       [ 9.1857214 , 10.5135088 ],
       [ 9.05552673,  9.05947399],
       [ 8.38491344,  8.50446606],
       [ 7.67676306,  7.79054546]])]2024-03-06 17:48:46.876174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6JI5 ph vector generated, counter: 58
2024-03-06 17:48:50.795671: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:50.838214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:51.896025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JJR ph vector generated, counter: 59
2024-03-06 17:48:55.163495: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:55.206851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:48:56.188474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JK6 ph vector generated, counter: 60
2024-03-06 17:48:59.816943: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:48:59.859415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:00.733197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JKE ph vector generated, counter: 61
2024-03-06 17:49:03.869959: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:03.914706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:04.825443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JNR ph vector generated, counter: 62
2024-03-06 17:49:07.949451: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:07.992917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:08.911528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JOL ph vector generated, counter: 63
2024-03-06 17:49:12.041238: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:12.083584: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:13.154833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JOU ph vector generated, counter: 64
2024-03-06 17:49:16.365493: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:16.408256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:17.297581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JPE ph vector generated, counter: 65
2024-03-06 17:49:20.434343: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:20.476658: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:21.357001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JT4 ph vector generated, counter: 66
2024-03-06 17:49:24.585459: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:24.627623: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:25.712643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JUU ph vector generated, counter: 67
2024-03-06 17:49:28.952142: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:28.994028: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:30.064140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JUX ph vector generated, counter: 68
2024-03-06 17:49:33.424503: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:33.467366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:34.502095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JVO ph vector generated, counter: 69
2024-03-06 17:49:37.837084: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:37.880473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:38.840023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.322856 ), (0., 1.3246607), (0., 1.3250475), (0., 1.325356 ),
       (0., 1.3253669), (0., 1.3253891), (0., 1.3255084), (0., 1.3255808),
       (0., 1.3274047), (0., 1.3274384), (0., 1.3274406), (0., 1.3274605),
       (0., 1.3279191), (0., 1.3279357), (0., 1.3279418), (0., 1.3281608),
       (0., 1.3286731), (0., 1.3288503), (0., 1.3289055), (0., 1.3290796),
       (0., 1.3291142), (0., 1.3292599), (0., 1.3293083), (0., 1.3294555),
       (0., 1.329497 ), (0., 1.3295153), (0., 1.3296542), (0., 1.3297379),
       (0., 1.3297871), (0., 1.3299067), (0., 1.3299353), (0., 1.330311 ),
       (0., 1.3303349), (0., 1.3303504), (0., 1.3303736), (0., 1.330435 ),
       (0., 1.3305414), (0., 1.3305476), (0., 1.3307943), (0., 1.3309692),
       (0., 1.3310051), (0., 1.3311746), (0., 1.3313867), (0., 1.3314424),
       (0., 1.3316463), (0., 1.3317376), (0., 1.3317822), (0., 1.3318124),
       (0., 1.3321038), (0., 1.332453 ), (0., 1.3324984), (0., 1.332665 ),
       (0., 1.3328162), (0., 1.3328853), (0., 1.3331974), (0., 1.3332105),
       (0., 1.333226 ), (0., 1.3333522), (0., 1.3335319), (0., 1.3337351),
       (0., 1.33398  ), (0., 1.3341485), (0., 1.3341768), (0., 1.3343381),
       (0., 1.334352 ), (0., 1.3346646), (0., 1.334756 ), (0., 1.3347898),
       (0., 1.3349719), (0., 1.3349942), (0., 1.3350515), (0., 1.3352002),
       (0., 1.3352833), (0., 1.3354859), (0., 1.3356475), (0., 1.3360426),
       (0., 1.3360523), (0., 1.3361592), (0., 1.3364487), (0., 1.3365837),
       (0., 1.3371931), (0., 1.337558 ), (0., 1.33787  ), (0., 1.3385861),
       (0., 1.3390156), (0., 1.3408396), (0., 1.3531836), (0., 1.380805 ),
       (0., 1.3904201), (0., 1.4128152), (0., 1.4157809), (0., 1.4161441),
       (0., 1.4221073), (0., 1.4364824), (0., 1.4391176), (0., 1.4429822),
       (0., 1.4446818), (0., 1.4453036), (0., 1.446058 ), (0., 1.4464214),
       (0., 1.4469742), (0., 1.4483184), (0., 1.4484035), (0., 1.450005 ),
       (0., 1.4513762), (0., 1.4517313), (0., 1.4517809), (0., 1.4519032),
       (0., 1.4519767), (0., 1.4521807), (0., 1.4522183), (0., 1.4524701),
       (0., 1.4527714), (0., 1.452972 ), (0., 1.453656 ), (0., 1.453801 ),
       (0., 1.4543132), (0., 1.4543549), (0., 1.4544502), (0., 1.4545925),
       (0., 1.4546833), (0., 1.4548862), (0., 1.4549342), (0., 1.4553779),
       (0., 1.4554135), (0., 1.4555372), (0., 1.4555876), (0., 1.4557824),
       (0., 1.4559323), (0., 1.4561659), (0., 1.4564157), (0., 1.4564463),
       (0., 1.4564755), (0., 1.456545 ), (0., 1.456558 ), (0., 1.4566079),
       (0., 1.4566925), (0., 1.4567047), (0., 1.4568548), (0., 1.4569407),
       (0., 1.4570626), (0., 1.4571402), (0., 1.4572004), (0., 1.4573274),
       (0., 1.4575267), (0., 1.4575443), (0., 1.4575874), (0., 1.4578809),
       (0., 1.4579716), (0., 1.4582703), (0., 1.4582736), (0., 1.45833  ),
       (0., 1.4585565), (0., 1.4585963), (0., 1.4586334), (0., 1.4587126),
       (0., 1.458998 ), (0., 1.4590584), (0., 1.4594016), (0., 1.4596024),
       (0., 1.4596395), (0., 1.4597045), (0., 1.4597379), (0., 1.4597605),
       (0., 1.4598815), (0., 1.4601189), (0., 1.4601235), (0., 1.4602506),
       (0., 1.4604201), (0., 1.4604728), (0., 1.4610908), (0., 1.4611961),
       (0., 1.4613127), (0., 1.4617437), (0., 1.4620585), (0., 1.4620601),
       (0., 1.4622977), (0., 1.4623362), (0., 1.4624264), (0., 1.463513 ),
       (0., 1.4674469), (0., 1.4682987), (0., 1.469615 ), (0., 1.4736556),
       (0., 1.4918954), (0., 1.4931693), (0., 1.50013  ), (0., 1.5066426),
       (0., 1.5079359), (0., 1.5088475), (0., 1.5115789), (0., 1.5131189),
       (0., 1.515306 ), (0., 1.5161823), (0., 1.516488 ), (0., 1.5167599),
       (0., 1.5173352), (0., 1.5176731), (0., 1.5179273), (0., 1.5179776),
       (0., 1.5182266), (0., 1.5183159), (0., 1.5187206), (0., 1.5189555),
       (0., 1.5190284), (0., 1.5197004), (0., 1.5197064), (0., 1.5198345),
       (0., 1.5200821), (0., 1.5203253), (0., 1.5204074), (0., 1.5205866),
       (0., 1.5206766), (0., 1.5206982), (0., 1.5207835), (0., 1.5208081),
       (0., 1.5208083), (0., 1.5211153), (0., 1.5212736), (0., 1.5212747),
       (0., 1.5212905), (0., 1.5213064), (0., 1.5213348), (0., 1.5214088),
       (0., 1.5214424), (0., 1.5216886), (0., 1.5217217), (0., 1.5217476),
       (0., 1.5217854), (0., 1.5218737), (0., 1.5220761), (0., 1.522257 ),
       (0., 1.5222709), (0., 1.5225334), (0., 1.5226358), (0., 1.5226836),
       (0., 1.5226921), (0., 1.5228705), (0., 1.5231318), (0., 1.5232515),
       (0., 1.5233914), (0., 1.5234221), (0., 1.5234473), (0., 1.5235014),
       (0., 1.5236509), (0., 1.5237092), (0., 1.5238116), (0., 1.5240173),
       (0., 1.524134 ), (0., 1.5241541), (0., 1.5244919), (0., 1.5245831),
       (0., 1.5246572), (0., 1.5248177), (0., 1.5249597), (0., 1.5249645),
       (0., 1.5249997), (0., 1.5250053), (0., 1.525059 ), (0., 1.5252275),
       (0., 1.5252898), (0., 1.5254688), (0., 1.5255013), (0., 1.5256132),
       (0., 1.5259155), (0., 1.5261772), (0., 1.526314 ), (0., 1.5264614),
       (0., 1.5269682), (0., 1.5271177), (0., 1.5273787), (0., 1.5278342),
       (0., 1.5278362), (0., 1.5280126), (0., 1.5285238), (0., 1.5288558),
       (0., 1.5294434), (0., 1.5295786), (0., 1.5306063), (0., 1.5332513),
       (0., 1.5375732), (0., 1.5483031), (0., 1.5500269), (0., 1.5539595),
       (0., 1.5551968), (0., 1.5631541), (0., 1.5656606), (0., 1.5725557),
       (0., 1.574919 ), (0., 1.5751817), (0., 1.5774475), (0., 1.5797071),
       (0., 1.5804338), (0., 1.5812991), (0., 1.5814644), (0., 1.5829871),
       (0., 1.5832582), (0., 1.5840493), (0., 1.5847847), (0., 1.5855039),
       (0., 1.5857186), (0., 1.587122 ), (0., 1.5884155), (0., 1.5885136),
       (0., 1.588759 ), (0., 1.5903263), (0., 1.590733 ), (0., 1.590997 ),
       (0., 1.5917603), (0., 1.592696 ), (0., 1.5927426), (0., 1.5938984),
       (0., 1.5942945), (0., 1.5962667), (0., 1.5969626), (0., 1.5984375),
       (0., 1.5988382), (0., 1.6146463), (0., 1.6156064), (0., 1.6169364),
       (0., 1.6181934), (0., 1.6228877), (0., 1.6449926), (0., 1.6555333),
       (0., 1.6797842), (0., 1.6832527), (0., 1.6841973), (0., 1.6865631),
       (0., 1.6940852), (0., 1.6944821), (0., 1.6947442), (0., 1.6952813),
       (0., 1.6965733), (0., 1.6967807), (0., 1.6970593), (0., 1.6970651),
       (0., 1.6993321), (0., 1.7000724), (0., 1.7030532), (0., 1.7043487),
       (0., 1.7047263), (0., 1.7059462), (0., 1.7071755), (0., 1.7079675),
       (0., 1.7085375), (0., 1.7098958), (0., 1.7108252), (0., 1.7108948),
       (0., 1.7169845), (0., 1.7187638), (0., 1.722203 ), (0., 1.72461  ),
       (0., 1.7248037), (0., 1.7303386), (0., 1.7313368), (0., 1.748179 ),
       (0., 1.7553487), (0., 1.7556511), (0., 1.7574602), (0., 1.7593955),
       (0., 1.7599018), (0., 1.7624131), (0., 1.7640276), (0., 1.7647215),
       (0., 1.7648858), (0., 1.7659587), (0., 1.7665938), (0., 1.7667344),
       (0., 1.770812 ), (0., 1.7729422), (0., 1.7749132), (0., 1.77555  ),
       (0., 1.7785095), (0., 1.7796934), (0., 1.7802854), (0., 1.7809885),
       (0., 1.7825271), (0., 1.7849418), (0., 1.7860196), (0., 1.786596 ),
       (0., 1.7904997), (0., 1.7929807), (0., 1.8007722)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(7.046316 , 7.420981 ), (6.4859548, 6.586089 ),
       (5.677265 , 5.690634 ), (5.4861956, 6.0061307),
       (5.457804 , 5.655609 ), (5.272144 , 5.6157923),
       (4.7995944, 6.3262353), (4.5298524, 5.246265 ),
       (4.475683 , 7.132227 ), (4.458396 , 4.695331 ),
       (4.3402166, 4.3607345), (4.262748 , 4.4528375),
       (4.1906147, 4.5203915), (4.1739283, 4.4055805),
       (4.166182 , 4.2181873), (4.131566 , 4.560526 ),
       (4.0626974, 4.4310484), (4.046838 , 4.1204767),
       (4.044769 , 4.4145565), (4.0379224, 4.2927637),
       (4.0089536, 4.1392007), (4.00161  , 4.5325103),
       (3.9996078, 4.6433144), (3.997571 , 4.7233467),
       (3.9805672, 4.788835 ), (3.9566128, 4.144624 ),
       (3.922282 , 4.4638877), (3.9208565, 4.8631926),
       (3.8996754, 4.3166504), (3.8949366, 5.4613743),
       (3.8929787, 4.7525063), (3.8400354, 3.9368093),
       (3.8208294, 4.871897 ), (3.7927995, 7.5697107),
       (3.761796 , 7.2376637)], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.555832, 10.572051 ), (5.660768,  5.8793545)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3228559494018555), (0.0, 1.3246606588363647), (0.0, 1.325047492980957), (0.0, 1.3253560066223145), (0.0, 1.3253668546676636), (0.0, 1.3253891468048096), (0.0, 1.3255083560943604), (0.0, 1.3255808353424072), (0.0, 1.3274047374725342), (0.0, 1.3274383544921875), (0.0, 1.327440619468689), (0.0, 1.327460527420044), (0.0, 1.3279191255569458), (0.0, 1.3279356956481934), (0.0, 1.3279417753219604), (0.0, 1.3281607627868652), (0.0, 1.3286731243133545), (0.0, 1.328850269317627), (0.0, 1.328905463218689), (0.0, 1.3290796279907227), (0.0, 1.3291141986846924), (0.0, 1.3292598724365234), (0.0, 1.329308271408081), (0.0, 1.3294554948806763), (0.0, 1.32949697971344), (0.0, 1.3295153379440308), (0.0, 1.3296542167663574), (0.0, 1.329737901687622), (0.0, 1.3297871351242065), (0.0, 1.329906702041626), (0.0, 1.3299353122711182), (0.0, 1.3303109407424927), (0.0, 1.3303349018096924), (0.0, 1.330350399017334), (0.0, 1.3303736448287964), (0.0, 1.330435037612915), (0.0, 1.3305413722991943), (0.0, 1.330547571182251), (0.0, 1.330794334411621), (0.0, 1.330969214439392), (0.0, 1.3310050964355469), (0.0, 1.331174612045288), (0.0, 1.331386685371399), (0.0, 1.3314423561096191), (0.0, 1.3316463232040405), (0.0, 1.3317376375198364), (0.0, 1.3317822217941284), (0.0, 1.3318123817443848), (0.0, 1.3321038484573364), (0.0, 1.3324530124664307), (0.0, 1.3324984312057495), (0.0, 1.332664966583252), (0.0, 1.332816243171692), (0.0, 1.3328852653503418), (0.0, 1.3331973552703857), (0.0, 1.3332104682922363), (0.0, 1.333225965499878), (0.0, 1.3333522081375122), (0.0, 1.3335318565368652), (0.0, 1.3337351083755493), (0.0, 1.3339799642562866), (0.0, 1.3341485261917114), (0.0, 1.334176778793335), (0.0, 1.3343380689620972), (0.0, 1.3343520164489746), (0.0, 1.3346645832061768), (0.0, 1.3347560167312622), (0.0, 1.334789752960205), (0.0, 1.3349719047546387), (0.0, 1.3349941968917847), (0.0, 1.3350515365600586), (0.0, 1.3352001905441284), (0.0, 1.3352832794189453), (0.0, 1.3354859352111816), (0.0, 1.335647463798523), (0.0, 1.3360426425933838), (0.0, 1.3360522985458374), (0.0, 1.3361592292785645), (0.0, 1.3364486694335938), (0.0, 1.3365837335586548), (0.0, 1.3371931314468384), (0.0, 1.3375580310821533), (0.0, 1.3378700017929077), (0.0, 1.3385860919952393), (0.0, 1.3390156030654907), (0.0, 1.3408396244049072), (0.0, 1.353183627128601), (0.0, 1.3808050155639648), (0.0, 1.3904200792312622), (0.0, 1.4128152132034302), (0.0, 1.4157809019088745), (0.0, 1.4161441326141357), (0.0, 1.4221073389053345), (0.0, 1.4364824295043945), (0.0, 1.4391175508499146), (0.0, 1.4429821968078613), (0.0, 1.4446817636489868), (0.0, 1.4453035593032837), (0.0, 1.4460580348968506), (0.0, 1.4464213848114014), (0.0, 1.4469741582870483), (0.0, 1.448318362236023), (0.0, 1.4484034776687622), (0.0, 1.450005054473877), (0.0, 1.45137619972229), (0.0, 1.4517313241958618), (0.0, 1.451780915260315), (0.0, 1.451903223991394), (0.0, 1.4519766569137573), (0.0, 1.4521807432174683), (0.0, 1.4522182941436768), (0.0, 1.452470064163208), (0.0, 1.4527714252471924), (0.0, 1.4529720544815063), (0.0, 1.4536559581756592), (0.0, 1.4538010358810425), (0.0, 1.4543131589889526), (0.0, 1.4543548822402954), (0.0, 1.454450249671936), (0.0, 1.4545924663543701), (0.0, 1.4546833038330078), (0.0, 1.4548861980438232), (0.0, 1.4549342393875122), (0.0, 1.4553779363632202), (0.0, 1.4554134607315063), (0.0, 1.45553719997406), (0.0, 1.45558762550354), (0.0, 1.455782413482666), (0.0, 1.4559322595596313), (0.0, 1.4561659097671509), (0.0, 1.4564156532287598), (0.0, 1.4564462900161743), (0.0, 1.4564754962921143), (0.0, 1.4565449953079224), (0.0, 1.4565579891204834), (0.0, 1.4566079378128052), (0.0, 1.4566924571990967), (0.0, 1.4567047357559204), (0.0, 1.4568548202514648), (0.0, 1.4569406509399414), (0.0, 1.4570626020431519), (0.0, 1.4571402072906494), (0.0, 1.4572004079818726), (0.0, 1.4573273658752441), (0.0, 1.457526683807373), (0.0, 1.4575443267822266), (0.0, 1.4575873613357544), (0.0, 1.4578808546066284), (0.0, 1.4579715728759766), (0.0, 1.4582703113555908), (0.0, 1.4582736492156982), (0.0, 1.4583300352096558), (0.0, 1.4585565328598022), (0.0, 1.4585963487625122), (0.0, 1.4586334228515625), (0.0, 1.4587125778198242), (0.0, 1.4589979648590088), (0.0, 1.459058403968811), (0.0, 1.4594016075134277), (0.0, 1.4596023559570312), (0.0, 1.459639549255371), (0.0, 1.4597045183181763), (0.0, 1.4597378969192505), (0.0, 1.4597605466842651), (0.0, 1.4598815441131592), (0.0, 1.4601188898086548), (0.0, 1.4601235389709473), (0.0, 1.4602506160736084), (0.0, 1.4604201316833496), (0.0, 1.460472822189331), (0.0, 1.4610908031463623), (0.0, 1.4611960649490356), (0.0, 1.4613126516342163), (0.0, 1.461743712425232), (0.0, 1.4620585441589355), (0.0, 1.4620600938796997), (0.0, 1.4622976779937744), (0.0, 1.4623361825942993), (0.0, 1.4624264240264893), (0.0, 1.4635130167007446), (0.0, 1.4674469232559204), (0.0, 1.4682986736297607), (0.0, 1.4696149826049805), (0.0, 1.4736555814743042), (0.0, 1.4918954372406006), (0.0, 1.4931693077087402), (0.0, 1.5001300573349), (0.0, 1.5066425800323486), (0.0, 1.507935881614685), (0.0, 1.5088474750518799), (0.0, 1.511578917503357), (0.0, 1.513118863105774), (0.0, 1.515305995941162), (0.0, 1.51618230342865), (0.0, 1.516487956047058), (0.0, 1.5167598724365234), (0.0, 1.5173351764678955), (0.0, 1.517673134803772), (0.0, 1.5179272890090942), (0.0, 1.5179775953292847), (0.0, 1.5182266235351562), (0.0, 1.5183159112930298), (0.0, 1.5187206268310547), (0.0, 1.5189554691314697), (0.0, 1.5190284252166748), (0.0, 1.5197004079818726), (0.0, 1.51970636844635), (0.0, 1.5198345184326172), (0.0, 1.5200821161270142), (0.0, 1.5203253030776978), (0.0, 1.5204074382781982), (0.0, 1.520586609840393), (0.0, 1.520676612854004), (0.0, 1.5206981897354126), (0.0, 1.520783543586731), (0.0, 1.5208081007003784), (0.0, 1.5208083391189575), (0.0, 1.5211153030395508), (0.0, 1.5212736129760742), (0.0, 1.5212746858596802), (0.0, 1.5212905406951904), (0.0, 1.5213063955307007), (0.0, 1.5213347673416138), (0.0, 1.5214087963104248), (0.0, 1.5214424133300781), (0.0, 1.5216885805130005), (0.0, 1.5217217206954956), (0.0, 1.5217475891113281), (0.0, 1.5217853784561157), (0.0, 1.5218737125396729), (0.0, 1.52207612991333), (0.0, 1.5222569704055786), (0.0, 1.522270917892456), (0.0, 1.5225334167480469), (0.0, 1.522635817527771), (0.0, 1.5226836204528809), (0.0, 1.522692084312439), (0.0, 1.5228705406188965), (0.0, 1.5231318473815918), (0.0, 1.5232515335083008), (0.0, 1.5233913660049438), (0.0, 1.523422122001648), (0.0, 1.5234472751617432), (0.0, 1.5235013961791992), (0.0, 1.523650884628296), (0.0, 1.5237091779708862), (0.0, 1.5238115787506104), (0.0, 1.524017333984375), (0.0, 1.5241340398788452), (0.0, 1.5241540670394897), (0.0, 1.5244919061660767), (0.0, 1.524583101272583), (0.0, 1.5246572494506836), (0.0, 1.524817705154419), (0.0, 1.524959683418274), (0.0, 1.524964451789856), (0.0, 1.524999737739563), (0.0, 1.5250053405761719), (0.0, 1.5250589847564697), (0.0, 1.5252275466918945), (0.0, 1.52528977394104), (0.0, 1.5254688262939453), (0.0, 1.5255012512207031), (0.0, 1.5256131887435913), (0.0, 1.525915503501892), (0.0, 1.526177167892456), (0.0, 1.5263140201568604), (0.0, 1.5264613628387451), (0.0, 1.526968240737915), (0.0, 1.5271177291870117), (0.0, 1.5273786783218384), (0.0, 1.527834177017212), (0.0, 1.5278362035751343), (0.0, 1.5280126333236694), (0.0, 1.5285238027572632), (0.0, 1.528855800628662), (0.0, 1.529443383216858), (0.0, 1.5295785665512085), (0.0, 1.5306062698364258), (0.0, 1.5332512855529785), (0.0, 1.537573218345642), (0.0, 1.5483031272888184), (0.0, 1.5500268936157227), (0.0, 1.5539594888687134), (0.0, 1.555196762084961), (0.0, 1.5631541013717651), (0.0, 1.5656605958938599), (0.0, 1.572555661201477), (0.0, 1.5749189853668213), (0.0, 1.5751817226409912), (0.0, 1.577447533607483), (0.0, 1.579707145690918), (0.0, 1.5804338455200195), (0.0, 1.581299066543579), (0.0, 1.581464409828186), (0.0, 1.5829870700836182), (0.0, 1.5832581520080566), (0.0, 1.5840493440628052), (0.0, 1.584784746170044), (0.0, 1.5855039358139038), (0.0, 1.5857186317443848), (0.0, 1.5871219635009766), (0.0, 1.588415503501892), (0.0, 1.5885136127471924), (0.0, 1.588758945465088), (0.0, 1.5903263092041016), (0.0, 1.5907330513000488), (0.0, 1.5909969806671143), (0.0, 1.591760277748108), (0.0, 1.592695951461792), (0.0, 1.5927425622940063), (0.0, 1.5938984155654907), (0.0, 1.594294548034668), (0.0, 1.596266746520996), (0.0, 1.596962571144104), (0.0, 1.5984375476837158), (0.0, 1.598838210105896), (0.0, 1.614646315574646), (0.0, 1.615606427192688), (0.0, 1.616936445236206), (0.0, 1.6181933879852295), (0.0, 1.6228877305984497), (0.0, 1.6449925899505615), (0.0, 1.6555333137512207), (0.0, 1.6797841787338257), (0.0, 1.6832526922225952), (0.0, 1.6841973066329956), (0.0, 1.6865631341934204), (0.0, 1.6940852403640747), (0.0, 1.6944820880889893), (0.0, 1.6947442293167114), (0.0, 1.6952812671661377), (0.0, 1.696573257446289), (0.0, 1.6967806816101074), (0.0, 1.6970592737197876), (0.0, 1.6970651149749756), (0.0, 1.6993321180343628), (0.0, 1.7000724077224731), (0.0, 1.7030532360076904), (0.0, 1.7043486833572388), (0.0, 1.7047263383865356), (0.0, 1.7059462070465088), (0.0, 1.7071754932403564), (0.0, 1.7079675197601318), (0.0, 1.7085374593734741), (0.0, 1.7098958492279053), (0.0, 1.7108252048492432), (0.0, 1.7108948230743408), (0.0, 1.716984510421753), (0.0, 1.718763828277588), (0.0, 1.722203016281128), (0.0, 1.7246099710464478), (0.0, 1.7248036861419678), (0.0, 1.7303385734558105), (0.0, 1.7313368320465088), (0.0, 1.7481789588928223), (0.0, 1.7553486824035645), (0.0, 1.7556511163711548), (0.0, 1.7574602365493774), (0.0, 1.7593954801559448), (0.0, 1.759901762008667), (0.0, 1.7624131441116333), (0.0, 1.7640275955200195), (0.0, 1.7647215127944946), (0.0, 1.7648857831954956), (0.0, 1.7659586668014526), (0.0, 1.7665938138961792), (0.0, 1.7667343616485596), (0.0, 1.7708120346069336), (0.0, 1.7729421854019165), (0.0, 1.7749131917953491), (0.0, 1.7755500078201294), (0.0, 1.778509497642517), (0.0, 1.779693365097046), (0.0, 1.780285358428955), (0.0, 1.7809884548187256), (0.0, 1.7825270891189575), (0.0, 1.7849417924880981), (0.0, 1.7860195636749268), (0.0, 1.7865959405899048), (0.0, 1.7904996871948242), (0.0, 1.792980670928955), (0.0, 1.8007721900939941)], [(7.046316146850586, 7.420980930328369), (6.485954761505127, 6.586089134216309), (5.677265167236328, 5.690633773803711), (5.4861955642700195, 6.006130695343018), (5.457804203033447, 5.655609130859375), (5.272143840789795, 5.615792274475098), (4.799594402313232, 6.326235294342041), (4.529852390289307, 5.246264934539795), (4.475683212280273, 7.132226943969727), (4.458395957946777, 4.69533109664917), (4.340216636657715, 4.360734462738037), (4.262747764587402, 4.4528374671936035), (4.190614700317383, 4.520391464233398), (4.173928260803223, 4.405580520629883), (4.166182041168213, 4.21818733215332), (4.131566047668457, 4.560525894165039), (4.062697410583496, 4.431048393249512), (4.04683780670166, 4.120476722717285), (4.044768810272217, 4.414556503295898), (4.037922382354736, 4.292763710021973), (4.00895357131958, 4.139200687408447), (4.001609802246094, 4.532510280609131), (3.999607801437378, 4.643314361572266), (3.9975709915161133, 4.723346710205078), (3.980567216873169, 4.788835048675537), (3.9566128253936768, 4.14462423324585), (3.9222819805145264, 4.463887691497803), (3.920856475830078, 4.863192558288574), (3.8996753692626953, 4.316650390625), (3.8949365615844727, 5.461374282836914), (3.8929786682128906, 4.752506256103516), (3.8400354385375977, 3.9368093013763428), (3.820829391479492, 4.871897220611572), (3.792799472808838, 7.569710731506348), (3.761795997619629, 7.237663745880127)], [(9.555831909179688, 10.572051048278809), (5.660768032073975, 5.879354476928711)]]
[array([[0.        , 1.32285595],
       [0.        , 1.32466066],
       [0.        , 1.32504749],
       [0.        , 1.32535601],
       [0.        , 1.32536685],
       [0.        , 1.32538915],
       [0.        , 1.32550836],
       [0.        , 1.32558084],
       [0.        , 1.32740474],
       [0.        , 1.32743835],
       [0.        , 1.32744062],
       [0.        , 1.32746053],
       [0.        , 1.32791913],
       [0.        , 1.3279357 ],
       [0.        , 1.32794178],
       [0.        , 1.32816076],
       [0.        , 1.32867312],
       [0.        , 1.32885027],
       [0.        , 1.32890546],
       [0.        , 1.32907963],
       [0.        , 1.3291142 ],
       [0.        , 1.32925987],
       [0.        , 1.32930827],
       [0.        , 1.32945549],
       [0.        , 1.32949698],
       [0.        , 1.32951534],
       [0.        , 1.32965422],
       [0.        , 1.3297379 ],
       [0.        , 1.32978714],
       [0.        , 1.3299067 ],
       [0.        , 1.32993531],
       [0.        , 1.33031094],
       [0.        , 1.3303349 ],
       [0.        , 1.3303504 ],
       [0.        , 1.33037364],
       [0.        , 1.33043504],
       [0.        , 1.33054137],
       [0.        , 1.33054757],
       [0.        , 1.33079433],
       [0.        , 1.33096921],
       [0.        , 1.3310051 ],
       [0.        , 1.33117461],
       [0.        , 1.33138669],
       [0.        , 1.33144236],
       [0.        , 1.33164632],
       [0.        , 1.33173764],
       [0.        , 1.33178222],
       [0.        , 1.33181238],
       [0.        , 1.33210385],
       [0.        , 1.33245301],
       [0.        , 1.33249843],
       [0.        , 1.33266497],
       [0.        , 1.33281624],
       [0.        , 1.33288527],
       [0.        , 1.33319736],
       [0.        , 1.33321047],
       [0.        , 1.33322597],
       [0.        , 1.33335221],
       [0.        , 1.33353186],
       [0.        , 1.33373511],
       [0.        , 1.33397996],
       [0.        , 1.33414853],
       [0.        , 1.33417678],
       [0.        , 1.33433807],
       [0.        , 1.33435202],
       [0.        , 1.33466458],
       [0.        , 1.33475602],
       [0.        , 1.33478975],
       [0.        , 1.3349719 ],
       [0.        , 1.3349942 ],
       [0.        , 1.33505154],
       [0.        , 1.33520019],
       [0.        , 1.33528328],
       [0.        , 1.33548594],
       [0.        , 1.33564746],
       [0.        , 1.33604264],
       [0.        , 1.3360523 ],
       [0.        , 1.33615923],
       [0.        , 1.33644867],
       [0.        , 1.33658373],
       [0.        , 1.33719313],
       [0.        , 1.33755803],
       [0.        , 1.33787   ],
       [0.        , 1.33858609],
       [0.        , 1.3390156 ],
       [0.        , 1.34083962],
       [0.        , 1.35318363],
       [0.        , 1.38080502],
       [0.        , 1.39042008],
       [0.        , 1.41281521],
       [0.        , 1.4157809 ],
       [0.        , 1.41614413],
       [0.        , 1.42210734],
       [0.        , 1.43648243],
       [0.        , 1.43911755],
       [0.        , 1.4429822 ],
       [0.        , 1.44468176],
       [0.        , 1.44530356],
       [0.        , 1.44605803],
       [0.        , 1.44642138],
       [0.        , 1.44697416],
       [0.        , 1.44831836],
       [0.        , 1.44840348],
       [0.        , 1.45000505],
       [0.        , 1.4513762 ],
       [0.        , 1.45173132],
       [0.        , 1.45178092],
       [0.        , 1.45190322],
       [0.        , 1.45197666],
       [0.        , 1.45218074],
       [0.        , 1.45221829],
       [0.        , 1.45247006],
       [0.        , 1.45277143],
       [0.        , 1.45297205],
       [0.        , 1.45365596],
       [0.        , 1.45380104],
       [0.        , 1.45431316],
       [0.        , 1.45435488],
       [0.        , 1.45445025],
       [0.        , 1.45459247],
       [0.        , 1.4546833 ],
       [0.        , 1.4548862 ],
       [0.        , 1.45493424],
       [0.        , 1.45537794],
       [0.        , 1.45541346],
       [0.        , 1.4555372 ],
       [0.        , 1.45558763],
       [0.        , 1.45578241],
       [0.        , 1.45593226],
       [0.        , 1.45616591],
       [0.        , 1.45641565],
       [0.        , 1.45644629],
       [0.        , 1.4564755 ],
       [0.        , 1.456545  ],
       [0.        , 1.45655799],
       [0.        , 1.45660794],
       [0.        , 1.45669246],
       [0.        , 1.45670474],
       [0.        , 1.45685482],
       [0.        , 1.45694065],
       [0.        , 1.4570626 ],
       [0.        , 1.45714021],
       [0.        , 1.45720041],
       [0.        , 1.45732737],
       [0.        , 1.45752668],
       [0.        , 1.45754433],
       [0.        , 1.45758736],
       [0.        , 1.45788085],
       [0.        , 1.45797157],
       [0.        , 1.45827031],
       [0.        , 1.45827365],
       [0.        , 1.45833004],
       [0.        , 1.45855653],
       [0.        , 1.45859635],
       [0.        , 1.45863342],
       [0.        , 1.45871258],
       [0.        , 1.45899796],
       [0.        , 1.4590584 ],
       [0.        , 1.45940161],
       [0.        , 1.45960236],
       [0.        , 1.45963955],
       [0.        , 1.45970452],
       [0.        , 1.4597379 ],
       [0.        , 1.45976055],
       [0.        , 1.45988154],
       [0.        , 1.46011889],
       [0.        , 1.46012354],
       [0.        , 1.46025062],
       [0.        , 1.46042013],
       [0.        , 1.46047282],
       [0.        , 1.4610908 ],
       [0.        , 1.46119606],
       [0.        , 1.46131265],
       [0.        , 1.46174371],
       [0.        , 1.46205854],
       [0.        , 1.46206009],
       [0.        , 1.46229768],
       [0.        , 1.46233618],
       [0.        , 1.46242642],
       [0.        , 1.46351302],
       [0.        , 1.46744692],
       [0.        , 1.46829867],
       [0.        , 1.46961498],
       [0.        , 1.47365558],
       [0.        , 1.49189544],
       [0.        , 1.49316931],
       [0.        , 1.50013006],
       [0.        , 1.50664258],
       [0.        , 1.50793588],
       [0.        , 1.50884748],
       [0.        , 1.51157892],
       [0.        , 1.51311886],
       [0.        , 1.515306  ],
       [0.        , 1.5161823 ],
       [0.        , 1.51648796],
       [0.        , 1.51675987],
       [0.        , 1.51733518],
       [0.        , 1.51767313],
       [0.        , 1.51792729],
       [0.        , 1.5179776 ],
       [0.        , 1.51822662],
       [0.        , 1.51831591],
       [0.        , 1.51872063],
       [0.        , 1.51895547],
       [0.        , 1.51902843],
       [0.        , 1.51970041],
       [0.        , 1.51970637],
       [0.        , 1.51983452],
       [0.        , 1.52008212],
       [0.        , 1.5203253 ],
       [0.        , 1.52040744],
       [0.        , 1.52058661],
       [0.        , 1.52067661],
       [0.        , 1.52069819],
       [0.        , 1.52078354],
       [0.        , 1.5208081 ],
       [0.        , 1.52080834],
       [0.        , 1.5211153 ],
       [0.        , 1.52127361],
       [0.        , 1.52127469],
       [0.        , 1.52129054],
       [0.        , 1.5213064 ],
       [0.        , 1.52133477],
       [0.        , 1.5214088 ],
       [0.        , 1.52144241],
       [0.        , 1.52168858],
       [0.        , 1.52172172],
       [0.        , 1.52174759],
       [0.        , 1.52178538],
       [0.        , 1.52187371],
       [0.        , 1.52207613],
       [0.        , 1.52225697],
       [0.        , 1.52227092],
       [0.        , 1.52253342],
       [0.        , 1.52263582],
       [0.        , 1.52268362],
       [0.        , 1.52269208],
       [0.        , 1.52287054],
       [0.        , 1.52313185],
       [0.        , 1.52325153],
       [0.        , 1.52339137],
       [0.        , 1.52342212],
       [0.        , 1.52344728],
       [0.        , 1.5235014 ],
       [0.        , 1.52365088],
       [0.        , 1.52370918],
       [0.        , 1.52381158],
       [0.        , 1.52401733],
       [0.        , 1.52413404],
       [0.        , 1.52415407],
       [0.        , 1.52449191],
       [0.        , 1.5245831 ],
       [0.        , 1.52465725],
       [0.        , 1.52481771],
       [0.        , 1.52495968],
       [0.        , 1.52496445],
       [0.        , 1.52499974],
       [0.        , 1.52500534],
       [0.        , 1.52505898],
       [0.        , 1.52522755],
       [0.        , 1.52528977],
       [0.        , 1.52546883],
       [0.        , 1.52550125],
       [0.        , 1.52561319],
       [0.        , 1.5259155 ],
       [0.        , 1.52617717],
       [0.        , 1.52631402],
       [0.        , 1.52646136],
       [0.        , 1.52696824],
       [0.        , 1.52711773],
       [0.        , 1.52737868],
       [0.        , 1.52783418],
       [0.        , 1.5278362 ],
       [0.        , 1.52801263],
       [0.        , 1.5285238 ],
       [0.        , 1.5288558 ],
       [0.        , 1.52944338],
       [0.        , 1.52957857],
       [0.        , 1.53060627],
       [0.        , 1.53325129],
       [0.        , 1.53757322],
       [0.        , 1.54830313],
       [0.        , 1.55002689],
       [0.        , 1.55395949],
       [0.        , 1.55519676],
       [0.        , 1.5631541 ],
       [0.        , 1.5656606 ],
       [0.        , 1.57255566],
       [0.        , 1.57491899],
       [0.        , 1.57518172],
       [0.        , 1.57744753],
       [0.        , 1.57970715],
       [0.        , 1.58043385],
       [0.        , 1.58129907],
       [0.        , 1.58146441],
       [0.        , 1.58298707],
       [0.        , 1.58325815],
       [0.        , 1.58404934],
       [0.        , 1.58478475],
       [0.        , 1.58550394],
       [0.        , 1.58571863],
       [0.        , 1.58712196],
       [0.        , 1.5884155 ],
       [0.        , 1.58851361],
       [0.        , 1.58875895],
       [0.        , 1.59032631],
       [0.        , 1.59073305],
       [0.        , 1.59099698],
       [0.        , 1.59176028],
       [0.        , 1.59269595],
       [0.        , 1.59274256],
       [0.        , 1.59389842],
       [0.        , 1.59429455],
       [0.        , 1.59626675],
       [0.        , 1.59696257],
       [0.        , 1.59843755],
       [0.        , 1.59883821],
       [0.        , 1.61464632],
       [0.        , 1.61560643],
       [0.        , 1.61693645],
       [0.        , 1.61819339],
       [0.        , 1.62288773],
       [0.        , 1.64499259],
       [0.        , 1.65553331],
       [0.        , 1.67978418],
       [0.        , 1.68325269],
       [0.        , 1.68419731],
       [0.        , 1.68656313],
       [0.        , 1.69408524],
       [0.        , 1.69448209],
       [0.        , 1.69474423],
       [0.        , 1.69528127],
       [0.        , 1.69657326],
       [0.        , 1.69678068],
       [0.        , 1.69705927],
       [0.        , 1.69706511],
       [0.        , 1.69933212],
       [0.        , 1.70007241],
       [0.        , 1.70305324],
       [0.        , 1.70434868],
       [0.        , 1.70472634],
       [0.        , 1.70594621],
       [0.        , 1.70717549],
       [0.        , 1.70796752],
       [0.        , 1.70853746],
       [0.        , 1.70989585],
       [0.        , 1.7108252 ],
       [0.        , 1.71089482],
       [0.        , 1.71698451],
       [0.        , 1.71876383],
       [0.        , 1.72220302],
       [0.        , 1.72460997],
       [0.        , 1.72480369],
       [0.        , 1.73033857],
       [0.        , 1.73133683],
       [0.        , 1.74817896],
       [0.        , 1.75534868],
       [0.        , 1.75565112],
       [0.        , 1.75746024],
       [0.        , 1.75939548],
       [0.        , 1.75990176],
       [0.        , 1.76241314],
       [0.        , 1.7640276 ],
       [0.        , 1.76472151],
       [0.        , 1.76488578],
       [0.        , 1.76595867],
       [0.        , 1.76659381],
       [0.        , 1.76673436],
       [0.        , 1.77081203],
       [0.        , 1.77294219],
       [0.        , 1.77491319],
       [0.        , 1.77555001],
       [0.        , 1.7785095 ],
       [0.        , 1.77969337],
       [0.        , 1.78028536],
       [0.        , 1.78098845],
       [0.        , 1.78252709],
       [0.        , 1.78494179],
       [0.        , 1.78601956],
       [0.        , 1.78659594],
       [0.        , 1.79049969],
       [0.        , 1.79298067],
       [0.        , 1.80077219]]), array([[7.04631615, 7.42098093],
       [6.48595476, 6.58608913],
       [5.67726517, 5.69063377],
       [5.48619556, 6.0061307 ],
       [5.4578042 , 5.65560913],
       [5.27214384, 5.61579227],
       [4.7995944 , 6.32623529],
       [4.52985239, 5.24626493],
       [4.47568321, 7.13222694],
       [4.45839596, 4.6953311 ],
       [4.34021664, 4.36073446],
       [4.26274776, 4.45283747],
       [4.1906147 , 4.52039146],
       [4.17392826, 4.40558052],
       [4.16618204, 4.21818733],
       [4.13156605, 4.56052589],
       [4.06269741, 4.43104839],
       [4.04683781, 4.12047672],
       [4.04476881, 4.4145565 ],
       [4.03792238, 4.29276371],
       [4.00895357, 4.13920069],
       [4.0016098 , 4.53251028],
       [3.9996078 , 4.64331436],
       [3.99757099, 4.72334671],
       [3.98056722, 4.78883505],
       [3.95661283, 4.14462423],
       [3.92228198, 4.46388769],
       [3.92085648, 4.86319256],
       [3.89967537, 4.31665039],
       [3.89493656, 5.46137428],
       [3.89297867, 4.75250626],
       [3.84003544, 3.9368093 ],
       [3.82082939, 4.87189722],
       [3.79279947, 7.56971073],
       [3.761796  , 7.23766375]]), array([[ 9.55583191, 10.57205105],
       [ 5.66076803,  5.87935448]])]2024-03-06 17:49:42.683230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6JVX ph vector generated, counter: 70
2024-03-06 17:49:46.808327: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:46.851260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:47.928553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3195108), (0., 1.3222336), (0., 1.322473 ), (0., 1.3228793),
       (0., 1.3229972), (0., 1.3251866), (0., 1.3252918), (0., 1.3256067),
       (0., 1.3264773), (0., 1.3265611), (0., 1.3266163), (0., 1.3268284),
       (0., 1.3277414), (0., 1.3277634), (0., 1.3278714), (0., 1.3279284),
       (0., 1.3280023), (0., 1.328009 ), (0., 1.3282421), (0., 1.3283831),
       (0., 1.3284835), (0., 1.3285818), (0., 1.3291142), (0., 1.3295497),
       (0., 1.3296207), (0., 1.3296305), (0., 1.3296443), (0., 1.3296463),
       (0., 1.330091 ), (0., 1.330193 ), (0., 1.3304337), (0., 1.3305079),
       (0., 1.3306376), (0., 1.3306451), (0., 1.3307515), (0., 1.3309155),
       (0., 1.3309611), (0., 1.3310095), (0., 1.3310236), (0., 1.3311384),
       (0., 1.3311759), (0., 1.3313746), (0., 1.3314987), (0., 1.3316414),
       (0., 1.3318213), (0., 1.3318852), (0., 1.3319043), (0., 1.3320227),
       (0., 1.3320874), (0., 1.3327233), (0., 1.3331336), (0., 1.3331426),
       (0., 1.3334692), (0., 1.3337877), (0., 1.3339752), (0., 1.3340306),
       (0., 1.3340414), (0., 1.3341461), (0., 1.3350639), (0., 1.3350798),
       (0., 1.3351359), (0., 1.335348 ), (0., 1.3353913), (0., 1.3356469),
       (0., 1.3357286), (0., 1.3357381), (0., 1.3357576), (0., 1.3358417),
       (0., 1.3359811), (0., 1.3361498), (0., 1.3362039), (0., 1.3362291),
       (0., 1.3364894), (0., 1.3366956), (0., 1.3368809), (0., 1.3371624),
       (0., 1.3386714), (0., 1.3387463), (0., 1.3388894), (0., 1.3388906),
       (0., 1.3392719), (0., 1.3397198), (0., 1.3401356), (0., 1.341528 ),
       (0., 1.3433017), (0., 1.3454202), (0., 1.3596083), (0., 1.3642521),
       (0., 1.3806752), (0., 1.3918856), (0., 1.3927488), (0., 1.3959035),
       (0., 1.4127823), (0., 1.4261669), (0., 1.4412585), (0., 1.4434692),
       (0., 1.4444032), (0., 1.4446945), (0., 1.4471731), (0., 1.447656 ),
       (0., 1.4480556), (0., 1.4483224), (0., 1.4494003), (0., 1.4501905),
       (0., 1.4503858), (0., 1.4504629), (0., 1.4505264), (0., 1.451015 ),
       (0., 1.4523089), (0., 1.4523958), (0., 1.4526821), (0., 1.4528546),
       (0., 1.4536403), (0., 1.4541264), (0., 1.4544206), (0., 1.4544662),
       (0., 1.4547282), (0., 1.4549698), (0., 1.4550033), (0., 1.4552653),
       (0., 1.455695 ), (0., 1.4558101), (0., 1.4558171), (0., 1.456131 ),
       (0., 1.4564027), (0., 1.4565113), (0., 1.4567192), (0., 1.4568352),
       (0., 1.4569077), (0., 1.4571992), (0., 1.4572953), (0., 1.4574133),
       (0., 1.4575864), (0., 1.4576806), (0., 1.4577258), (0., 1.4580321),
       (0., 1.4580519), (0., 1.4580954), (0., 1.4581659), (0., 1.4584016),
       (0., 1.4589977), (0., 1.4591475), (0., 1.4592395), (0., 1.4592532),
       (0., 1.4594284), (0., 1.4594331), (0., 1.4594551), (0., 1.4594774),
       (0., 1.4595418), (0., 1.4596443), (0., 1.4598426), (0., 1.4598731),
       (0., 1.4599144), (0., 1.4600083), (0., 1.4600427), (0., 1.4601241),
       (0., 1.4603976), (0., 1.4604639), (0., 1.4604862), (0., 1.4607438),
       (0., 1.460894 ), (0., 1.4609436), (0., 1.4609524), (0., 1.4610887),
       (0., 1.4610946), (0., 1.4615029), (0., 1.4616214), (0., 1.4617175),
       (0., 1.4617581), (0., 1.4619514), (0., 1.4620008), (0., 1.4621598),
       (0., 1.4621863), (0., 1.4622204), (0., 1.4624012), (0., 1.4624326),
       (0., 1.4624408), (0., 1.4626126), (0., 1.4634858), (0., 1.4636599),
       (0., 1.4638463), (0., 1.4640332), (0., 1.4643767), (0., 1.4661015),
       (0., 1.468537 ), (0., 1.4686103), (0., 1.4692601), (0., 1.4758503),
       (0., 1.4780996), (0., 1.483611 ), (0., 1.4907465), (0., 1.494186 ),
       (0., 1.4959134), (0., 1.5076429), (0., 1.5085094), (0., 1.5092772),
       (0., 1.5114615), (0., 1.512125 ), (0., 1.5134887), (0., 1.5145649),
       (0., 1.5146865), (0., 1.5160586), (0., 1.5162053), (0., 1.5163336),
       (0., 1.5169691), (0., 1.5173584), (0., 1.51737  ), (0., 1.5176259),
       (0., 1.5179483), (0., 1.5184127), (0., 1.5186253), (0., 1.5195236),
       (0., 1.5196885), (0., 1.5198087), (0., 1.5201781), (0., 1.5202394),
       (0., 1.5202897), (0., 1.5203626), (0., 1.5207374), (0., 1.5208989),
       (0., 1.521225 ), (0., 1.521292 ), (0., 1.5217233), (0., 1.5218003),
       (0., 1.5218879), (0., 1.5220191), (0., 1.5220405), (0., 1.5222527),
       (0., 1.522419 ), (0., 1.5225232), (0., 1.5225257), (0., 1.5225697),
       (0., 1.5225835), (0., 1.5228672), (0., 1.5231187), (0., 1.5234065),
       (0., 1.5235345), (0., 1.5235367), (0., 1.5235882), (0., 1.5235894),
       (0., 1.5236092), (0., 1.5237892), (0., 1.5238055), (0., 1.5238309),
       (0., 1.5240687), (0., 1.524206 ), (0., 1.5242435), (0., 1.5242815),
       (0., 1.524461 ), (0., 1.5244917), (0., 1.5245587), (0., 1.5247118),
       (0., 1.5249542), (0., 1.5251281), (0., 1.5252142), (0., 1.5255555),
       (0., 1.5257304), (0., 1.5258144), (0., 1.5261331), (0., 1.5261645),
       (0., 1.5262085), (0., 1.5262867), (0., 1.5263909), (0., 1.5266337),
       (0., 1.5266707), (0., 1.5266879), (0., 1.5267116), (0., 1.5272391),
       (0., 1.5273839), (0., 1.5275106), (0., 1.5285774), (0., 1.528831 ),
       (0., 1.528963 ), (0., 1.5289868), (0., 1.5292684), (0., 1.5301203),
       (0., 1.5303193), (0., 1.5307547), (0., 1.5312564), (0., 1.5314392),
       (0., 1.5337523), (0., 1.5357007), (0., 1.5399314), (0., 1.5400128),
       (0., 1.5444272), (0., 1.5503726), (0., 1.5510917), (0., 1.5511703),
       (0., 1.5520666), (0., 1.5526367), (0., 1.5564651), (0., 1.5574043),
       (0., 1.5574049), (0., 1.5615445), (0., 1.5638318), (0., 1.5655622),
       (0., 1.5688664), (0., 1.5781826), (0., 1.578476 ), (0., 1.5826964),
       (0., 1.5846708), (0., 1.5913918), (0., 1.5926428), (0., 1.5994724),
       (0., 1.5996051), (0., 1.5999376), (0., 1.6000929), (0., 1.6024166),
       (0., 1.6029423), (0., 1.6056732), (0., 1.6063244), (0., 1.606832 ),
       (0., 1.6081145), (0., 1.6082029), (0., 1.6168102), (0., 1.617981 ),
       (0., 1.6194487), (0., 1.6246861), (0., 1.6262605), (0., 1.6273062),
       (0., 1.6277469), (0., 1.6372716), (0., 1.6381756), (0., 1.6521019),
       (0., 1.6556782), (0., 1.6615108), (0., 1.6627573), (0., 1.6653273),
       (0., 1.6680917), (0., 1.6689743), (0., 1.6742847), (0., 1.6746216),
       (0., 1.6766645), (0., 1.686406 ), (0., 1.6948334), (0., 1.6962492),
       (0., 1.7009712), (0., 1.7029737), (0., 1.709783 ), (0., 1.7127824),
       (0., 1.7137458), (0., 1.7157414), (0., 1.7161549), (0., 1.7193573),
       (0., 1.7211024), (0., 1.7217187), (0., 1.7256824), (0., 1.7313744),
       (0., 1.734398 ), (0., 1.7377902), (0., 1.7393   ), (0., 1.7483517),
       (0., 1.7597216), (0., 1.7600187), (0., 1.7641574), (0., 1.7660557),
       (0., 1.7661642), (0., 1.7678863), (0., 1.7742825), (0., 1.7751973),
       (0., 1.7774832), (0., 1.7793468), (0., 1.7798527), (0., 1.7803396),
       (0., 1.78285  ), (0., 1.7830824), (0., 1.7856457), (0., 1.7919868),
       (0., 1.7936056), (0., 1.7961391), (0., 1.8034517), (0., 1.8041532),
       (0., 1.8065506), (0., 1.8124177), (0., 1.8144226), (0., 1.8187692),
       (0., 1.8348105), (0., 1.8487576), (0., 1.8506038), (0., 1.8581241),
       (0., 1.8648361), (0., 1.8728663), (0., 1.8875608)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(6.971148 , 7.338352 ), (6.4561687, 6.6162357),
       (5.4775004, 5.6632085), (5.419609 , 6.037091 ),
       (5.1967344, 5.5381055), (4.8092704, 6.3431296),
       (4.5753927, 5.2467527), (4.526213 , 7.122392 ),
       (4.351686 , 4.592145 ), (4.322611 , 4.4154377),
       (4.192762 , 4.391132 ), (4.1918297, 4.4498444),
       (4.151241 , 4.172493 ), (4.126977 , 4.51019  ),
       (4.1079082, 4.1198115), (4.084164 , 4.1289153),
       (4.0817256, 4.5421643), (4.0795164, 4.314735 ),
       (4.0303535, 4.7274957), (4.0270085, 4.4260325),
       (3.9992926, 4.158838 ), (3.9966512, 4.5520964),
       (3.9790285, 4.505591 ), (3.9784195, 4.3999686),
       (3.9718242, 4.407338 ), (3.969943 , 4.102098 ),
       (3.956458 , 4.8127723), (3.9500353, 5.401966 ),
       (3.9486916, 4.5837355), (3.9460168, 4.675962 ),
       (3.9229188, 4.7212825), (3.9066107, 4.071672 ),
       (3.8335123, 4.822985 ), (3.7947977, 7.2418537),
       (3.746677 , 7.61841  )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.455571 , 10.608684 ), (6.3907385,  6.4407573),
       (5.5670795,  5.833927 )],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.319510817527771), (0.0, 1.3222335577011108), (0.0, 1.3224730491638184), (0.0, 1.3228793144226074), (0.0, 1.3229972124099731), (0.0, 1.3251866102218628), (0.0, 1.3252917528152466), (0.0, 1.3256067037582397), (0.0, 1.326477289199829), (0.0, 1.3265610933303833), (0.0, 1.3266162872314453), (0.0, 1.3268283605575562), (0.0, 1.3277413845062256), (0.0, 1.3277634382247925), (0.0, 1.3278714418411255), (0.0, 1.3279284238815308), (0.0, 1.3280023336410522), (0.0, 1.328009009361267), (0.0, 1.3282420635223389), (0.0, 1.3283830881118774), (0.0, 1.3284834623336792), (0.0, 1.3285818099975586), (0.0, 1.3291141986846924), (0.0, 1.3295496702194214), (0.0, 1.3296207189559937), (0.0, 1.3296304941177368), (0.0, 1.3296443223953247), (0.0, 1.329646348953247), (0.0, 1.3300909996032715), (0.0, 1.330193042755127), (0.0, 1.33043372631073), (0.0, 1.3305078744888306), (0.0, 1.3306375741958618), (0.0, 1.3306450843811035), (0.0, 1.3307515382766724), (0.0, 1.3309154510498047), (0.0, 1.3309611082077026), (0.0, 1.3310095071792603), (0.0, 1.3310235738754272), (0.0, 1.3311383724212646), (0.0, 1.3311759233474731), (0.0, 1.3313746452331543), (0.0, 1.3314987421035767), (0.0, 1.331641435623169), (0.0, 1.331821322441101), (0.0, 1.3318852186203003), (0.0, 1.3319042921066284), (0.0, 1.3320226669311523), (0.0, 1.3320873975753784), (0.0, 1.3327232599258423), (0.0, 1.333133578300476), (0.0, 1.333142638206482), (0.0, 1.3334691524505615), (0.0, 1.3337876796722412), (0.0, 1.3339751958847046), (0.0, 1.3340306282043457), (0.0, 1.3340413570404053), (0.0, 1.3341461420059204), (0.0, 1.3350639343261719), (0.0, 1.3350797891616821), (0.0, 1.3351359367370605), (0.0, 1.3353480100631714), (0.0, 1.3353912830352783), (0.0, 1.3356468677520752), (0.0, 1.335728645324707), (0.0, 1.3357380628585815), (0.0, 1.3357576131820679), (0.0, 1.3358416557312012), (0.0, 1.3359811305999756), (0.0, 1.33614981174469), (0.0, 1.336203932762146), (0.0, 1.3362290859222412), (0.0, 1.3364894390106201), (0.0, 1.3366955518722534), (0.0, 1.3368809223175049), (0.0, 1.3371623754501343), (0.0, 1.3386714458465576), (0.0, 1.3387463092803955), (0.0, 1.3388893604278564), (0.0, 1.338890552520752), (0.0, 1.339271903038025), (0.0, 1.3397197723388672), (0.0, 1.3401355743408203), (0.0, 1.341528058052063), (0.0, 1.3433016538619995), (0.0, 1.345420241355896), (0.0, 1.3596082925796509), (0.0, 1.3642520904541016), (0.0, 1.380675196647644), (0.0, 1.3918856382369995), (0.0, 1.3927488327026367), (0.0, 1.395903468132019), (0.0, 1.4127823114395142), (0.0, 1.4261668920516968), (0.0, 1.4412585496902466), (0.0, 1.4434691667556763), (0.0, 1.4444031715393066), (0.0, 1.4446945190429688), (0.0, 1.4471731185913086), (0.0, 1.4476560354232788), (0.0, 1.448055624961853), (0.0, 1.4483224153518677), (0.0, 1.4494003057479858), (0.0, 1.450190544128418), (0.0, 1.4503858089447021), (0.0, 1.4504629373550415), (0.0, 1.4505263566970825), (0.0, 1.4510149955749512), (0.0, 1.4523088932037354), (0.0, 1.4523957967758179), (0.0, 1.4526821374893188), (0.0, 1.4528546333312988), (0.0, 1.453640341758728), (0.0, 1.4541263580322266), (0.0, 1.454420566558838), (0.0, 1.4544662237167358), (0.0, 1.4547282457351685), (0.0, 1.4549697637557983), (0.0, 1.455003261566162), (0.0, 1.4552652835845947), (0.0, 1.4556950330734253), (0.0, 1.4558100700378418), (0.0, 1.4558171033859253), (0.0, 1.4561309814453125), (0.0, 1.4564026594161987), (0.0, 1.4565112590789795), (0.0, 1.456719160079956), (0.0, 1.456835150718689), (0.0, 1.4569077491760254), (0.0, 1.457199215888977), (0.0, 1.457295298576355), (0.0, 1.4574133157730103), (0.0, 1.457586407661438), (0.0, 1.457680583000183), (0.0, 1.4577257633209229), (0.0, 1.4580321311950684), (0.0, 1.4580519199371338), (0.0, 1.4580954313278198), (0.0, 1.4581658840179443), (0.0, 1.4584015607833862), (0.0, 1.4589977264404297), (0.0, 1.4591474533081055), (0.0, 1.4592394828796387), (0.0, 1.459253191947937), (0.0, 1.4594284296035767), (0.0, 1.4594330787658691), (0.0, 1.459455132484436), (0.0, 1.459477424621582), (0.0, 1.4595417976379395), (0.0, 1.4596443176269531), (0.0, 1.459842562675476), (0.0, 1.459873080253601), (0.0, 1.4599144458770752), (0.0, 1.4600082635879517), (0.0, 1.4600427150726318), (0.0, 1.460124135017395), (0.0, 1.4603976011276245), (0.0, 1.4604638814926147), (0.0, 1.4604861736297607), (0.0, 1.46074378490448), (0.0, 1.460893988609314), (0.0, 1.460943579673767), (0.0, 1.4609524011611938), (0.0, 1.4610886573791504), (0.0, 1.461094617843628), (0.0, 1.4615029096603394), (0.0, 1.4616214036941528), (0.0, 1.4617174863815308), (0.0, 1.4617581367492676), (0.0, 1.4619513750076294), (0.0, 1.462000846862793), (0.0, 1.4621597528457642), (0.0, 1.462186336517334), (0.0, 1.4622204303741455), (0.0, 1.4624011516571045), (0.0, 1.462432622909546), (0.0, 1.462440848350525), (0.0, 1.4626126289367676), (0.0, 1.463485836982727), (0.0, 1.4636598825454712), (0.0, 1.4638463258743286), (0.0, 1.4640332460403442), (0.0, 1.46437668800354), (0.0, 1.4661015272140503), (0.0, 1.4685369729995728), (0.0, 1.4686102867126465), (0.0, 1.4692600965499878), (0.0, 1.4758503437042236), (0.0, 1.4780995845794678), (0.0, 1.483610987663269), (0.0, 1.4907464981079102), (0.0, 1.4941860437393188), (0.0, 1.4959133863449097), (0.0, 1.5076428651809692), (0.0, 1.5085093975067139), (0.0, 1.5092772245407104), (0.0, 1.5114614963531494), (0.0, 1.512125015258789), (0.0, 1.5134886503219604), (0.0, 1.514564871788025), (0.0, 1.5146864652633667), (0.0, 1.5160585641860962), (0.0, 1.5162053108215332), (0.0, 1.5163335800170898), (0.0, 1.516969084739685), (0.0, 1.517358422279358), (0.0, 1.5173699855804443), (0.0, 1.5176259279251099), (0.0, 1.5179482698440552), (0.0, 1.518412709236145), (0.0, 1.518625259399414), (0.0, 1.5195236206054688), (0.0, 1.5196884870529175), (0.0, 1.5198086500167847), (0.0, 1.5201780796051025), (0.0, 1.5202393531799316), (0.0, 1.520289659500122), (0.0, 1.5203626155853271), (0.0, 1.5207374095916748), (0.0, 1.5208989381790161), (0.0, 1.5212249755859375), (0.0, 1.521291971206665), (0.0, 1.5217232704162598), (0.0, 1.5218002796173096), (0.0, 1.5218878984451294), (0.0, 1.5220191478729248), (0.0, 1.5220404863357544), (0.0, 1.5222526788711548), (0.0, 1.5224189758300781), (0.0, 1.5225231647491455), (0.0, 1.522525668144226), (0.0, 1.5225696563720703), (0.0, 1.5225834846496582), (0.0, 1.522867202758789), (0.0, 1.5231187343597412), (0.0, 1.5234065055847168), (0.0, 1.5235345363616943), (0.0, 1.5235366821289062), (0.0, 1.5235881805419922), (0.0, 1.5235893726348877), (0.0, 1.5236091613769531), (0.0, 1.5237891674041748), (0.0, 1.5238054990768433), (0.0, 1.5238308906555176), (0.0, 1.5240687131881714), (0.0, 1.5242060422897339), (0.0, 1.5242434740066528), (0.0, 1.5242815017700195), (0.0, 1.524461030960083), (0.0, 1.5244916677474976), (0.0, 1.524558663368225), (0.0, 1.5247118473052979), (0.0, 1.5249541997909546), (0.0, 1.5251281261444092), (0.0, 1.5252141952514648), (0.0, 1.5255554914474487), (0.0, 1.5257303714752197), (0.0, 1.525814414024353), (0.0, 1.5261330604553223), (0.0, 1.5261645317077637), (0.0, 1.526208519935608), (0.0, 1.5262867212295532), (0.0, 1.5263909101486206), (0.0, 1.5266337394714355), (0.0, 1.5266706943511963), (0.0, 1.5266878604888916), (0.0, 1.5267115831375122), (0.0, 1.5272390842437744), (0.0, 1.5273839235305786), (0.0, 1.527510643005371), (0.0, 1.528577446937561), (0.0, 1.5288310050964355), (0.0, 1.5289629697799683), (0.0, 1.5289868116378784), (0.0, 1.5292683839797974), (0.0, 1.5301202535629272), (0.0, 1.530319333076477), (0.0, 1.5307546854019165), (0.0, 1.5312564373016357), (0.0, 1.531439185142517), (0.0, 1.5337523221969604), (0.0, 1.5357006788253784), (0.0, 1.5399314165115356), (0.0, 1.5400128364562988), (0.0, 1.5444271564483643), (0.0, 1.55037260055542), (0.0, 1.5510916709899902), (0.0, 1.5511703491210938), (0.0, 1.5520665645599365), (0.0, 1.552636742591858), (0.0, 1.5564651489257812), (0.0, 1.5574042797088623), (0.0, 1.55740487575531), (0.0, 1.5615445375442505), (0.0, 1.5638318061828613), (0.0, 1.5655622482299805), (0.0, 1.5688663721084595), (0.0, 1.578182578086853), (0.0, 1.5784759521484375), (0.0, 1.5826964378356934), (0.0, 1.5846707820892334), (0.0, 1.5913918018341064), (0.0, 1.5926427841186523), (0.0, 1.5994724035263062), (0.0, 1.5996050834655762), (0.0, 1.5999375581741333), (0.0, 1.600092887878418), (0.0, 1.6024166345596313), (0.0, 1.6029423475265503), (0.0, 1.6056731939315796), (0.0, 1.6063244342803955), (0.0, 1.6068320274353027), (0.0, 1.60811448097229), (0.0, 1.6082029342651367), (0.0, 1.6168102025985718), (0.0, 1.61798095703125), (0.0, 1.6194486618041992), (0.0, 1.6246861219406128), (0.0, 1.62626051902771), (0.0, 1.6273062229156494), (0.0, 1.6277469396591187), (0.0, 1.6372716426849365), (0.0, 1.6381756067276), (0.0, 1.6521018743515015), (0.0, 1.655678153038025), (0.0, 1.6615108251571655), (0.0, 1.6627572774887085), (0.0, 1.6653273105621338), (0.0, 1.6680916547775269), (0.0, 1.6689742803573608), (0.0, 1.6742846965789795), (0.0, 1.67462158203125), (0.0, 1.6766644716262817), (0.0, 1.6864060163497925), (0.0, 1.6948333978652954), (0.0, 1.69624924659729), (0.0, 1.700971245765686), (0.0, 1.70297372341156), (0.0, 1.7097829580307007), (0.0, 1.712782382965088), (0.0, 1.7137458324432373), (0.0, 1.7157413959503174), (0.0, 1.716154932975769), (0.0, 1.7193572521209717), (0.0, 1.7211023569107056), (0.0, 1.721718668937683), (0.0, 1.7256823778152466), (0.0, 1.7313743829727173), (0.0, 1.7343980073928833), (0.0, 1.7377902269363403), (0.0, 1.739300012588501), (0.0, 1.7483516931533813), (0.0, 1.7597216367721558), (0.0, 1.7600187063217163), (0.0, 1.7641574144363403), (0.0, 1.766055703163147), (0.0, 1.7661641836166382), (0.0, 1.7678862810134888), (0.0, 1.774282455444336), (0.0, 1.7751972675323486), (0.0, 1.7774832248687744), (0.0, 1.7793468236923218), (0.0, 1.7798527479171753), (0.0, 1.7803395986557007), (0.0, 1.7828500270843506), (0.0, 1.783082365989685), (0.0, 1.7856457233428955), (0.0, 1.7919868230819702), (0.0, 1.7936055660247803), (0.0, 1.796139121055603), (0.0, 1.803451657295227), (0.0, 1.8041532039642334), (0.0, 1.8065506219863892), (0.0, 1.81241774559021), (0.0, 1.814422607421875), (0.0, 1.8187692165374756), (0.0, 1.834810495376587), (0.0, 1.8487576246261597), (0.0, 1.8506038188934326), (0.0, 1.8581241369247437), (0.0, 1.8648360967636108), (0.0, 1.8728662729263306), (0.0, 1.8875608444213867)], [(6.9711480140686035, 7.338352203369141), (6.4561686515808105, 6.616235733032227), (5.4775004386901855, 5.663208484649658), (5.419609069824219, 6.03709077835083), (5.196734428405762, 5.538105487823486), (4.80927038192749, 6.343129634857178), (4.575392723083496, 5.246752738952637), (4.5262131690979, 7.122392177581787), (4.351686000823975, 4.592144966125488), (4.322610855102539, 4.415437698364258), (4.1927618980407715, 4.39113187789917), (4.191829681396484, 4.4498443603515625), (4.151240825653076, 4.172492980957031), (4.12697696685791, 4.510190010070801), (4.107908248901367, 4.119811534881592), (4.084164142608643, 4.128915309906006), (4.081725597381592, 4.542164325714111), (4.079516410827637, 4.314734935760498), (4.030353546142578, 4.7274956703186035), (4.027008533477783, 4.426032543182373), (3.9992926120758057, 4.158837795257568), (3.9966511726379395, 4.552096366882324), (3.9790284633636475, 4.505590915679932), (3.978419542312622, 4.39996862411499), (3.9718241691589355, 4.4073381423950195), (3.969943046569824, 4.102097988128662), (3.95645809173584, 4.812772274017334), (3.950035333633423, 5.401966094970703), (3.9486916065216064, 4.583735466003418), (3.946016788482666, 4.675961971282959), (3.9229187965393066, 4.721282482147217), (3.9066107273101807, 4.071671962738037), (3.833512306213379, 4.8229851722717285), (3.794797658920288, 7.241853713989258), (3.7466769218444824, 7.618410110473633)], [(9.455571174621582, 10.608683586120605), (6.390738487243652, 6.4407572746276855), (5.567079544067383, 5.833927154541016)]]
[array([[0.        , 1.31951082],
       [0.        , 1.32223356],
       [0.        , 1.32247305],
       [0.        , 1.32287931],
       [0.        , 1.32299721],
       [0.        , 1.32518661],
       [0.        , 1.32529175],
       [0.        , 1.3256067 ],
       [0.        , 1.32647729],
       [0.        , 1.32656109],
       [0.        , 1.32661629],
       [0.        , 1.32682836],
       [0.        , 1.32774138],
       [0.        , 1.32776344],
       [0.        , 1.32787144],
       [0.        , 1.32792842],
       [0.        , 1.32800233],
       [0.        , 1.32800901],
       [0.        , 1.32824206],
       [0.        , 1.32838309],
       [0.        , 1.32848346],
       [0.        , 1.32858181],
       [0.        , 1.3291142 ],
       [0.        , 1.32954967],
       [0.        , 1.32962072],
       [0.        , 1.32963049],
       [0.        , 1.32964432],
       [0.        , 1.32964635],
       [0.        , 1.330091  ],
       [0.        , 1.33019304],
       [0.        , 1.33043373],
       [0.        , 1.33050787],
       [0.        , 1.33063757],
       [0.        , 1.33064508],
       [0.        , 1.33075154],
       [0.        , 1.33091545],
       [0.        , 1.33096111],
       [0.        , 1.33100951],
       [0.        , 1.33102357],
       [0.        , 1.33113837],
       [0.        , 1.33117592],
       [0.        , 1.33137465],
       [0.        , 1.33149874],
       [0.        , 1.33164144],
       [0.        , 1.33182132],
       [0.        , 1.33188522],
       [0.        , 1.33190429],
       [0.        , 1.33202267],
       [0.        , 1.3320874 ],
       [0.        , 1.33272326],
       [0.        , 1.33313358],
       [0.        , 1.33314264],
       [0.        , 1.33346915],
       [0.        , 1.33378768],
       [0.        , 1.3339752 ],
       [0.        , 1.33403063],
       [0.        , 1.33404136],
       [0.        , 1.33414614],
       [0.        , 1.33506393],
       [0.        , 1.33507979],
       [0.        , 1.33513594],
       [0.        , 1.33534801],
       [0.        , 1.33539128],
       [0.        , 1.33564687],
       [0.        , 1.33572865],
       [0.        , 1.33573806],
       [0.        , 1.33575761],
       [0.        , 1.33584166],
       [0.        , 1.33598113],
       [0.        , 1.33614981],
       [0.        , 1.33620393],
       [0.        , 1.33622909],
       [0.        , 1.33648944],
       [0.        , 1.33669555],
       [0.        , 1.33688092],
       [0.        , 1.33716238],
       [0.        , 1.33867145],
       [0.        , 1.33874631],
       [0.        , 1.33888936],
       [0.        , 1.33889055],
       [0.        , 1.3392719 ],
       [0.        , 1.33971977],
       [0.        , 1.34013557],
       [0.        , 1.34152806],
       [0.        , 1.34330165],
       [0.        , 1.34542024],
       [0.        , 1.35960829],
       [0.        , 1.36425209],
       [0.        , 1.3806752 ],
       [0.        , 1.39188564],
       [0.        , 1.39274883],
       [0.        , 1.39590347],
       [0.        , 1.41278231],
       [0.        , 1.42616689],
       [0.        , 1.44125855],
       [0.        , 1.44346917],
       [0.        , 1.44440317],
       [0.        , 1.44469452],
       [0.        , 1.44717312],
       [0.        , 1.44765604],
       [0.        , 1.44805562],
       [0.        , 1.44832242],
       [0.        , 1.44940031],
       [0.        , 1.45019054],
       [0.        , 1.45038581],
       [0.        , 1.45046294],
       [0.        , 1.45052636],
       [0.        , 1.451015  ],
       [0.        , 1.45230889],
       [0.        , 1.4523958 ],
       [0.        , 1.45268214],
       [0.        , 1.45285463],
       [0.        , 1.45364034],
       [0.        , 1.45412636],
       [0.        , 1.45442057],
       [0.        , 1.45446622],
       [0.        , 1.45472825],
       [0.        , 1.45496976],
       [0.        , 1.45500326],
       [0.        , 1.45526528],
       [0.        , 1.45569503],
       [0.        , 1.45581007],
       [0.        , 1.4558171 ],
       [0.        , 1.45613098],
       [0.        , 1.45640266],
       [0.        , 1.45651126],
       [0.        , 1.45671916],
       [0.        , 1.45683515],
       [0.        , 1.45690775],
       [0.        , 1.45719922],
       [0.        , 1.4572953 ],
       [0.        , 1.45741332],
       [0.        , 1.45758641],
       [0.        , 1.45768058],
       [0.        , 1.45772576],
       [0.        , 1.45803213],
       [0.        , 1.45805192],
       [0.        , 1.45809543],
       [0.        , 1.45816588],
       [0.        , 1.45840156],
       [0.        , 1.45899773],
       [0.        , 1.45914745],
       [0.        , 1.45923948],
       [0.        , 1.45925319],
       [0.        , 1.45942843],
       [0.        , 1.45943308],
       [0.        , 1.45945513],
       [0.        , 1.45947742],
       [0.        , 1.4595418 ],
       [0.        , 1.45964432],
       [0.        , 1.45984256],
       [0.        , 1.45987308],
       [0.        , 1.45991445],
       [0.        , 1.46000826],
       [0.        , 1.46004272],
       [0.        , 1.46012414],
       [0.        , 1.4603976 ],
       [0.        , 1.46046388],
       [0.        , 1.46048617],
       [0.        , 1.46074378],
       [0.        , 1.46089399],
       [0.        , 1.46094358],
       [0.        , 1.4609524 ],
       [0.        , 1.46108866],
       [0.        , 1.46109462],
       [0.        , 1.46150291],
       [0.        , 1.4616214 ],
       [0.        , 1.46171749],
       [0.        , 1.46175814],
       [0.        , 1.46195138],
       [0.        , 1.46200085],
       [0.        , 1.46215975],
       [0.        , 1.46218634],
       [0.        , 1.46222043],
       [0.        , 1.46240115],
       [0.        , 1.46243262],
       [0.        , 1.46244085],
       [0.        , 1.46261263],
       [0.        , 1.46348584],
       [0.        , 1.46365988],
       [0.        , 1.46384633],
       [0.        , 1.46403325],
       [0.        , 1.46437669],
       [0.        , 1.46610153],
       [0.        , 1.46853697],
       [0.        , 1.46861029],
       [0.        , 1.4692601 ],
       [0.        , 1.47585034],
       [0.        , 1.47809958],
       [0.        , 1.48361099],
       [0.        , 1.4907465 ],
       [0.        , 1.49418604],
       [0.        , 1.49591339],
       [0.        , 1.50764287],
       [0.        , 1.5085094 ],
       [0.        , 1.50927722],
       [0.        , 1.5114615 ],
       [0.        , 1.51212502],
       [0.        , 1.51348865],
       [0.        , 1.51456487],
       [0.        , 1.51468647],
       [0.        , 1.51605856],
       [0.        , 1.51620531],
       [0.        , 1.51633358],
       [0.        , 1.51696908],
       [0.        , 1.51735842],
       [0.        , 1.51736999],
       [0.        , 1.51762593],
       [0.        , 1.51794827],
       [0.        , 1.51841271],
       [0.        , 1.51862526],
       [0.        , 1.51952362],
       [0.        , 1.51968849],
       [0.        , 1.51980865],
       [0.        , 1.52017808],
       [0.        , 1.52023935],
       [0.        , 1.52028966],
       [0.        , 1.52036262],
       [0.        , 1.52073741],
       [0.        , 1.52089894],
       [0.        , 1.52122498],
       [0.        , 1.52129197],
       [0.        , 1.52172327],
       [0.        , 1.52180028],
       [0.        , 1.5218879 ],
       [0.        , 1.52201915],
       [0.        , 1.52204049],
       [0.        , 1.52225268],
       [0.        , 1.52241898],
       [0.        , 1.52252316],
       [0.        , 1.52252567],
       [0.        , 1.52256966],
       [0.        , 1.52258348],
       [0.        , 1.5228672 ],
       [0.        , 1.52311873],
       [0.        , 1.52340651],
       [0.        , 1.52353454],
       [0.        , 1.52353668],
       [0.        , 1.52358818],
       [0.        , 1.52358937],
       [0.        , 1.52360916],
       [0.        , 1.52378917],
       [0.        , 1.5238055 ],
       [0.        , 1.52383089],
       [0.        , 1.52406871],
       [0.        , 1.52420604],
       [0.        , 1.52424347],
       [0.        , 1.5242815 ],
       [0.        , 1.52446103],
       [0.        , 1.52449167],
       [0.        , 1.52455866],
       [0.        , 1.52471185],
       [0.        , 1.5249542 ],
       [0.        , 1.52512813],
       [0.        , 1.5252142 ],
       [0.        , 1.52555549],
       [0.        , 1.52573037],
       [0.        , 1.52581441],
       [0.        , 1.52613306],
       [0.        , 1.52616453],
       [0.        , 1.52620852],
       [0.        , 1.52628672],
       [0.        , 1.52639091],
       [0.        , 1.52663374],
       [0.        , 1.52667069],
       [0.        , 1.52668786],
       [0.        , 1.52671158],
       [0.        , 1.52723908],
       [0.        , 1.52738392],
       [0.        , 1.52751064],
       [0.        , 1.52857745],
       [0.        , 1.52883101],
       [0.        , 1.52896297],
       [0.        , 1.52898681],
       [0.        , 1.52926838],
       [0.        , 1.53012025],
       [0.        , 1.53031933],
       [0.        , 1.53075469],
       [0.        , 1.53125644],
       [0.        , 1.53143919],
       [0.        , 1.53375232],
       [0.        , 1.53570068],
       [0.        , 1.53993142],
       [0.        , 1.54001284],
       [0.        , 1.54442716],
       [0.        , 1.5503726 ],
       [0.        , 1.55109167],
       [0.        , 1.55117035],
       [0.        , 1.55206656],
       [0.        , 1.55263674],
       [0.        , 1.55646515],
       [0.        , 1.55740428],
       [0.        , 1.55740488],
       [0.        , 1.56154454],
       [0.        , 1.56383181],
       [0.        , 1.56556225],
       [0.        , 1.56886637],
       [0.        , 1.57818258],
       [0.        , 1.57847595],
       [0.        , 1.58269644],
       [0.        , 1.58467078],
       [0.        , 1.5913918 ],
       [0.        , 1.59264278],
       [0.        , 1.5994724 ],
       [0.        , 1.59960508],
       [0.        , 1.59993756],
       [0.        , 1.60009289],
       [0.        , 1.60241663],
       [0.        , 1.60294235],
       [0.        , 1.60567319],
       [0.        , 1.60632443],
       [0.        , 1.60683203],
       [0.        , 1.60811448],
       [0.        , 1.60820293],
       [0.        , 1.6168102 ],
       [0.        , 1.61798096],
       [0.        , 1.61944866],
       [0.        , 1.62468612],
       [0.        , 1.62626052],
       [0.        , 1.62730622],
       [0.        , 1.62774694],
       [0.        , 1.63727164],
       [0.        , 1.63817561],
       [0.        , 1.65210187],
       [0.        , 1.65567815],
       [0.        , 1.66151083],
       [0.        , 1.66275728],
       [0.        , 1.66532731],
       [0.        , 1.66809165],
       [0.        , 1.66897428],
       [0.        , 1.6742847 ],
       [0.        , 1.67462158],
       [0.        , 1.67666447],
       [0.        , 1.68640602],
       [0.        , 1.6948334 ],
       [0.        , 1.69624925],
       [0.        , 1.70097125],
       [0.        , 1.70297372],
       [0.        , 1.70978296],
       [0.        , 1.71278238],
       [0.        , 1.71374583],
       [0.        , 1.7157414 ],
       [0.        , 1.71615493],
       [0.        , 1.71935725],
       [0.        , 1.72110236],
       [0.        , 1.72171867],
       [0.        , 1.72568238],
       [0.        , 1.73137438],
       [0.        , 1.73439801],
       [0.        , 1.73779023],
       [0.        , 1.73930001],
       [0.        , 1.74835169],
       [0.        , 1.75972164],
       [0.        , 1.76001871],
       [0.        , 1.76415741],
       [0.        , 1.7660557 ],
       [0.        , 1.76616418],
       [0.        , 1.76788628],
       [0.        , 1.77428246],
       [0.        , 1.77519727],
       [0.        , 1.77748322],
       [0.        , 1.77934682],
       [0.        , 1.77985275],
       [0.        , 1.7803396 ],
       [0.        , 1.78285003],
       [0.        , 1.78308237],
       [0.        , 1.78564572],
       [0.        , 1.79198682],
       [0.        , 1.79360557],
       [0.        , 1.79613912],
       [0.        , 1.80345166],
       [0.        , 1.8041532 ],
       [0.        , 1.80655062],
       [0.        , 1.81241775],
       [0.        , 1.81442261],
       [0.        , 1.81876922],
       [0.        , 1.8348105 ],
       [0.        , 1.84875762],
       [0.        , 1.85060382],
       [0.        , 1.85812414],
       [0.        , 1.8648361 ],
       [0.        , 1.87286627],
       [0.        , 1.88756084]]), array([[6.97114801, 7.3383522 ],
       [6.45616865, 6.61623573],
       [5.47750044, 5.66320848],
       [5.41960907, 6.03709078],
       [5.19673443, 5.53810549],
       [4.80927038, 6.34312963],
       [4.57539272, 5.24675274],
       [4.52621317, 7.12239218],
       [4.351686  , 4.59214497],
       [4.32261086, 4.4154377 ],
       [4.1927619 , 4.39113188],
       [4.19182968, 4.44984436],
       [4.15124083, 4.17249298],
       [4.12697697, 4.51019001],
       [4.10790825, 4.11981153],
       [4.08416414, 4.12891531],
       [4.0817256 , 4.54216433],
       [4.07951641, 4.31473494],
       [4.03035355, 4.72749567],
       [4.02700853, 4.42603254],
       [3.99929261, 4.1588378 ],
       [3.99665117, 4.55209637],
       [3.97902846, 4.50559092],
       [3.97841954, 4.39996862],
       [3.97182417, 4.40733814],
       [3.96994305, 4.10209799],
       [3.95645809, 4.81277227],
       [3.95003533, 5.40196609],
       [3.94869161, 4.58373547],
       [3.94601679, 4.67596197],
       [3.9229188 , 4.72128248],
       [3.90661073, 4.07167196],
       [3.83351231, 4.82298517],
       [3.79479766, 7.24185371],
       [3.74667692, 7.61841011]]), array([[ 9.45557117, 10.60868359],
       [ 6.39073849,  6.44075727],
       [ 5.56707954,  5.83392715]])]2024-03-06 17:49:51.643272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6JVY ph vector generated, counter: 71
2024-03-06 17:49:55.736958: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:49:55.780315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:49:57.131465: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JWM ph vector generated, counter: 72
2024-03-06 17:50:01.719160: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:01.761841: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:02.856311: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JWN ph vector generated, counter: 73
2024-03-06 17:50:06.607178: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:06.650471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:07.885180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JXD ph vector generated, counter: 74
2024-03-06 17:50:11.784857: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:11.830508: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:13.121774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JXT ph vector generated, counter: 75
2024-03-06 17:50:16.501823: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:16.545351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:17.453859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JYP ph vector generated, counter: 76
2024-03-06 17:50:20.632621: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:20.675636: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:21.567762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6JYQ ph vector generated, counter: 77
2024-03-06 17:50:24.765888: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:24.808991: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:25.752799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6K1K ph vector generated, counter: 78
2024-03-06 17:50:28.964721: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:29.008215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:30.034893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6K3F ph vector generated, counter: 79
2024-03-06 17:50:33.773373: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:33.816315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:34.700180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6K9P ph vector generated, counter: 80
2024-03-06 17:50:37.813437: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:37.859372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:38.767032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6K9Y ph vector generated, counter: 81
2024-03-06 17:50:42.043501: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:42.087278: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:43.242262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KA9 ph vector generated, counter: 82
2024-03-06 17:50:46.560488: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:46.603372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:47.464404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAE ph vector generated, counter: 83
2024-03-06 17:50:50.712323: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:50.756860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:51.640009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAH ph vector generated, counter: 84
2024-03-06 17:50:54.901027: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:54.944002: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:50:55.784712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAI ph vector generated, counter: 85
2024-03-06 17:50:59.264894: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:50:59.308391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:00.435148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAO ph vector generated, counter: 86
2024-03-06 17:51:04.120009: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:04.163365: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:05.033629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAP ph vector generated, counter: 87
2024-03-06 17:51:08.184907: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:08.227971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:09.090880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAQ ph vector generated, counter: 88
2024-03-06 17:51:12.183162: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:12.225579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:13.248605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAR ph vector generated, counter: 89
2024-03-06 17:51:16.196155: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:16.238994: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:17.288040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAS ph vector generated, counter: 90
2024-03-06 17:51:20.433548: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:20.477252: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:21.317938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAT ph vector generated, counter: 91
2024-03-06 17:51:24.609436: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:24.652119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:25.514057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAU ph vector generated, counter: 92
2024-03-06 17:51:28.903406: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:28.948911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:29.820000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAV ph vector generated, counter: 93
2024-03-06 17:51:32.944987: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:32.987849: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:34.045928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAX ph vector generated, counter: 94
2024-03-06 17:51:37.104280: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:37.146853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:38.044884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAY ph vector generated, counter: 95
2024-03-06 17:51:41.142530: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:41.184987: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:42.053205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KAZ ph vector generated, counter: 96
2024-03-06 17:51:45.462307: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:45.505701: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:46.405154: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB0 ph vector generated, counter: 97
2024-03-06 17:51:49.511217: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:49.553957: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:50.605047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB1 ph vector generated, counter: 98
2024-03-06 17:51:53.814870: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:53.864337: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:54.710763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB2 ph vector generated, counter: 99
2024-03-06 17:51:57.992418: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:51:58.035722: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:51:58.959111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB3 ph vector generated, counter: 100
2024-03-06 17:52:02.205493: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:02.248671: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:03.319355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB4 ph vector generated, counter: 101
2024-03-06 17:52:06.946245: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:06.989201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:08.019583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB5 ph vector generated, counter: 102
2024-03-06 17:52:11.219760: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:11.262236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:12.134624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB6 ph vector generated, counter: 103
2024-03-06 17:52:15.295007: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:15.337847: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:16.219067: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB7 ph vector generated, counter: 104
2024-03-06 17:52:19.331780: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:19.374595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:20.243337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB8 ph vector generated, counter: 105
2024-03-06 17:52:23.368284: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:23.410903: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:24.619730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KB9 ph vector generated, counter: 106
2024-03-06 17:52:27.713399: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:27.777496: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:28.647825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KBA ph vector generated, counter: 107
2024-03-06 17:52:31.691592: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:31.734426: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:32.593708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KBB ph vector generated, counter: 108
2024-03-06 17:52:35.755254: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:35.798247: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:36.676662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KBE ph vector generated, counter: 109
2024-03-06 17:52:39.907268: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:39.950154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:40.943767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KBP ph vector generated, counter: 110
2024-03-06 17:52:44.131152: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:44.187969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:45.225343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KC5 ph vector generated, counter: 111
2024-03-06 17:52:48.444922: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:48.488140: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:49.380575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KC6 ph vector generated, counter: 112
2024-03-06 17:52:52.645350: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:52:52.690562: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:52:53.597775: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3187879), (0., 1.3235793), (0., 1.3296503), (0., 1.3297192),
       (0., 1.3300731), (0., 1.3307449), (0., 1.3314658), (0., 1.3323286),
       (0., 1.3324658), (0., 1.3325208), (0., 1.3328354), (0., 1.3330718),
       (0., 1.3337774), (0., 1.3337969), (0., 1.3341168), (0., 1.3341486),
       (0., 1.3341575), (0., 1.3348118), (0., 1.334833 ), (0., 1.33484  ),
       (0., 1.3351274), (0., 1.3352509), (0., 1.3354502), (0., 1.3355765),
       (0., 1.3356568), (0., 1.3356615), (0., 1.3357267), (0., 1.3357468),
       (0., 1.3357564), (0., 1.3357887), (0., 1.3358866), (0., 1.3359429),
       (0., 1.3359462), (0., 1.3362539), (0., 1.3362546), (0., 1.3362674),
       (0., 1.3363115), (0., 1.3363723), (0., 1.3365244), (0., 1.3366063),
       (0., 1.3366103), (0., 1.3366195), (0., 1.3366244), (0., 1.336752 ),
       (0., 1.3367947), (0., 1.3368315), (0., 1.3368973), (0., 1.336939 ),
       (0., 1.336965 ), (0., 1.3370242), (0., 1.3370433), (0., 1.3370486),
       (0., 1.3371036), (0., 1.3371146), (0., 1.3372806), (0., 1.3373263),
       (0., 1.3374739), (0., 1.3374865), (0., 1.3374889), (0., 1.337589 ),
       (0., 1.3376138), (0., 1.337774 ), (0., 1.3381312), (0., 1.3382741),
       (0., 1.3383287), (0., 1.3383313), (0., 1.3383576), (0., 1.3383887),
       (0., 1.3383942), (0., 1.3384315), (0., 1.3385499), (0., 1.33858  ),
       (0., 1.3386803), (0., 1.3387973), (0., 1.3388357), (0., 1.3388727),
       (0., 1.3388749), (0., 1.3389133), (0., 1.3389387), (0., 1.3390179),
       (0., 1.3390548), (0., 1.3390956), (0., 1.3391243), (0., 1.3391293),
       (0., 1.3391707), (0., 1.3391827), (0., 1.3393592), (0., 1.3393898),
       (0., 1.3394041), (0., 1.3394178), (0., 1.3394341), (0., 1.3394886),
       (0., 1.339489 ), (0., 1.3396076), (0., 1.3397816), (0., 1.3399435),
       (0., 1.3400227), (0., 1.3400539), (0., 1.3401139), (0., 1.3401663),
       (0., 1.3401691), (0., 1.340171 ), (0., 1.3401963), (0., 1.340236 ),
       (0., 1.3406208), (0., 1.3406436), (0., 1.3408009), (0., 1.3416134),
       (0., 1.3419415), (0., 1.3420359), (0., 1.3428286), (0., 1.3437136),
       (0., 1.3494843), (0., 1.3528794), (0., 1.3539896), (0., 1.3541124),
       (0., 1.3543754), (0., 1.355532 ), (0., 1.3568673), (0., 1.3614128),
       (0., 1.3682613), (0., 1.3803568), (0., 1.422157 ), (0., 1.4262099),
       (0., 1.4450996), (0., 1.4458833), (0., 1.4468966), (0., 1.4473597),
       (0., 1.4480333), (0., 1.448195 ), (0., 1.4492674), (0., 1.4492985),
       (0., 1.449618 ), (0., 1.4497756), (0., 1.4509075), (0., 1.4517643),
       (0., 1.4522014), (0., 1.4523004), (0., 1.4523783), (0., 1.4525844),
       (0., 1.4530461), (0., 1.4530687), (0., 1.453127 ), (0., 1.4531492),
       (0., 1.4531504), (0., 1.4531525), (0., 1.4533381), (0., 1.4534125),
       (0., 1.4534814), (0., 1.4535232), (0., 1.4536667), (0., 1.4536946),
       (0., 1.453815 ), (0., 1.4538362), (0., 1.4539   ), (0., 1.4539893),
       (0., 1.4541028), (0., 1.4541773), (0., 1.4543704), (0., 1.4544239),
       (0., 1.4544724), (0., 1.4545139), (0., 1.454566 ), (0., 1.4546189),
       (0., 1.4547426), (0., 1.4548092), (0., 1.454845 ), (0., 1.4548666),
       (0., 1.4549531), (0., 1.4549725), (0., 1.455017 ), (0., 1.4551364),
       (0., 1.4552215), (0., 1.455371 ), (0., 1.455413 ), (0., 1.4554868),
       (0., 1.4556688), (0., 1.4557168), (0., 1.4557742), (0., 1.4557902),
       (0., 1.4558135), (0., 1.4558605), (0., 1.4558936), (0., 1.4559214),
       (0., 1.4559307), (0., 1.4560302), (0., 1.4561262), (0., 1.4561839),
       (0., 1.4562559), (0., 1.4562947), (0., 1.4563193), (0., 1.4563222),
       (0., 1.4563462), (0., 1.4564433), (0., 1.4564773), (0., 1.4564914),
       (0., 1.456531 ), (0., 1.4565363), (0., 1.4565365), (0., 1.4565815),
       (0., 1.456588 ), (0., 1.4567324), (0., 1.4567405), (0., 1.4567561),
       (0., 1.4567666), (0., 1.4567924), (0., 1.4569949), (0., 1.457004 ),
       (0., 1.4570428), (0., 1.4570951), (0., 1.4573959), (0., 1.4574747),
       (0., 1.4576064), (0., 1.4576361), (0., 1.4576371), (0., 1.457668 ),
       (0., 1.4576861), (0., 1.4577093), (0., 1.4578289), (0., 1.4580394),
       (0., 1.4582204), (0., 1.4582208), (0., 1.4584255), (0., 1.458474 ),
       (0., 1.4586275), (0., 1.4588479), (0., 1.4588534), (0., 1.4589214),
       (0., 1.4590619), (0., 1.4594953), (0., 1.4599367), (0., 1.4601829),
       (0., 1.4603636), (0., 1.4609597), (0., 1.461114 ), (0., 1.4617541),
       (0., 1.462379 ), (0., 1.4633362), (0., 1.4644612), (0., 1.4646776),
       (0., 1.4661417), (0., 1.4750807), (0., 1.4909979), (0., 1.4941218),
       (0., 1.5115408), (0., 1.5196683), (0., 1.52092  ), (0., 1.5219004),
       (0., 1.5223432), (0., 1.5229511), (0., 1.5232517), (0., 1.5233284),
       (0., 1.5234244), (0., 1.5234542), (0., 1.5234646), (0., 1.5239068),
       (0., 1.5239726), (0., 1.5240618), (0., 1.5242951), (0., 1.5244787),
       (0., 1.5246177), (0., 1.5249143), (0., 1.5249356), (0., 1.5249518),
       (0., 1.5251538), (0., 1.5251853), (0., 1.5252936), (0., 1.5255053),
       (0., 1.5255407), (0., 1.5257145), (0., 1.5258417), (0., 1.5258515),
       (0., 1.5258611), (0., 1.5258722), (0., 1.525949 ), (0., 1.5261542),
       (0., 1.5263139), (0., 1.526553 ), (0., 1.5265969), (0., 1.5266396),
       (0., 1.52667  ), (0., 1.5269504), (0., 1.5269827), (0., 1.5269924),
       (0., 1.527136 ), (0., 1.527179 ), (0., 1.5272051), (0., 1.5272266),
       (0., 1.5272868), (0., 1.5273517), (0., 1.5273696), (0., 1.5273707),
       (0., 1.5274316), (0., 1.5275086), (0., 1.5275556), (0., 1.5275961),
       (0., 1.5276318), (0., 1.5276698), (0., 1.5277411), (0., 1.5278891),
       (0., 1.5279201), (0., 1.5281194), (0., 1.5281771), (0., 1.528185 ),
       (0., 1.5281935), (0., 1.5282164), (0., 1.5282549), (0., 1.5282785),
       (0., 1.5283083), (0., 1.5283341), (0., 1.528352 ), (0., 1.52844  ),
       (0., 1.5284482), (0., 1.5284934), (0., 1.528579 ), (0., 1.5285823),
       (0., 1.5287069), (0., 1.5287894), (0., 1.5287932), (0., 1.5287958),
       (0., 1.528802 ), (0., 1.5288069), (0., 1.5288275), (0., 1.528828 ),
       (0., 1.528861 ), (0., 1.5288818), (0., 1.528888 ), (0., 1.5289111),
       (0., 1.5289565), (0., 1.5289626), (0., 1.5289888), (0., 1.5290866),
       (0., 1.529102 ), (0., 1.5291338), (0., 1.5292693), (0., 1.5293623),
       (0., 1.5293715), (0., 1.5294368), (0., 1.5295148), (0., 1.5295155),
       (0., 1.5295408), (0., 1.5296183), (0., 1.5296223), (0., 1.5296392),
       (0., 1.52968  ), (0., 1.529712 ), (0., 1.5298163), (0., 1.5298436),
       (0., 1.5298866), (0., 1.5299907), (0., 1.5301237), (0., 1.5301791),
       (0., 1.530305 ), (0., 1.5303441), (0., 1.5304912), (0., 1.5307432),
       (0., 1.5309174), (0., 1.5311096), (0., 1.531126 ), (0., 1.5311651),
       (0., 1.5312562), (0., 1.536497 ), (0., 1.5545752), (0., 1.5556891),
       (0., 1.6014683)], dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(6.882918 , 6.9065504), (6.2367296, 6.760612 ),
       (6.220985 , 7.173173 ), (5.8572025, 6.046898 ),
       (5.791317 , 6.1869826), (5.5131974, 5.915728 ),
       (5.44608  , 7.265909 ), (5.1593366, 5.2604012),
       (5.1466107, 6.090982 ), (4.8723087, 5.2933135),
       (4.8606167, 5.790511 ), (4.8057704, 4.868752 ),
       (4.768782 , 5.076026 ), (4.7034044, 5.1957626),
       (4.57944  , 4.590262 ), (4.257584 , 4.766236 ),
       (4.2559466, 4.3702736), (4.214995 , 4.708717 ),
       (4.2127514, 4.475148 ), (4.200039 , 5.5775895),
       (4.1678247, 4.262177 ), (4.1323   , 4.4153905),
       (4.109806 , 4.2760644), (4.0837374, 4.360137 ),
       (4.0826926, 4.550361 ), (4.0714636, 4.321701 ),
       (4.0345087, 4.301218 ), (4.0228076, 4.382547 ),
       (4.0206294, 4.4610896), (4.0159426, 4.448833 ),
       (4.0095305, 4.3278017), (4.0094957, 4.496901 ),
       (4.0057573, 4.7904463), (4.0041137, 7.1060376),
       (4.0007625, 4.701115 ), (3.984813 , 4.435349 ),
       (3.968074 , 4.218702 ), (3.9604983, 6.169647 ),
       (3.9541352, 4.6656966), (3.9347813, 8.791749 ),
       (3.890806 , 4.8302493), (3.8832176, 4.245813 ),
       (3.8585305, 4.461918 ), (3.8408008, 4.5476108),
       (3.8125885, 8.217692 ), (3.8118725, 4.343894 ),
       (3.7721374, 5.433536 ), (3.7552068, 4.779747 ),
       (3.7497149, 4.139087 ), (3.7470324, 4.549523 ),
       (3.7094543, 8.269007 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(11.069982, 11.09571  ), ( 9.854337, 10.746029 ),
       ( 9.839183,  9.845576 ), ( 9.233094, 11.281074 ),
       ( 4.816653,  4.9941974)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.318787932395935), (0.0, 1.3235793113708496), (0.0, 1.3296502828598022), (0.0, 1.3297191858291626), (0.0, 1.3300731182098389), (0.0, 1.3307448625564575), (0.0, 1.3314658403396606), (0.0, 1.3323285579681396), (0.0, 1.3324657678604126), (0.0, 1.332520842552185), (0.0, 1.3328354358673096), (0.0, 1.3330718278884888), (0.0, 1.3337774276733398), (0.0, 1.3337968587875366), (0.0, 1.334116816520691), (0.0, 1.334148645401001), (0.0, 1.3341574668884277), (0.0, 1.334811806678772), (0.0, 1.334833025932312), (0.0, 1.3348400592803955), (0.0, 1.335127353668213), (0.0, 1.3352508544921875), (0.0, 1.3354501724243164), (0.0, 1.3355765342712402), (0.0, 1.335656762123108), (0.0, 1.33566153049469), (0.0, 1.3357267379760742), (0.0, 1.3357467651367188), (0.0, 1.3357564210891724), (0.0, 1.3357887268066406), (0.0, 1.3358865976333618), (0.0, 1.3359428644180298), (0.0, 1.3359462022781372), (0.0, 1.3362538814544678), (0.0, 1.336254596710205), (0.0, 1.336267352104187), (0.0, 1.3363114595413208), (0.0, 1.3363722562789917), (0.0, 1.3365243673324585), (0.0, 1.3366062641143799), (0.0, 1.3366103172302246), (0.0, 1.33661949634552), (0.0, 1.3366243839263916), (0.0, 1.3367520570755005), (0.0, 1.3367947340011597), (0.0, 1.3368314504623413), (0.0, 1.3368972539901733), (0.0, 1.3369389772415161), (0.0, 1.3369649648666382), (0.0, 1.337024211883545), (0.0, 1.337043285369873), (0.0, 1.3370486497879028), (0.0, 1.3371036052703857), (0.0, 1.3371145725250244), (0.0, 1.3372806310653687), (0.0, 1.3373262882232666), (0.0, 1.3374738693237305), (0.0, 1.3374865055084229), (0.0, 1.3374888896942139), (0.0, 1.3375890254974365), (0.0, 1.337613821029663), (0.0, 1.3377740383148193), (0.0, 1.3381311893463135), (0.0, 1.3382741212844849), (0.0, 1.3383287191390991), (0.0, 1.3383313417434692), (0.0, 1.3383575677871704), (0.0, 1.3383886814117432), (0.0, 1.3383941650390625), (0.0, 1.338431477546692), (0.0, 1.3385498523712158), (0.0, 1.3385800123214722), (0.0, 1.3386802673339844), (0.0, 1.3387973308563232), (0.0, 1.3388357162475586), (0.0, 1.3388726711273193), (0.0, 1.3388749361038208), (0.0, 1.3389133214950562), (0.0, 1.3389387130737305), (0.0, 1.3390178680419922), (0.0, 1.339054822921753), (0.0, 1.3390955924987793), (0.0, 1.339124321937561), (0.0, 1.3391293287277222), (0.0, 1.3391706943511963), (0.0, 1.339182734489441), (0.0, 1.339359164237976), (0.0, 1.3393898010253906), (0.0, 1.3394041061401367), (0.0, 1.339417815208435), (0.0, 1.3394341468811035), (0.0, 1.3394886255264282), (0.0, 1.3394889831542969), (0.0, 1.3396075963974), (0.0, 1.339781641960144), (0.0, 1.339943528175354), (0.0, 1.3400226831436157), (0.0, 1.340053915977478), (0.0, 1.340113878250122), (0.0, 1.3401663303375244), (0.0, 1.340169072151184), (0.0, 1.340170979499817), (0.0, 1.3401962518692017), (0.0, 1.340235948562622), (0.0, 1.340620756149292), (0.0, 1.3406436443328857), (0.0, 1.3408008813858032), (0.0, 1.3416134119033813), (0.0, 1.341941475868225), (0.0, 1.3420358896255493), (0.0, 1.342828631401062), (0.0, 1.343713641166687), (0.0, 1.3494843244552612), (0.0, 1.3528794050216675), (0.0, 1.353989601135254), (0.0, 1.3541123867034912), (0.0, 1.3543753623962402), (0.0, 1.3555320501327515), (0.0, 1.3568673133850098), (0.0, 1.361412763595581), (0.0, 1.3682613372802734), (0.0, 1.380356788635254), (0.0, 1.4221570491790771), (0.0, 1.4262099266052246), (0.0, 1.4450995922088623), (0.0, 1.4458832740783691), (0.0, 1.4468965530395508), (0.0, 1.4473596811294556), (0.0, 1.448033332824707), (0.0, 1.448194980621338), (0.0, 1.4492673873901367), (0.0, 1.4492985010147095), (0.0, 1.4496179819107056), (0.0, 1.4497755765914917), (0.0, 1.4509074687957764), (0.0, 1.4517643451690674), (0.0, 1.4522013664245605), (0.0, 1.4523004293441772), (0.0, 1.452378273010254), (0.0, 1.4525843858718872), (0.0, 1.4530460834503174), (0.0, 1.453068733215332), (0.0, 1.4531270265579224), (0.0, 1.4531491994857788), (0.0, 1.4531503915786743), (0.0, 1.4531525373458862), (0.0, 1.4533381462097168), (0.0, 1.4534125328063965), (0.0, 1.4534814357757568), (0.0, 1.4535231590270996), (0.0, 1.4536666870117188), (0.0, 1.4536945819854736), (0.0, 1.45381498336792), (0.0, 1.45383620262146), (0.0, 1.4538999795913696), (0.0, 1.4539892673492432), (0.0, 1.4541027545928955), (0.0, 1.4541772603988647), (0.0, 1.454370379447937), (0.0, 1.4544239044189453), (0.0, 1.4544724225997925), (0.0, 1.4545139074325562), (0.0, 1.4545660018920898), (0.0, 1.4546189308166504), (0.0, 1.4547425508499146), (0.0, 1.4548091888427734), (0.0, 1.4548449516296387), (0.0, 1.454866647720337), (0.0, 1.4549530744552612), (0.0, 1.454972505569458), (0.0, 1.4550169706344604), (0.0, 1.4551364183425903), (0.0, 1.4552215337753296), (0.0, 1.4553710222244263), (0.0, 1.4554129838943481), (0.0, 1.45548677444458), (0.0, 1.4556688070297241), (0.0, 1.455716848373413), (0.0, 1.455774188041687), (0.0, 1.4557901620864868), (0.0, 1.4558135271072388), (0.0, 1.4558604955673218), (0.0, 1.455893635749817), (0.0, 1.4559214115142822), (0.0, 1.4559307098388672), (0.0, 1.456030249595642), (0.0, 1.4561262130737305), (0.0, 1.456183910369873), (0.0, 1.4562559127807617), (0.0, 1.4562946557998657), (0.0, 1.4563193321228027), (0.0, 1.456322193145752), (0.0, 1.4563461542129517), (0.0, 1.4564433097839355), (0.0, 1.4564772844314575), (0.0, 1.4564913511276245), (0.0, 1.456531047821045), (0.0, 1.4565362930297852), (0.0, 1.4565365314483643), (0.0, 1.456581473350525), (0.0, 1.4565880298614502), (0.0, 1.4567323923110962), (0.0, 1.4567404985427856), (0.0, 1.4567561149597168), (0.0, 1.4567666053771973), (0.0, 1.4567923545837402), (0.0, 1.456994891166687), (0.0, 1.4570039510726929), (0.0, 1.4570428133010864), (0.0, 1.4570951461791992), (0.0, 1.4573959112167358), (0.0, 1.457474708557129), (0.0, 1.4576064348220825), (0.0, 1.4576361179351807), (0.0, 1.457637071609497), (0.0, 1.4576679468154907), (0.0, 1.4576860666275024), (0.0, 1.4577093124389648), (0.0, 1.4578288793563843), (0.0, 1.458039402961731), (0.0, 1.458220362663269), (0.0, 1.4582208395004272), (0.0, 1.458425521850586), (0.0, 1.458474040031433), (0.0, 1.458627462387085), (0.0, 1.4588478803634644), (0.0, 1.4588533639907837), (0.0, 1.4589214324951172), (0.0, 1.459061861038208), (0.0, 1.4594953060150146), (0.0, 1.4599367380142212), (0.0, 1.4601829051971436), (0.0, 1.4603636264801025), (0.0, 1.4609596729278564), (0.0, 1.4611140489578247), (0.0, 1.4617540836334229), (0.0, 1.462378978729248), (0.0, 1.4633362293243408), (0.0, 1.4644612073898315), (0.0, 1.4646775722503662), (0.0, 1.466141700744629), (0.0, 1.4750807285308838), (0.0, 1.4909979104995728), (0.0, 1.494121789932251), (0.0, 1.5115407705307007), (0.0, 1.5196683406829834), (0.0, 1.5209200382232666), (0.0, 1.5219004154205322), (0.0, 1.5223431587219238), (0.0, 1.5229511260986328), (0.0, 1.5232516527175903), (0.0, 1.523328423500061), (0.0, 1.5234243869781494), (0.0, 1.523454189300537), (0.0, 1.523464560508728), (0.0, 1.5239068269729614), (0.0, 1.5239726305007935), (0.0, 1.5240617990493774), (0.0, 1.5242950916290283), (0.0, 1.5244786739349365), (0.0, 1.5246176719665527), (0.0, 1.524914264678955), (0.0, 1.5249356031417847), (0.0, 1.5249518156051636), (0.0, 1.5251537561416626), (0.0, 1.5251853466033936), (0.0, 1.5252935886383057), (0.0, 1.5255053043365479), (0.0, 1.5255407094955444), (0.0, 1.5257145166397095), (0.0, 1.5258417129516602), (0.0, 1.5258514881134033), (0.0, 1.525861144065857), (0.0, 1.5258722305297852), (0.0, 1.5259490013122559), (0.0, 1.5261541604995728), (0.0, 1.5263139009475708), (0.0, 1.5265530347824097), (0.0, 1.5265969038009644), (0.0, 1.5266395807266235), (0.0, 1.526669979095459), (0.0, 1.5269503593444824), (0.0, 1.5269826650619507), (0.0, 1.5269924402236938), (0.0, 1.527135968208313), (0.0, 1.5271790027618408), (0.0, 1.5272051095962524), (0.0, 1.5272265672683716), (0.0, 1.5272867679595947), (0.0, 1.5273517370224), (0.0, 1.5273696184158325), (0.0, 1.5273706912994385), (0.0, 1.527431607246399), (0.0, 1.5275086164474487), (0.0, 1.5275555849075317), (0.0, 1.527596116065979), (0.0, 1.5276317596435547), (0.0, 1.5276697874069214), (0.0, 1.5277410745620728), (0.0, 1.5278891324996948), (0.0, 1.527920126914978), (0.0, 1.528119444847107), (0.0, 1.5281771421432495), (0.0, 1.5281850099563599), (0.0, 1.528193473815918), (0.0, 1.5282163619995117), (0.0, 1.5282548666000366), (0.0, 1.5282784700393677), (0.0, 1.5283082723617554), (0.0, 1.528334140777588), (0.0, 1.5283520221710205), (0.0, 1.528439998626709), (0.0, 1.528448224067688), (0.0, 1.5284934043884277), (0.0, 1.5285789966583252), (0.0, 1.5285823345184326), (0.0, 1.5287069082260132), (0.0, 1.5287894010543823), (0.0, 1.528793215751648), (0.0, 1.528795838356018), (0.0, 1.5288020372390747), (0.0, 1.5288069248199463), (0.0, 1.5288275480270386), (0.0, 1.5288280248641968), (0.0, 1.5288610458374023), (0.0, 1.5288817882537842), (0.0, 1.5288879871368408), (0.0, 1.5289111137390137), (0.0, 1.5289565324783325), (0.0, 1.5289626121520996), (0.0, 1.5289888381958008), (0.0, 1.5290865898132324), (0.0, 1.5291019678115845), (0.0, 1.5291337966918945), (0.0, 1.5292693376541138), (0.0, 1.5293623208999634), (0.0, 1.5293715000152588), (0.0, 1.5294368267059326), (0.0, 1.5295147895812988), (0.0, 1.5295155048370361), (0.0, 1.529540777206421), (0.0, 1.529618263244629), (0.0, 1.5296223163604736), (0.0, 1.5296392440795898), (0.0, 1.5296800136566162), (0.0, 1.5297119617462158), (0.0, 1.5298162698745728), (0.0, 1.5298435688018799), (0.0, 1.5298866033554077), (0.0, 1.5299906730651855), (0.0, 1.5301237106323242), (0.0, 1.5301791429519653), (0.0, 1.530305027961731), (0.0, 1.5303441286087036), (0.0, 1.5304912328720093), (0.0, 1.5307432413101196), (0.0, 1.5309174060821533), (0.0, 1.5311095714569092), (0.0, 1.5311260223388672), (0.0, 1.5311651229858398), (0.0, 1.5312561988830566), (0.0, 1.5364969968795776), (0.0, 1.5545752048492432), (0.0, 1.5556890964508057), (0.0, 1.6014683246612549)], [(6.882917881011963, 6.906550407409668), (6.236729621887207, 6.7606120109558105), (6.220984935760498, 7.173172950744629), (5.857202529907227, 6.046897888183594), (5.791316986083984, 6.18698263168335), (5.513197422027588, 5.9157280921936035), (5.446080207824707, 7.265909194946289), (5.159336566925049, 5.260401248931885), (5.146610736846924, 6.090981960296631), (4.872308731079102, 5.293313503265381), (4.860616683959961, 5.790511131286621), (4.805770397186279, 4.8687520027160645), (4.768782138824463, 5.07602596282959), (4.703404426574707, 5.195762634277344), (4.579440116882324, 4.590261936187744), (4.257584095001221, 4.766235828399658), (4.255946636199951, 4.370273590087891), (4.21499490737915, 4.708716869354248), (4.212751388549805, 4.4751482009887695), (4.200038909912109, 5.577589511871338), (4.167824745178223, 4.262176990509033), (4.132299900054932, 4.415390491485596), (4.109806060791016, 4.276064395904541), (4.083737373352051, 4.360136985778809), (4.082692623138428, 4.550361156463623), (4.071463584899902, 4.3217010498046875), (4.03450870513916, 4.301218032836914), (4.022807598114014, 4.382546901702881), (4.020629405975342, 4.461089611053467), (4.015942573547363, 4.448832988739014), (4.009530544281006, 4.327801704406738), (4.009495735168457, 4.496901035308838), (4.0057573318481445, 4.7904462814331055), (4.004113674163818, 7.106037616729736), (4.000762462615967, 4.701115131378174), (3.9848129749298096, 4.435348987579346), (3.968074083328247, 4.2187018394470215), (3.960498332977295, 6.169647216796875), (3.9541351795196533, 4.665696620941162), (3.934781312942505, 8.791749000549316), (3.890805959701538, 4.830249309539795), (3.8832175731658936, 4.245812892913818), (3.8585305213928223, 4.461917877197266), (3.8408007621765137, 4.547610759735107), (3.8125884532928467, 8.217692375183105), (3.8118724822998047, 4.343894004821777), (3.772137403488159, 5.433536052703857), (3.755206823348999, 4.779747009277344), (3.7497148513793945, 4.139087200164795), (3.747032403945923, 4.549522876739502), (3.709454298019409, 8.269006729125977)], [(11.069981575012207, 11.095709800720215), (9.854336738586426, 10.746028900146484), (9.83918285369873, 9.845576286315918), (9.233094215393066, 11.281073570251465), (4.816652774810791, 4.994197368621826)]]
[array([[0.        , 1.31878793],
       [0.        , 1.32357931],
       [0.        , 1.32965028],
       [0.        , 1.32971919],
       [0.        , 1.33007312],
       [0.        , 1.33074486],
       [0.        , 1.33146584],
       [0.        , 1.33232856],
       [0.        , 1.33246577],
       [0.        , 1.33252084],
       [0.        , 1.33283544],
       [0.        , 1.33307183],
       [0.        , 1.33377743],
       [0.        , 1.33379686],
       [0.        , 1.33411682],
       [0.        , 1.33414865],
       [0.        , 1.33415747],
       [0.        , 1.33481181],
       [0.        , 1.33483303],
       [0.        , 1.33484006],
       [0.        , 1.33512735],
       [0.        , 1.33525085],
       [0.        , 1.33545017],
       [0.        , 1.33557653],
       [0.        , 1.33565676],
       [0.        , 1.33566153],
       [0.        , 1.33572674],
       [0.        , 1.33574677],
       [0.        , 1.33575642],
       [0.        , 1.33578873],
       [0.        , 1.3358866 ],
       [0.        , 1.33594286],
       [0.        , 1.3359462 ],
       [0.        , 1.33625388],
       [0.        , 1.3362546 ],
       [0.        , 1.33626735],
       [0.        , 1.33631146],
       [0.        , 1.33637226],
       [0.        , 1.33652437],
       [0.        , 1.33660626],
       [0.        , 1.33661032],
       [0.        , 1.3366195 ],
       [0.        , 1.33662438],
       [0.        , 1.33675206],
       [0.        , 1.33679473],
       [0.        , 1.33683145],
       [0.        , 1.33689725],
       [0.        , 1.33693898],
       [0.        , 1.33696496],
       [0.        , 1.33702421],
       [0.        , 1.33704329],
       [0.        , 1.33704865],
       [0.        , 1.33710361],
       [0.        , 1.33711457],
       [0.        , 1.33728063],
       [0.        , 1.33732629],
       [0.        , 1.33747387],
       [0.        , 1.33748651],
       [0.        , 1.33748889],
       [0.        , 1.33758903],
       [0.        , 1.33761382],
       [0.        , 1.33777404],
       [0.        , 1.33813119],
       [0.        , 1.33827412],
       [0.        , 1.33832872],
       [0.        , 1.33833134],
       [0.        , 1.33835757],
       [0.        , 1.33838868],
       [0.        , 1.33839417],
       [0.        , 1.33843148],
       [0.        , 1.33854985],
       [0.        , 1.33858001],
       [0.        , 1.33868027],
       [0.        , 1.33879733],
       [0.        , 1.33883572],
       [0.        , 1.33887267],
       [0.        , 1.33887494],
       [0.        , 1.33891332],
       [0.        , 1.33893871],
       [0.        , 1.33901787],
       [0.        , 1.33905482],
       [0.        , 1.33909559],
       [0.        , 1.33912432],
       [0.        , 1.33912933],
       [0.        , 1.33917069],
       [0.        , 1.33918273],
       [0.        , 1.33935916],
       [0.        , 1.3393898 ],
       [0.        , 1.33940411],
       [0.        , 1.33941782],
       [0.        , 1.33943415],
       [0.        , 1.33948863],
       [0.        , 1.33948898],
       [0.        , 1.3396076 ],
       [0.        , 1.33978164],
       [0.        , 1.33994353],
       [0.        , 1.34002268],
       [0.        , 1.34005392],
       [0.        , 1.34011388],
       [0.        , 1.34016633],
       [0.        , 1.34016907],
       [0.        , 1.34017098],
       [0.        , 1.34019625],
       [0.        , 1.34023595],
       [0.        , 1.34062076],
       [0.        , 1.34064364],
       [0.        , 1.34080088],
       [0.        , 1.34161341],
       [0.        , 1.34194148],
       [0.        , 1.34203589],
       [0.        , 1.34282863],
       [0.        , 1.34371364],
       [0.        , 1.34948432],
       [0.        , 1.35287941],
       [0.        , 1.3539896 ],
       [0.        , 1.35411239],
       [0.        , 1.35437536],
       [0.        , 1.35553205],
       [0.        , 1.35686731],
       [0.        , 1.36141276],
       [0.        , 1.36826134],
       [0.        , 1.38035679],
       [0.        , 1.42215705],
       [0.        , 1.42620993],
       [0.        , 1.44509959],
       [0.        , 1.44588327],
       [0.        , 1.44689655],
       [0.        , 1.44735968],
       [0.        , 1.44803333],
       [0.        , 1.44819498],
       [0.        , 1.44926739],
       [0.        , 1.4492985 ],
       [0.        , 1.44961798],
       [0.        , 1.44977558],
       [0.        , 1.45090747],
       [0.        , 1.45176435],
       [0.        , 1.45220137],
       [0.        , 1.45230043],
       [0.        , 1.45237827],
       [0.        , 1.45258439],
       [0.        , 1.45304608],
       [0.        , 1.45306873],
       [0.        , 1.45312703],
       [0.        , 1.4531492 ],
       [0.        , 1.45315039],
       [0.        , 1.45315254],
       [0.        , 1.45333815],
       [0.        , 1.45341253],
       [0.        , 1.45348144],
       [0.        , 1.45352316],
       [0.        , 1.45366669],
       [0.        , 1.45369458],
       [0.        , 1.45381498],
       [0.        , 1.4538362 ],
       [0.        , 1.45389998],
       [0.        , 1.45398927],
       [0.        , 1.45410275],
       [0.        , 1.45417726],
       [0.        , 1.45437038],
       [0.        , 1.4544239 ],
       [0.        , 1.45447242],
       [0.        , 1.45451391],
       [0.        , 1.454566  ],
       [0.        , 1.45461893],
       [0.        , 1.45474255],
       [0.        , 1.45480919],
       [0.        , 1.45484495],
       [0.        , 1.45486665],
       [0.        , 1.45495307],
       [0.        , 1.45497251],
       [0.        , 1.45501697],
       [0.        , 1.45513642],
       [0.        , 1.45522153],
       [0.        , 1.45537102],
       [0.        , 1.45541298],
       [0.        , 1.45548677],
       [0.        , 1.45566881],
       [0.        , 1.45571685],
       [0.        , 1.45577419],
       [0.        , 1.45579016],
       [0.        , 1.45581353],
       [0.        , 1.4558605 ],
       [0.        , 1.45589364],
       [0.        , 1.45592141],
       [0.        , 1.45593071],
       [0.        , 1.45603025],
       [0.        , 1.45612621],
       [0.        , 1.45618391],
       [0.        , 1.45625591],
       [0.        , 1.45629466],
       [0.        , 1.45631933],
       [0.        , 1.45632219],
       [0.        , 1.45634615],
       [0.        , 1.45644331],
       [0.        , 1.45647728],
       [0.        , 1.45649135],
       [0.        , 1.45653105],
       [0.        , 1.45653629],
       [0.        , 1.45653653],
       [0.        , 1.45658147],
       [0.        , 1.45658803],
       [0.        , 1.45673239],
       [0.        , 1.4567405 ],
       [0.        , 1.45675611],
       [0.        , 1.45676661],
       [0.        , 1.45679235],
       [0.        , 1.45699489],
       [0.        , 1.45700395],
       [0.        , 1.45704281],
       [0.        , 1.45709515],
       [0.        , 1.45739591],
       [0.        , 1.45747471],
       [0.        , 1.45760643],
       [0.        , 1.45763612],
       [0.        , 1.45763707],
       [0.        , 1.45766795],
       [0.        , 1.45768607],
       [0.        , 1.45770931],
       [0.        , 1.45782888],
       [0.        , 1.4580394 ],
       [0.        , 1.45822036],
       [0.        , 1.45822084],
       [0.        , 1.45842552],
       [0.        , 1.45847404],
       [0.        , 1.45862746],
       [0.        , 1.45884788],
       [0.        , 1.45885336],
       [0.        , 1.45892143],
       [0.        , 1.45906186],
       [0.        , 1.45949531],
       [0.        , 1.45993674],
       [0.        , 1.46018291],
       [0.        , 1.46036363],
       [0.        , 1.46095967],
       [0.        , 1.46111405],
       [0.        , 1.46175408],
       [0.        , 1.46237898],
       [0.        , 1.46333623],
       [0.        , 1.46446121],
       [0.        , 1.46467757],
       [0.        , 1.4661417 ],
       [0.        , 1.47508073],
       [0.        , 1.49099791],
       [0.        , 1.49412179],
       [0.        , 1.51154077],
       [0.        , 1.51966834],
       [0.        , 1.52092004],
       [0.        , 1.52190042],
       [0.        , 1.52234316],
       [0.        , 1.52295113],
       [0.        , 1.52325165],
       [0.        , 1.52332842],
       [0.        , 1.52342439],
       [0.        , 1.52345419],
       [0.        , 1.52346456],
       [0.        , 1.52390683],
       [0.        , 1.52397263],
       [0.        , 1.5240618 ],
       [0.        , 1.52429509],
       [0.        , 1.52447867],
       [0.        , 1.52461767],
       [0.        , 1.52491426],
       [0.        , 1.5249356 ],
       [0.        , 1.52495182],
       [0.        , 1.52515376],
       [0.        , 1.52518535],
       [0.        , 1.52529359],
       [0.        , 1.5255053 ],
       [0.        , 1.52554071],
       [0.        , 1.52571452],
       [0.        , 1.52584171],
       [0.        , 1.52585149],
       [0.        , 1.52586114],
       [0.        , 1.52587223],
       [0.        , 1.525949  ],
       [0.        , 1.52615416],
       [0.        , 1.5263139 ],
       [0.        , 1.52655303],
       [0.        , 1.5265969 ],
       [0.        , 1.52663958],
       [0.        , 1.52666998],
       [0.        , 1.52695036],
       [0.        , 1.52698267],
       [0.        , 1.52699244],
       [0.        , 1.52713597],
       [0.        , 1.527179  ],
       [0.        , 1.52720511],
       [0.        , 1.52722657],
       [0.        , 1.52728677],
       [0.        , 1.52735174],
       [0.        , 1.52736962],
       [0.        , 1.52737069],
       [0.        , 1.52743161],
       [0.        , 1.52750862],
       [0.        , 1.52755558],
       [0.        , 1.52759612],
       [0.        , 1.52763176],
       [0.        , 1.52766979],
       [0.        , 1.52774107],
       [0.        , 1.52788913],
       [0.        , 1.52792013],
       [0.        , 1.52811944],
       [0.        , 1.52817714],
       [0.        , 1.52818501],
       [0.        , 1.52819347],
       [0.        , 1.52821636],
       [0.        , 1.52825487],
       [0.        , 1.52827847],
       [0.        , 1.52830827],
       [0.        , 1.52833414],
       [0.        , 1.52835202],
       [0.        , 1.52844   ],
       [0.        , 1.52844822],
       [0.        , 1.5284934 ],
       [0.        , 1.528579  ],
       [0.        , 1.52858233],
       [0.        , 1.52870691],
       [0.        , 1.5287894 ],
       [0.        , 1.52879322],
       [0.        , 1.52879584],
       [0.        , 1.52880204],
       [0.        , 1.52880692],
       [0.        , 1.52882755],
       [0.        , 1.52882802],
       [0.        , 1.52886105],
       [0.        , 1.52888179],
       [0.        , 1.52888799],
       [0.        , 1.52891111],
       [0.        , 1.52895653],
       [0.        , 1.52896261],
       [0.        , 1.52898884],
       [0.        , 1.52908659],
       [0.        , 1.52910197],
       [0.        , 1.5291338 ],
       [0.        , 1.52926934],
       [0.        , 1.52936232],
       [0.        , 1.5293715 ],
       [0.        , 1.52943683],
       [0.        , 1.52951479],
       [0.        , 1.5295155 ],
       [0.        , 1.52954078],
       [0.        , 1.52961826],
       [0.        , 1.52962232],
       [0.        , 1.52963924],
       [0.        , 1.52968001],
       [0.        , 1.52971196],
       [0.        , 1.52981627],
       [0.        , 1.52984357],
       [0.        , 1.5298866 ],
       [0.        , 1.52999067],
       [0.        , 1.53012371],
       [0.        , 1.53017914],
       [0.        , 1.53030503],
       [0.        , 1.53034413],
       [0.        , 1.53049123],
       [0.        , 1.53074324],
       [0.        , 1.53091741],
       [0.        , 1.53110957],
       [0.        , 1.53112602],
       [0.        , 1.53116512],
       [0.        , 1.5312562 ],
       [0.        , 1.536497  ],
       [0.        , 1.5545752 ],
       [0.        , 1.5556891 ],
       [0.        , 1.60146832]]), array([[6.88291788, 6.90655041],
       [6.23672962, 6.76061201],
       [6.22098494, 7.17317295],
       [5.85720253, 6.04689789],
       [5.79131699, 6.18698263],
       [5.51319742, 5.91572809],
       [5.44608021, 7.26590919],
       [5.15933657, 5.26040125],
       [5.14661074, 6.09098196],
       [4.87230873, 5.2933135 ],
       [4.86061668, 5.79051113],
       [4.8057704 , 4.868752  ],
       [4.76878214, 5.07602596],
       [4.70340443, 5.19576263],
       [4.57944012, 4.59026194],
       [4.2575841 , 4.76623583],
       [4.25594664, 4.37027359],
       [4.21499491, 4.70871687],
       [4.21275139, 4.4751482 ],
       [4.20003891, 5.57758951],
       [4.16782475, 4.26217699],
       [4.1322999 , 4.41539049],
       [4.10980606, 4.2760644 ],
       [4.08373737, 4.36013699],
       [4.08269262, 4.55036116],
       [4.07146358, 4.32170105],
       [4.03450871, 4.30121803],
       [4.0228076 , 4.3825469 ],
       [4.02062941, 4.46108961],
       [4.01594257, 4.44883299],
       [4.00953054, 4.3278017 ],
       [4.00949574, 4.49690104],
       [4.00575733, 4.79044628],
       [4.00411367, 7.10603762],
       [4.00076246, 4.70111513],
       [3.98481297, 4.43534899],
       [3.96807408, 4.21870184],
       [3.96049833, 6.16964722],
       [3.95413518, 4.66569662],
       [3.93478131, 8.791749  ],
       [3.89080596, 4.83024931],
       [3.88321757, 4.24581289],
       [3.85853052, 4.46191788],
       [3.84080076, 4.54761076],
       [3.81258845, 8.21769238],
       [3.81187248, 4.343894  ],
       [3.7721374 , 5.43353605],
       [3.75520682, 4.77974701],
       [3.74971485, 4.1390872 ],
       [3.7470324 , 4.54952288],
       [3.7094543 , 8.26900673]]), array([[11.06998158, 11.0957098 ],
       [ 9.85433674, 10.7460289 ],
       [ 9.83918285,  9.84557629],
       [ 9.23309422, 11.28107357],
       [ 4.81665277,  4.99419737]])]2024-03-06 17:52:57.671982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KCS ph vector generated, counter: 113
2024-03-06 17:53:01.780941: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:01.823636: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:02.738809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KDQ ph vector generated, counter: 114
2024-03-06 17:53:06.011471: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:06.054861: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:07.116312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KDR ph vector generated, counter: 115
2024-03-06 17:53:10.894156: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:10.937240: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:12.011065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KDS ph vector generated, counter: 116
2024-03-06 17:53:15.426269: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:15.468920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:16.402794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KE9 ph vector generated, counter: 117
2024-03-06 17:53:19.662300: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:19.705241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:20.574869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.2908044), (0., 1.3189961), (0., 1.3253789), (0., 1.3254772),
       (0., 1.3260852), (0., 1.3265165), (0., 1.3270636), (0., 1.3278638),
       (0., 1.3282549), (0., 1.3282701), (0., 1.3283324), (0., 1.3286089),
       (0., 1.3286363), (0., 1.3290046), (0., 1.3291683), (0., 1.3291975),
       (0., 1.3293393), (0., 1.3294052), (0., 1.3294541), (0., 1.3297904),
       (0., 1.3299017), (0., 1.3300123), (0., 1.330177 ), (0., 1.3306395),
       (0., 1.3306973), (0., 1.3307611), (0., 1.3307636), (0., 1.330766 ),
       (0., 1.3309209), (0., 1.3309381), (0., 1.3310398), (0., 1.3311452),
       (0., 1.3311927), (0., 1.331402 ), (0., 1.3314049), (0., 1.3315173),
       (0., 1.3316432), (0., 1.33187  ), (0., 1.3319938), (0., 1.3321815),
       (0., 1.3324147), (0., 1.332507 ), (0., 1.3326349), (0., 1.3328198),
       (0., 1.3328623), (0., 1.3331602), (0., 1.3333569), (0., 1.3333908),
       (0., 1.3334128), (0., 1.3334321), (0., 1.3334352), (0., 1.333535 ),
       (0., 1.3338071), (0., 1.3338157), (0., 1.3340122), (0., 1.3340815),
       (0., 1.3342388), (0., 1.3342569), (0., 1.3344887), (0., 1.3345493),
       (0., 1.3345613), (0., 1.334594 ), (0., 1.3346387), (0., 1.3347139),
       (0., 1.3347698), (0., 1.3348173), (0., 1.3349797), (0., 1.3349986),
       (0., 1.3350055), (0., 1.3350245), (0., 1.3351429), (0., 1.3351872),
       (0., 1.3356221), (0., 1.3357141), (0., 1.3358126), (0., 1.3358195),
       (0., 1.335842 ), (0., 1.3360857), (0., 1.3361336), (0., 1.3361396),
       (0., 1.3361545), (0., 1.3361945), (0., 1.3363923), (0., 1.3364284),
       (0., 1.3364681), (0., 1.3365351), (0., 1.3365622), (0., 1.336734 ),
       (0., 1.336755 ), (0., 1.3369296), (0., 1.3371725), (0., 1.3372852),
       (0., 1.3373269), (0., 1.3373297), (0., 1.3373841), (0., 1.3374158),
       (0., 1.3374871), (0., 1.3375926), (0., 1.3377705), (0., 1.337854 ),
       (0., 1.3378636), (0., 1.3381245), (0., 1.338446 ), (0., 1.338496 ),
       (0., 1.3386071), (0., 1.3386167), (0., 1.3389019), (0., 1.3390242),
       (0., 1.3391436), (0., 1.3392012), (0., 1.3394741), (0., 1.3396528),
       (0., 1.3397173), (0., 1.3398814), (0., 1.3402673), (0., 1.3408622),
       (0., 1.341012 ), (0., 1.3413541), (0., 1.3416355), (0., 1.3416505),
       (0., 1.3421081), (0., 1.3444155), (0., 1.3456497), (0., 1.3642724),
       (0., 1.3891846), (0., 1.396657 ), (0., 1.4037609), (0., 1.4406384),
       (0., 1.4456139), (0., 1.4464595), (0., 1.4508367), (0., 1.4529339),
       (0., 1.4532117), (0., 1.4534391), (0., 1.4535706), (0., 1.4537928),
       (0., 1.4538049), (0., 1.454655 ), (0., 1.4546556), (0., 1.4547176),
       (0., 1.4547659), (0., 1.4548258), (0., 1.4551837), (0., 1.4553474),
       (0., 1.4553539), (0., 1.4555252), (0., 1.4558234), (0., 1.4558414),
       (0., 1.4563625), (0., 1.4563715), (0., 1.4566517), (0., 1.4567921),
       (0., 1.4568313), (0., 1.4570801), (0., 1.4571058), (0., 1.4571096),
       (0., 1.4572074), (0., 1.4572893), (0., 1.4573462), (0., 1.4574162),
       (0., 1.4575418), (0., 1.4576104), (0., 1.4577097), (0., 1.457727 ),
       (0., 1.4578276), (0., 1.457942 ), (0., 1.4580929), (0., 1.4581254),
       (0., 1.4582162), (0., 1.4583436), (0., 1.4583685), (0., 1.4584411),
       (0., 1.458583 ), (0., 1.458586 ), (0., 1.4588217), (0., 1.4588373),
       (0., 1.4588394), (0., 1.4590839), (0., 1.4592335), (0., 1.459302 ),
       (0., 1.4593034), (0., 1.4593482), (0., 1.4593786), (0., 1.4595153),
       (0., 1.459744 ), (0., 1.4598308), (0., 1.4598585), (0., 1.4602041),
       (0., 1.4603018), (0., 1.460377 ), (0., 1.4604071), (0., 1.4604741),
       (0., 1.4607716), (0., 1.460785 ), (0., 1.4608198), (0., 1.4608786),
       (0., 1.4609654), (0., 1.461025 ), (0., 1.4610701), (0., 1.461147 ),
       (0., 1.4611698), (0., 1.4611727), (0., 1.4612067), (0., 1.4612415),
       (0., 1.4614023), (0., 1.4615605), (0., 1.4615889), (0., 1.4616488),
       (0., 1.4617872), (0., 1.4617882), (0., 1.4618586), (0., 1.461917 ),
       (0., 1.4619418), (0., 1.4620179), (0., 1.4620279), (0., 1.4621482),
       (0., 1.462171 ), (0., 1.462182 ), (0., 1.4622215), (0., 1.4622328),
       (0., 1.4625819), (0., 1.4627048), (0., 1.4628253), (0., 1.4628611),
       (0., 1.4628996), (0., 1.4629917), (0., 1.4631861), (0., 1.4632404),
       (0., 1.4634321), (0., 1.4635072), (0., 1.4635432), (0., 1.4638393),
       (0., 1.4639596), (0., 1.4642506), (0., 1.4649731), (0., 1.4649758),
       (0., 1.4652462), (0., 1.4658308), (0., 1.4658936), (0., 1.4659551),
       (0., 1.4663916), (0., 1.4667002), (0., 1.4667908), (0., 1.466816 ),
       (0., 1.4675075), (0., 1.4677316), (0., 1.4681534), (0., 1.4683026),
       (0., 1.4699144), (0., 1.4715966), (0., 1.4727716), (0., 1.4775095),
       (0., 1.4794264), (0., 1.4800255), (0., 1.5093956), (0., 1.5126041),
       (0., 1.5128584), (0., 1.514441 ), (0., 1.5155393), (0., 1.5157077),
       (0., 1.5157528), (0., 1.5158662), (0., 1.5164641), (0., 1.5167482),
       (0., 1.5170839), (0., 1.5171313), (0., 1.5179105), (0., 1.5181953),
       (0., 1.5184228), (0., 1.518716 ), (0., 1.5187733), (0., 1.5189707),
       (0., 1.5190939), (0., 1.5193338), (0., 1.5196877), (0., 1.519889 ),
       (0., 1.5199302), (0., 1.51996  ), (0., 1.5202026), (0., 1.5202464),
       (0., 1.5202796), (0., 1.5204818), (0., 1.5205564), (0., 1.5207648),
       (0., 1.5208371), (0., 1.5208578), (0., 1.5210419), (0., 1.5210977),
       (0., 1.521183 ), (0., 1.5212053), (0., 1.521221 ), (0., 1.5212992),
       (0., 1.5213118), (0., 1.5213847), (0., 1.5214603), (0., 1.5215617),
       (0., 1.5217204), (0., 1.5217359), (0., 1.5217364), (0., 1.52178  ),
       (0., 1.5218121), (0., 1.5218412), (0., 1.521861 ), (0., 1.5218755),
       (0., 1.5218892), (0., 1.5219369), (0., 1.522052 ), (0., 1.5222365),
       (0., 1.5222615), (0., 1.5222954), (0., 1.5222956), (0., 1.5224599),
       (0., 1.5224857), (0., 1.5227655), (0., 1.5227784), (0., 1.5230349),
       (0., 1.5230569), (0., 1.5230728), (0., 1.5231149), (0., 1.523128 ),
       (0., 1.5231996), (0., 1.523344 ), (0., 1.5233457), (0., 1.5233546),
       (0., 1.5235268), (0., 1.5235413), (0., 1.5236026), (0., 1.5236151),
       (0., 1.5236284), (0., 1.5239384), (0., 1.5239731), (0., 1.5239766),
       (0., 1.5240818), (0., 1.52427  ), (0., 1.5242921), (0., 1.5243125),
       (0., 1.5244628), (0., 1.5245806), (0., 1.5245832), (0., 1.5246998),
       (0., 1.5247151), (0., 1.5247948), (0., 1.524873 ), (0., 1.5249645),
       (0., 1.5250232), (0., 1.5250686), (0., 1.5250978), (0., 1.5251125),
       (0., 1.525114 ), (0., 1.5254767), (0., 1.525554 ), (0., 1.525598 ),
       (0., 1.5256394), (0., 1.5257382), (0., 1.525758 ), (0., 1.5257616),
       (0., 1.5259434), (0., 1.5259622), (0., 1.5260807), (0., 1.5264567),
       (0., 1.5267442), (0., 1.5268795), (0., 1.5268798), (0., 1.5269111),
       (0., 1.5269606), (0., 1.5273461), (0., 1.5275055), (0., 1.5282516),
       (0., 1.5282645), (0., 1.5286797), (0., 1.528845 ), (0., 1.5289121),
       (0., 1.5290753), (0., 1.5291946), (0., 1.5299071), (0., 1.5299311),
       (0., 1.5303059), (0., 1.5305822), (0., 1.5308459), (0., 1.5523207)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.47255  , 8.531128 ), (8.001783 , 8.4583845),
       (6.9962497, 7.5615034), (6.9877305, 7.2085066),
       (6.6107526, 6.9019465), (6.605765 , 7.2378306),
       (6.469449 , 7.014482 ), (6.4485974, 7.06576  ),
       (6.2180185, 7.814379 ), (6.1059875, 7.2822447),
       (6.095446 , 7.0418763), (5.984579 , 6.066814 ),
       (5.9490657, 6.5014544), (5.867684 , 6.2712646),
       (5.6818643, 5.946941 ), (5.567272 , 8.52363  ),
       (5.5052133, 5.602578 ), (5.3739543, 8.350748 ),
       (5.2824297, 8.611108 ), (5.0731583, 6.860866 ),
       (4.9580927, 7.2251205), (4.4872828, 4.8299904),
       (4.486223 , 7.8628983), (4.4245872, 7.9876795),
       (4.357798 , 4.468585 ), (4.052916 , 4.271149 ),
       (3.9486055, 4.0188875), (3.948069 , 4.285784 ),
       (3.9351761, 4.7272487), (3.9043872, 3.9076314),
       (3.711708 , 8.535208 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.537729,  9.627317 ), (9.440768, 10.06595  ),
       (9.117783, 10.746545 ), (8.35596 ,  8.473534 ),
       (7.645828,  7.7144947)], dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.290804386138916), (0.0, 1.3189960718154907), (0.0, 1.3253788948059082), (0.0, 1.3254772424697876), (0.0, 1.3260852098464966), (0.0, 1.3265165090560913), (0.0, 1.3270635604858398), (0.0, 1.3278638124465942), (0.0, 1.3282549381256104), (0.0, 1.3282700777053833), (0.0, 1.3283324241638184), (0.0, 1.3286088705062866), (0.0, 1.3286362886428833), (0.0, 1.3290046453475952), (0.0, 1.3291683197021484), (0.0, 1.3291975259780884), (0.0, 1.3293392658233643), (0.0, 1.3294051885604858), (0.0, 1.3294540643692017), (0.0, 1.3297903537750244), (0.0, 1.3299016952514648), (0.0, 1.330012321472168), (0.0, 1.3301769495010376), (0.0, 1.3306394815444946), (0.0, 1.3306972980499268), (0.0, 1.3307610750198364), (0.0, 1.330763578414917), (0.0, 1.330765962600708), (0.0, 1.330920934677124), (0.0, 1.3309381008148193), (0.0, 1.3310397863388062), (0.0, 1.331145167350769), (0.0, 1.3311927318572998), (0.0, 1.3314019441604614), (0.0, 1.3314049243927002), (0.0, 1.3315173387527466), (0.0, 1.3316432237625122), (0.0, 1.3318699598312378), (0.0, 1.331993818283081), (0.0, 1.332181453704834), (0.0, 1.3324147462844849), (0.0, 1.3325070142745972), (0.0, 1.3326349258422852), (0.0, 1.3328198194503784), (0.0, 1.3328622579574585), (0.0, 1.333160161972046), (0.0, 1.3333568572998047), (0.0, 1.3333908319473267), (0.0, 1.333412766456604), (0.0, 1.3334320783615112), (0.0, 1.3334351778030396), (0.0, 1.3335349559783936), (0.0, 1.333807110786438), (0.0, 1.3338156938552856), (0.0, 1.3340121507644653), (0.0, 1.3340815305709839), (0.0, 1.3342387676239014), (0.0, 1.334256887435913), (0.0, 1.3344887495040894), (0.0, 1.3345493078231812), (0.0, 1.3345613479614258), (0.0, 1.3345940113067627), (0.0, 1.3346387147903442), (0.0, 1.3347139358520508), (0.0, 1.33476984500885), (0.0, 1.3348172903060913), (0.0, 1.3349796533584595), (0.0, 1.334998607635498), (0.0, 1.335005521774292), (0.0, 1.3350244760513306), (0.0, 1.3351428508758545), (0.0, 1.3351871967315674), (0.0, 1.3356220722198486), (0.0, 1.3357141017913818), (0.0, 1.3358125686645508), (0.0, 1.3358194828033447), (0.0, 1.3358420133590698), (0.0, 1.3360856771469116), (0.0, 1.336133599281311), (0.0, 1.3361395597457886), (0.0, 1.3361544609069824), (0.0, 1.3361945152282715), (0.0, 1.3363922834396362), (0.0, 1.3364284038543701), (0.0, 1.3364681005477905), (0.0, 1.336535096168518), (0.0, 1.336562156677246), (0.0, 1.3367340564727783), (0.0, 1.3367550373077393), (0.0, 1.3369295597076416), (0.0, 1.337172508239746), (0.0, 1.3372851610183716), (0.0, 1.3373268842697144), (0.0, 1.3373297452926636), (0.0, 1.3373841047286987), (0.0, 1.3374158143997192), (0.0, 1.3374871015548706), (0.0, 1.337592601776123), (0.0, 1.3377704620361328), (0.0, 1.337854027748108), (0.0, 1.337863564491272), (0.0, 1.3381245136260986), (0.0, 1.338446021080017), (0.0, 1.3384959697723389), (0.0, 1.3386070728302002), (0.0, 1.3386167287826538), (0.0, 1.3389018774032593), (0.0, 1.3390241861343384), (0.0, 1.3391436338424683), (0.0, 1.3392012119293213), (0.0, 1.339474081993103), (0.0, 1.3396527767181396), (0.0, 1.3397172689437866), (0.0, 1.339881420135498), (0.0, 1.340267300605774), (0.0, 1.3408621549606323), (0.0, 1.3410120010375977), (0.0, 1.3413541316986084), (0.0, 1.3416354656219482), (0.0, 1.3416504859924316), (0.0, 1.342108130455017), (0.0, 1.344415545463562), (0.0, 1.3456497192382812), (0.0, 1.3642723560333252), (0.0, 1.389184594154358), (0.0, 1.3966569900512695), (0.0, 1.4037609100341797), (0.0, 1.4406384229660034), (0.0, 1.4456138610839844), (0.0, 1.4464595317840576), (0.0, 1.4508366584777832), (0.0, 1.45293390750885), (0.0, 1.4532116651535034), (0.0, 1.4534391164779663), (0.0, 1.4535706043243408), (0.0, 1.4537928104400635), (0.0, 1.453804850578308), (0.0, 1.4546550512313843), (0.0, 1.454655647277832), (0.0, 1.4547176361083984), (0.0, 1.4547659158706665), (0.0, 1.454825758934021), (0.0, 1.455183744430542), (0.0, 1.4553474187850952), (0.0, 1.455353856086731), (0.0, 1.4555251598358154), (0.0, 1.4558234214782715), (0.0, 1.4558414220809937), (0.0, 1.4563624858856201), (0.0, 1.456371545791626), (0.0, 1.4566516876220703), (0.0, 1.4567921161651611), (0.0, 1.4568313360214233), (0.0, 1.4570801258087158), (0.0, 1.4571057558059692), (0.0, 1.4571095705032349), (0.0, 1.457207441329956), (0.0, 1.4572893381118774), (0.0, 1.4573462009429932), (0.0, 1.4574161767959595), (0.0, 1.457541823387146), (0.0, 1.4576103687286377), (0.0, 1.4577096700668335), (0.0, 1.4577269554138184), (0.0, 1.4578275680541992), (0.0, 1.457942008972168), (0.0, 1.4580929279327393), (0.0, 1.458125352859497), (0.0, 1.4582161903381348), (0.0, 1.4583436250686646), (0.0, 1.4583685398101807), (0.0, 1.458441138267517), (0.0, 1.4585829973220825), (0.0, 1.4585859775543213), (0.0, 1.4588216543197632), (0.0, 1.4588372707366943), (0.0, 1.4588394165039062), (0.0, 1.459083914756775), (0.0, 1.4592335224151611), (0.0, 1.4593019485473633), (0.0, 1.459303379058838), (0.0, 1.459348201751709), (0.0, 1.4593786001205444), (0.0, 1.4595153331756592), (0.0, 1.4597439765930176), (0.0, 1.4598307609558105), (0.0, 1.4598585367202759), (0.0, 1.4602041244506836), (0.0, 1.4603017568588257), (0.0, 1.4603769779205322), (0.0, 1.4604071378707886), (0.0, 1.4604741334915161), (0.0, 1.4607715606689453), (0.0, 1.4607850313186646), (0.0, 1.4608198404312134), (0.0, 1.460878610610962), (0.0, 1.4609653949737549), (0.0, 1.4610249996185303), (0.0, 1.4610700607299805), (0.0, 1.4611469507217407), (0.0, 1.4611698389053345), (0.0, 1.4611726999282837), (0.0, 1.4612066745758057), (0.0, 1.4612414836883545), (0.0, 1.4614022970199585), (0.0, 1.4615604877471924), (0.0, 1.4615888595581055), (0.0, 1.4616488218307495), (0.0, 1.461787223815918), (0.0, 1.4617881774902344), (0.0, 1.4618586301803589), (0.0, 1.4619170427322388), (0.0, 1.4619418382644653), (0.0, 1.4620178937911987), (0.0, 1.462027907371521), (0.0, 1.4621481895446777), (0.0, 1.462170958518982), (0.0, 1.4621820449829102), (0.0, 1.4622215032577515), (0.0, 1.4622328281402588), (0.0, 1.4625818729400635), (0.0, 1.4627047777175903), (0.0, 1.4628252983093262), (0.0, 1.4628610610961914), (0.0, 1.4628995656967163), (0.0, 1.462991714477539), (0.0, 1.4631861448287964), (0.0, 1.463240385055542), (0.0, 1.4634320735931396), (0.0, 1.4635071754455566), (0.0, 1.463543176651001), (0.0, 1.4638392925262451), (0.0, 1.4639595746994019), (0.0, 1.4642505645751953), (0.0, 1.4649730920791626), (0.0, 1.4649758338928223), (0.0, 1.4652462005615234), (0.0, 1.4658308029174805), (0.0, 1.4658936262130737), (0.0, 1.465955138206482), (0.0, 1.4663915634155273), (0.0, 1.4667001962661743), (0.0, 1.466790795326233), (0.0, 1.4668159484863281), (0.0, 1.4675074815750122), (0.0, 1.4677315950393677), (0.0, 1.4681533575057983), (0.0, 1.468302607536316), (0.0, 1.469914436340332), (0.0, 1.471596598625183), (0.0, 1.4727716445922852), (0.0, 1.4775094985961914), (0.0, 1.479426383972168), (0.0, 1.4800255298614502), (0.0, 1.5093955993652344), (0.0, 1.5126041173934937), (0.0, 1.5128583908081055), (0.0, 1.5144410133361816), (0.0, 1.515539288520813), (0.0, 1.5157077312469482), (0.0, 1.5157527923583984), (0.0, 1.5158661603927612), (0.0, 1.516464114189148), (0.0, 1.5167481899261475), (0.0, 1.5170838832855225), (0.0, 1.5171313285827637), (0.0, 1.5179104804992676), (0.0, 1.5181952714920044), (0.0, 1.5184228420257568), (0.0, 1.5187159776687622), (0.0, 1.5187733173370361), (0.0, 1.5189707279205322), (0.0, 1.5190938711166382), (0.0, 1.519333839416504), (0.0, 1.5196876525878906), (0.0, 1.519888997077942), (0.0, 1.5199302434921265), (0.0, 1.5199600458145142), (0.0, 1.52020263671875), (0.0, 1.5202463865280151), (0.0, 1.5202796459197998), (0.0, 1.520481824874878), (0.0, 1.5205564498901367), (0.0, 1.5207648277282715), (0.0, 1.5208370685577393), (0.0, 1.520857810974121), (0.0, 1.5210418701171875), (0.0, 1.5210976600646973), (0.0, 1.5211830139160156), (0.0, 1.5212053060531616), (0.0, 1.5212210416793823), (0.0, 1.5212992429733276), (0.0, 1.5213117599487305), (0.0, 1.5213847160339355), (0.0, 1.5214602947235107), (0.0, 1.5215617418289185), (0.0, 1.5217204093933105), (0.0, 1.5217359066009521), (0.0, 1.5217363834381104), (0.0, 1.521780014038086), (0.0, 1.521812081336975), (0.0, 1.5218411684036255), (0.0, 1.521860957145691), (0.0, 1.5218755006790161), (0.0, 1.5218892097473145), (0.0, 1.5219368934631348), (0.0, 1.5220520496368408), (0.0, 1.5222364664077759), (0.0, 1.5222615003585815), (0.0, 1.522295355796814), (0.0, 1.522295594215393), (0.0, 1.522459864616394), (0.0, 1.5224857330322266), (0.0, 1.5227655172348022), (0.0, 1.5227783918380737), (0.0, 1.523034930229187), (0.0, 1.5230568647384644), (0.0, 1.5230728387832642), (0.0, 1.5231149196624756), (0.0, 1.5231280326843262), (0.0, 1.5231995582580566), (0.0, 1.5233440399169922), (0.0, 1.523345708847046), (0.0, 1.5233546495437622), (0.0, 1.5235267877578735), (0.0, 1.5235413312911987), (0.0, 1.5236026048660278), (0.0, 1.5236151218414307), (0.0, 1.5236283540725708), (0.0, 1.5239384174346924), (0.0, 1.5239731073379517), (0.0, 1.5239765644073486), (0.0, 1.524081826210022), (0.0, 1.5242700576782227), (0.0, 1.5242921113967896), (0.0, 1.5243124961853027), (0.0, 1.5244628190994263), (0.0, 1.5245805978775024), (0.0, 1.5245832204818726), (0.0, 1.5246998071670532), (0.0, 1.5247150659561157), (0.0, 1.5247948169708252), (0.0, 1.5248730182647705), (0.0, 1.524964451789856), (0.0, 1.5250232219696045), (0.0, 1.5250686407089233), (0.0, 1.5250978469848633), (0.0, 1.525112509727478), (0.0, 1.5251140594482422), (0.0, 1.5254766941070557), (0.0, 1.5255539417266846), (0.0, 1.5255980491638184), (0.0, 1.5256394147872925), (0.0, 1.52573823928833), (0.0, 1.5257580280303955), (0.0, 1.525761604309082), (0.0, 1.525943398475647), (0.0, 1.525962233543396), (0.0, 1.5260807275772095), (0.0, 1.5264567136764526), (0.0, 1.5267442464828491), (0.0, 1.5268795490264893), (0.0, 1.5268797874450684), (0.0, 1.5269111394882202), (0.0, 1.5269606113433838), (0.0, 1.527346134185791), (0.0, 1.5275055170059204), (0.0, 1.5282516479492188), (0.0, 1.5282645225524902), (0.0, 1.5286797285079956), (0.0, 1.528844952583313), (0.0, 1.52891206741333), (0.0, 1.529075264930725), (0.0, 1.5291945934295654), (0.0, 1.5299071073532104), (0.0, 1.5299310684204102), (0.0, 1.5303058624267578), (0.0, 1.5305821895599365), (0.0, 1.5308458805084229), (0.0, 1.5523207187652588)], [(8.472550392150879, 8.5311279296875), (8.00178337097168, 8.45838451385498), (6.996249675750732, 7.5615034103393555), (6.987730503082275, 7.2085065841674805), (6.610752582550049, 6.901946544647217), (6.605764865875244, 7.237830638885498), (6.469449043273926, 7.014482021331787), (6.448597431182861, 7.065760135650635), (6.218018531799316, 7.8143792152404785), (6.105987548828125, 7.282244682312012), (6.0954461097717285, 7.041876316070557), (5.984579086303711, 6.066813945770264), (5.949065685272217, 6.5014543533325195), (5.8676838874816895, 6.271264553070068), (5.681864261627197, 5.946940898895264), (5.567272186279297, 8.523630142211914), (5.505213260650635, 5.602578163146973), (5.3739542961120605, 8.350748062133789), (5.2824296951293945, 8.61110782623291), (5.073158264160156, 6.860866069793701), (4.95809268951416, 7.225120544433594), (4.487282752990723, 4.829990386962891), (4.486223220825195, 7.862898349761963), (4.424587249755859, 7.987679481506348), (4.357798099517822, 4.468585014343262), (4.052916049957275, 4.271149158477783), (3.948605537414551, 4.018887519836426), (3.9480690956115723, 4.285783767700195), (3.935176134109497, 4.727248668670654), (3.9043872356414795, 3.9076313972473145), (3.7117080688476562, 8.535207748413086)], [(9.537729263305664, 9.627317428588867), (9.440768241882324, 10.065950393676758), (9.117782592773438, 10.74654483795166), (8.35595989227295, 8.473533630371094), (7.645827770233154, 7.714494705200195)]]
[array([[0.        , 1.29080439],
       [0.        , 1.31899607],
       [0.        , 1.32537889],
       [0.        , 1.32547724],
       [0.        , 1.32608521],
       [0.        , 1.32651651],
       [0.        , 1.32706356],
       [0.        , 1.32786381],
       [0.        , 1.32825494],
       [0.        , 1.32827008],
       [0.        , 1.32833242],
       [0.        , 1.32860887],
       [0.        , 1.32863629],
       [0.        , 1.32900465],
       [0.        , 1.32916832],
       [0.        , 1.32919753],
       [0.        , 1.32933927],
       [0.        , 1.32940519],
       [0.        , 1.32945406],
       [0.        , 1.32979035],
       [0.        , 1.3299017 ],
       [0.        , 1.33001232],
       [0.        , 1.33017695],
       [0.        , 1.33063948],
       [0.        , 1.3306973 ],
       [0.        , 1.33076108],
       [0.        , 1.33076358],
       [0.        , 1.33076596],
       [0.        , 1.33092093],
       [0.        , 1.3309381 ],
       [0.        , 1.33103979],
       [0.        , 1.33114517],
       [0.        , 1.33119273],
       [0.        , 1.33140194],
       [0.        , 1.33140492],
       [0.        , 1.33151734],
       [0.        , 1.33164322],
       [0.        , 1.33186996],
       [0.        , 1.33199382],
       [0.        , 1.33218145],
       [0.        , 1.33241475],
       [0.        , 1.33250701],
       [0.        , 1.33263493],
       [0.        , 1.33281982],
       [0.        , 1.33286226],
       [0.        , 1.33316016],
       [0.        , 1.33335686],
       [0.        , 1.33339083],
       [0.        , 1.33341277],
       [0.        , 1.33343208],
       [0.        , 1.33343518],
       [0.        , 1.33353496],
       [0.        , 1.33380711],
       [0.        , 1.33381569],
       [0.        , 1.33401215],
       [0.        , 1.33408153],
       [0.        , 1.33423877],
       [0.        , 1.33425689],
       [0.        , 1.33448875],
       [0.        , 1.33454931],
       [0.        , 1.33456135],
       [0.        , 1.33459401],
       [0.        , 1.33463871],
       [0.        , 1.33471394],
       [0.        , 1.33476985],
       [0.        , 1.33481729],
       [0.        , 1.33497965],
       [0.        , 1.33499861],
       [0.        , 1.33500552],
       [0.        , 1.33502448],
       [0.        , 1.33514285],
       [0.        , 1.3351872 ],
       [0.        , 1.33562207],
       [0.        , 1.3357141 ],
       [0.        , 1.33581257],
       [0.        , 1.33581948],
       [0.        , 1.33584201],
       [0.        , 1.33608568],
       [0.        , 1.3361336 ],
       [0.        , 1.33613956],
       [0.        , 1.33615446],
       [0.        , 1.33619452],
       [0.        , 1.33639228],
       [0.        , 1.3364284 ],
       [0.        , 1.3364681 ],
       [0.        , 1.3365351 ],
       [0.        , 1.33656216],
       [0.        , 1.33673406],
       [0.        , 1.33675504],
       [0.        , 1.33692956],
       [0.        , 1.33717251],
       [0.        , 1.33728516],
       [0.        , 1.33732688],
       [0.        , 1.33732975],
       [0.        , 1.3373841 ],
       [0.        , 1.33741581],
       [0.        , 1.3374871 ],
       [0.        , 1.3375926 ],
       [0.        , 1.33777046],
       [0.        , 1.33785403],
       [0.        , 1.33786356],
       [0.        , 1.33812451],
       [0.        , 1.33844602],
       [0.        , 1.33849597],
       [0.        , 1.33860707],
       [0.        , 1.33861673],
       [0.        , 1.33890188],
       [0.        , 1.33902419],
       [0.        , 1.33914363],
       [0.        , 1.33920121],
       [0.        , 1.33947408],
       [0.        , 1.33965278],
       [0.        , 1.33971727],
       [0.        , 1.33988142],
       [0.        , 1.3402673 ],
       [0.        , 1.34086215],
       [0.        , 1.341012  ],
       [0.        , 1.34135413],
       [0.        , 1.34163547],
       [0.        , 1.34165049],
       [0.        , 1.34210813],
       [0.        , 1.34441555],
       [0.        , 1.34564972],
       [0.        , 1.36427236],
       [0.        , 1.38918459],
       [0.        , 1.39665699],
       [0.        , 1.40376091],
       [0.        , 1.44063842],
       [0.        , 1.44561386],
       [0.        , 1.44645953],
       [0.        , 1.45083666],
       [0.        , 1.45293391],
       [0.        , 1.45321167],
       [0.        , 1.45343912],
       [0.        , 1.4535706 ],
       [0.        , 1.45379281],
       [0.        , 1.45380485],
       [0.        , 1.45465505],
       [0.        , 1.45465565],
       [0.        , 1.45471764],
       [0.        , 1.45476592],
       [0.        , 1.45482576],
       [0.        , 1.45518374],
       [0.        , 1.45534742],
       [0.        , 1.45535386],
       [0.        , 1.45552516],
       [0.        , 1.45582342],
       [0.        , 1.45584142],
       [0.        , 1.45636249],
       [0.        , 1.45637155],
       [0.        , 1.45665169],
       [0.        , 1.45679212],
       [0.        , 1.45683134],
       [0.        , 1.45708013],
       [0.        , 1.45710576],
       [0.        , 1.45710957],
       [0.        , 1.45720744],
       [0.        , 1.45728934],
       [0.        , 1.4573462 ],
       [0.        , 1.45741618],
       [0.        , 1.45754182],
       [0.        , 1.45761037],
       [0.        , 1.45770967],
       [0.        , 1.45772696],
       [0.        , 1.45782757],
       [0.        , 1.45794201],
       [0.        , 1.45809293],
       [0.        , 1.45812535],
       [0.        , 1.45821619],
       [0.        , 1.45834363],
       [0.        , 1.45836854],
       [0.        , 1.45844114],
       [0.        , 1.458583  ],
       [0.        , 1.45858598],
       [0.        , 1.45882165],
       [0.        , 1.45883727],
       [0.        , 1.45883942],
       [0.        , 1.45908391],
       [0.        , 1.45923352],
       [0.        , 1.45930195],
       [0.        , 1.45930338],
       [0.        , 1.4593482 ],
       [0.        , 1.4593786 ],
       [0.        , 1.45951533],
       [0.        , 1.45974398],
       [0.        , 1.45983076],
       [0.        , 1.45985854],
       [0.        , 1.46020412],
       [0.        , 1.46030176],
       [0.        , 1.46037698],
       [0.        , 1.46040714],
       [0.        , 1.46047413],
       [0.        , 1.46077156],
       [0.        , 1.46078503],
       [0.        , 1.46081984],
       [0.        , 1.46087861],
       [0.        , 1.46096539],
       [0.        , 1.461025  ],
       [0.        , 1.46107006],
       [0.        , 1.46114695],
       [0.        , 1.46116984],
       [0.        , 1.4611727 ],
       [0.        , 1.46120667],
       [0.        , 1.46124148],
       [0.        , 1.4614023 ],
       [0.        , 1.46156049],
       [0.        , 1.46158886],
       [0.        , 1.46164882],
       [0.        , 1.46178722],
       [0.        , 1.46178818],
       [0.        , 1.46185863],
       [0.        , 1.46191704],
       [0.        , 1.46194184],
       [0.        , 1.46201789],
       [0.        , 1.46202791],
       [0.        , 1.46214819],
       [0.        , 1.46217096],
       [0.        , 1.46218204],
       [0.        , 1.4622215 ],
       [0.        , 1.46223283],
       [0.        , 1.46258187],
       [0.        , 1.46270478],
       [0.        , 1.4628253 ],
       [0.        , 1.46286106],
       [0.        , 1.46289957],
       [0.        , 1.46299171],
       [0.        , 1.46318614],
       [0.        , 1.46324039],
       [0.        , 1.46343207],
       [0.        , 1.46350718],
       [0.        , 1.46354318],
       [0.        , 1.46383929],
       [0.        , 1.46395957],
       [0.        , 1.46425056],
       [0.        , 1.46497309],
       [0.        , 1.46497583],
       [0.        , 1.4652462 ],
       [0.        , 1.4658308 ],
       [0.        , 1.46589363],
       [0.        , 1.46595514],
       [0.        , 1.46639156],
       [0.        , 1.4667002 ],
       [0.        , 1.4667908 ],
       [0.        , 1.46681595],
       [0.        , 1.46750748],
       [0.        , 1.4677316 ],
       [0.        , 1.46815336],
       [0.        , 1.46830261],
       [0.        , 1.46991444],
       [0.        , 1.4715966 ],
       [0.        , 1.47277164],
       [0.        , 1.4775095 ],
       [0.        , 1.47942638],
       [0.        , 1.48002553],
       [0.        , 1.5093956 ],
       [0.        , 1.51260412],
       [0.        , 1.51285839],
       [0.        , 1.51444101],
       [0.        , 1.51553929],
       [0.        , 1.51570773],
       [0.        , 1.51575279],
       [0.        , 1.51586616],
       [0.        , 1.51646411],
       [0.        , 1.51674819],
       [0.        , 1.51708388],
       [0.        , 1.51713133],
       [0.        , 1.51791048],
       [0.        , 1.51819527],
       [0.        , 1.51842284],
       [0.        , 1.51871598],
       [0.        , 1.51877332],
       [0.        , 1.51897073],
       [0.        , 1.51909387],
       [0.        , 1.51933384],
       [0.        , 1.51968765],
       [0.        , 1.519889  ],
       [0.        , 1.51993024],
       [0.        , 1.51996005],
       [0.        , 1.52020264],
       [0.        , 1.52024639],
       [0.        , 1.52027965],
       [0.        , 1.52048182],
       [0.        , 1.52055645],
       [0.        , 1.52076483],
       [0.        , 1.52083707],
       [0.        , 1.52085781],
       [0.        , 1.52104187],
       [0.        , 1.52109766],
       [0.        , 1.52118301],
       [0.        , 1.52120531],
       [0.        , 1.52122104],
       [0.        , 1.52129924],
       [0.        , 1.52131176],
       [0.        , 1.52138472],
       [0.        , 1.52146029],
       [0.        , 1.52156174],
       [0.        , 1.52172041],
       [0.        , 1.52173591],
       [0.        , 1.52173638],
       [0.        , 1.52178001],
       [0.        , 1.52181208],
       [0.        , 1.52184117],
       [0.        , 1.52186096],
       [0.        , 1.5218755 ],
       [0.        , 1.52188921],
       [0.        , 1.52193689],
       [0.        , 1.52205205],
       [0.        , 1.52223647],
       [0.        , 1.5222615 ],
       [0.        , 1.52229536],
       [0.        , 1.52229559],
       [0.        , 1.52245986],
       [0.        , 1.52248573],
       [0.        , 1.52276552],
       [0.        , 1.52277839],
       [0.        , 1.52303493],
       [0.        , 1.52305686],
       [0.        , 1.52307284],
       [0.        , 1.52311492],
       [0.        , 1.52312803],
       [0.        , 1.52319956],
       [0.        , 1.52334404],
       [0.        , 1.52334571],
       [0.        , 1.52335465],
       [0.        , 1.52352679],
       [0.        , 1.52354133],
       [0.        , 1.5236026 ],
       [0.        , 1.52361512],
       [0.        , 1.52362835],
       [0.        , 1.52393842],
       [0.        , 1.52397311],
       [0.        , 1.52397656],
       [0.        , 1.52408183],
       [0.        , 1.52427006],
       [0.        , 1.52429211],
       [0.        , 1.5243125 ],
       [0.        , 1.52446282],
       [0.        , 1.5245806 ],
       [0.        , 1.52458322],
       [0.        , 1.52469981],
       [0.        , 1.52471507],
       [0.        , 1.52479482],
       [0.        , 1.52487302],
       [0.        , 1.52496445],
       [0.        , 1.52502322],
       [0.        , 1.52506864],
       [0.        , 1.52509785],
       [0.        , 1.52511251],
       [0.        , 1.52511406],
       [0.        , 1.52547669],
       [0.        , 1.52555394],
       [0.        , 1.52559805],
       [0.        , 1.52563941],
       [0.        , 1.52573824],
       [0.        , 1.52575803],
       [0.        , 1.5257616 ],
       [0.        , 1.5259434 ],
       [0.        , 1.52596223],
       [0.        , 1.52608073],
       [0.        , 1.52645671],
       [0.        , 1.52674425],
       [0.        , 1.52687955],
       [0.        , 1.52687979],
       [0.        , 1.52691114],
       [0.        , 1.52696061],
       [0.        , 1.52734613],
       [0.        , 1.52750552],
       [0.        , 1.52825165],
       [0.        , 1.52826452],
       [0.        , 1.52867973],
       [0.        , 1.52884495],
       [0.        , 1.52891207],
       [0.        , 1.52907526],
       [0.        , 1.52919459],
       [0.        , 1.52990711],
       [0.        , 1.52993107],
       [0.        , 1.53030586],
       [0.        , 1.53058219],
       [0.        , 1.53084588],
       [0.        , 1.55232072]]), array([[8.47255039, 8.53112793],
       [8.00178337, 8.45838451],
       [6.99624968, 7.56150341],
       [6.9877305 , 7.20850658],
       [6.61075258, 6.90194654],
       [6.60576487, 7.23783064],
       [6.46944904, 7.01448202],
       [6.44859743, 7.06576014],
       [6.21801853, 7.81437922],
       [6.10598755, 7.28224468],
       [6.09544611, 7.04187632],
       [5.98457909, 6.06681395],
       [5.94906569, 6.50145435],
       [5.86768389, 6.27126455],
       [5.68186426, 5.9469409 ],
       [5.56727219, 8.52363014],
       [5.50521326, 5.60257816],
       [5.3739543 , 8.35074806],
       [5.2824297 , 8.61110783],
       [5.07315826, 6.86086607],
       [4.95809269, 7.22512054],
       [4.48728275, 4.82999039],
       [4.48622322, 7.86289835],
       [4.42458725, 7.98767948],
       [4.3577981 , 4.46858501],
       [4.05291605, 4.27114916],
       [3.94860554, 4.01888752],
       [3.9480691 , 4.28578377],
       [3.93517613, 4.72724867],
       [3.90438724, 3.9076314 ],
       [3.71170807, 8.53520775]]), array([[ 9.53772926,  9.62731743],
       [ 9.44076824, 10.06595039],
       [ 9.11778259, 10.74654484],
       [ 8.35595989,  8.47353363],
       [ 7.64582777,  7.71449471]])]2024-03-06 17:53:24.579002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KEC ph vector generated, counter: 118
2024-03-06 17:53:28.498065: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:28.541416: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:29.605158: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3200511), (0., 1.3204652), (0., 1.321599 ), (0., 1.3217331),
       (0., 1.3219175), (0., 1.3221833), (0., 1.322461 ), (0., 1.322918 ),
       (0., 1.3230362), (0., 1.3231107), (0., 1.3247439), (0., 1.324789 ),
       (0., 1.3255538), (0., 1.3257428), (0., 1.3261504), (0., 1.3263564),
       (0., 1.3267281), (0., 1.326774 ), (0., 1.3269733), (0., 1.3269856),
       (0., 1.3276887), (0., 1.328455 ), (0., 1.3286926), (0., 1.3286936),
       (0., 1.3287358), (0., 1.3287607), (0., 1.3288646), (0., 1.3293124),
       (0., 1.329318 ), (0., 1.3294545), (0., 1.3297703), (0., 1.329908 ),
       (0., 1.330019 ), (0., 1.3300234), (0., 1.3300588), (0., 1.3301136),
       (0., 1.33031  ), (0., 1.3303276), (0., 1.3304353), (0., 1.3306576),
       (0., 1.3306621), (0., 1.3306781), (0., 1.3306803), (0., 1.330709 ),
       (0., 1.3307271), (0., 1.3307389), (0., 1.3308697), (0., 1.3310819),
       (0., 1.3315648), (0., 1.3315834), (0., 1.3315889), (0., 1.3316265),
       (0., 1.3316358), (0., 1.3318042), (0., 1.3318278), (0., 1.3318572),
       (0., 1.3319753), (0., 1.332103 ), (0., 1.3321193), (0., 1.3322308),
       (0., 1.3324773), (0., 1.3325821), (0., 1.3327066), (0., 1.3327595),
       (0., 1.3328192), (0., 1.3330542), (0., 1.3332777), (0., 1.3334384),
       (0., 1.3334503), (0., 1.3335154), (0., 1.3337033), (0., 1.3337566),
       (0., 1.3338354), (0., 1.3339069), (0., 1.3340971), (0., 1.3341906),
       (0., 1.3344188), (0., 1.3344201), (0., 1.3345343), (0., 1.3345377),
       (0., 1.3346255), (0., 1.3347061), (0., 1.3347664), (0., 1.3348467),
       (0., 1.3348935), (0., 1.3350533), (0., 1.3351234), (0., 1.3352691),
       (0., 1.3353544), (0., 1.3355674), (0., 1.3357083), (0., 1.3357309),
       (0., 1.335883 ), (0., 1.33615  ), (0., 1.3364388), (0., 1.3364927),
       (0., 1.3366936), (0., 1.3368274), (0., 1.337039 ), (0., 1.3370658),
       (0., 1.3371204), (0., 1.3372484), (0., 1.3372965), (0., 1.3375876),
       (0., 1.3379856), (0., 1.3379933), (0., 1.3380286), (0., 1.3383037),
       (0., 1.3384483), (0., 1.3388143), (0., 1.3389171), (0., 1.3395162),
       (0., 1.3397013), (0., 1.3397101), (0., 1.3400854), (0., 1.3401715),
       (0., 1.3403035), (0., 1.3403888), (0., 1.3408297), (0., 1.3412344),
       (0., 1.3413043), (0., 1.3415531), (0., 1.3416369), (0., 1.3420138),
       (0., 1.3430505), (0., 1.3586226), (0., 1.4396185), (0., 1.4447544),
       (0., 1.4471424), (0., 1.4480443), (0., 1.4480453), (0., 1.4489996),
       (0., 1.4490715), (0., 1.4490905), (0., 1.4493064), (0., 1.4493862),
       (0., 1.4501799), (0., 1.4512565), (0., 1.4515345), (0., 1.4516215),
       (0., 1.4521909), (0., 1.4522195), (0., 1.4524304), (0., 1.4524522),
       (0., 1.4527426), (0., 1.4530002), (0., 1.4530762), (0., 1.4532503),
       (0., 1.4535066), (0., 1.4536492), (0., 1.4538927), (0., 1.4539056),
       (0., 1.4545686), (0., 1.4545832), (0., 1.4546087), (0., 1.4546728),
       (0., 1.454805 ), (0., 1.4549528), (0., 1.454979 ), (0., 1.4550334),
       (0., 1.455076 ), (0., 1.4553009), (0., 1.4553074), (0., 1.4554526),
       (0., 1.4556162), (0., 1.4557204), (0., 1.4558347), (0., 1.4558706),
       (0., 1.4559056), (0., 1.456023 ), (0., 1.456128 ), (0., 1.4562657),
       (0., 1.4564421), (0., 1.4564993), (0., 1.4565558), (0., 1.4566267),
       (0., 1.4566574), (0., 1.4568024), (0., 1.4568063), (0., 1.4569921),
       (0., 1.4570932), (0., 1.4572068), (0., 1.4573045), (0., 1.4573601),
       (0., 1.4574162), (0., 1.4574482), (0., 1.4574677), (0., 1.4574783),
       (0., 1.457672 ), (0., 1.4578807), (0., 1.45792  ), (0., 1.4580469),
       (0., 1.458146 ), (0., 1.4583946), (0., 1.4587716), (0., 1.4589076),
       (0., 1.4589409), (0., 1.4589785), (0., 1.4593494), (0., 1.4594595),
       (0., 1.4594768), (0., 1.4595184), (0., 1.4597024), (0., 1.4597764),
       (0., 1.4598677), (0., 1.4600494), (0., 1.4600555), (0., 1.4601263),
       (0., 1.4601367), (0., 1.4602298), (0., 1.4602377), (0., 1.4606034),
       (0., 1.4607035), (0., 1.4607972), (0., 1.4610693), (0., 1.4611362),
       (0., 1.4614551), (0., 1.4614682), (0., 1.4617461), (0., 1.4619993),
       (0., 1.462022 ), (0., 1.4620737), (0., 1.4622028), (0., 1.4622535),
       (0., 1.4625027), (0., 1.4625689), (0., 1.4626433), (0., 1.4627388),
       (0., 1.4628415), (0., 1.4633902), (0., 1.4635514), (0., 1.4637617),
       (0., 1.4640503), (0., 1.464346 ), (0., 1.4646088), (0., 1.4648418),
       (0., 1.4648851), (0., 1.465095 ), (0., 1.465216 ), (0., 1.465287 ),
       (0., 1.4654222), (0., 1.4655745), (0., 1.4656564), (0., 1.4659292),
       (0., 1.4664291), (0., 1.4668424), (0., 1.4671866), (0., 1.4678211),
       (0., 1.4679846), (0., 1.4684291), (0., 1.4696112), (0., 1.4749364),
       (0., 1.4767333), (0., 1.5079411), (0., 1.5105041), (0., 1.5105469),
       (0., 1.5115576), (0., 1.5124295), (0., 1.512728 ), (0., 1.5127409),
       (0., 1.5131426), (0., 1.5135458), (0., 1.5136409), (0., 1.5138762),
       (0., 1.5139791), (0., 1.5144062), (0., 1.5146102), (0., 1.5148547),
       (0., 1.5153521), (0., 1.5157788), (0., 1.5159115), (0., 1.5159122),
       (0., 1.5162321), (0., 1.5167831), (0., 1.5169016), (0., 1.5170577),
       (0., 1.5171499), (0., 1.5173584), (0., 1.5175046), (0., 1.5175107),
       (0., 1.5175681), (0., 1.517683 ), (0., 1.5176903), (0., 1.5177021),
       (0., 1.5177433), (0., 1.5178211), (0., 1.5178801), (0., 1.5179372),
       (0., 1.5179911), (0., 1.5180304), (0., 1.5181501), (0., 1.518269 ),
       (0., 1.5187201), (0., 1.5187874), (0., 1.5190281), (0., 1.5193139),
       (0., 1.5198795), (0., 1.5198905), (0., 1.5200982), (0., 1.5201391),
       (0., 1.5203612), (0., 1.5205122), (0., 1.5205293), (0., 1.5206441),
       (0., 1.5207474), (0., 1.5208699), (0., 1.5209616), (0., 1.5210527),
       (0., 1.5211374), (0., 1.5213693), (0., 1.521417 ), (0., 1.5214602),
       (0., 1.5216272), (0., 1.5216326), (0., 1.521852 ), (0., 1.5220097),
       (0., 1.5220318), (0., 1.5220926), (0., 1.5221065), (0., 1.5223846),
       (0., 1.522556 ), (0., 1.5228359), (0., 1.5230333), (0., 1.5230631),
       (0., 1.5230888), (0., 1.5232319), (0., 1.5233076), (0., 1.5233561),
       (0., 1.5234569), (0., 1.5234611), (0., 1.5235068), (0., 1.5235975),
       (0., 1.5236143), (0., 1.5236392), (0., 1.5237702), (0., 1.5240191),
       (0., 1.5240493), (0., 1.5243049), (0., 1.5243233), (0., 1.5243331),
       (0., 1.524481 ), (0., 1.5246023), (0., 1.5248798), (0., 1.5248884),
       (0., 1.52491  ), (0., 1.524967 ), (0., 1.5250872), (0., 1.5252246),
       (0., 1.5253104), (0., 1.5253154), (0., 1.525557 ), (0., 1.5255594),
       (0., 1.5256834), (0., 1.5259316), (0., 1.5261045), (0., 1.5261072),
       (0., 1.526442 ), (0., 1.5265508), (0., 1.5266206), (0., 1.5266485),
       (0., 1.5266768), (0., 1.5267361), (0., 1.5273684), (0., 1.5279679),
       (0., 1.5282964), (0., 1.5282993), (0., 1.5285342), (0., 1.5286874),
       (0., 1.5296046), (0., 1.5298151), (0., 1.5300757), (0., 1.5302385),
       (0., 1.5303952), (0., 1.5309216), (0., 1.5309557), (0., 1.5310787),
       (0., 1.532149 ), (0., 1.5328575), (0., 1.5361234), (0., 1.5386769)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.216617 , 8.6175585), (6.9424925, 7.3301067),
       (6.927011 , 6.961509 ), (6.864673 , 7.051202 ),
       (6.822866 , 7.538847 ), (6.5142374, 6.8330607),
       (6.4622188, 7.0169477), (6.420634 , 6.949892 ),
       (6.218221 , 7.831189 ), (6.149647 , 6.9723144),
       (6.0405664, 7.275818 ), (5.9134054, 6.532627 ),
       (5.7840953, 6.030611 ), (5.5662184, 5.5978274),
       (5.5255785, 5.8314924), (5.443465 , 8.278724 ),
       (5.4379377, 5.694264 ), (5.404099 , 8.659989 ),
       (5.123411 , 5.2610836), (5.108413 , 6.5935884),
       (4.9565997, 7.323836 ), (4.787271 , 4.7955327),
       (4.5560646, 7.8548813), (4.092445 , 7.9473343),
       (3.9091077, 4.3156366), (3.8928518, 4.7265453),
       (3.8569767, 3.899037 ), (3.8370543, 4.3031025),
       (3.7057712, 8.61574  ), (3.6490068, 8.646297 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.553569, 10.141475), (9.362193,  9.438893),
       (9.161054, 10.773989), (8.36392 ,  8.398337),
       (8.349709,  8.389325)], dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3200510740280151), (0.0, 1.3204652070999146), (0.0, 1.321599006652832), (0.0, 1.3217331171035767), (0.0, 1.3219175338745117), (0.0, 1.3221832513809204), (0.0, 1.3224610090255737), (0.0, 1.3229180574417114), (0.0, 1.3230361938476562), (0.0, 1.3231106996536255), (0.0, 1.3247438669204712), (0.0, 1.324789047241211), (0.0, 1.3255537748336792), (0.0, 1.3257428407669067), (0.0, 1.3261504173278809), (0.0, 1.3263564109802246), (0.0, 1.326728105545044), (0.0, 1.326774001121521), (0.0, 1.32697331905365), (0.0, 1.3269855976104736), (0.0, 1.3276886940002441), (0.0, 1.3284549713134766), (0.0, 1.3286925554275513), (0.0, 1.3286936283111572), (0.0, 1.3287358283996582), (0.0, 1.3287607431411743), (0.0, 1.328864574432373), (0.0, 1.3293124437332153), (0.0, 1.3293180465698242), (0.0, 1.3294545412063599), (0.0, 1.3297703266143799), (0.0, 1.329908013343811), (0.0, 1.3300189971923828), (0.0, 1.3300234079360962), (0.0, 1.3300588130950928), (0.0, 1.3301136493682861), (0.0, 1.3303099870681763), (0.0, 1.3303276300430298), (0.0, 1.3304352760314941), (0.0, 1.3306576013565063), (0.0, 1.3306621313095093), (0.0, 1.330678105354309), (0.0, 1.330680251121521), (0.0, 1.3307089805603027), (0.0, 1.3307271003723145), (0.0, 1.33073890209198), (0.0, 1.3308696746826172), (0.0, 1.3310818672180176), (0.0, 1.3315647840499878), (0.0, 1.3315833806991577), (0.0, 1.331588864326477), (0.0, 1.331626534461975), (0.0, 1.33163583278656), (0.0, 1.3318041563034058), (0.0, 1.3318277597427368), (0.0, 1.3318572044372559), (0.0, 1.3319753408432007), (0.0, 1.3321030139923096), (0.0, 1.332119345664978), (0.0, 1.332230806350708), (0.0, 1.332477331161499), (0.0, 1.3325821161270142), (0.0, 1.3327065706253052), (0.0, 1.3327594995498657), (0.0, 1.3328192234039307), (0.0, 1.3330541849136353), (0.0, 1.333277702331543), (0.0, 1.3334383964538574), (0.0, 1.3334503173828125), (0.0, 1.3335154056549072), (0.0, 1.3337032794952393), (0.0, 1.3337565660476685), (0.0, 1.3338353633880615), (0.0, 1.333906888961792), (0.0, 1.334097146987915), (0.0, 1.3341906070709229), (0.0, 1.334418773651123), (0.0, 1.334420084953308), (0.0, 1.3345342874526978), (0.0, 1.3345377445220947), (0.0, 1.334625482559204), (0.0, 1.3347060680389404), (0.0, 1.3347663879394531), (0.0, 1.3348467350006104), (0.0, 1.3348934650421143), (0.0, 1.3350533246994019), (0.0, 1.3351234197616577), (0.0, 1.3352690935134888), (0.0, 1.3353544473648071), (0.0, 1.3355673551559448), (0.0, 1.3357082605361938), (0.0, 1.3357309103012085), (0.0, 1.3358830213546753), (0.0, 1.336150050163269), (0.0, 1.336438775062561), (0.0, 1.336492657661438), (0.0, 1.3366936445236206), (0.0, 1.3368273973464966), (0.0, 1.3370389938354492), (0.0, 1.3370658159255981), (0.0, 1.3371204137802124), (0.0, 1.33724844455719), (0.0, 1.337296485900879), (0.0, 1.337587594985962), (0.0, 1.337985634803772), (0.0, 1.3379932641983032), (0.0, 1.3380285501480103), (0.0, 1.3383036851882935), (0.0, 1.3384482860565186), (0.0, 1.3388142585754395), (0.0, 1.3389171361923218), (0.0, 1.3395161628723145), (0.0, 1.3397012948989868), (0.0, 1.3397101163864136), (0.0, 1.3400853872299194), (0.0, 1.340171456336975), (0.0, 1.3403035402297974), (0.0, 1.3403887748718262), (0.0, 1.3408297300338745), (0.0, 1.3412344455718994), (0.0, 1.3413043022155762), (0.0, 1.3415530920028687), (0.0, 1.3416368961334229), (0.0, 1.3420138359069824), (0.0, 1.343050479888916), (0.0, 1.3586225509643555), (0.0, 1.439618468284607), (0.0, 1.4447543621063232), (0.0, 1.4471423625946045), (0.0, 1.4480443000793457), (0.0, 1.448045253753662), (0.0, 1.4489996433258057), (0.0, 1.4490715265274048), (0.0, 1.4490904808044434), (0.0, 1.4493063688278198), (0.0, 1.4493862390518188), (0.0, 1.450179934501648), (0.0, 1.451256513595581), (0.0, 1.4515345096588135), (0.0, 1.4516215324401855), (0.0, 1.45219087600708), (0.0, 1.4522194862365723), (0.0, 1.4524303674697876), (0.0, 1.4524521827697754), (0.0, 1.452742576599121), (0.0, 1.4530001878738403), (0.0, 1.4530762434005737), (0.0, 1.4532502889633179), (0.0, 1.453506588935852), (0.0, 1.4536491632461548), (0.0, 1.453892707824707), (0.0, 1.4539055824279785), (0.0, 1.45456862449646), (0.0, 1.4545831680297852), (0.0, 1.454608678817749), (0.0, 1.4546728134155273), (0.0, 1.4548050165176392), (0.0, 1.4549528360366821), (0.0, 1.4549789428710938), (0.0, 1.4550334215164185), (0.0, 1.455075979232788), (0.0, 1.4553009271621704), (0.0, 1.4553073644638062), (0.0, 1.455452561378479), (0.0, 1.4556162357330322), (0.0, 1.4557204246520996), (0.0, 1.4558347463607788), (0.0, 1.4558706283569336), (0.0, 1.455905556678772), (0.0, 1.4560229778289795), (0.0, 1.4561280012130737), (0.0, 1.4562656879425049), (0.0, 1.45644211769104), (0.0, 1.4564993381500244), (0.0, 1.4565558433532715), (0.0, 1.4566266536712646), (0.0, 1.4566574096679688), (0.0, 1.4568023681640625), (0.0, 1.4568063020706177), (0.0, 1.4569921493530273), (0.0, 1.4570932388305664), (0.0, 1.4572068452835083), (0.0, 1.4573044776916504), (0.0, 1.4573601484298706), (0.0, 1.4574161767959595), (0.0, 1.4574482440948486), (0.0, 1.4574676752090454), (0.0, 1.4574782848358154), (0.0, 1.4576719999313354), (0.0, 1.4578807353973389), (0.0, 1.457919955253601), (0.0, 1.4580469131469727), (0.0, 1.4581459760665894), (0.0, 1.4583946466445923), (0.0, 1.4587715864181519), (0.0, 1.4589076042175293), (0.0, 1.458940863609314), (0.0, 1.458978533744812), (0.0, 1.4593493938446045), (0.0, 1.4594595432281494), (0.0, 1.4594768285751343), (0.0, 1.4595184326171875), (0.0, 1.4597023725509644), (0.0, 1.4597764015197754), (0.0, 1.4598677158355713), (0.0, 1.4600493907928467), (0.0, 1.4600554704666138), (0.0, 1.460126280784607), (0.0, 1.4601366519927979), (0.0, 1.460229754447937), (0.0, 1.460237741470337), (0.0, 1.4606033563613892), (0.0, 1.4607034921646118), (0.0, 1.4607971906661987), (0.0, 1.4610693454742432), (0.0, 1.4611362218856812), (0.0, 1.4614551067352295), (0.0, 1.46146821975708), (0.0, 1.461746096611023), (0.0, 1.4619992971420288), (0.0, 1.4620219469070435), (0.0, 1.4620736837387085), (0.0, 1.462202787399292), (0.0, 1.462253451347351), (0.0, 1.4625027179718018), (0.0, 1.4625688791275024), (0.0, 1.4626432657241821), (0.0, 1.4627387523651123), (0.0, 1.462841510772705), (0.0, 1.4633902311325073), (0.0, 1.46355140209198), (0.0, 1.4637616872787476), (0.0, 1.46405029296875), (0.0, 1.4643460512161255), (0.0, 1.4646087884902954), (0.0, 1.4648418426513672), (0.0, 1.4648851156234741), (0.0, 1.465095043182373), (0.0, 1.465216040611267), (0.0, 1.4652869701385498), (0.0, 1.4654221534729004), (0.0, 1.4655745029449463), (0.0, 1.4656563997268677), (0.0, 1.4659291505813599), (0.0, 1.4664291143417358), (0.0, 1.4668424129486084), (0.0, 1.4671865701675415), (0.0, 1.4678211212158203), (0.0, 1.4679845571517944), (0.0, 1.4684290885925293), (0.0, 1.4696111679077148), (0.0, 1.4749363660812378), (0.0, 1.4767333269119263), (0.0, 1.5079411268234253), (0.0, 1.510504126548767), (0.0, 1.5105469226837158), (0.0, 1.5115575790405273), (0.0, 1.5124294757843018), (0.0, 1.512727975845337), (0.0, 1.5127408504486084), (0.0, 1.5131425857543945), (0.0, 1.5135457515716553), (0.0, 1.5136408805847168), (0.0, 1.51387619972229), (0.0, 1.5139790773391724), (0.0, 1.5144062042236328), (0.0, 1.5146101713180542), (0.0, 1.5148546695709229), (0.0, 1.5153521299362183), (0.0, 1.5157787799835205), (0.0, 1.5159114599227905), (0.0, 1.5159121751785278), (0.0, 1.5162321329116821), (0.0, 1.5167831182479858), (0.0, 1.5169016122817993), (0.0, 1.5170576572418213), (0.0, 1.5171499252319336), (0.0, 1.517358422279358), (0.0, 1.5175045728683472), (0.0, 1.5175106525421143), (0.0, 1.5175681114196777), (0.0, 1.5176830291748047), (0.0, 1.5176903009414673), (0.0, 1.5177021026611328), (0.0, 1.5177433490753174), (0.0, 1.5178210735321045), (0.0, 1.5178800821304321), (0.0, 1.517937183380127), (0.0, 1.517991065979004), (0.0, 1.5180304050445557), (0.0, 1.5181500911712646), (0.0, 1.5182689428329468), (0.0, 1.5187201499938965), (0.0, 1.5187873840332031), (0.0, 1.5190280675888062), (0.0, 1.519313931465149), (0.0, 1.5198794603347778), (0.0, 1.519890546798706), (0.0, 1.5200982093811035), (0.0, 1.5201390981674194), (0.0, 1.5203611850738525), (0.0, 1.5205122232437134), (0.0, 1.5205292701721191), (0.0, 1.5206440687179565), (0.0, 1.520747423171997), (0.0, 1.5208698511123657), (0.0, 1.5209616422653198), (0.0, 1.5210527181625366), (0.0, 1.5211373567581177), (0.0, 1.5213693380355835), (0.0, 1.5214170217514038), (0.0, 1.5214601755142212), (0.0, 1.5216271877288818), (0.0, 1.5216325521469116), (0.0, 1.5218520164489746), (0.0, 1.5220097303390503), (0.0, 1.5220317840576172), (0.0, 1.522092580795288), (0.0, 1.5221065282821655), (0.0, 1.5223846435546875), (0.0, 1.522555947303772), (0.0, 1.5228358507156372), (0.0, 1.5230332612991333), (0.0, 1.523063063621521), (0.0, 1.523088812828064), (0.0, 1.523231863975525), (0.0, 1.5233075618743896), (0.0, 1.5233560800552368), (0.0, 1.5234569311141968), (0.0, 1.523461103439331), (0.0, 1.523506760597229), (0.0, 1.5235974788665771), (0.0, 1.5236142873764038), (0.0, 1.52363920211792), (0.0, 1.5237702131271362), (0.0, 1.5240191221237183), (0.0, 1.5240492820739746), (0.0, 1.5243048667907715), (0.0, 1.5243233442306519), (0.0, 1.524333119392395), (0.0, 1.5244810581207275), (0.0, 1.5246022939682007), (0.0, 1.524879813194275), (0.0, 1.5248883962631226), (0.0, 1.5249099731445312), (0.0, 1.5249669551849365), (0.0, 1.5250872373580933), (0.0, 1.5252245664596558), (0.0, 1.5253103971481323), (0.0, 1.5253154039382935), (0.0, 1.525557041168213), (0.0, 1.525559425354004), (0.0, 1.5256834030151367), (0.0, 1.5259315967559814), (0.0, 1.52610445022583), (0.0, 1.5261071920394897), (0.0, 1.526442050933838), (0.0, 1.5265507698059082), (0.0, 1.526620626449585), (0.0, 1.5266485214233398), (0.0, 1.5266767740249634), (0.0, 1.5267361402511597), (0.0, 1.527368426322937), (0.0, 1.527967929840088), (0.0, 1.5282963514328003), (0.0, 1.528299331665039), (0.0, 1.528534173965454), (0.0, 1.5286873579025269), (0.0, 1.5296045541763306), (0.0, 1.5298150777816772), (0.0, 1.5300756692886353), (0.0, 1.5302385091781616), (0.0, 1.5303951501846313), (0.0, 1.5309215784072876), (0.0, 1.5309556722640991), (0.0, 1.5310786962509155), (0.0, 1.5321489572525024), (0.0, 1.5328575372695923), (0.0, 1.5361233949661255), (0.0, 1.5386768579483032)], [(8.2166166305542, 8.617558479309082), (6.942492485046387, 7.330106735229492), (6.927011013031006, 6.9615092277526855), (6.864673137664795, 7.051201820373535), (6.822865962982178, 7.538846969604492), (6.514237403869629, 6.8330607414245605), (6.462218761444092, 7.0169477462768555), (6.420633792877197, 6.949892044067383), (6.218221187591553, 7.831189155578613), (6.149647235870361, 6.972314357757568), (6.040566444396973, 7.27581787109375), (5.913405418395996, 6.532627105712891), (5.784095287322998, 6.030611038208008), (5.566218376159668, 5.597827434539795), (5.525578498840332, 5.8314924240112305), (5.443465232849121, 8.27872371673584), (5.4379377365112305, 5.694263935089111), (5.404098987579346, 8.659989356994629), (5.123411178588867, 5.261083602905273), (5.108413219451904, 6.593588352203369), (4.956599712371826, 7.323835849761963), (4.787271022796631, 4.795532703399658), (4.556064605712891, 7.854881286621094), (4.092444896697998, 7.947334289550781), (3.9091076850891113, 4.31563663482666), (3.8928518295288086, 4.726545333862305), (3.8569767475128174, 3.8990368843078613), (3.8370542526245117, 4.303102493286133), (3.7057712078094482, 8.615739822387695), (3.6490068435668945, 8.646297454833984)], [(9.553568840026855, 10.141474723815918), (9.36219310760498, 9.43889331817627), (9.161053657531738, 10.773988723754883), (8.363920211791992, 8.398337364196777), (8.349708557128906, 8.389325141906738)]]
[array([[0.        , 1.32005107],
       [0.        , 1.32046521],
       [0.        , 1.32159901],
       [0.        , 1.32173312],
       [0.        , 1.32191753],
       [0.        , 1.32218325],
       [0.        , 1.32246101],
       [0.        , 1.32291806],
       [0.        , 1.32303619],
       [0.        , 1.3231107 ],
       [0.        , 1.32474387],
       [0.        , 1.32478905],
       [0.        , 1.32555377],
       [0.        , 1.32574284],
       [0.        , 1.32615042],
       [0.        , 1.32635641],
       [0.        , 1.32672811],
       [0.        , 1.326774  ],
       [0.        , 1.32697332],
       [0.        , 1.3269856 ],
       [0.        , 1.32768869],
       [0.        , 1.32845497],
       [0.        , 1.32869256],
       [0.        , 1.32869363],
       [0.        , 1.32873583],
       [0.        , 1.32876074],
       [0.        , 1.32886457],
       [0.        , 1.32931244],
       [0.        , 1.32931805],
       [0.        , 1.32945454],
       [0.        , 1.32977033],
       [0.        , 1.32990801],
       [0.        , 1.330019  ],
       [0.        , 1.33002341],
       [0.        , 1.33005881],
       [0.        , 1.33011365],
       [0.        , 1.33030999],
       [0.        , 1.33032763],
       [0.        , 1.33043528],
       [0.        , 1.3306576 ],
       [0.        , 1.33066213],
       [0.        , 1.33067811],
       [0.        , 1.33068025],
       [0.        , 1.33070898],
       [0.        , 1.3307271 ],
       [0.        , 1.3307389 ],
       [0.        , 1.33086967],
       [0.        , 1.33108187],
       [0.        , 1.33156478],
       [0.        , 1.33158338],
       [0.        , 1.33158886],
       [0.        , 1.33162653],
       [0.        , 1.33163583],
       [0.        , 1.33180416],
       [0.        , 1.33182776],
       [0.        , 1.3318572 ],
       [0.        , 1.33197534],
       [0.        , 1.33210301],
       [0.        , 1.33211935],
       [0.        , 1.33223081],
       [0.        , 1.33247733],
       [0.        , 1.33258212],
       [0.        , 1.33270657],
       [0.        , 1.3327595 ],
       [0.        , 1.33281922],
       [0.        , 1.33305418],
       [0.        , 1.3332777 ],
       [0.        , 1.3334384 ],
       [0.        , 1.33345032],
       [0.        , 1.33351541],
       [0.        , 1.33370328],
       [0.        , 1.33375657],
       [0.        , 1.33383536],
       [0.        , 1.33390689],
       [0.        , 1.33409715],
       [0.        , 1.33419061],
       [0.        , 1.33441877],
       [0.        , 1.33442008],
       [0.        , 1.33453429],
       [0.        , 1.33453774],
       [0.        , 1.33462548],
       [0.        , 1.33470607],
       [0.        , 1.33476639],
       [0.        , 1.33484674],
       [0.        , 1.33489347],
       [0.        , 1.33505332],
       [0.        , 1.33512342],
       [0.        , 1.33526909],
       [0.        , 1.33535445],
       [0.        , 1.33556736],
       [0.        , 1.33570826],
       [0.        , 1.33573091],
       [0.        , 1.33588302],
       [0.        , 1.33615005],
       [0.        , 1.33643878],
       [0.        , 1.33649266],
       [0.        , 1.33669364],
       [0.        , 1.3368274 ],
       [0.        , 1.33703899],
       [0.        , 1.33706582],
       [0.        , 1.33712041],
       [0.        , 1.33724844],
       [0.        , 1.33729649],
       [0.        , 1.33758759],
       [0.        , 1.33798563],
       [0.        , 1.33799326],
       [0.        , 1.33802855],
       [0.        , 1.33830369],
       [0.        , 1.33844829],
       [0.        , 1.33881426],
       [0.        , 1.33891714],
       [0.        , 1.33951616],
       [0.        , 1.33970129],
       [0.        , 1.33971012],
       [0.        , 1.34008539],
       [0.        , 1.34017146],
       [0.        , 1.34030354],
       [0.        , 1.34038877],
       [0.        , 1.34082973],
       [0.        , 1.34123445],
       [0.        , 1.3413043 ],
       [0.        , 1.34155309],
       [0.        , 1.3416369 ],
       [0.        , 1.34201384],
       [0.        , 1.34305048],
       [0.        , 1.35862255],
       [0.        , 1.43961847],
       [0.        , 1.44475436],
       [0.        , 1.44714236],
       [0.        , 1.4480443 ],
       [0.        , 1.44804525],
       [0.        , 1.44899964],
       [0.        , 1.44907153],
       [0.        , 1.44909048],
       [0.        , 1.44930637],
       [0.        , 1.44938624],
       [0.        , 1.45017993],
       [0.        , 1.45125651],
       [0.        , 1.45153451],
       [0.        , 1.45162153],
       [0.        , 1.45219088],
       [0.        , 1.45221949],
       [0.        , 1.45243037],
       [0.        , 1.45245218],
       [0.        , 1.45274258],
       [0.        , 1.45300019],
       [0.        , 1.45307624],
       [0.        , 1.45325029],
       [0.        , 1.45350659],
       [0.        , 1.45364916],
       [0.        , 1.45389271],
       [0.        , 1.45390558],
       [0.        , 1.45456862],
       [0.        , 1.45458317],
       [0.        , 1.45460868],
       [0.        , 1.45467281],
       [0.        , 1.45480502],
       [0.        , 1.45495284],
       [0.        , 1.45497894],
       [0.        , 1.45503342],
       [0.        , 1.45507598],
       [0.        , 1.45530093],
       [0.        , 1.45530736],
       [0.        , 1.45545256],
       [0.        , 1.45561624],
       [0.        , 1.45572042],
       [0.        , 1.45583475],
       [0.        , 1.45587063],
       [0.        , 1.45590556],
       [0.        , 1.45602298],
       [0.        , 1.456128  ],
       [0.        , 1.45626569],
       [0.        , 1.45644212],
       [0.        , 1.45649934],
       [0.        , 1.45655584],
       [0.        , 1.45662665],
       [0.        , 1.45665741],
       [0.        , 1.45680237],
       [0.        , 1.4568063 ],
       [0.        , 1.45699215],
       [0.        , 1.45709324],
       [0.        , 1.45720685],
       [0.        , 1.45730448],
       [0.        , 1.45736015],
       [0.        , 1.45741618],
       [0.        , 1.45744824],
       [0.        , 1.45746768],
       [0.        , 1.45747828],
       [0.        , 1.457672  ],
       [0.        , 1.45788074],
       [0.        , 1.45791996],
       [0.        , 1.45804691],
       [0.        , 1.45814598],
       [0.        , 1.45839465],
       [0.        , 1.45877159],
       [0.        , 1.4589076 ],
       [0.        , 1.45894086],
       [0.        , 1.45897853],
       [0.        , 1.45934939],
       [0.        , 1.45945954],
       [0.        , 1.45947683],
       [0.        , 1.45951843],
       [0.        , 1.45970237],
       [0.        , 1.4597764 ],
       [0.        , 1.45986772],
       [0.        , 1.46004939],
       [0.        , 1.46005547],
       [0.        , 1.46012628],
       [0.        , 1.46013665],
       [0.        , 1.46022975],
       [0.        , 1.46023774],
       [0.        , 1.46060336],
       [0.        , 1.46070349],
       [0.        , 1.46079719],
       [0.        , 1.46106935],
       [0.        , 1.46113622],
       [0.        , 1.46145511],
       [0.        , 1.46146822],
       [0.        , 1.4617461 ],
       [0.        , 1.4619993 ],
       [0.        , 1.46202195],
       [0.        , 1.46207368],
       [0.        , 1.46220279],
       [0.        , 1.46225345],
       [0.        , 1.46250272],
       [0.        , 1.46256888],
       [0.        , 1.46264327],
       [0.        , 1.46273875],
       [0.        , 1.46284151],
       [0.        , 1.46339023],
       [0.        , 1.4635514 ],
       [0.        , 1.46376169],
       [0.        , 1.46405029],
       [0.        , 1.46434605],
       [0.        , 1.46460879],
       [0.        , 1.46484184],
       [0.        , 1.46488512],
       [0.        , 1.46509504],
       [0.        , 1.46521604],
       [0.        , 1.46528697],
       [0.        , 1.46542215],
       [0.        , 1.4655745 ],
       [0.        , 1.4656564 ],
       [0.        , 1.46592915],
       [0.        , 1.46642911],
       [0.        , 1.46684241],
       [0.        , 1.46718657],
       [0.        , 1.46782112],
       [0.        , 1.46798456],
       [0.        , 1.46842909],
       [0.        , 1.46961117],
       [0.        , 1.47493637],
       [0.        , 1.47673333],
       [0.        , 1.50794113],
       [0.        , 1.51050413],
       [0.        , 1.51054692],
       [0.        , 1.51155758],
       [0.        , 1.51242948],
       [0.        , 1.51272798],
       [0.        , 1.51274085],
       [0.        , 1.51314259],
       [0.        , 1.51354575],
       [0.        , 1.51364088],
       [0.        , 1.5138762 ],
       [0.        , 1.51397908],
       [0.        , 1.5144062 ],
       [0.        , 1.51461017],
       [0.        , 1.51485467],
       [0.        , 1.51535213],
       [0.        , 1.51577878],
       [0.        , 1.51591146],
       [0.        , 1.51591218],
       [0.        , 1.51623213],
       [0.        , 1.51678312],
       [0.        , 1.51690161],
       [0.        , 1.51705766],
       [0.        , 1.51714993],
       [0.        , 1.51735842],
       [0.        , 1.51750457],
       [0.        , 1.51751065],
       [0.        , 1.51756811],
       [0.        , 1.51768303],
       [0.        , 1.5176903 ],
       [0.        , 1.5177021 ],
       [0.        , 1.51774335],
       [0.        , 1.51782107],
       [0.        , 1.51788008],
       [0.        , 1.51793718],
       [0.        , 1.51799107],
       [0.        , 1.51803041],
       [0.        , 1.51815009],
       [0.        , 1.51826894],
       [0.        , 1.51872015],
       [0.        , 1.51878738],
       [0.        , 1.51902807],
       [0.        , 1.51931393],
       [0.        , 1.51987946],
       [0.        , 1.51989055],
       [0.        , 1.52009821],
       [0.        , 1.5201391 ],
       [0.        , 1.52036119],
       [0.        , 1.52051222],
       [0.        , 1.52052927],
       [0.        , 1.52064407],
       [0.        , 1.52074742],
       [0.        , 1.52086985],
       [0.        , 1.52096164],
       [0.        , 1.52105272],
       [0.        , 1.52113736],
       [0.        , 1.52136934],
       [0.        , 1.52141702],
       [0.        , 1.52146018],
       [0.        , 1.52162719],
       [0.        , 1.52163255],
       [0.        , 1.52185202],
       [0.        , 1.52200973],
       [0.        , 1.52203178],
       [0.        , 1.52209258],
       [0.        , 1.52210653],
       [0.        , 1.52238464],
       [0.        , 1.52255595],
       [0.        , 1.52283585],
       [0.        , 1.52303326],
       [0.        , 1.52306306],
       [0.        , 1.52308881],
       [0.        , 1.52323186],
       [0.        , 1.52330756],
       [0.        , 1.52335608],
       [0.        , 1.52345693],
       [0.        , 1.5234611 ],
       [0.        , 1.52350676],
       [0.        , 1.52359748],
       [0.        , 1.52361429],
       [0.        , 1.5236392 ],
       [0.        , 1.52377021],
       [0.        , 1.52401912],
       [0.        , 1.52404928],
       [0.        , 1.52430487],
       [0.        , 1.52432334],
       [0.        , 1.52433312],
       [0.        , 1.52448106],
       [0.        , 1.52460229],
       [0.        , 1.52487981],
       [0.        , 1.5248884 ],
       [0.        , 1.52490997],
       [0.        , 1.52496696],
       [0.        , 1.52508724],
       [0.        , 1.52522457],
       [0.        , 1.5253104 ],
       [0.        , 1.5253154 ],
       [0.        , 1.52555704],
       [0.        , 1.52555943],
       [0.        , 1.5256834 ],
       [0.        , 1.5259316 ],
       [0.        , 1.52610445],
       [0.        , 1.52610719],
       [0.        , 1.52644205],
       [0.        , 1.52655077],
       [0.        , 1.52662063],
       [0.        , 1.52664852],
       [0.        , 1.52667677],
       [0.        , 1.52673614],
       [0.        , 1.52736843],
       [0.        , 1.52796793],
       [0.        , 1.52829635],
       [0.        , 1.52829933],
       [0.        , 1.52853417],
       [0.        , 1.52868736],
       [0.        , 1.52960455],
       [0.        , 1.52981508],
       [0.        , 1.53007567],
       [0.        , 1.53023851],
       [0.        , 1.53039515],
       [0.        , 1.53092158],
       [0.        , 1.53095567],
       [0.        , 1.5310787 ],
       [0.        , 1.53214896],
       [0.        , 1.53285754],
       [0.        , 1.53612339],
       [0.        , 1.53867686]]), array([[8.21661663, 8.61755848],
       [6.94249249, 7.33010674],
       [6.92701101, 6.96150923],
       [6.86467314, 7.05120182],
       [6.82286596, 7.53884697],
       [6.5142374 , 6.83306074],
       [6.46221876, 7.01694775],
       [6.42063379, 6.94989204],
       [6.21822119, 7.83118916],
       [6.14964724, 6.97231436],
       [6.04056644, 7.27581787],
       [5.91340542, 6.53262711],
       [5.78409529, 6.03061104],
       [5.56621838, 5.59782743],
       [5.5255785 , 5.83149242],
       [5.44346523, 8.27872372],
       [5.43793774, 5.69426394],
       [5.40409899, 8.65998936],
       [5.12341118, 5.2610836 ],
       [5.10841322, 6.59358835],
       [4.95659971, 7.32383585],
       [4.78727102, 4.7955327 ],
       [4.55606461, 7.85488129],
       [4.0924449 , 7.94733429],
       [3.90910769, 4.31563663],
       [3.89285183, 4.72654533],
       [3.85697675, 3.89903688],
       [3.83705425, 4.30310249],
       [3.70577121, 8.61573982],
       [3.64900684, 8.64629745]]), array([[ 9.55356884, 10.14147472],
       [ 9.36219311,  9.43889332],
       [ 9.16105366, 10.77398872],
       [ 8.36392021,  8.39833736],
       [ 8.34970856,  8.38932514]])]2024-03-06 17:53:33.650321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KEE ph vector generated, counter: 119
2024-03-06 17:53:37.607092: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:37.650196: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:38.788384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3106678), (0., 1.3112817), (0., 1.3240908), (0., 1.326025 ),
       (0., 1.3260357), (0., 1.3263385), (0., 1.3273655), (0., 1.327369 ),
       (0., 1.3273822), (0., 1.3274949), (0., 1.3276482), (0., 1.3278707),
       (0., 1.3282197), (0., 1.3290873), (0., 1.3293264), (0., 1.3294133),
       (0., 1.3295331), (0., 1.3296287), (0., 1.3298844), (0., 1.3299036),
       (0., 1.329968 ), (0., 1.3300073), (0., 1.3302187), (0., 1.3302344),
       (0., 1.3304532), (0., 1.3305396), (0., 1.3305866), (0., 1.330638 ),
       (0., 1.3306925), (0., 1.3307594), (0., 1.3310707), (0., 1.3311142),
       (0., 1.3311496), (0., 1.3311851), (0., 1.331206 ), (0., 1.3313954),
       (0., 1.3314353), (0., 1.3316567), (0., 1.3316858), (0., 1.3317506),
       (0., 1.3319309), (0., 1.3320131), (0., 1.3321512), (0., 1.3322382),
       (0., 1.332557 ), (0., 1.3325655), (0., 1.3326273), (0., 1.3327613),
       (0., 1.3328665), (0., 1.3330718), (0., 1.3331089), (0., 1.3331469),
       (0., 1.3331988), (0., 1.3332953), (0., 1.3334352), (0., 1.3334987),
       (0., 1.3335265), (0., 1.3336923), (0., 1.3336996), (0., 1.3337134),
       (0., 1.3339858), (0., 1.334052 ), (0., 1.3341467), (0., 1.3341714),
       (0., 1.3342638), (0., 1.334291 ), (0., 1.3343285), (0., 1.334403 ),
       (0., 1.334428 ), (0., 1.3344467), (0., 1.3345532), (0., 1.3346727),
       (0., 1.3348469), (0., 1.3349806), (0., 1.3350444), (0., 1.3351033),
       (0., 1.3352014), (0., 1.3352481), (0., 1.3352816), (0., 1.335325 ),
       (0., 1.3356102), (0., 1.3356104), (0., 1.3357676), (0., 1.335903 ),
       (0., 1.3359115), (0., 1.3363156), (0., 1.3363633), (0., 1.3363947),
       (0., 1.3364286), (0., 1.3365275), (0., 1.3365963), (0., 1.336808 ),
       (0., 1.3368311), (0., 1.3368834), (0., 1.337113 ), (0., 1.3373419),
       (0., 1.3373725), (0., 1.3376018), (0., 1.33764  ), (0., 1.3376724),
       (0., 1.3377718), (0., 1.3377936), (0., 1.338037 ), (0., 1.3380868),
       (0., 1.3382095), (0., 1.3384335), (0., 1.3384798), (0., 1.3384838),
       (0., 1.338764 ), (0., 1.3387966), (0., 1.3391743), (0., 1.3392886),
       (0., 1.3396456), (0., 1.3396481), (0., 1.3398032), (0., 1.339905 ),
       (0., 1.3399583), (0., 1.3407505), (0., 1.3409503), (0., 1.3422767),
       (0., 1.3423585), (0., 1.3423734), (0., 1.3433676), (0., 1.3462106),
       (0., 1.3884499), (0., 1.4143945), (0., 1.4314178), (0., 1.441292 ),
       (0., 1.4482867), (0., 1.4497747), (0., 1.4499304), (0., 1.4510779),
       (0., 1.451826 ), (0., 1.4523401), (0., 1.4526161), (0., 1.452938 ),
       (0., 1.4533255), (0., 1.4535079), (0., 1.4539355), (0., 1.4548546),
       (0., 1.4550905), (0., 1.4553298), (0., 1.4554306), (0., 1.4554399),
       (0., 1.4554416), (0., 1.4554592), (0., 1.4557825), (0., 1.4559146),
       (0., 1.4559168), (0., 1.4561387), (0., 1.4561683), (0., 1.4561714),
       (0., 1.4561775), (0., 1.4563359), (0., 1.4564148), (0., 1.4564503),
       (0., 1.4566087), (0., 1.4566269), (0., 1.4567678), (0., 1.4568052),
       (0., 1.4570699), (0., 1.4572308), (0., 1.4573007), (0., 1.4574568),
       (0., 1.4576545), (0., 1.4577239), (0., 1.4577813), (0., 1.457885 ),
       (0., 1.4579302), (0., 1.4579898), (0., 1.4580442), (0., 1.4582868),
       (0., 1.4583056), (0., 1.4583259), (0., 1.4583318), (0., 1.4583338),
       (0., 1.4583899), (0., 1.4584633), (0., 1.4584708), (0., 1.4585094),
       (0., 1.458613 ), (0., 1.4586223), (0., 1.4588913), (0., 1.4589306),
       (0., 1.4589351), (0., 1.4589487), (0., 1.459138 ), (0., 1.4591721),
       (0., 1.4591899), (0., 1.4592364), (0., 1.4593838), (0., 1.459445 ),
       (0., 1.4595202), (0., 1.4595343), (0., 1.4595355), (0., 1.4596285),
       (0., 1.4596603), (0., 1.459869 ), (0., 1.4598743), (0., 1.4600352),
       (0., 1.460046 ), (0., 1.46005  ), (0., 1.4602878), (0., 1.4603854),
       (0., 1.4604155), (0., 1.4604485), (0., 1.4604604), (0., 1.460606 ),
       (0., 1.4607142), (0., 1.4608096), (0., 1.4608587), (0., 1.4608966),
       (0., 1.4609337), (0., 1.4610059), (0., 1.4611062), (0., 1.4611789),
       (0., 1.4612851), (0., 1.4613321), (0., 1.4613711), (0., 1.4615445),
       (0., 1.4616323), (0., 1.4617232), (0., 1.4619389), (0., 1.4620006),
       (0., 1.4620342), (0., 1.462236 ), (0., 1.4622458), (0., 1.462392 ),
       (0., 1.4624478), (0., 1.4625924), (0., 1.462636 ), (0., 1.4627262),
       (0., 1.4629145), (0., 1.4631804), (0., 1.4634521), (0., 1.4635972),
       (0., 1.4643391), (0., 1.4644078), (0., 1.4646087), (0., 1.4652851),
       (0., 1.465935 ), (0., 1.4661802), (0., 1.4668905), (0., 1.46693  ),
       (0., 1.4674574), (0., 1.4682558), (0., 1.4683309), (0., 1.468533 ),
       (0., 1.4694325), (0., 1.4712534), (0., 1.4735622), (0., 1.4740047),
       (0., 1.4991156), (0., 1.5021064), (0., 1.5099552), (0., 1.5101163),
       (0., 1.511125 ), (0., 1.5126013), (0., 1.5145575), (0., 1.5152955),
       (0., 1.5154383), (0., 1.5157665), (0., 1.5162292), (0., 1.5164701),
       (0., 1.5166302), (0., 1.5168568), (0., 1.5176203), (0., 1.5177728),
       (0., 1.5178452), (0., 1.5180113), (0., 1.5180761), (0., 1.5181066),
       (0., 1.5182779), (0., 1.5186546), (0., 1.5186853), (0., 1.5189077),
       (0., 1.5189291), (0., 1.5196944), (0., 1.5197061), (0., 1.5197929),
       (0., 1.520214 ), (0., 1.5202312), (0., 1.520296 ), (0., 1.5203618),
       (0., 1.5204743), (0., 1.5204773), (0., 1.5205928), (0., 1.5206101),
       (0., 1.5206637), (0., 1.5208017), (0., 1.5210445), (0., 1.5211345),
       (0., 1.5213672), (0., 1.5213975), (0., 1.5214007), (0., 1.5215747),
       (0., 1.521589 ), (0., 1.5215899), (0., 1.5217038), (0., 1.5217814),
       (0., 1.5218487), (0., 1.5219566), (0., 1.5220172), (0., 1.5220349),
       (0., 1.5221165), (0., 1.5222412), (0., 1.5222453), (0., 1.5222512),
       (0., 1.5224093), (0., 1.5224164), (0., 1.5226656), (0., 1.5226902),
       (0., 1.5227494), (0., 1.522803 ), (0., 1.5229384), (0., 1.5229644),
       (0., 1.5229791), (0., 1.5230634), (0., 1.5231298), (0., 1.5232756),
       (0., 1.5235673), (0., 1.5235952), (0., 1.523654 ), (0., 1.523676 ),
       (0., 1.5236869), (0., 1.523691 ), (0., 1.523704 ), (0., 1.5237967),
       (0., 1.523813 ), (0., 1.5238547), (0., 1.5239035), (0., 1.5240537),
       (0., 1.5240824), (0., 1.5240934), (0., 1.5241299), (0., 1.5241513),
       (0., 1.5241879), (0., 1.5242506), (0., 1.5242739), (0., 1.5244375),
       (0., 1.5245636), (0., 1.5245816), (0., 1.5247607), (0., 1.5247741),
       (0., 1.5248332), (0., 1.5250932), (0., 1.5250983), (0., 1.5251853),
       (0., 1.5252005), (0., 1.5252768), (0., 1.5253006), (0., 1.5253724),
       (0., 1.5258595), (0., 1.5260477), (0., 1.5260501), (0., 1.5261428),
       (0., 1.5261644), (0., 1.5262305), (0., 1.5268878), (0., 1.5269004),
       (0., 1.5269756), (0., 1.527154 ), (0., 1.5272996), (0., 1.5276587),
       (0., 1.5277312), (0., 1.5278343), (0., 1.5281221), (0., 1.5283834),
       (0., 1.5283996), (0., 1.5284289), (0., 1.528544 ), (0., 1.5295631),
       (0., 1.5298584), (0., 1.5303164), (0., 1.5305656), (0., 1.530714 ),
       (0., 1.530926 ), (0., 1.531348 ), (0., 1.5342529), (0., 1.5557451)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.501368 , 8.5072565), (7.8873124, 8.385689 ),
       (7.032221 , 7.1798677), (7.001886 , 7.552196 ),
       (6.656803 , 7.2110076), (6.556734 , 6.8702946),
       (6.4768705, 7.0484676), (6.4469376, 6.968675 ),
       (6.2026067, 7.858549 ), (6.1348257, 7.262186 ),
       (6.121199 , 6.9844193), (6.075904 , 6.089506 ),
       (5.9113655, 6.427675 ), (5.663813 , 5.9552965),
       (5.6167765, 5.7361255), (5.5589714, 5.6531205),
       (5.494987 , 5.527699 ), (5.4234247, 8.376854 ),
       (5.42197  , 8.569818 ), (5.3084846, 8.641015 ),
       (4.9873257, 6.816541 ), (4.93573  , 7.28584  ),
       (4.4952154, 7.8735523), (4.4340267, 4.517694 ),
       (4.2584596, 7.911962 ), (4.0219903, 4.0393004),
       (4.010729 , 4.2941966), (3.9756796, 4.339149 ),
       (3.9620342, 4.7424946), (3.6506855, 8.53577  )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.555952 ,  9.6701145), (9.447096 , 10.098509 ),
       (9.125313 , 10.739255 ), (8.44692  ,  8.483988 ),
       (8.397829 ,  8.507083 ), (7.6530213,  7.747749 )],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3106677532196045), (0.0, 1.311281681060791), (0.0, 1.324090838432312), (0.0, 1.3260250091552734), (0.0, 1.326035737991333), (0.0, 1.326338529586792), (0.0, 1.327365517616272), (0.0, 1.327368974685669), (0.0, 1.327382206916809), (0.0, 1.3274948596954346), (0.0, 1.3276481628417969), (0.0, 1.3278707265853882), (0.0, 1.3282196521759033), (0.0, 1.329087257385254), (0.0, 1.3293263912200928), (0.0, 1.3294132947921753), (0.0, 1.3295331001281738), (0.0, 1.3296287059783936), (0.0, 1.32988440990448), (0.0, 1.3299036026000977), (0.0, 1.329967975616455), (0.0, 1.3300073146820068), (0.0, 1.3302186727523804), (0.0, 1.330234408378601), (0.0, 1.3304531574249268), (0.0, 1.330539584159851), (0.0, 1.330586552619934), (0.0, 1.33063805103302), (0.0, 1.3306925296783447), (0.0, 1.3307594060897827), (0.0, 1.3310706615447998), (0.0, 1.3311141729354858), (0.0, 1.3311495780944824), (0.0, 1.3311851024627686), (0.0, 1.33120596408844), (0.0, 1.3313953876495361), (0.0, 1.3314353227615356), (0.0, 1.3316566944122314), (0.0, 1.3316857814788818), (0.0, 1.3317506313323975), (0.0, 1.3319308757781982), (0.0, 1.3320131301879883), (0.0, 1.332151174545288), (0.0, 1.3322381973266602), (0.0, 1.332556962966919), (0.0, 1.3325655460357666), (0.0, 1.332627296447754), (0.0, 1.332761287689209), (0.0, 1.3328665494918823), (0.0, 1.3330718278884888), (0.0, 1.333108901977539), (0.0, 1.3331469297409058), (0.0, 1.3331987857818604), (0.0, 1.3332953453063965), (0.0, 1.3334351778030396), (0.0, 1.3334987163543701), (0.0, 1.3335264921188354), (0.0, 1.3336923122406006), (0.0, 1.3336995840072632), (0.0, 1.333713412284851), (0.0, 1.3339858055114746), (0.0, 1.3340519666671753), (0.0, 1.3341467380523682), (0.0, 1.3341714143753052), (0.0, 1.334263801574707), (0.0, 1.3342909812927246), (0.0, 1.334328532218933), (0.0, 1.3344030380249023), (0.0, 1.3344279527664185), (0.0, 1.334446668624878), (0.0, 1.3345532417297363), (0.0, 1.3346726894378662), (0.0, 1.3348468542099), (0.0, 1.3349806070327759), (0.0, 1.3350443840026855), (0.0, 1.3351032733917236), (0.0, 1.335201382637024), (0.0, 1.3352481126785278), (0.0, 1.3352816104888916), (0.0, 1.335325002670288), (0.0, 1.3356101512908936), (0.0, 1.3356103897094727), (0.0, 1.3357676267623901), (0.0, 1.3359030485153198), (0.0, 1.335911512374878), (0.0, 1.336315631866455), (0.0, 1.3363633155822754), (0.0, 1.3363946676254272), (0.0, 1.3364286422729492), (0.0, 1.3365274667739868), (0.0, 1.3365962505340576), (0.0, 1.3368079662322998), (0.0, 1.3368310928344727), (0.0, 1.3368834257125854), (0.0, 1.3371130228042603), (0.0, 1.3373419046401978), (0.0, 1.3373725414276123), (0.0, 1.3376017808914185), (0.0, 1.3376400470733643), (0.0, 1.3376723527908325), (0.0, 1.3377717733383179), (0.0, 1.3377935886383057), (0.0, 1.3380370140075684), (0.0, 1.3380868434906006), (0.0, 1.3382095098495483), (0.0, 1.3384335041046143), (0.0, 1.33847975730896), (0.0, 1.3384838104248047), (0.0, 1.338763952255249), (0.0, 1.338796615600586), (0.0, 1.3391742706298828), (0.0, 1.339288592338562), (0.0, 1.3396456241607666), (0.0, 1.3396481275558472), (0.0, 1.3398032188415527), (0.0, 1.339905023574829), (0.0, 1.3399583101272583), (0.0, 1.3407504558563232), (0.0, 1.3409502506256104), (0.0, 1.342276692390442), (0.0, 1.3423584699630737), (0.0, 1.3423733711242676), (0.0, 1.343367576599121), (0.0, 1.3462105989456177), (0.0, 1.3884499073028564), (0.0, 1.414394497871399), (0.0, 1.4314178228378296), (0.0, 1.4412920475006104), (0.0, 1.4482866525650024), (0.0, 1.4497747421264648), (0.0, 1.4499304294586182), (0.0, 1.451077938079834), (0.0, 1.4518259763717651), (0.0, 1.4523401260375977), (0.0, 1.4526160955429077), (0.0, 1.4529379606246948), (0.0, 1.4533255100250244), (0.0, 1.453507900238037), (0.0, 1.4539355039596558), (0.0, 1.4548546075820923), (0.0, 1.4550905227661133), (0.0, 1.4553297758102417), (0.0, 1.4554306268692017), (0.0, 1.4554399251937866), (0.0, 1.4554415941238403), (0.0, 1.4554592370986938), (0.0, 1.4557825326919556), (0.0, 1.4559146165847778), (0.0, 1.4559167623519897), (0.0, 1.4561387300491333), (0.0, 1.456168293952942), (0.0, 1.4561713933944702), (0.0, 1.4561774730682373), (0.0, 1.4563359022140503), (0.0, 1.456414818763733), (0.0, 1.456450343132019), (0.0, 1.4566086530685425), (0.0, 1.4566268920898438), (0.0, 1.4567677974700928), (0.0, 1.4568052291870117), (0.0, 1.4570698738098145), (0.0, 1.457230806350708), (0.0, 1.4573006629943848), (0.0, 1.4574568271636963), (0.0, 1.4576544761657715), (0.0, 1.45772385597229), (0.0, 1.4577813148498535), (0.0, 1.4578850269317627), (0.0, 1.4579302072525024), (0.0, 1.4579898118972778), (0.0, 1.458044171333313), (0.0, 1.4582867622375488), (0.0, 1.4583055973052979), (0.0, 1.4583258628845215), (0.0, 1.458331823348999), (0.0, 1.4583338499069214), (0.0, 1.4583898782730103), (0.0, 1.4584633111953735), (0.0, 1.4584708213806152), (0.0, 1.4585094451904297), (0.0, 1.4586130380630493), (0.0, 1.4586223363876343), (0.0, 1.4588912725448608), (0.0, 1.4589306116104126), (0.0, 1.4589351415634155), (0.0, 1.4589487314224243), (0.0, 1.459138035774231), (0.0, 1.4591721296310425), (0.0, 1.4591898918151855), (0.0, 1.4592363834381104), (0.0, 1.4593838453292847), (0.0, 1.4594449996948242), (0.0, 1.4595202207565308), (0.0, 1.4595342874526978), (0.0, 1.4595354795455933), (0.0, 1.4596284627914429), (0.0, 1.459660291671753), (0.0, 1.4598690271377563), (0.0, 1.4598742723464966), (0.0, 1.4600352048873901), (0.0, 1.4600460529327393), (0.0, 1.4600499868392944), (0.0, 1.4602878093719482), (0.0, 1.4603854417800903), (0.0, 1.4604154825210571), (0.0, 1.4604485034942627), (0.0, 1.4604604244232178), (0.0, 1.4606059789657593), (0.0, 1.4607142210006714), (0.0, 1.460809588432312), (0.0, 1.460858702659607), (0.0, 1.460896611213684), (0.0, 1.4609336853027344), (0.0, 1.4610059261322021), (0.0, 1.4611061811447144), (0.0, 1.4611788988113403), (0.0, 1.46128511428833), (0.0, 1.461332082748413), (0.0, 1.4613710641860962), (0.0, 1.4615445137023926), (0.0, 1.461632251739502), (0.0, 1.4617232084274292), (0.0, 1.4619388580322266), (0.0, 1.4620006084442139), (0.0, 1.4620342254638672), (0.0, 1.4622360467910767), (0.0, 1.4622458219528198), (0.0, 1.462391972541809), (0.0, 1.4624477624893188), (0.0, 1.462592363357544), (0.0, 1.4626359939575195), (0.0, 1.4627262353897095), (0.0, 1.4629144668579102), (0.0, 1.463180422782898), (0.0, 1.4634521007537842), (0.0, 1.4635971784591675), (0.0, 1.4643391370773315), (0.0, 1.4644078016281128), (0.0, 1.4646086692810059), (0.0, 1.465285062789917), (0.0, 1.4659349918365479), (0.0, 1.4661802053451538), (0.0, 1.4668904542922974), (0.0, 1.4669300317764282), (0.0, 1.4674574136734009), (0.0, 1.4682557582855225), (0.0, 1.4683308601379395), (0.0, 1.4685330390930176), (0.0, 1.4694324731826782), (0.0, 1.4712533950805664), (0.0, 1.473562240600586), (0.0, 1.4740047454833984), (0.0, 1.4991155862808228), (0.0, 1.5021064281463623), (0.0, 1.5099551677703857), (0.0, 1.5101163387298584), (0.0, 1.5111249685287476), (0.0, 1.5126012563705444), (0.0, 1.5145574808120728), (0.0, 1.5152955055236816), (0.0, 1.5154383182525635), (0.0, 1.5157665014266968), (0.0, 1.5162291526794434), (0.0, 1.5164700746536255), (0.0, 1.5166301727294922), (0.0, 1.5168567895889282), (0.0, 1.517620325088501), (0.0, 1.5177727937698364), (0.0, 1.5178451538085938), (0.0, 1.5180113315582275), (0.0, 1.5180760622024536), (0.0, 1.5181065797805786), (0.0, 1.518277883529663), (0.0, 1.5186545848846436), (0.0, 1.5186853408813477), (0.0, 1.5189076662063599), (0.0, 1.518929123878479), (0.0, 1.519694447517395), (0.0, 1.519706130027771), (0.0, 1.519792914390564), (0.0, 1.5202139616012573), (0.0, 1.5202312469482422), (0.0, 1.5202959775924683), (0.0, 1.5203617811203003), (0.0, 1.5204743146896362), (0.0, 1.520477294921875), (0.0, 1.5205928087234497), (0.0, 1.5206100940704346), (0.0, 1.5206637382507324), (0.0, 1.5208016633987427), (0.0, 1.5210444927215576), (0.0, 1.5211344957351685), (0.0, 1.5213671922683716), (0.0, 1.5213974714279175), (0.0, 1.5214006900787354), (0.0, 1.5215747356414795), (0.0, 1.5215890407562256), (0.0, 1.5215898752212524), (0.0, 1.521703839302063), (0.0, 1.5217814445495605), (0.0, 1.5218486785888672), (0.0, 1.5219565629959106), (0.0, 1.522017240524292), (0.0, 1.5220348834991455), (0.0, 1.5221165418624878), (0.0, 1.522241234779358), (0.0, 1.5222452878952026), (0.0, 1.5222512483596802), (0.0, 1.5224093198776245), (0.0, 1.522416353225708), (0.0, 1.5226656198501587), (0.0, 1.5226901769638062), (0.0, 1.522749423980713), (0.0, 1.5228029489517212), (0.0, 1.5229383707046509), (0.0, 1.522964358329773), (0.0, 1.5229791402816772), (0.0, 1.5230634212493896), (0.0, 1.5231298208236694), (0.0, 1.52327561378479), (0.0, 1.5235673189163208), (0.0, 1.5235952138900757), (0.0, 1.5236539840698242), (0.0, 1.5236760377883911), (0.0, 1.5236868858337402), (0.0, 1.5236910581588745), (0.0, 1.5237040519714355), (0.0, 1.5237966775894165), (0.0, 1.523813009262085), (0.0, 1.5238547325134277), (0.0, 1.523903489112854), (0.0, 1.524053692817688), (0.0, 1.5240824222564697), (0.0, 1.5240933895111084), (0.0, 1.524129867553711), (0.0, 1.52415132522583), (0.0, 1.5241879224777222), (0.0, 1.5242506265640259), (0.0, 1.5242738723754883), (0.0, 1.5244375467300415), (0.0, 1.5245635509490967), (0.0, 1.5245815515518188), (0.0, 1.5247607231140137), (0.0, 1.5247740745544434), (0.0, 1.5248332023620605), (0.0, 1.5250931978225708), (0.0, 1.5250983238220215), (0.0, 1.5251853466033936), (0.0, 1.5252004861831665), (0.0, 1.525276780128479), (0.0, 1.5253006219863892), (0.0, 1.5253723859786987), (0.0, 1.5258594751358032), (0.0, 1.526047706604004), (0.0, 1.526050090789795), (0.0, 1.5261428356170654), (0.0, 1.5261644124984741), (0.0, 1.5262304544448853), (0.0, 1.5268877744674683), (0.0, 1.5269004106521606), (0.0, 1.5269756317138672), (0.0, 1.5271539688110352), (0.0, 1.5272996425628662), (0.0, 1.5276587009429932), (0.0, 1.52773118019104), (0.0, 1.5278342962265015), (0.0, 1.528122067451477), (0.0, 1.5283833742141724), (0.0, 1.5283995866775513), (0.0, 1.5284289121627808), (0.0, 1.5285439491271973), (0.0, 1.529563069343567), (0.0, 1.5298583507537842), (0.0, 1.5303163528442383), (0.0, 1.530565619468689), (0.0, 1.5307140350341797), (0.0, 1.530925989151001), (0.0, 1.5313479900360107), (0.0, 1.5342528820037842), (0.0, 1.5557451248168945)], [(8.501367568969727, 8.507256507873535), (7.887312412261963, 8.385688781738281), (7.032220840454102, 7.179867744445801), (7.001885890960693, 7.552196025848389), (6.656803131103516, 7.211007595062256), (6.556734085083008, 6.870294570922852), (6.476870536804199, 7.048467636108398), (6.446937561035156, 6.968675136566162), (6.202606678009033, 7.858549118041992), (6.134825706481934, 7.262186050415039), (6.121199131011963, 6.984419345855713), (6.07590389251709, 6.089506149291992), (5.911365509033203, 6.427674770355225), (5.66381311416626, 5.955296516418457), (5.616776466369629, 5.736125469207764), (5.558971405029297, 5.653120517730713), (5.4949870109558105, 5.527698993682861), (5.42342472076416, 8.376853942871094), (5.421969890594482, 8.569817543029785), (5.3084845542907715, 8.64101505279541), (4.987325668334961, 6.8165411949157715), (4.93572998046875, 7.285840034484863), (4.49521541595459, 7.873552322387695), (4.434026718139648, 4.517693996429443), (4.258459568023682, 7.911962032318115), (4.0219902992248535, 4.039300441741943), (4.01072883605957, 4.294196605682373), (3.975679636001587, 4.339148998260498), (3.962034225463867, 4.742494583129883), (3.6506855487823486, 8.535770416259766)], [(9.555952072143555, 9.670114517211914), (9.44709587097168, 10.098508834838867), (9.125312805175781, 10.73925495147705), (8.446920394897461, 8.483987808227539), (8.397829055786133, 8.50708293914795), (7.653021335601807, 7.747748851776123)]]
[array([[0.        , 1.31066775],
       [0.        , 1.31128168],
       [0.        , 1.32409084],
       [0.        , 1.32602501],
       [0.        , 1.32603574],
       [0.        , 1.32633853],
       [0.        , 1.32736552],
       [0.        , 1.32736897],
       [0.        , 1.32738221],
       [0.        , 1.32749486],
       [0.        , 1.32764816],
       [0.        , 1.32787073],
       [0.        , 1.32821965],
       [0.        , 1.32908726],
       [0.        , 1.32932639],
       [0.        , 1.32941329],
       [0.        , 1.3295331 ],
       [0.        , 1.32962871],
       [0.        , 1.32988441],
       [0.        , 1.3299036 ],
       [0.        , 1.32996798],
       [0.        , 1.33000731],
       [0.        , 1.33021867],
       [0.        , 1.33023441],
       [0.        , 1.33045316],
       [0.        , 1.33053958],
       [0.        , 1.33058655],
       [0.        , 1.33063805],
       [0.        , 1.33069253],
       [0.        , 1.33075941],
       [0.        , 1.33107066],
       [0.        , 1.33111417],
       [0.        , 1.33114958],
       [0.        , 1.3311851 ],
       [0.        , 1.33120596],
       [0.        , 1.33139539],
       [0.        , 1.33143532],
       [0.        , 1.33165669],
       [0.        , 1.33168578],
       [0.        , 1.33175063],
       [0.        , 1.33193088],
       [0.        , 1.33201313],
       [0.        , 1.33215117],
       [0.        , 1.3322382 ],
       [0.        , 1.33255696],
       [0.        , 1.33256555],
       [0.        , 1.3326273 ],
       [0.        , 1.33276129],
       [0.        , 1.33286655],
       [0.        , 1.33307183],
       [0.        , 1.3331089 ],
       [0.        , 1.33314693],
       [0.        , 1.33319879],
       [0.        , 1.33329535],
       [0.        , 1.33343518],
       [0.        , 1.33349872],
       [0.        , 1.33352649],
       [0.        , 1.33369231],
       [0.        , 1.33369958],
       [0.        , 1.33371341],
       [0.        , 1.33398581],
       [0.        , 1.33405197],
       [0.        , 1.33414674],
       [0.        , 1.33417141],
       [0.        , 1.3342638 ],
       [0.        , 1.33429098],
       [0.        , 1.33432853],
       [0.        , 1.33440304],
       [0.        , 1.33442795],
       [0.        , 1.33444667],
       [0.        , 1.33455324],
       [0.        , 1.33467269],
       [0.        , 1.33484685],
       [0.        , 1.33498061],
       [0.        , 1.33504438],
       [0.        , 1.33510327],
       [0.        , 1.33520138],
       [0.        , 1.33524811],
       [0.        , 1.33528161],
       [0.        , 1.335325  ],
       [0.        , 1.33561015],
       [0.        , 1.33561039],
       [0.        , 1.33576763],
       [0.        , 1.33590305],
       [0.        , 1.33591151],
       [0.        , 1.33631563],
       [0.        , 1.33636332],
       [0.        , 1.33639467],
       [0.        , 1.33642864],
       [0.        , 1.33652747],
       [0.        , 1.33659625],
       [0.        , 1.33680797],
       [0.        , 1.33683109],
       [0.        , 1.33688343],
       [0.        , 1.33711302],
       [0.        , 1.3373419 ],
       [0.        , 1.33737254],
       [0.        , 1.33760178],
       [0.        , 1.33764005],
       [0.        , 1.33767235],
       [0.        , 1.33777177],
       [0.        , 1.33779359],
       [0.        , 1.33803701],
       [0.        , 1.33808684],
       [0.        , 1.33820951],
       [0.        , 1.3384335 ],
       [0.        , 1.33847976],
       [0.        , 1.33848381],
       [0.        , 1.33876395],
       [0.        , 1.33879662],
       [0.        , 1.33917427],
       [0.        , 1.33928859],
       [0.        , 1.33964562],
       [0.        , 1.33964813],
       [0.        , 1.33980322],
       [0.        , 1.33990502],
       [0.        , 1.33995831],
       [0.        , 1.34075046],
       [0.        , 1.34095025],
       [0.        , 1.34227669],
       [0.        , 1.34235847],
       [0.        , 1.34237337],
       [0.        , 1.34336758],
       [0.        , 1.3462106 ],
       [0.        , 1.38844991],
       [0.        , 1.4143945 ],
       [0.        , 1.43141782],
       [0.        , 1.44129205],
       [0.        , 1.44828665],
       [0.        , 1.44977474],
       [0.        , 1.44993043],
       [0.        , 1.45107794],
       [0.        , 1.45182598],
       [0.        , 1.45234013],
       [0.        , 1.4526161 ],
       [0.        , 1.45293796],
       [0.        , 1.45332551],
       [0.        , 1.4535079 ],
       [0.        , 1.4539355 ],
       [0.        , 1.45485461],
       [0.        , 1.45509052],
       [0.        , 1.45532978],
       [0.        , 1.45543063],
       [0.        , 1.45543993],
       [0.        , 1.45544159],
       [0.        , 1.45545924],
       [0.        , 1.45578253],
       [0.        , 1.45591462],
       [0.        , 1.45591676],
       [0.        , 1.45613873],
       [0.        , 1.45616829],
       [0.        , 1.45617139],
       [0.        , 1.45617747],
       [0.        , 1.4563359 ],
       [0.        , 1.45641482],
       [0.        , 1.45645034],
       [0.        , 1.45660865],
       [0.        , 1.45662689],
       [0.        , 1.4567678 ],
       [0.        , 1.45680523],
       [0.        , 1.45706987],
       [0.        , 1.45723081],
       [0.        , 1.45730066],
       [0.        , 1.45745683],
       [0.        , 1.45765448],
       [0.        , 1.45772386],
       [0.        , 1.45778131],
       [0.        , 1.45788503],
       [0.        , 1.45793021],
       [0.        , 1.45798981],
       [0.        , 1.45804417],
       [0.        , 1.45828676],
       [0.        , 1.4583056 ],
       [0.        , 1.45832586],
       [0.        , 1.45833182],
       [0.        , 1.45833385],
       [0.        , 1.45838988],
       [0.        , 1.45846331],
       [0.        , 1.45847082],
       [0.        , 1.45850945],
       [0.        , 1.45861304],
       [0.        , 1.45862234],
       [0.        , 1.45889127],
       [0.        , 1.45893061],
       [0.        , 1.45893514],
       [0.        , 1.45894873],
       [0.        , 1.45913804],
       [0.        , 1.45917213],
       [0.        , 1.45918989],
       [0.        , 1.45923638],
       [0.        , 1.45938385],
       [0.        , 1.459445  ],
       [0.        , 1.45952022],
       [0.        , 1.45953429],
       [0.        , 1.45953548],
       [0.        , 1.45962846],
       [0.        , 1.45966029],
       [0.        , 1.45986903],
       [0.        , 1.45987427],
       [0.        , 1.4600352 ],
       [0.        , 1.46004605],
       [0.        , 1.46004999],
       [0.        , 1.46028781],
       [0.        , 1.46038544],
       [0.        , 1.46041548],
       [0.        , 1.4604485 ],
       [0.        , 1.46046042],
       [0.        , 1.46060598],
       [0.        , 1.46071422],
       [0.        , 1.46080959],
       [0.        , 1.4608587 ],
       [0.        , 1.46089661],
       [0.        , 1.46093369],
       [0.        , 1.46100593],
       [0.        , 1.46110618],
       [0.        , 1.4611789 ],
       [0.        , 1.46128511],
       [0.        , 1.46133208],
       [0.        , 1.46137106],
       [0.        , 1.46154451],
       [0.        , 1.46163225],
       [0.        , 1.46172321],
       [0.        , 1.46193886],
       [0.        , 1.46200061],
       [0.        , 1.46203423],
       [0.        , 1.46223605],
       [0.        , 1.46224582],
       [0.        , 1.46239197],
       [0.        , 1.46244776],
       [0.        , 1.46259236],
       [0.        , 1.46263599],
       [0.        , 1.46272624],
       [0.        , 1.46291447],
       [0.        , 1.46318042],
       [0.        , 1.4634521 ],
       [0.        , 1.46359718],
       [0.        , 1.46433914],
       [0.        , 1.4644078 ],
       [0.        , 1.46460867],
       [0.        , 1.46528506],
       [0.        , 1.46593499],
       [0.        , 1.46618021],
       [0.        , 1.46689045],
       [0.        , 1.46693003],
       [0.        , 1.46745741],
       [0.        , 1.46825576],
       [0.        , 1.46833086],
       [0.        , 1.46853304],
       [0.        , 1.46943247],
       [0.        , 1.4712534 ],
       [0.        , 1.47356224],
       [0.        , 1.47400475],
       [0.        , 1.49911559],
       [0.        , 1.50210643],
       [0.        , 1.50995517],
       [0.        , 1.51011634],
       [0.        , 1.51112497],
       [0.        , 1.51260126],
       [0.        , 1.51455748],
       [0.        , 1.51529551],
       [0.        , 1.51543832],
       [0.        , 1.5157665 ],
       [0.        , 1.51622915],
       [0.        , 1.51647007],
       [0.        , 1.51663017],
       [0.        , 1.51685679],
       [0.        , 1.51762033],
       [0.        , 1.51777279],
       [0.        , 1.51784515],
       [0.        , 1.51801133],
       [0.        , 1.51807606],
       [0.        , 1.51810658],
       [0.        , 1.51827788],
       [0.        , 1.51865458],
       [0.        , 1.51868534],
       [0.        , 1.51890767],
       [0.        , 1.51892912],
       [0.        , 1.51969445],
       [0.        , 1.51970613],
       [0.        , 1.51979291],
       [0.        , 1.52021396],
       [0.        , 1.52023125],
       [0.        , 1.52029598],
       [0.        , 1.52036178],
       [0.        , 1.52047431],
       [0.        , 1.52047729],
       [0.        , 1.52059281],
       [0.        , 1.52061009],
       [0.        , 1.52066374],
       [0.        , 1.52080166],
       [0.        , 1.52104449],
       [0.        , 1.5211345 ],
       [0.        , 1.52136719],
       [0.        , 1.52139747],
       [0.        , 1.52140069],
       [0.        , 1.52157474],
       [0.        , 1.52158904],
       [0.        , 1.52158988],
       [0.        , 1.52170384],
       [0.        , 1.52178144],
       [0.        , 1.52184868],
       [0.        , 1.52195656],
       [0.        , 1.52201724],
       [0.        , 1.52203488],
       [0.        , 1.52211654],
       [0.        , 1.52224123],
       [0.        , 1.52224529],
       [0.        , 1.52225125],
       [0.        , 1.52240932],
       [0.        , 1.52241635],
       [0.        , 1.52266562],
       [0.        , 1.52269018],
       [0.        , 1.52274942],
       [0.        , 1.52280295],
       [0.        , 1.52293837],
       [0.        , 1.52296436],
       [0.        , 1.52297914],
       [0.        , 1.52306342],
       [0.        , 1.52312982],
       [0.        , 1.52327561],
       [0.        , 1.52356732],
       [0.        , 1.52359521],
       [0.        , 1.52365398],
       [0.        , 1.52367604],
       [0.        , 1.52368689],
       [0.        , 1.52369106],
       [0.        , 1.52370405],
       [0.        , 1.52379668],
       [0.        , 1.52381301],
       [0.        , 1.52385473],
       [0.        , 1.52390349],
       [0.        , 1.52405369],
       [0.        , 1.52408242],
       [0.        , 1.52409339],
       [0.        , 1.52412987],
       [0.        , 1.52415133],
       [0.        , 1.52418792],
       [0.        , 1.52425063],
       [0.        , 1.52427387],
       [0.        , 1.52443755],
       [0.        , 1.52456355],
       [0.        , 1.52458155],
       [0.        , 1.52476072],
       [0.        , 1.52477407],
       [0.        , 1.5248332 ],
       [0.        , 1.5250932 ],
       [0.        , 1.52509832],
       [0.        , 1.52518535],
       [0.        , 1.52520049],
       [0.        , 1.52527678],
       [0.        , 1.52530062],
       [0.        , 1.52537239],
       [0.        , 1.52585948],
       [0.        , 1.52604771],
       [0.        , 1.52605009],
       [0.        , 1.52614284],
       [0.        , 1.52616441],
       [0.        , 1.52623045],
       [0.        , 1.52688777],
       [0.        , 1.52690041],
       [0.        , 1.52697563],
       [0.        , 1.52715397],
       [0.        , 1.52729964],
       [0.        , 1.5276587 ],
       [0.        , 1.52773118],
       [0.        , 1.5278343 ],
       [0.        , 1.52812207],
       [0.        , 1.52838337],
       [0.        , 1.52839959],
       [0.        , 1.52842891],
       [0.        , 1.52854395],
       [0.        , 1.52956307],
       [0.        , 1.52985835],
       [0.        , 1.53031635],
       [0.        , 1.53056562],
       [0.        , 1.53071404],
       [0.        , 1.53092599],
       [0.        , 1.53134799],
       [0.        , 1.53425288],
       [0.        , 1.55574512]]), array([[8.50136757, 8.50725651],
       [7.88731241, 8.38568878],
       [7.03222084, 7.17986774],
       [7.00188589, 7.55219603],
       [6.65680313, 7.2110076 ],
       [6.55673409, 6.87029457],
       [6.47687054, 7.04846764],
       [6.44693756, 6.96867514],
       [6.20260668, 7.85854912],
       [6.13482571, 7.26218605],
       [6.12119913, 6.98441935],
       [6.07590389, 6.08950615],
       [5.91136551, 6.42767477],
       [5.66381311, 5.95529652],
       [5.61677647, 5.73612547],
       [5.55897141, 5.65312052],
       [5.49498701, 5.52769899],
       [5.42342472, 8.37685394],
       [5.42196989, 8.56981754],
       [5.30848455, 8.64101505],
       [4.98732567, 6.81654119],
       [4.93572998, 7.28584003],
       [4.49521542, 7.87355232],
       [4.43402672, 4.517694  ],
       [4.25845957, 7.91196203],
       [4.0219903 , 4.03930044],
       [4.01072884, 4.29419661],
       [3.97567964, 4.339149  ],
       [3.96203423, 4.74249458],
       [3.65068555, 8.53577042]]), array([[ 9.55595207,  9.67011452],
       [ 9.44709587, 10.09850883],
       [ 9.12531281, 10.73925495],
       [ 8.44692039,  8.48398781],
       [ 8.39782906,  8.50708294],
       [ 7.65302134,  7.74774885]])]2024-03-06 17:53:42.974533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KEH ph vector generated, counter: 120
2024-03-06 17:53:46.876044: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:46.919325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:47.824820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.271388 ), (0., 1.2889435), (0., 1.3202701), (0., 1.3232542),
       (0., 1.3238803), (0., 1.3247736), (0., 1.3250965), (0., 1.3253438),
       (0., 1.3253961), (0., 1.3261234), (0., 1.3263004), (0., 1.3266464),
       (0., 1.3267573), (0., 1.327501 ), (0., 1.327528 ), (0., 1.327668 ),
       (0., 1.3278303), (0., 1.3279501), (0., 1.3282489), (0., 1.3283701),
       (0., 1.3286384), (0., 1.328952 ), (0., 1.3291215), (0., 1.3293631),
       (0., 1.3297094), (0., 1.3297856), (0., 1.329846 ), (0., 1.3302363),
       (0., 1.330462 ), (0., 1.3304945), (0., 1.3305869), (0., 1.3310223),
       (0., 1.331287 ), (0., 1.3313665), (0., 1.3315563), (0., 1.3316004),
       (0., 1.3316964), (0., 1.3317181), (0., 1.3317696), (0., 1.3317897),
       (0., 1.3318468), (0., 1.332025 ), (0., 1.3321216), (0., 1.3321689),
       (0., 1.3322126), (0., 1.3326222), (0., 1.3328185), (0., 1.3329462),
       (0., 1.3329521), (0., 1.332958 ), (0., 1.3330549), (0., 1.3331298),
       (0., 1.3332916), (0., 1.3332995), (0., 1.333318 ), (0., 1.3334129),
       (0., 1.3336028), (0., 1.3337318), (0., 1.3337569), (0., 1.3337888),
       (0., 1.3338196), (0., 1.3339033), (0., 1.3339814), (0., 1.334047 ),
       (0., 1.3341765), (0., 1.3341767), (0., 1.3342396), (0., 1.3344741),
       (0., 1.3346738), (0., 1.3348744), (0., 1.3349519), (0., 1.3351276),
       (0., 1.3351609), (0., 1.3353121), (0., 1.335342 ), (0., 1.3354478),
       (0., 1.3355002), (0., 1.3357955), (0., 1.3358345), (0., 1.3362101),
       (0., 1.336268 ), (0., 1.3362966), (0., 1.3363483), (0., 1.3363692),
       (0., 1.3364859), (0., 1.3365319), (0., 1.3366033), (0., 1.3367367),
       (0., 1.3367405), (0., 1.3367722), (0., 1.33683  ), (0., 1.3369706),
       (0., 1.3370824), (0., 1.3373108), (0., 1.3373374), (0., 1.3374984),
       (0., 1.3375491), (0., 1.337607 ), (0., 1.337721 ), (0., 1.337728 ),
       (0., 1.3377671), (0., 1.3379166), (0., 1.3380029), (0., 1.3382931),
       (0., 1.3383586), (0., 1.3384827), (0., 1.3386844), (0., 1.3386862),
       (0., 1.3389586), (0., 1.3391789), (0., 1.3394555), (0., 1.3399564),
       (0., 1.3400612), (0., 1.3401415), (0., 1.3401663), (0., 1.3402987),
       (0., 1.3405266), (0., 1.3405881), (0., 1.340991 ), (0., 1.3411644),
       (0., 1.3414204), (0., 1.3419853), (0., 1.3424926), (0., 1.342767 ),
       (0., 1.4017557), (0., 1.4206917), (0., 1.428367 ), (0., 1.4371206),
       (0., 1.4455985), (0., 1.4491295), (0., 1.4509087), (0., 1.4523623),
       (0., 1.4528407), (0., 1.4528488), (0., 1.4532548), (0., 1.4534683),
       (0., 1.4537377), (0., 1.4539521), (0., 1.4545892), (0., 1.4548482),
       (0., 1.4549162), (0., 1.4550631), (0., 1.4550928), (0., 1.4551191),
       (0., 1.4551227), (0., 1.4551994), (0., 1.4552076), (0., 1.4552976),
       (0., 1.4553697), (0., 1.4553918), (0., 1.4554757), (0., 1.4555072),
       (0., 1.4556024), (0., 1.4559181), (0., 1.4559408), (0., 1.455964 ),
       (0., 1.4561741), (0., 1.4562064), (0., 1.4562242), (0., 1.4562258),
       (0., 1.4562716), (0., 1.4562895), (0., 1.4564114), (0., 1.4567274),
       (0., 1.4568555), (0., 1.4569619), (0., 1.4572846), (0., 1.4575435),
       (0., 1.4576261), (0., 1.4576826), (0., 1.4577746), (0., 1.4580792),
       (0., 1.45812  ), (0., 1.4582748), (0., 1.4582881), (0., 1.4584721),
       (0., 1.458498 ), (0., 1.4585443), (0., 1.4585779), (0., 1.4586906),
       (0., 1.4587659), (0., 1.4588231), (0., 1.4588236), (0., 1.4588382),
       (0., 1.4589121), (0., 1.458947 ), (0., 1.4590837), (0., 1.4591279),
       (0., 1.4592361), (0., 1.4594734), (0., 1.4595193), (0., 1.45954  ),
       (0., 1.4595509), (0., 1.45963  ), (0., 1.4596338), (0., 1.4596758),
       (0., 1.4597083), (0., 1.4598343), (0., 1.460022 ), (0., 1.4601191),
       (0., 1.460244 ), (0., 1.4602487), (0., 1.4603415), (0., 1.4604614),
       (0., 1.4605384), (0., 1.4605514), (0., 1.460768 ), (0., 1.4608226),
       (0., 1.4608492), (0., 1.4610329), (0., 1.4610418), (0., 1.4612223),
       (0., 1.4614296), (0., 1.4614946), (0., 1.4615428), (0., 1.4615587),
       (0., 1.4617573), (0., 1.4617597), (0., 1.4617794), (0., 1.4618409),
       (0., 1.4620827), (0., 1.4624056), (0., 1.4624442), (0., 1.4625577),
       (0., 1.4626136), (0., 1.4626806), (0., 1.4627707), (0., 1.4627824),
       (0., 1.4627844), (0., 1.4627959), (0., 1.4628652), (0., 1.4630171),
       (0., 1.4633148), (0., 1.4634601), (0., 1.4635102), (0., 1.4635861),
       (0., 1.4636381), (0., 1.463726 ), (0., 1.4641095), (0., 1.4641333),
       (0., 1.4641463), (0., 1.4642117), (0., 1.4651912), (0., 1.4669383),
       (0., 1.4670264), (0., 1.4685537), (0., 1.4686705), (0., 1.4688711),
       (0., 1.4689271), (0., 1.4691019), (0., 1.4705961), (0., 1.4748832),
       (0., 1.5035149), (0., 1.5108359), (0., 1.5139277), (0., 1.5155082),
       (0., 1.5157354), (0., 1.5164791), (0., 1.5166545), (0., 1.5167089),
       (0., 1.5167602), (0., 1.5172917), (0., 1.5174251), (0., 1.517528 ),
       (0., 1.5175662), (0., 1.5175852), (0., 1.5177213), (0., 1.5182536),
       (0., 1.5184743), (0., 1.5188376), (0., 1.5188662), (0., 1.5189158),
       (0., 1.5189786), (0., 1.519027 ), (0., 1.5191901), (0., 1.5192871),
       (0., 1.5193923), (0., 1.5194178), (0., 1.5196481), (0., 1.5199293),
       (0., 1.5200745), (0., 1.5203396), (0., 1.5204792), (0., 1.5206254),
       (0., 1.5206276), (0., 1.5206927), (0., 1.5208566), (0., 1.5209752),
       (0., 1.5210607), (0., 1.5211289), (0., 1.5212374), (0., 1.5212604),
       (0., 1.5212762), (0., 1.5213062), (0., 1.5214597), (0., 1.5214758),
       (0., 1.5215104), (0., 1.5215757), (0., 1.5215971), (0., 1.5216404),
       (0., 1.5217766), (0., 1.5220373), (0., 1.5223079), (0., 1.5224235),
       (0., 1.5224891), (0., 1.5225613), (0., 1.5225945), (0., 1.5226378),
       (0., 1.5227205), (0., 1.5229074), (0., 1.5229098), (0., 1.5230145),
       (0., 1.5230966), (0., 1.5231675), (0., 1.5231959), (0., 1.5232586),
       (0., 1.5232681), (0., 1.5234053), (0., 1.5234064), (0., 1.5234939),
       (0., 1.5236018), (0., 1.5236763), (0., 1.5236932), (0., 1.5238626),
       (0., 1.5238774), (0., 1.5239102), (0., 1.5239114), (0., 1.5239924),
       (0., 1.5240278), (0., 1.5240308), (0., 1.5240426), (0., 1.5241827),
       (0., 1.5242698), (0., 1.5243366), (0., 1.5244694), (0., 1.5246198),
       (0., 1.5246657), (0., 1.524698 ), (0., 1.5247611), (0., 1.5247916),
       (0., 1.5249301), (0., 1.5250006), (0., 1.5250993), (0., 1.5251478),
       (0., 1.5252024), (0., 1.5252709), (0., 1.5252819), (0., 1.5253257),
       (0., 1.5254091), (0., 1.5257841), (0., 1.5258267), (0., 1.5258648),
       (0., 1.5261165), (0., 1.5262192), (0., 1.5262907), (0., 1.5263093),
       (0., 1.5263667), (0., 1.5265   ), (0., 1.5267866), (0., 1.526828 ),
       (0., 1.5268803), (0., 1.5268959), (0., 1.5271971), (0., 1.527555 ),
       (0., 1.5275555), (0., 1.5276316), (0., 1.5277461), (0., 1.5281808),
       (0., 1.5287898), (0., 1.5289056), (0., 1.5289229), (0., 1.5299579),
       (0., 1.5305332), (0., 1.5306643), (0., 1.5308357), (0., 1.5325739),
       (0., 1.5420913), (0., 1.548881 ), (0., 1.613496 ), (0., 1.6326672)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.450287 , 8.502152 ), (7.780253 , 8.308199 ),
       (6.9688168, 7.531099 ), (6.9686446, 7.1589212),
       (6.6108794, 6.9078403), (6.564316 , 7.183895 ),
       (6.4688797, 6.990532 ), (6.438244 , 7.06247  ),
       (6.2007194, 7.806253 ), (6.1086555, 7.1597257),
       (6.0723085, 7.005544 ), (6.0132365, 6.0172176),
       (5.889496 , 6.4086027), (5.6594434, 5.9557242),
       (5.598385 , 6.019035 ), (5.579732 , 8.513681 ),
       (5.4768906, 5.602895 ), (5.296947 , 8.32852  ),
       (5.293331 , 8.467715 ), (5.10626  , 5.134923 ),
       (5.064034 , 6.8063393), (4.940686 , 7.1703243),
       (4.5797186, 4.8642716), (4.4734926, 7.835865 ),
       (4.3751745, 4.45249  ), (4.360169 , 7.9662805),
       (4.0125914, 4.2684693), (4.007017 , 4.0882835),
       (3.928581 , 4.688472 ), (3.9230118, 4.296885 ),
       (3.9005418, 3.9176142), (3.6997426, 8.5167675)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.51652 ,  9.612755 ), (9.420275, 10.0792055),
       (9.110514, 10.730769 ), (8.385576,  8.403459 ),
       (8.337422,  8.460422 ), (7.643832,  7.6512637)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.271388053894043), (0.0, 1.2889435291290283), (0.0, 1.32027006149292), (0.0, 1.3232542276382446), (0.0, 1.3238803148269653), (0.0, 1.3247735500335693), (0.0, 1.3250964879989624), (0.0, 1.3253438472747803), (0.0, 1.3253960609436035), (0.0, 1.3261233568191528), (0.0, 1.3263003826141357), (0.0, 1.3266464471817017), (0.0, 1.3267573118209839), (0.0, 1.3275010585784912), (0.0, 1.3275279998779297), (0.0, 1.3276679515838623), (0.0, 1.3278303146362305), (0.0, 1.327950119972229), (0.0, 1.3282488584518433), (0.0, 1.3283700942993164), (0.0, 1.3286384344100952), (0.0, 1.3289519548416138), (0.0, 1.329121470451355), (0.0, 1.3293631076812744), (0.0, 1.3297094106674194), (0.0, 1.3297855854034424), (0.0, 1.3298460245132446), (0.0, 1.3302363157272339), (0.0, 1.3304619789123535), (0.0, 1.3304945230484009), (0.0, 1.3305869102478027), (0.0, 1.3310222625732422), (0.0, 1.3312870264053345), (0.0, 1.3313665390014648), (0.0, 1.3315563201904297), (0.0, 1.3316004276275635), (0.0, 1.3316963911056519), (0.0, 1.33171808719635), (0.0, 1.331769585609436), (0.0, 1.3317897319793701), (0.0, 1.331846833229065), (0.0, 1.3320250511169434), (0.0, 1.3321216106414795), (0.0, 1.3321689367294312), (0.0, 1.3322125673294067), (0.0, 1.3326221704483032), (0.0, 1.3328185081481934), (0.0, 1.3329461812973022), (0.0, 1.3329521417617798), (0.0, 1.3329579830169678), (0.0, 1.3330549001693726), (0.0, 1.3331297636032104), (0.0, 1.3332916498184204), (0.0, 1.3332995176315308), (0.0, 1.3333179950714111), (0.0, 1.3334128856658936), (0.0, 1.333602786064148), (0.0, 1.333731770515442), (0.0, 1.333756923675537), (0.0, 1.3337887525558472), (0.0, 1.3338196277618408), (0.0, 1.3339033126831055), (0.0, 1.3339813947677612), (0.0, 1.3340469598770142), (0.0, 1.3341765403747559), (0.0, 1.3341766595840454), (0.0, 1.3342396020889282), (0.0, 1.3344740867614746), (0.0, 1.3346737623214722), (0.0, 1.3348743915557861), (0.0, 1.3349518775939941), (0.0, 1.335127592086792), (0.0, 1.3351608514785767), (0.0, 1.3353121280670166), (0.0, 1.3353420495986938), (0.0, 1.3354477882385254), (0.0, 1.3355002403259277), (0.0, 1.335795521736145), (0.0, 1.3358345031738281), (0.0, 1.3362101316452026), (0.0, 1.3362679481506348), (0.0, 1.336296558380127), (0.0, 1.336348295211792), (0.0, 1.3363691568374634), (0.0, 1.3364858627319336), (0.0, 1.3365318775177002), (0.0, 1.3366032838821411), (0.0, 1.3367366790771484), (0.0, 1.336740493774414), (0.0, 1.3367722034454346), (0.0, 1.3368300199508667), (0.0, 1.336970567703247), (0.0, 1.3370823860168457), (0.0, 1.337310791015625), (0.0, 1.3373373746871948), (0.0, 1.337498426437378), (0.0, 1.337549090385437), (0.0, 1.3376070261001587), (0.0, 1.3377209901809692), (0.0, 1.3377280235290527), (0.0, 1.3377671241760254), (0.0, 1.337916612625122), (0.0, 1.3380029201507568), (0.0, 1.3382930755615234), (0.0, 1.3383586406707764), (0.0, 1.3384827375411987), (0.0, 1.3386844396591187), (0.0, 1.338686227798462), (0.0, 1.3389586210250854), (0.0, 1.3391789197921753), (0.0, 1.339455485343933), (0.0, 1.3399564027786255), (0.0, 1.3400611877441406), (0.0, 1.3401415348052979), (0.0, 1.3401663303375244), (0.0, 1.3402986526489258), (0.0, 1.3405265808105469), (0.0, 1.340588092803955), (0.0, 1.3409910202026367), (0.0, 1.3411643505096436), (0.0, 1.3414204120635986), (0.0, 1.3419853448867798), (0.0, 1.3424925804138184), (0.0, 1.3427670001983643), (0.0, 1.401755690574646), (0.0, 1.420691728591919), (0.0, 1.428367018699646), (0.0, 1.4371205568313599), (0.0, 1.4455984830856323), (0.0, 1.4491294622421265), (0.0, 1.4509086608886719), (0.0, 1.452362298965454), (0.0, 1.4528406858444214), (0.0, 1.4528487920761108), (0.0, 1.4532548189163208), (0.0, 1.4534683227539062), (0.0, 1.453737735748291), (0.0, 1.4539520740509033), (0.0, 1.4545892477035522), (0.0, 1.4548481702804565), (0.0, 1.45491623878479), (0.0, 1.4550631046295166), (0.0, 1.4550927877426147), (0.0, 1.4551191329956055), (0.0, 1.455122709274292), (0.0, 1.4551993608474731), (0.0, 1.4552075862884521), (0.0, 1.455297589302063), (0.0, 1.4553697109222412), (0.0, 1.455391764640808), (0.0, 1.4554756879806519), (0.0, 1.4555071592330933), (0.0, 1.4556024074554443), (0.0, 1.4559180736541748), (0.0, 1.455940842628479), (0.0, 1.4559639692306519), (0.0, 1.4561741352081299), (0.0, 1.4562064409255981), (0.0, 1.4562242031097412), (0.0, 1.4562257528305054), (0.0, 1.4562716484069824), (0.0, 1.456289529800415), (0.0, 1.456411361694336), (0.0, 1.456727385520935), (0.0, 1.4568555355072021), (0.0, 1.4569618701934814), (0.0, 1.4572845697402954), (0.0, 1.4575434923171997), (0.0, 1.4576261043548584), (0.0, 1.4576826095581055), (0.0, 1.4577746391296387), (0.0, 1.458079218864441), (0.0, 1.4581199884414673), (0.0, 1.4582748413085938), (0.0, 1.4582880735397339), (0.0, 1.4584721326828003), (0.0, 1.4584980010986328), (0.0, 1.4585442543029785), (0.0, 1.4585778713226318), (0.0, 1.4586906433105469), (0.0, 1.4587658643722534), (0.0, 1.4588230848312378), (0.0, 1.458823561668396), (0.0, 1.4588382244110107), (0.0, 1.4589121341705322), (0.0, 1.458946943283081), (0.0, 1.4590836763381958), (0.0, 1.4591279029846191), (0.0, 1.4592361450195312), (0.0, 1.4594733715057373), (0.0, 1.4595192670822144), (0.0, 1.4595400094985962), (0.0, 1.4595508575439453), (0.0, 1.459630012512207), (0.0, 1.4596338272094727), (0.0, 1.4596757888793945), (0.0, 1.459708333015442), (0.0, 1.459834337234497), (0.0, 1.46002197265625), (0.0, 1.4601191282272339), (0.0, 1.460244059562683), (0.0, 1.4602487087249756), (0.0, 1.460341453552246), (0.0, 1.4604613780975342), (0.0, 1.460538387298584), (0.0, 1.460551381111145), (0.0, 1.4607679843902588), (0.0, 1.460822582244873), (0.0, 1.4608491659164429), (0.0, 1.4610328674316406), (0.0, 1.461041808128357), (0.0, 1.4612222909927368), (0.0, 1.4614295959472656), (0.0, 1.4614945650100708), (0.0, 1.4615428447723389), (0.0, 1.4615586996078491), (0.0, 1.4617573022842407), (0.0, 1.4617596864700317), (0.0, 1.4617793560028076), (0.0, 1.4618408679962158), (0.0, 1.4620827436447144), (0.0, 1.4624055624008179), (0.0, 1.4624441862106323), (0.0, 1.4625576734542847), (0.0, 1.462613582611084), (0.0, 1.4626805782318115), (0.0, 1.462770700454712), (0.0, 1.462782382965088), (0.0, 1.4627844095230103), (0.0, 1.4627958536148071), (0.0, 1.4628652334213257), (0.0, 1.4630171060562134), (0.0, 1.4633147716522217), (0.0, 1.463460087776184), (0.0, 1.4635101556777954), (0.0, 1.4635860919952393), (0.0, 1.4636380672454834), (0.0, 1.4637260437011719), (0.0, 1.4641095399856567), (0.0, 1.4641332626342773), (0.0, 1.4641462564468384), (0.0, 1.4642117023468018), (0.0, 1.4651912450790405), (0.0, 1.4669382572174072), (0.0, 1.4670263528823853), (0.0, 1.4685536623001099), (0.0, 1.4686704874038696), (0.0, 1.4688711166381836), (0.0, 1.4689271450042725), (0.0, 1.469101905822754), (0.0, 1.4705960750579834), (0.0, 1.4748831987380981), (0.0, 1.5035148859024048), (0.0, 1.510835886001587), (0.0, 1.513927698135376), (0.0, 1.5155081748962402), (0.0, 1.515735387802124), (0.0, 1.5164791345596313), (0.0, 1.5166544914245605), (0.0, 1.5167088508605957), (0.0, 1.516760230064392), (0.0, 1.5172916650772095), (0.0, 1.5174250602722168), (0.0, 1.5175280570983887), (0.0, 1.517566204071045), (0.0, 1.5175851583480835), (0.0, 1.5177212953567505), (0.0, 1.5182535648345947), (0.0, 1.5184743404388428), (0.0, 1.518837571144104), (0.0, 1.5188661813735962), (0.0, 1.5189157724380493), (0.0, 1.5189785957336426), (0.0, 1.5190269947052002), (0.0, 1.5191900730133057), (0.0, 1.519287109375), (0.0, 1.5193922519683838), (0.0, 1.5194177627563477), (0.0, 1.5196480751037598), (0.0, 1.51992928981781), (0.0, 1.520074486732483), (0.0, 1.5203396081924438), (0.0, 1.5204792022705078), (0.0, 1.520625352859497), (0.0, 1.5206276178359985), (0.0, 1.5206927061080933), (0.0, 1.5208566188812256), (0.0, 1.5209752321243286), (0.0, 1.5210607051849365), (0.0, 1.5211288928985596), (0.0, 1.5212373733520508), (0.0, 1.521260380744934), (0.0, 1.5212762355804443), (0.0, 1.5213061571121216), (0.0, 1.521459698677063), (0.0, 1.5214757919311523), (0.0, 1.521510362625122), (0.0, 1.521575689315796), (0.0, 1.521597146987915), (0.0, 1.521640419960022), (0.0, 1.521776556968689), (0.0, 1.5220372676849365), (0.0, 1.5223078727722168), (0.0, 1.522423505783081), (0.0, 1.522489070892334), (0.0, 1.5225613117218018), (0.0, 1.5225944519042969), (0.0, 1.5226378440856934), (0.0, 1.522720456123352), (0.0, 1.5229073762893677), (0.0, 1.5229097604751587), (0.0, 1.5230145454406738), (0.0, 1.5230965614318848), (0.0, 1.5231674909591675), (0.0, 1.5231958627700806), (0.0, 1.5232585668563843), (0.0, 1.5232681035995483), (0.0, 1.5234053134918213), (0.0, 1.5234063863754272), (0.0, 1.5234938859939575), (0.0, 1.523601770401001), (0.0, 1.5236762762069702), (0.0, 1.5236932039260864), (0.0, 1.523862600326538), (0.0, 1.5238773822784424), (0.0, 1.5239101648330688), (0.0, 1.5239113569259644), (0.0, 1.5239924192428589), (0.0, 1.5240278244018555), (0.0, 1.5240308046340942), (0.0, 1.5240426063537598), (0.0, 1.524182677268982), (0.0, 1.5242698192596436), (0.0, 1.524336576461792), (0.0, 1.5244693756103516), (0.0, 1.5246198177337646), (0.0, 1.5246657133102417), (0.0, 1.52469801902771), (0.0, 1.5247610807418823), (0.0, 1.5247915983200073), (0.0, 1.5249301195144653), (0.0, 1.5250005722045898), (0.0, 1.525099277496338), (0.0, 1.525147795677185), (0.0, 1.5252023935317993), (0.0, 1.525270938873291), (0.0, 1.5252819061279297), (0.0, 1.5253256559371948), (0.0, 1.5254091024398804), (0.0, 1.5257841348648071), (0.0, 1.5258266925811768), (0.0, 1.525864839553833), (0.0, 1.5261164903640747), (0.0, 1.5262192487716675), (0.0, 1.5262906551361084), (0.0, 1.5263092517852783), (0.0, 1.5263667106628418), (0.0, 1.5264999866485596), (0.0, 1.5267865657806396), (0.0, 1.5268280506134033), (0.0, 1.5268802642822266), (0.0, 1.5268958806991577), (0.0, 1.5271971225738525), (0.0, 1.527554988861084), (0.0, 1.5275554656982422), (0.0, 1.5276316404342651), (0.0, 1.5277460813522339), (0.0, 1.5281808376312256), (0.0, 1.528789758682251), (0.0, 1.5289056301116943), (0.0, 1.5289229154586792), (0.0, 1.529957890510559), (0.0, 1.5305331945419312), (0.0, 1.530664324760437), (0.0, 1.530835747718811), (0.0, 1.532573938369751), (0.0, 1.5420912504196167), (0.0, 1.5488810539245605), (0.0, 1.613495945930481), (0.0, 1.6326671838760376)], [(8.450286865234375, 8.502152442932129), (7.780252933502197, 8.308198928833008), (6.968816757202148, 7.53109884262085), (6.968644618988037, 7.158921241760254), (6.610879421234131, 6.907840251922607), (6.5643157958984375, 7.183895111083984), (6.468879699707031, 6.990531921386719), (6.438243865966797, 7.062469959259033), (6.200719356536865, 7.806252956390381), (6.1086554527282715, 7.159725666046143), (6.072308540344238, 7.005544185638428), (6.0132365226745605, 6.017217636108398), (5.889495849609375, 6.408602714538574), (5.659443378448486, 5.955724239349365), (5.598384857177734, 6.0190348625183105), (5.5797319412231445, 8.513681411743164), (5.476890563964844, 5.6028947830200195), (5.296947002410889, 8.328519821166992), (5.293331146240234, 8.4677152633667), (5.106259822845459, 5.134922981262207), (5.0640339851379395, 6.806339263916016), (4.940686225891113, 7.170324325561523), (4.579718589782715, 4.864271640777588), (4.473492622375488, 7.835865020751953), (4.375174522399902, 4.452489852905273), (4.360168933868408, 7.966280460357666), (4.012591361999512, 4.268469333648682), (4.007017135620117, 4.088283538818359), (3.9285809993743896, 4.688471794128418), (3.9230117797851562, 4.296885013580322), (3.9005417823791504, 3.917614221572876), (3.699742555618286, 8.516767501831055)], [(9.516519546508789, 9.612754821777344), (9.42027473449707, 10.079205513000488), (9.110513687133789, 10.730769157409668), (8.385576248168945, 8.403458595275879), (8.337422370910645, 8.460421562194824), (7.643832206726074, 7.65126371383667)]]
[array([[0.        , 1.27138805],
       [0.        , 1.28894353],
       [0.        , 1.32027006],
       [0.        , 1.32325423],
       [0.        , 1.32388031],
       [0.        , 1.32477355],
       [0.        , 1.32509649],
       [0.        , 1.32534385],
       [0.        , 1.32539606],
       [0.        , 1.32612336],
       [0.        , 1.32630038],
       [0.        , 1.32664645],
       [0.        , 1.32675731],
       [0.        , 1.32750106],
       [0.        , 1.327528  ],
       [0.        , 1.32766795],
       [0.        , 1.32783031],
       [0.        , 1.32795012],
       [0.        , 1.32824886],
       [0.        , 1.32837009],
       [0.        , 1.32863843],
       [0.        , 1.32895195],
       [0.        , 1.32912147],
       [0.        , 1.32936311],
       [0.        , 1.32970941],
       [0.        , 1.32978559],
       [0.        , 1.32984602],
       [0.        , 1.33023632],
       [0.        , 1.33046198],
       [0.        , 1.33049452],
       [0.        , 1.33058691],
       [0.        , 1.33102226],
       [0.        , 1.33128703],
       [0.        , 1.33136654],
       [0.        , 1.33155632],
       [0.        , 1.33160043],
       [0.        , 1.33169639],
       [0.        , 1.33171809],
       [0.        , 1.33176959],
       [0.        , 1.33178973],
       [0.        , 1.33184683],
       [0.        , 1.33202505],
       [0.        , 1.33212161],
       [0.        , 1.33216894],
       [0.        , 1.33221257],
       [0.        , 1.33262217],
       [0.        , 1.33281851],
       [0.        , 1.33294618],
       [0.        , 1.33295214],
       [0.        , 1.33295798],
       [0.        , 1.3330549 ],
       [0.        , 1.33312976],
       [0.        , 1.33329165],
       [0.        , 1.33329952],
       [0.        , 1.333318  ],
       [0.        , 1.33341289],
       [0.        , 1.33360279],
       [0.        , 1.33373177],
       [0.        , 1.33375692],
       [0.        , 1.33378875],
       [0.        , 1.33381963],
       [0.        , 1.33390331],
       [0.        , 1.33398139],
       [0.        , 1.33404696],
       [0.        , 1.33417654],
       [0.        , 1.33417666],
       [0.        , 1.3342396 ],
       [0.        , 1.33447409],
       [0.        , 1.33467376],
       [0.        , 1.33487439],
       [0.        , 1.33495188],
       [0.        , 1.33512759],
       [0.        , 1.33516085],
       [0.        , 1.33531213],
       [0.        , 1.33534205],
       [0.        , 1.33544779],
       [0.        , 1.33550024],
       [0.        , 1.33579552],
       [0.        , 1.3358345 ],
       [0.        , 1.33621013],
       [0.        , 1.33626795],
       [0.        , 1.33629656],
       [0.        , 1.3363483 ],
       [0.        , 1.33636916],
       [0.        , 1.33648586],
       [0.        , 1.33653188],
       [0.        , 1.33660328],
       [0.        , 1.33673668],
       [0.        , 1.33674049],
       [0.        , 1.3367722 ],
       [0.        , 1.33683002],
       [0.        , 1.33697057],
       [0.        , 1.33708239],
       [0.        , 1.33731079],
       [0.        , 1.33733737],
       [0.        , 1.33749843],
       [0.        , 1.33754909],
       [0.        , 1.33760703],
       [0.        , 1.33772099],
       [0.        , 1.33772802],
       [0.        , 1.33776712],
       [0.        , 1.33791661],
       [0.        , 1.33800292],
       [0.        , 1.33829308],
       [0.        , 1.33835864],
       [0.        , 1.33848274],
       [0.        , 1.33868444],
       [0.        , 1.33868623],
       [0.        , 1.33895862],
       [0.        , 1.33917892],
       [0.        , 1.33945549],
       [0.        , 1.3399564 ],
       [0.        , 1.34006119],
       [0.        , 1.34014153],
       [0.        , 1.34016633],
       [0.        , 1.34029865],
       [0.        , 1.34052658],
       [0.        , 1.34058809],
       [0.        , 1.34099102],
       [0.        , 1.34116435],
       [0.        , 1.34142041],
       [0.        , 1.34198534],
       [0.        , 1.34249258],
       [0.        , 1.342767  ],
       [0.        , 1.40175569],
       [0.        , 1.42069173],
       [0.        , 1.42836702],
       [0.        , 1.43712056],
       [0.        , 1.44559848],
       [0.        , 1.44912946],
       [0.        , 1.45090866],
       [0.        , 1.4523623 ],
       [0.        , 1.45284069],
       [0.        , 1.45284879],
       [0.        , 1.45325482],
       [0.        , 1.45346832],
       [0.        , 1.45373774],
       [0.        , 1.45395207],
       [0.        , 1.45458925],
       [0.        , 1.45484817],
       [0.        , 1.45491624],
       [0.        , 1.4550631 ],
       [0.        , 1.45509279],
       [0.        , 1.45511913],
       [0.        , 1.45512271],
       [0.        , 1.45519936],
       [0.        , 1.45520759],
       [0.        , 1.45529759],
       [0.        , 1.45536971],
       [0.        , 1.45539176],
       [0.        , 1.45547569],
       [0.        , 1.45550716],
       [0.        , 1.45560241],
       [0.        , 1.45591807],
       [0.        , 1.45594084],
       [0.        , 1.45596397],
       [0.        , 1.45617414],
       [0.        , 1.45620644],
       [0.        , 1.4562242 ],
       [0.        , 1.45622575],
       [0.        , 1.45627165],
       [0.        , 1.45628953],
       [0.        , 1.45641136],
       [0.        , 1.45672739],
       [0.        , 1.45685554],
       [0.        , 1.45696187],
       [0.        , 1.45728457],
       [0.        , 1.45754349],
       [0.        , 1.4576261 ],
       [0.        , 1.45768261],
       [0.        , 1.45777464],
       [0.        , 1.45807922],
       [0.        , 1.45811999],
       [0.        , 1.45827484],
       [0.        , 1.45828807],
       [0.        , 1.45847213],
       [0.        , 1.458498  ],
       [0.        , 1.45854425],
       [0.        , 1.45857787],
       [0.        , 1.45869064],
       [0.        , 1.45876586],
       [0.        , 1.45882308],
       [0.        , 1.45882356],
       [0.        , 1.45883822],
       [0.        , 1.45891213],
       [0.        , 1.45894694],
       [0.        , 1.45908368],
       [0.        , 1.4591279 ],
       [0.        , 1.45923615],
       [0.        , 1.45947337],
       [0.        , 1.45951927],
       [0.        , 1.45954001],
       [0.        , 1.45955086],
       [0.        , 1.45963001],
       [0.        , 1.45963383],
       [0.        , 1.45967579],
       [0.        , 1.45970833],
       [0.        , 1.45983434],
       [0.        , 1.46002197],
       [0.        , 1.46011913],
       [0.        , 1.46024406],
       [0.        , 1.46024871],
       [0.        , 1.46034145],
       [0.        , 1.46046138],
       [0.        , 1.46053839],
       [0.        , 1.46055138],
       [0.        , 1.46076798],
       [0.        , 1.46082258],
       [0.        , 1.46084917],
       [0.        , 1.46103287],
       [0.        , 1.46104181],
       [0.        , 1.46122229],
       [0.        , 1.4614296 ],
       [0.        , 1.46149457],
       [0.        , 1.46154284],
       [0.        , 1.4615587 ],
       [0.        , 1.4617573 ],
       [0.        , 1.46175969],
       [0.        , 1.46177936],
       [0.        , 1.46184087],
       [0.        , 1.46208274],
       [0.        , 1.46240556],
       [0.        , 1.46244419],
       [0.        , 1.46255767],
       [0.        , 1.46261358],
       [0.        , 1.46268058],
       [0.        , 1.4627707 ],
       [0.        , 1.46278238],
       [0.        , 1.46278441],
       [0.        , 1.46279585],
       [0.        , 1.46286523],
       [0.        , 1.46301711],
       [0.        , 1.46331477],
       [0.        , 1.46346009],
       [0.        , 1.46351016],
       [0.        , 1.46358609],
       [0.        , 1.46363807],
       [0.        , 1.46372604],
       [0.        , 1.46410954],
       [0.        , 1.46413326],
       [0.        , 1.46414626],
       [0.        , 1.4642117 ],
       [0.        , 1.46519125],
       [0.        , 1.46693826],
       [0.        , 1.46702635],
       [0.        , 1.46855366],
       [0.        , 1.46867049],
       [0.        , 1.46887112],
       [0.        , 1.46892715],
       [0.        , 1.46910191],
       [0.        , 1.47059608],
       [0.        , 1.4748832 ],
       [0.        , 1.50351489],
       [0.        , 1.51083589],
       [0.        , 1.5139277 ],
       [0.        , 1.51550817],
       [0.        , 1.51573539],
       [0.        , 1.51647913],
       [0.        , 1.51665449],
       [0.        , 1.51670885],
       [0.        , 1.51676023],
       [0.        , 1.51729167],
       [0.        , 1.51742506],
       [0.        , 1.51752806],
       [0.        , 1.5175662 ],
       [0.        , 1.51758516],
       [0.        , 1.5177213 ],
       [0.        , 1.51825356],
       [0.        , 1.51847434],
       [0.        , 1.51883757],
       [0.        , 1.51886618],
       [0.        , 1.51891577],
       [0.        , 1.5189786 ],
       [0.        , 1.51902699],
       [0.        , 1.51919007],
       [0.        , 1.51928711],
       [0.        , 1.51939225],
       [0.        , 1.51941776],
       [0.        , 1.51964808],
       [0.        , 1.51992929],
       [0.        , 1.52007449],
       [0.        , 1.52033961],
       [0.        , 1.5204792 ],
       [0.        , 1.52062535],
       [0.        , 1.52062762],
       [0.        , 1.52069271],
       [0.        , 1.52085662],
       [0.        , 1.52097523],
       [0.        , 1.52106071],
       [0.        , 1.52112889],
       [0.        , 1.52123737],
       [0.        , 1.52126038],
       [0.        , 1.52127624],
       [0.        , 1.52130616],
       [0.        , 1.5214597 ],
       [0.        , 1.52147579],
       [0.        , 1.52151036],
       [0.        , 1.52157569],
       [0.        , 1.52159715],
       [0.        , 1.52164042],
       [0.        , 1.52177656],
       [0.        , 1.52203727],
       [0.        , 1.52230787],
       [0.        , 1.52242351],
       [0.        , 1.52248907],
       [0.        , 1.52256131],
       [0.        , 1.52259445],
       [0.        , 1.52263784],
       [0.        , 1.52272046],
       [0.        , 1.52290738],
       [0.        , 1.52290976],
       [0.        , 1.52301455],
       [0.        , 1.52309656],
       [0.        , 1.52316749],
       [0.        , 1.52319586],
       [0.        , 1.52325857],
       [0.        , 1.5232681 ],
       [0.        , 1.52340531],
       [0.        , 1.52340639],
       [0.        , 1.52349389],
       [0.        , 1.52360177],
       [0.        , 1.52367628],
       [0.        , 1.5236932 ],
       [0.        , 1.5238626 ],
       [0.        , 1.52387738],
       [0.        , 1.52391016],
       [0.        , 1.52391136],
       [0.        , 1.52399242],
       [0.        , 1.52402782],
       [0.        , 1.5240308 ],
       [0.        , 1.52404261],
       [0.        , 1.52418268],
       [0.        , 1.52426982],
       [0.        , 1.52433658],
       [0.        , 1.52446938],
       [0.        , 1.52461982],
       [0.        , 1.52466571],
       [0.        , 1.52469802],
       [0.        , 1.52476108],
       [0.        , 1.5247916 ],
       [0.        , 1.52493012],
       [0.        , 1.52500057],
       [0.        , 1.52509928],
       [0.        , 1.5251478 ],
       [0.        , 1.52520239],
       [0.        , 1.52527094],
       [0.        , 1.52528191],
       [0.        , 1.52532566],
       [0.        , 1.5254091 ],
       [0.        , 1.52578413],
       [0.        , 1.52582669],
       [0.        , 1.52586484],
       [0.        , 1.52611649],
       [0.        , 1.52621925],
       [0.        , 1.52629066],
       [0.        , 1.52630925],
       [0.        , 1.52636671],
       [0.        , 1.52649999],
       [0.        , 1.52678657],
       [0.        , 1.52682805],
       [0.        , 1.52688026],
       [0.        , 1.52689588],
       [0.        , 1.52719712],
       [0.        , 1.52755499],
       [0.        , 1.52755547],
       [0.        , 1.52763164],
       [0.        , 1.52774608],
       [0.        , 1.52818084],
       [0.        , 1.52878976],
       [0.        , 1.52890563],
       [0.        , 1.52892292],
       [0.        , 1.52995789],
       [0.        , 1.53053319],
       [0.        , 1.53066432],
       [0.        , 1.53083575],
       [0.        , 1.53257394],
       [0.        , 1.54209125],
       [0.        , 1.54888105],
       [0.        , 1.61349595],
       [0.        , 1.63266718]]), array([[8.45028687, 8.50215244],
       [7.78025293, 8.30819893],
       [6.96881676, 7.53109884],
       [6.96864462, 7.15892124],
       [6.61087942, 6.90784025],
       [6.5643158 , 7.18389511],
       [6.4688797 , 6.99053192],
       [6.43824387, 7.06246996],
       [6.20071936, 7.80625296],
       [6.10865545, 7.15972567],
       [6.07230854, 7.00554419],
       [6.01323652, 6.01721764],
       [5.88949585, 6.40860271],
       [5.65944338, 5.95572424],
       [5.59838486, 6.01903486],
       [5.57973194, 8.51368141],
       [5.47689056, 5.60289478],
       [5.296947  , 8.32851982],
       [5.29333115, 8.46771526],
       [5.10625982, 5.13492298],
       [5.06403399, 6.80633926],
       [4.94068623, 7.17032433],
       [4.57971859, 4.86427164],
       [4.47349262, 7.83586502],
       [4.37517452, 4.45248985],
       [4.36016893, 7.96628046],
       [4.01259136, 4.26846933],
       [4.00701714, 4.08828354],
       [3.928581  , 4.68847179],
       [3.92301178, 4.29688501],
       [3.90054178, 3.91761422],
       [3.69974256, 8.5167675 ]]), array([[ 9.51651955,  9.61275482],
       [ 9.42027473, 10.07920551],
       [ 9.11051369, 10.73076916],
       [ 8.38557625,  8.4034586 ],
       [ 8.33742237,  8.46042156],
       [ 7.64383221,  7.65126371]])]2024-03-06 17:53:51.844634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KEI ph vector generated, counter: 121
2024-03-06 17:53:55.749743: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:53:55.792933: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:53:56.831626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.1733587), (0., 1.2983888), (0., 1.3248479), (0., 1.3249744),
       (0., 1.3250514), (0., 1.3258477), (0., 1.3262028), (0., 1.3263808),
       (0., 1.3264135), (0., 1.3273021), (0., 1.3281982), (0., 1.3284762),
       (0., 1.3285291), (0., 1.3286985), (0., 1.3288428), (0., 1.3291786),
       (0., 1.3292885), (0., 1.3298398), (0., 1.3306099), (0., 1.3306876),
       (0., 1.3307071), (0., 1.330954 ), (0., 1.3310032), (0., 1.3310199),
       (0., 1.3311946), (0., 1.3315485), (0., 1.3316679), (0., 1.3316964),
       (0., 1.3317379), (0., 1.3318218), (0., 1.3319255), (0., 1.332034 ),
       (0., 1.3321261), (0., 1.3321271), (0., 1.3321735), (0., 1.3323507),
       (0., 1.3323637), (0., 1.3323777), (0., 1.3323914), (0., 1.3325922),
       (0., 1.3326391), (0., 1.3326825), (0., 1.3327045), (0., 1.3327702),
       (0., 1.3328441), (0., 1.3333361), (0., 1.3333588), (0., 1.3334159),
       (0., 1.3335452), (0., 1.3338008), (0., 1.334184 ), (0., 1.334284 ),
       (0., 1.3342967), (0., 1.3346127), (0., 1.334683 ), (0., 1.3347019),
       (0., 1.3347651), (0., 1.3349638), (0., 1.3350018), (0., 1.3350606),
       (0., 1.3350749), (0., 1.3351051), (0., 1.3351995), (0., 1.3352742),
       (0., 1.3356144), (0., 1.3356482), (0., 1.3358339), (0., 1.3358699),
       (0., 1.335979 ), (0., 1.3360164), (0., 1.3360274), (0., 1.3361565),
       (0., 1.3361592), (0., 1.336351 ), (0., 1.3364371), (0., 1.3364443),
       (0., 1.336496 ), (0., 1.3365673), (0., 1.3366032), (0., 1.336679 ),
       (0., 1.3366954), (0., 1.336792 ), (0., 1.3368343), (0., 1.3369715),
       (0., 1.3371187), (0., 1.3371574), (0., 1.3371735), (0., 1.337354 ),
       (0., 1.3373681), (0., 1.3373752), (0., 1.3374966), (0., 1.3375086),
       (0., 1.3375264), (0., 1.337746 ), (0., 1.3378226), (0., 1.3378298),
       (0., 1.3378663), (0., 1.3379017), (0., 1.3379138), (0., 1.3379463),
       (0., 1.3380263), (0., 1.3380858), (0., 1.3381059), (0., 1.3381722),
       (0., 1.3383223), (0., 1.3383893), (0., 1.3387855), (0., 1.3388511),
       (0., 1.3391225), (0., 1.3394818), (0., 1.3396742), (0., 1.3400242),
       (0., 1.3401057), (0., 1.3401709), (0., 1.340566 ), (0., 1.3406404),
       (0., 1.3409944), (0., 1.3412567), (0., 1.3415445), (0., 1.3424176),
       (0., 1.3424736), (0., 1.3429825), (0., 1.3437644), (0., 1.3460023),
       (0., 1.3503422), (0., 1.4066468), (0., 1.40866  ), (0., 1.4421663),
       (0., 1.4497622), (0., 1.4513713), (0., 1.451787 ), (0., 1.4519209),
       (0., 1.4529109), (0., 1.4529883), (0., 1.4532081), (0., 1.4532927),
       (0., 1.4538089), (0., 1.4538181), (0., 1.4540895), (0., 1.4544121),
       (0., 1.4545137), (0., 1.4546107), (0., 1.454977 ), (0., 1.455231 ),
       (0., 1.455241 ), (0., 1.4554744), (0., 1.4557431), (0., 1.4558586),
       (0., 1.4558859), (0., 1.4561257), (0., 1.4561567), (0., 1.4562671),
       (0., 1.4564676), (0., 1.4567806), (0., 1.456845 ), (0., 1.4570631),
       (0., 1.4570765), (0., 1.4571933), (0., 1.4573996), (0., 1.4576374),
       (0., 1.4578278), (0., 1.4579923), (0., 1.4582927), (0., 1.4583011),
       (0., 1.4583681), (0., 1.4586135), (0., 1.4586922), (0., 1.4587249),
       (0., 1.4587418), (0., 1.4588822), (0., 1.4589772), (0., 1.4592084),
       (0., 1.45924  ), (0., 1.4592826), (0., 1.4593785), (0., 1.4594691),
       (0., 1.4595255), (0., 1.4595515), (0., 1.4596549), (0., 1.4596764),
       (0., 1.4600075), (0., 1.4600158), (0., 1.4600286), (0., 1.4600693),
       (0., 1.4600753), (0., 1.4601002), (0., 1.4601821), (0., 1.4601825),
       (0., 1.4602388), (0., 1.460409 ), (0., 1.4604197), (0., 1.4606483),
       (0., 1.4608777), (0., 1.4609071), (0., 1.4610431), (0., 1.4610628),
       (0., 1.4613166), (0., 1.4614176), (0., 1.4614189), (0., 1.4615182),
       (0., 1.4616925), (0., 1.4617479), (0., 1.4618692), (0., 1.46202  ),
       (0., 1.4620823), (0., 1.4621748), (0., 1.4621869), (0., 1.4621912),
       (0., 1.4622304), (0., 1.4622347), (0., 1.4623011), (0., 1.4624395),
       (0., 1.4624971), (0., 1.462527 ), (0., 1.4625369), (0., 1.4626038),
       (0., 1.4626077), (0., 1.4626086), (0., 1.4626402), (0., 1.4626831),
       (0., 1.46271  ), (0., 1.4627321), (0., 1.4627571), (0., 1.4627596),
       (0., 1.4628787), (0., 1.4632375), (0., 1.4634484), (0., 1.4636441),
       (0., 1.4636779), (0., 1.4637613), (0., 1.4639082), (0., 1.4642682),
       (0., 1.4643512), (0., 1.464525 ), (0., 1.4647421), (0., 1.4648786),
       (0., 1.4655476), (0., 1.4656149), (0., 1.465696 ), (0., 1.466425 ),
       (0., 1.4677119), (0., 1.4678762), (0., 1.4681653), (0., 1.4683847),
       (0., 1.4704639), (0., 1.4712136), (0., 1.471532 ), (0., 1.4722602),
       (0., 1.4734912), (0., 1.4737461), (0., 1.475889 ), (0., 1.4876317),
       (0., 1.4953821), (0., 1.5123132), (0., 1.5127844), (0., 1.5136696),
       (0., 1.5155709), (0., 1.5157104), (0., 1.5165718), (0., 1.5166613),
       (0., 1.5169033), (0., 1.5174646), (0., 1.517624 ), (0., 1.5180691),
       (0., 1.5181118), (0., 1.5181999), (0., 1.5188236), (0., 1.519153 ),
       (0., 1.5193511), (0., 1.5195723), (0., 1.5196939), (0., 1.5198072),
       (0., 1.519896 ), (0., 1.5201604), (0., 1.5202018), (0., 1.520308 ),
       (0., 1.5203362), (0., 1.5204691), (0., 1.5205604), (0., 1.5206481),
       (0., 1.5210496), (0., 1.5212172), (0., 1.5213002), (0., 1.5213479),
       (0., 1.5213952), (0., 1.5215417), (0., 1.52163  ), (0., 1.5217962),
       (0., 1.521852 ), (0., 1.5218631), (0., 1.5218939), (0., 1.5219581),
       (0., 1.5221144), (0., 1.5221449), (0., 1.5221716), (0., 1.5225852),
       (0., 1.5225984), (0., 1.5226104), (0., 1.5226591), (0., 1.5227093),
       (0., 1.522784 ), (0., 1.5228201), (0., 1.52283  ), (0., 1.5230103),
       (0., 1.5230496), (0., 1.5230567), (0., 1.5230681), (0., 1.5230724),
       (0., 1.52312  ), (0., 1.5232165), (0., 1.5232842), (0., 1.5234504),
       (0., 1.5234864), (0., 1.5235924), (0., 1.5238993), (0., 1.5239515),
       (0., 1.5241226), (0., 1.5242704), (0., 1.5242822), (0., 1.524344 ),
       (0., 1.5243998), (0., 1.524445 ), (0., 1.5245521), (0., 1.5245769),
       (0., 1.5248004), (0., 1.5248979), (0., 1.525008 ), (0., 1.5250806),
       (0., 1.5252163), (0., 1.5253543), (0., 1.5253922), (0., 1.5255874),
       (0., 1.5256118), (0., 1.525701 ), (0., 1.5258355), (0., 1.5259215),
       (0., 1.5260736), (0., 1.5262164), (0., 1.5262791), (0., 1.5263478),
       (0., 1.5263994), (0., 1.5264829), (0., 1.52653  ), (0., 1.5266874),
       (0., 1.526846 ), (0., 1.5275519), (0., 1.5275577), (0., 1.5276484),
       (0., 1.5277921), (0., 1.5278758), (0., 1.5278819), (0., 1.5281316),
       (0., 1.5283002), (0., 1.5284088), (0., 1.528509 ), (0., 1.5285226),
       (0., 1.5286779), (0., 1.5286908), (0., 1.5289032), (0., 1.528924 ),
       (0., 1.5290116), (0., 1.5291468), (0., 1.5292137), (0., 1.5293264),
       (0., 1.5295957), (0., 1.529671 ), (0., 1.5300764), (0., 1.5302835),
       (0., 1.5304985), (0., 1.53065  ), (0., 1.530661 ), (0., 1.5310396),
       (0., 1.5326353), (0., 1.5331717), (0., 1.5334309), (0., 1.5338984),
       (0., 1.5344361), (0., 1.5410279), (0., 1.5417162), (0., 1.625975 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.453634 , 8.524659 ), (7.7114606, 8.217686 ),
       (7.1206045, 7.647975 ), (6.9681706, 7.017429 ),
       (6.9285893, 7.0931673), (6.600268 , 6.937559 ),
       (6.5562477, 7.1680336), (6.499357 , 7.145122 ),
       (6.4870524, 6.911668 ), (6.1946487, 7.8108673),
       (6.1382885, 7.196533 ), (6.052416 , 7.008308 ),
       (5.9016204, 6.435901 ), (5.759296 , 5.8502617),
       (5.652381 , 8.677394 ), (5.492175 , 5.5986986),
       (5.333202 , 8.252821 ), (5.197166 , 8.447136 ),
       (5.053234 , 6.8927217), (4.9650617, 7.2419024),
       (4.6538053, 4.842872 ), (4.49929  , 7.9176974),
       (4.4495506, 7.867078 ), (4.353369 , 4.3983297),
       (3.9865403, 4.2435374), (3.9394557, 3.9598057),
       (3.930811 , 4.0660267), (3.892877 , 4.6164618),
       (3.8144374, 4.259113 ), (3.6201181, 8.471172 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.566446,  9.628763 ), (9.46651 , 10.064187 ),
       (9.081903, 10.792788 ), (8.396577,  8.417847 ),
       (8.309642,  8.408697 ), (4.642838,  4.6483693)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.173358678817749), (0.0, 1.2983888387680054), (0.0, 1.324847936630249), (0.0, 1.3249744176864624), (0.0, 1.3250514268875122), (0.0, 1.3258477449417114), (0.0, 1.3262027502059937), (0.0, 1.3263808488845825), (0.0, 1.3264135122299194), (0.0, 1.327302098274231), (0.0, 1.3281981945037842), (0.0, 1.3284761905670166), (0.0, 1.3285291194915771), (0.0, 1.3286985158920288), (0.0, 1.3288427591323853), (0.0, 1.3291785717010498), (0.0, 1.3292884826660156), (0.0, 1.329839825630188), (0.0, 1.330609917640686), (0.0, 1.3306876420974731), (0.0, 1.33070707321167), (0.0, 1.3309539556503296), (0.0, 1.331003189086914), (0.0, 1.3310198783874512), (0.0, 1.3311946392059326), (0.0, 1.3315484523773193), (0.0, 1.3316679000854492), (0.0, 1.3316963911056519), (0.0, 1.3317378759384155), (0.0, 1.3318217992782593), (0.0, 1.3319255113601685), (0.0, 1.3320339918136597), (0.0, 1.3321261405944824), (0.0, 1.3321270942687988), (0.0, 1.332173466682434), (0.0, 1.332350730895996), (0.0, 1.3323637247085571), (0.0, 1.3323776721954346), (0.0, 1.332391381263733), (0.0, 1.332592248916626), (0.0, 1.3326390981674194), (0.0, 1.332682490348816), (0.0, 1.3327045440673828), (0.0, 1.3327702283859253), (0.0, 1.3328441381454468), (0.0, 1.3333361148834229), (0.0, 1.3333587646484375), (0.0, 1.3334158658981323), (0.0, 1.333545207977295), (0.0, 1.3338007926940918), (0.0, 1.3341840505599976), (0.0, 1.3342839479446411), (0.0, 1.334296703338623), (0.0, 1.3346127271652222), (0.0, 1.3346829414367676), (0.0, 1.3347018957138062), (0.0, 1.334765076637268), (0.0, 1.3349637985229492), (0.0, 1.335001826286316), (0.0, 1.3350605964660645), (0.0, 1.3350749015808105), (0.0, 1.335105061531067), (0.0, 1.3351994752883911), (0.0, 1.3352742195129395), (0.0, 1.3356144428253174), (0.0, 1.3356481790542603), (0.0, 1.3358339071273804), (0.0, 1.3358699083328247), (0.0, 1.3359789848327637), (0.0, 1.3360164165496826), (0.0, 1.3360273838043213), (0.0, 1.3361564874649048), (0.0, 1.3361592292785645), (0.0, 1.3363510370254517), (0.0, 1.3364371061325073), (0.0, 1.3364442586898804), (0.0, 1.3364959955215454), (0.0, 1.3365672826766968), (0.0, 1.3366031646728516), (0.0, 1.3366789817810059), (0.0, 1.3366954326629639), (0.0, 1.3367919921875), (0.0, 1.3368343114852905), (0.0, 1.3369715213775635), (0.0, 1.3371187448501587), (0.0, 1.3371573686599731), (0.0, 1.3371734619140625), (0.0, 1.3373539447784424), (0.0, 1.337368130683899), (0.0, 1.3373751640319824), (0.0, 1.3374966382980347), (0.0, 1.3375085592269897), (0.0, 1.3375264406204224), (0.0, 1.337746024131775), (0.0, 1.3378225564956665), (0.0, 1.337829828262329), (0.0, 1.3378663063049316), (0.0, 1.3379017114639282), (0.0, 1.3379137516021729), (0.0, 1.3379462957382202), (0.0, 1.3380262851715088), (0.0, 1.3380857706069946), (0.0, 1.3381059169769287), (0.0, 1.338172197341919), (0.0, 1.3383222818374634), (0.0, 1.338389277458191), (0.0, 1.3387855291366577), (0.0, 1.3388510942459106), (0.0, 1.3391225337982178), (0.0, 1.3394818305969238), (0.0, 1.3396742343902588), (0.0, 1.3400242328643799), (0.0, 1.340105652809143), (0.0, 1.3401708602905273), (0.0, 1.3405660390853882), (0.0, 1.3406404256820679), (0.0, 1.3409943580627441), (0.0, 1.3412567377090454), (0.0, 1.341544508934021), (0.0, 1.342417597770691), (0.0, 1.3424736261367798), (0.0, 1.342982530593872), (0.0, 1.3437644243240356), (0.0, 1.3460023403167725), (0.0, 1.3503421545028687), (0.0, 1.4066468477249146), (0.0, 1.4086600542068481), (0.0, 1.4421663284301758), (0.0, 1.449762225151062), (0.0, 1.4513713121414185), (0.0, 1.451786994934082), (0.0, 1.4519208669662476), (0.0, 1.4529109001159668), (0.0, 1.4529882669448853), (0.0, 1.453208088874817), (0.0, 1.453292727470398), (0.0, 1.4538089036941528), (0.0, 1.4538180828094482), (0.0, 1.4540895223617554), (0.0, 1.4544121026992798), (0.0, 1.454513669013977), (0.0, 1.4546107053756714), (0.0, 1.454977035522461), (0.0, 1.455230951309204), (0.0, 1.4552409648895264), (0.0, 1.4554743766784668), (0.0, 1.4557430744171143), (0.0, 1.455858588218689), (0.0, 1.455885887145996), (0.0, 1.4561257362365723), (0.0, 1.4561567306518555), (0.0, 1.4562671184539795), (0.0, 1.456467628479004), (0.0, 1.4567805528640747), (0.0, 1.4568450450897217), (0.0, 1.45706307888031), (0.0, 1.4570765495300293), (0.0, 1.4571932554244995), (0.0, 1.457399606704712), (0.0, 1.4576374292373657), (0.0, 1.4578278064727783), (0.0, 1.4579923152923584), (0.0, 1.4582927227020264), (0.0, 1.458301067352295), (0.0, 1.4583680629730225), (0.0, 1.4586135149002075), (0.0, 1.458692193031311), (0.0, 1.458724856376648), (0.0, 1.4587417840957642), (0.0, 1.458882212638855), (0.0, 1.458977222442627), (0.0, 1.459208369255066), (0.0, 1.4592399597167969), (0.0, 1.459282636642456), (0.0, 1.4593784809112549), (0.0, 1.4594690799713135), (0.0, 1.459525465965271), (0.0, 1.459551453590393), (0.0, 1.4596549272537231), (0.0, 1.4596763849258423), (0.0, 1.4600075483322144), (0.0, 1.4600157737731934), (0.0, 1.4600286483764648), (0.0, 1.4600692987442017), (0.0, 1.4600752592086792), (0.0, 1.4601001739501953), (0.0, 1.4601820707321167), (0.0, 1.460182547569275), (0.0, 1.4602388143539429), (0.0, 1.4604090452194214), (0.0, 1.4604196548461914), (0.0, 1.4606482982635498), (0.0, 1.4608776569366455), (0.0, 1.4609071016311646), (0.0, 1.461043119430542), (0.0, 1.4610627889633179), (0.0, 1.4613165855407715), (0.0, 1.461417555809021), (0.0, 1.461418867111206), (0.0, 1.4615181684494019), (0.0, 1.461692452430725), (0.0, 1.4617478847503662), (0.0, 1.461869239807129), (0.0, 1.4620200395584106), (0.0, 1.4620822668075562), (0.0, 1.4621747732162476), (0.0, 1.4621869325637817), (0.0, 1.4621912240982056), (0.0, 1.4622304439544678), (0.0, 1.4622347354888916), (0.0, 1.4623011350631714), (0.0, 1.4624395370483398), (0.0, 1.4624971151351929), (0.0, 1.4625270366668701), (0.0, 1.4625369310379028), (0.0, 1.4626038074493408), (0.0, 1.462607741355896), (0.0, 1.4626085758209229), (0.0, 1.4626401662826538), (0.0, 1.462683081626892), (0.0, 1.4627100229263306), (0.0, 1.4627320766448975), (0.0, 1.4627571105957031), (0.0, 1.4627596139907837), (0.0, 1.462878704071045), (0.0, 1.4632375240325928), (0.0, 1.463448405265808), (0.0, 1.4636441469192505), (0.0, 1.4636778831481934), (0.0, 1.463761329650879), (0.0, 1.4639081954956055), (0.0, 1.4642682075500488), (0.0, 1.4643511772155762), (0.0, 1.4645249843597412), (0.0, 1.4647420644760132), (0.0, 1.4648785591125488), (0.0, 1.4655475616455078), (0.0, 1.465614914894104), (0.0, 1.4656959772109985), (0.0, 1.4664249420166016), (0.0, 1.4677119255065918), (0.0, 1.4678761959075928), (0.0, 1.4681652784347534), (0.0, 1.4683847427368164), (0.0, 1.4704638719558716), (0.0, 1.4712135791778564), (0.0, 1.4715319871902466), (0.0, 1.4722602367401123), (0.0, 1.4734911918640137), (0.0, 1.4737460613250732), (0.0, 1.475888967514038), (0.0, 1.4876316785812378), (0.0, 1.4953820705413818), (0.0, 1.5123132467269897), (0.0, 1.5127843618392944), (0.0, 1.5136696100234985), (0.0, 1.515570878982544), (0.0, 1.5157103538513184), (0.0, 1.5165717601776123), (0.0, 1.516661286354065), (0.0, 1.516903281211853), (0.0, 1.5174646377563477), (0.0, 1.517624020576477), (0.0, 1.5180691480636597), (0.0, 1.5181118249893188), (0.0, 1.5181999206542969), (0.0, 1.5188236236572266), (0.0, 1.5191529989242554), (0.0, 1.5193511247634888), (0.0, 1.5195722579956055), (0.0, 1.5196938514709473), (0.0, 1.51980721950531), (0.0, 1.5198960304260254), (0.0, 1.520160436630249), (0.0, 1.5202018022537231), (0.0, 1.520308017730713), (0.0, 1.5203361511230469), (0.0, 1.520469069480896), (0.0, 1.520560383796692), (0.0, 1.5206481218338013), (0.0, 1.5210496187210083), (0.0, 1.5212172269821167), (0.0, 1.521300196647644), (0.0, 1.5213478803634644), (0.0, 1.521395206451416), (0.0, 1.521541714668274), (0.0, 1.521630048751831), (0.0, 1.5217962265014648), (0.0, 1.5218520164489746), (0.0, 1.5218631029129028), (0.0, 1.521893858909607), (0.0, 1.5219581127166748), (0.0, 1.5221143960952759), (0.0, 1.5221449136734009), (0.0, 1.5221716165542603), (0.0, 1.522585153579712), (0.0, 1.522598385810852), (0.0, 1.5226104259490967), (0.0, 1.5226590633392334), (0.0, 1.5227092504501343), (0.0, 1.5227839946746826), (0.0, 1.5228201150894165), (0.0, 1.5228300094604492), (0.0, 1.52301025390625), (0.0, 1.5230495929718018), (0.0, 1.5230567455291748), (0.0, 1.5230680704116821), (0.0, 1.523072361946106), (0.0, 1.5231200456619263), (0.0, 1.5232164859771729), (0.0, 1.5232841968536377), (0.0, 1.5234503746032715), (0.0, 1.5234863758087158), (0.0, 1.5235923528671265), (0.0, 1.5238993167877197), (0.0, 1.523951530456543), (0.0, 1.5241225957870483), (0.0, 1.5242704153060913), (0.0, 1.5242822170257568), (0.0, 1.5243439674377441), (0.0, 1.524399757385254), (0.0, 1.5244450569152832), (0.0, 1.5245521068572998), (0.0, 1.5245769023895264), (0.0, 1.524800419807434), (0.0, 1.5248979330062866), (0.0, 1.525007963180542), (0.0, 1.5250805616378784), (0.0, 1.5252163410186768), (0.0, 1.525354266166687), (0.0, 1.5253921747207642), (0.0, 1.5255874395370483), (0.0, 1.5256117582321167), (0.0, 1.5257010459899902), (0.0, 1.5258355140686035), (0.0, 1.5259214639663696), (0.0, 1.5260735750198364), (0.0, 1.5262163877487183), (0.0, 1.526279091835022), (0.0, 1.5263477563858032), (0.0, 1.5263993740081787), (0.0, 1.5264829397201538), (0.0, 1.5265300273895264), (0.0, 1.5266873836517334), (0.0, 1.5268460512161255), (0.0, 1.5275518894195557), (0.0, 1.5275577306747437), (0.0, 1.5276484489440918), (0.0, 1.5277920961380005), (0.0, 1.5278757810592651), (0.0, 1.5278818607330322), (0.0, 1.5281316041946411), (0.0, 1.528300166130066), (0.0, 1.5284087657928467), (0.0, 1.5285090208053589), (0.0, 1.5285226106643677), (0.0, 1.5286779403686523), (0.0, 1.5286908149719238), (0.0, 1.5289032459259033), (0.0, 1.5289239883422852), (0.0, 1.529011607170105), (0.0, 1.5291467905044556), (0.0, 1.5292136669158936), (0.0, 1.5293264389038086), (0.0, 1.5295957326889038), (0.0, 1.5296709537506104), (0.0, 1.5300763845443726), (0.0, 1.5302834510803223), (0.0, 1.5304985046386719), (0.0, 1.530650019645691), (0.0, 1.5306609869003296), (0.0, 1.5310395956039429), (0.0, 1.5326353311538696), (0.0, 1.5331716537475586), (0.0, 1.5334309339523315), (0.0, 1.5338983535766602), (0.0, 1.5344361066818237), (0.0, 1.5410279035568237), (0.0, 1.54171621799469), (0.0, 1.6259750127792358)], [(8.453634262084961, 8.524659156799316), (7.711460590362549, 8.21768569946289), (7.120604515075684, 7.647974967956543), (6.968170642852783, 7.017428874969482), (6.928589344024658, 7.093167304992676), (6.6002678871154785, 6.937559127807617), (6.556247711181641, 7.168033599853516), (6.499357223510742, 7.145122051239014), (6.4870524406433105, 6.911667823791504), (6.194648742675781, 7.8108673095703125), (6.138288497924805, 7.196533203125), (6.05241584777832, 7.008307933807373), (5.901620388031006, 6.435901165008545), (5.75929594039917, 5.850261688232422), (5.65238094329834, 8.677393913269043), (5.492175102233887, 5.598698616027832), (5.333201885223389, 8.25282096862793), (5.1971659660339355, 8.447135925292969), (5.053234100341797, 6.892721652984619), (4.965061664581299, 7.2419023513793945), (4.653805255889893, 4.842872142791748), (4.4992899894714355, 7.917697429656982), (4.449550628662109, 7.867077827453613), (4.353369235992432, 4.398329734802246), (3.9865403175354004, 4.243537425994873), (3.93945574760437, 3.959805727005005), (3.9308109283447266, 4.06602668762207), (3.8928771018981934, 4.616461753845215), (3.8144373893737793, 4.25911283493042), (3.6201181411743164, 8.471172332763672)], [(9.566446304321289, 9.628763198852539), (9.466509819030762, 10.064187049865723), (9.081903457641602, 10.792787551879883), (8.396576881408691, 8.4178466796875), (8.30964183807373, 8.408697128295898), (4.642838001251221, 4.648369312286377)]]
[array([[0.        , 1.17335868],
       [0.        , 1.29838884],
       [0.        , 1.32484794],
       [0.        , 1.32497442],
       [0.        , 1.32505143],
       [0.        , 1.32584774],
       [0.        , 1.32620275],
       [0.        , 1.32638085],
       [0.        , 1.32641351],
       [0.        , 1.3273021 ],
       [0.        , 1.32819819],
       [0.        , 1.32847619],
       [0.        , 1.32852912],
       [0.        , 1.32869852],
       [0.        , 1.32884276],
       [0.        , 1.32917857],
       [0.        , 1.32928848],
       [0.        , 1.32983983],
       [0.        , 1.33060992],
       [0.        , 1.33068764],
       [0.        , 1.33070707],
       [0.        , 1.33095396],
       [0.        , 1.33100319],
       [0.        , 1.33101988],
       [0.        , 1.33119464],
       [0.        , 1.33154845],
       [0.        , 1.3316679 ],
       [0.        , 1.33169639],
       [0.        , 1.33173788],
       [0.        , 1.3318218 ],
       [0.        , 1.33192551],
       [0.        , 1.33203399],
       [0.        , 1.33212614],
       [0.        , 1.33212709],
       [0.        , 1.33217347],
       [0.        , 1.33235073],
       [0.        , 1.33236372],
       [0.        , 1.33237767],
       [0.        , 1.33239138],
       [0.        , 1.33259225],
       [0.        , 1.3326391 ],
       [0.        , 1.33268249],
       [0.        , 1.33270454],
       [0.        , 1.33277023],
       [0.        , 1.33284414],
       [0.        , 1.33333611],
       [0.        , 1.33335876],
       [0.        , 1.33341587],
       [0.        , 1.33354521],
       [0.        , 1.33380079],
       [0.        , 1.33418405],
       [0.        , 1.33428395],
       [0.        , 1.3342967 ],
       [0.        , 1.33461273],
       [0.        , 1.33468294],
       [0.        , 1.3347019 ],
       [0.        , 1.33476508],
       [0.        , 1.3349638 ],
       [0.        , 1.33500183],
       [0.        , 1.3350606 ],
       [0.        , 1.3350749 ],
       [0.        , 1.33510506],
       [0.        , 1.33519948],
       [0.        , 1.33527422],
       [0.        , 1.33561444],
       [0.        , 1.33564818],
       [0.        , 1.33583391],
       [0.        , 1.33586991],
       [0.        , 1.33597898],
       [0.        , 1.33601642],
       [0.        , 1.33602738],
       [0.        , 1.33615649],
       [0.        , 1.33615923],
       [0.        , 1.33635104],
       [0.        , 1.33643711],
       [0.        , 1.33644426],
       [0.        , 1.336496  ],
       [0.        , 1.33656728],
       [0.        , 1.33660316],
       [0.        , 1.33667898],
       [0.        , 1.33669543],
       [0.        , 1.33679199],
       [0.        , 1.33683431],
       [0.        , 1.33697152],
       [0.        , 1.33711874],
       [0.        , 1.33715737],
       [0.        , 1.33717346],
       [0.        , 1.33735394],
       [0.        , 1.33736813],
       [0.        , 1.33737516],
       [0.        , 1.33749664],
       [0.        , 1.33750856],
       [0.        , 1.33752644],
       [0.        , 1.33774602],
       [0.        , 1.33782256],
       [0.        , 1.33782983],
       [0.        , 1.33786631],
       [0.        , 1.33790171],
       [0.        , 1.33791375],
       [0.        , 1.3379463 ],
       [0.        , 1.33802629],
       [0.        , 1.33808577],
       [0.        , 1.33810592],
       [0.        , 1.3381722 ],
       [0.        , 1.33832228],
       [0.        , 1.33838928],
       [0.        , 1.33878553],
       [0.        , 1.33885109],
       [0.        , 1.33912253],
       [0.        , 1.33948183],
       [0.        , 1.33967423],
       [0.        , 1.34002423],
       [0.        , 1.34010565],
       [0.        , 1.34017086],
       [0.        , 1.34056604],
       [0.        , 1.34064043],
       [0.        , 1.34099436],
       [0.        , 1.34125674],
       [0.        , 1.34154451],
       [0.        , 1.3424176 ],
       [0.        , 1.34247363],
       [0.        , 1.34298253],
       [0.        , 1.34376442],
       [0.        , 1.34600234],
       [0.        , 1.35034215],
       [0.        , 1.40664685],
       [0.        , 1.40866005],
       [0.        , 1.44216633],
       [0.        , 1.44976223],
       [0.        , 1.45137131],
       [0.        , 1.45178699],
       [0.        , 1.45192087],
       [0.        , 1.4529109 ],
       [0.        , 1.45298827],
       [0.        , 1.45320809],
       [0.        , 1.45329273],
       [0.        , 1.4538089 ],
       [0.        , 1.45381808],
       [0.        , 1.45408952],
       [0.        , 1.4544121 ],
       [0.        , 1.45451367],
       [0.        , 1.45461071],
       [0.        , 1.45497704],
       [0.        , 1.45523095],
       [0.        , 1.45524096],
       [0.        , 1.45547438],
       [0.        , 1.45574307],
       [0.        , 1.45585859],
       [0.        , 1.45588589],
       [0.        , 1.45612574],
       [0.        , 1.45615673],
       [0.        , 1.45626712],
       [0.        , 1.45646763],
       [0.        , 1.45678055],
       [0.        , 1.45684505],
       [0.        , 1.45706308],
       [0.        , 1.45707655],
       [0.        , 1.45719326],
       [0.        , 1.45739961],
       [0.        , 1.45763743],
       [0.        , 1.45782781],
       [0.        , 1.45799232],
       [0.        , 1.45829272],
       [0.        , 1.45830107],
       [0.        , 1.45836806],
       [0.        , 1.45861351],
       [0.        , 1.45869219],
       [0.        , 1.45872486],
       [0.        , 1.45874178],
       [0.        , 1.45888221],
       [0.        , 1.45897722],
       [0.        , 1.45920837],
       [0.        , 1.45923996],
       [0.        , 1.45928264],
       [0.        , 1.45937848],
       [0.        , 1.45946908],
       [0.        , 1.45952547],
       [0.        , 1.45955145],
       [0.        , 1.45965493],
       [0.        , 1.45967638],
       [0.        , 1.46000755],
       [0.        , 1.46001577],
       [0.        , 1.46002865],
       [0.        , 1.4600693 ],
       [0.        , 1.46007526],
       [0.        , 1.46010017],
       [0.        , 1.46018207],
       [0.        , 1.46018255],
       [0.        , 1.46023881],
       [0.        , 1.46040905],
       [0.        , 1.46041965],
       [0.        , 1.4606483 ],
       [0.        , 1.46087766],
       [0.        , 1.4609071 ],
       [0.        , 1.46104312],
       [0.        , 1.46106279],
       [0.        , 1.46131659],
       [0.        , 1.46141756],
       [0.        , 1.46141887],
       [0.        , 1.46151817],
       [0.        , 1.46169245],
       [0.        , 1.46174788],
       [0.        , 1.46186924],
       [0.        , 1.46202004],
       [0.        , 1.46208227],
       [0.        , 1.46217477],
       [0.        , 1.46218693],
       [0.        , 1.46219122],
       [0.        , 1.46223044],
       [0.        , 1.46223474],
       [0.        , 1.46230114],
       [0.        , 1.46243954],
       [0.        , 1.46249712],
       [0.        , 1.46252704],
       [0.        , 1.46253693],
       [0.        , 1.46260381],
       [0.        , 1.46260774],
       [0.        , 1.46260858],
       [0.        , 1.46264017],
       [0.        , 1.46268308],
       [0.        , 1.46271002],
       [0.        , 1.46273208],
       [0.        , 1.46275711],
       [0.        , 1.46275961],
       [0.        , 1.4628787 ],
       [0.        , 1.46323752],
       [0.        , 1.46344841],
       [0.        , 1.46364415],
       [0.        , 1.46367788],
       [0.        , 1.46376133],
       [0.        , 1.4639082 ],
       [0.        , 1.46426821],
       [0.        , 1.46435118],
       [0.        , 1.46452498],
       [0.        , 1.46474206],
       [0.        , 1.46487856],
       [0.        , 1.46554756],
       [0.        , 1.46561491],
       [0.        , 1.46569598],
       [0.        , 1.46642494],
       [0.        , 1.46771193],
       [0.        , 1.4678762 ],
       [0.        , 1.46816528],
       [0.        , 1.46838474],
       [0.        , 1.47046387],
       [0.        , 1.47121358],
       [0.        , 1.47153199],
       [0.        , 1.47226024],
       [0.        , 1.47349119],
       [0.        , 1.47374606],
       [0.        , 1.47588897],
       [0.        , 1.48763168],
       [0.        , 1.49538207],
       [0.        , 1.51231325],
       [0.        , 1.51278436],
       [0.        , 1.51366961],
       [0.        , 1.51557088],
       [0.        , 1.51571035],
       [0.        , 1.51657176],
       [0.        , 1.51666129],
       [0.        , 1.51690328],
       [0.        , 1.51746464],
       [0.        , 1.51762402],
       [0.        , 1.51806915],
       [0.        , 1.51811182],
       [0.        , 1.51819992],
       [0.        , 1.51882362],
       [0.        , 1.519153  ],
       [0.        , 1.51935112],
       [0.        , 1.51957226],
       [0.        , 1.51969385],
       [0.        , 1.51980722],
       [0.        , 1.51989603],
       [0.        , 1.52016044],
       [0.        , 1.5202018 ],
       [0.        , 1.52030802],
       [0.        , 1.52033615],
       [0.        , 1.52046907],
       [0.        , 1.52056038],
       [0.        , 1.52064812],
       [0.        , 1.52104962],
       [0.        , 1.52121723],
       [0.        , 1.5213002 ],
       [0.        , 1.52134788],
       [0.        , 1.52139521],
       [0.        , 1.52154171],
       [0.        , 1.52163005],
       [0.        , 1.52179623],
       [0.        , 1.52185202],
       [0.        , 1.5218631 ],
       [0.        , 1.52189386],
       [0.        , 1.52195811],
       [0.        , 1.5221144 ],
       [0.        , 1.52214491],
       [0.        , 1.52217162],
       [0.        , 1.52258515],
       [0.        , 1.52259839],
       [0.        , 1.52261043],
       [0.        , 1.52265906],
       [0.        , 1.52270925],
       [0.        , 1.52278399],
       [0.        , 1.52282012],
       [0.        , 1.52283001],
       [0.        , 1.52301025],
       [0.        , 1.52304959],
       [0.        , 1.52305675],
       [0.        , 1.52306807],
       [0.        , 1.52307236],
       [0.        , 1.52312005],
       [0.        , 1.52321649],
       [0.        , 1.5232842 ],
       [0.        , 1.52345037],
       [0.        , 1.52348638],
       [0.        , 1.52359235],
       [0.        , 1.52389932],
       [0.        , 1.52395153],
       [0.        , 1.5241226 ],
       [0.        , 1.52427042],
       [0.        , 1.52428222],
       [0.        , 1.52434397],
       [0.        , 1.52439976],
       [0.        , 1.52444506],
       [0.        , 1.52455211],
       [0.        , 1.5245769 ],
       [0.        , 1.52480042],
       [0.        , 1.52489793],
       [0.        , 1.52500796],
       [0.        , 1.52508056],
       [0.        , 1.52521634],
       [0.        , 1.52535427],
       [0.        , 1.52539217],
       [0.        , 1.52558744],
       [0.        , 1.52561176],
       [0.        , 1.52570105],
       [0.        , 1.52583551],
       [0.        , 1.52592146],
       [0.        , 1.52607358],
       [0.        , 1.52621639],
       [0.        , 1.52627909],
       [0.        , 1.52634776],
       [0.        , 1.52639937],
       [0.        , 1.52648294],
       [0.        , 1.52653003],
       [0.        , 1.52668738],
       [0.        , 1.52684605],
       [0.        , 1.52755189],
       [0.        , 1.52755773],
       [0.        , 1.52764845],
       [0.        , 1.5277921 ],
       [0.        , 1.52787578],
       [0.        , 1.52788186],
       [0.        , 1.5281316 ],
       [0.        , 1.52830017],
       [0.        , 1.52840877],
       [0.        , 1.52850902],
       [0.        , 1.52852261],
       [0.        , 1.52867794],
       [0.        , 1.52869081],
       [0.        , 1.52890325],
       [0.        , 1.52892399],
       [0.        , 1.52901161],
       [0.        , 1.52914679],
       [0.        , 1.52921367],
       [0.        , 1.52932644],
       [0.        , 1.52959573],
       [0.        , 1.52967095],
       [0.        , 1.53007638],
       [0.        , 1.53028345],
       [0.        , 1.5304985 ],
       [0.        , 1.53065002],
       [0.        , 1.53066099],
       [0.        , 1.5310396 ],
       [0.        , 1.53263533],
       [0.        , 1.53317165],
       [0.        , 1.53343093],
       [0.        , 1.53389835],
       [0.        , 1.53443611],
       [0.        , 1.5410279 ],
       [0.        , 1.54171622],
       [0.        , 1.62597501]]), array([[8.45363426, 8.52465916],
       [7.71146059, 8.2176857 ],
       [7.12060452, 7.64797497],
       [6.96817064, 7.01742887],
       [6.92858934, 7.0931673 ],
       [6.60026789, 6.93755913],
       [6.55624771, 7.1680336 ],
       [6.49935722, 7.14512205],
       [6.48705244, 6.91166782],
       [6.19464874, 7.81086731],
       [6.1382885 , 7.1965332 ],
       [6.05241585, 7.00830793],
       [5.90162039, 6.43590117],
       [5.75929594, 5.85026169],
       [5.65238094, 8.67739391],
       [5.4921751 , 5.59869862],
       [5.33320189, 8.25282097],
       [5.19716597, 8.44713593],
       [5.0532341 , 6.89272165],
       [4.96506166, 7.24190235],
       [4.65380526, 4.84287214],
       [4.49928999, 7.91769743],
       [4.44955063, 7.86707783],
       [4.35336924, 4.39832973],
       [3.98654032, 4.24353743],
       [3.93945575, 3.95980573],
       [3.93081093, 4.06602669],
       [3.8928771 , 4.61646175],
       [3.81443739, 4.25911283],
       [3.62011814, 8.47117233]]), array([[ 9.5664463 ,  9.6287632 ],
       [ 9.46650982, 10.06418705],
       [ 9.08190346, 10.79278755],
       [ 8.39657688,  8.41784668],
       [ 8.30964184,  8.40869713],
       [ 4.642838  ,  4.64836931]])]2024-03-06 17:54:01.080882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KEJ ph vector generated, counter: 122
2024-03-06 17:54:05.082345: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:05.125310: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:05.989322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3131164), (0., 1.3201036), (0., 1.3228772), (0., 1.3237987),
       (0., 1.3252522), (0., 1.3265903), (0., 1.3272911), (0., 1.3276993),
       (0., 1.3279788), (0., 1.3286484), (0., 1.3286775), (0., 1.3289902),
       (0., 1.3290585), (0., 1.3291721), (0., 1.329484 ), (0., 1.3296375),
       (0., 1.3297298), (0., 1.3298829), (0., 1.3299289), (0., 1.3300884),
       (0., 1.3301519), (0., 1.3301771), (0., 1.33022  ), (0., 1.3303497),
       (0., 1.330451 ), (0., 1.3305609), (0., 1.3305856), (0., 1.3310666),
       (0., 1.3311526), (0., 1.3311689), (0., 1.3313224), (0., 1.3314779),
       (0., 1.3315097), (0., 1.3318051), (0., 1.3318485), (0., 1.3322583),
       (0., 1.3323737), (0., 1.3325373), (0., 1.3328031), (0., 1.3328502),
       (0., 1.3330907), (0., 1.333169 ), (0., 1.3331801), (0., 1.3334656),
       (0., 1.3335115), (0., 1.3336241), (0., 1.3337945), (0., 1.3339937),
       (0., 1.3340367), (0., 1.3342192), (0., 1.3343809), (0., 1.3343884),
       (0., 1.334428 ), (0., 1.3346223), (0., 1.3346248), (0., 1.3346293),
       (0., 1.3346344), (0., 1.3347429), (0., 1.3348835), (0., 1.3349035),
       (0., 1.3349558), (0., 1.3349655), (0., 1.3350983), (0., 1.3351943),
       (0., 1.3352343), (0., 1.3352703), (0., 1.3354795), (0., 1.335637 ),
       (0., 1.335681 ), (0., 1.3357801), (0., 1.3358049), (0., 1.3358121),
       (0., 1.3358669), (0., 1.3358976), (0., 1.3359642), (0., 1.3360008),
       (0., 1.3361001), (0., 1.3361027), (0., 1.336327 ), (0., 1.3363789),
       (0., 1.3363824), (0., 1.336431 ), (0., 1.3364508), (0., 1.3366746),
       (0., 1.3367066), (0., 1.336743 ), (0., 1.3367656), (0., 1.3368316),
       (0., 1.3368615), (0., 1.3369024), (0., 1.336919 ), (0., 1.3369492),
       (0., 1.337037 ), (0., 1.337048 ), (0., 1.3373665), (0., 1.3374345),
       (0., 1.3374995), (0., 1.3377564), (0., 1.3378223), (0., 1.3378725),
       (0., 1.3378825), (0., 1.3379579), (0., 1.3381711), (0., 1.3381786),
       (0., 1.3382313), (0., 1.3382835), (0., 1.3385493), (0., 1.3392185),
       (0., 1.3395431), (0., 1.3396311), (0., 1.3397286), (0., 1.3401108),
       (0., 1.3406183), (0., 1.3406717), (0., 1.3406969), (0., 1.340785 ),
       (0., 1.3418049), (0., 1.3418204), (0., 1.3421153), (0., 1.342825 ),
       (0., 1.3429198), (0., 1.3435531), (0., 1.3440157), (0., 1.3488388),
       (0., 1.3728153), (0., 1.378316 ), (0., 1.3863587), (0., 1.4413915),
       (0., 1.4504995), (0., 1.450858 ), (0., 1.45152  ), (0., 1.4518698),
       (0., 1.4521595), (0., 1.45263  ), (0., 1.4534547), (0., 1.4535053),
       (0., 1.4540375), (0., 1.4542086), (0., 1.4543161), (0., 1.4543785),
       (0., 1.4545876), (0., 1.4545943), (0., 1.4548594), (0., 1.4549162),
       (0., 1.4549907), (0., 1.455219 ), (0., 1.4553313), (0., 1.455355 ),
       (0., 1.455422 ), (0., 1.4555179), (0., 1.4556519), (0., 1.4557137),
       (0., 1.4557823), (0., 1.4559052), (0., 1.4559244), (0., 1.4562159),
       (0., 1.4562428), (0., 1.4562606), (0., 1.4563704), (0., 1.4563769),
       (0., 1.4563977), (0., 1.4565401), (0., 1.456588 ), (0., 1.456703 ),
       (0., 1.4567945), (0., 1.4568613), (0., 1.4571365), (0., 1.4571657),
       (0., 1.4572319), (0., 1.4574357), (0., 1.4577469), (0., 1.4580863),
       (0., 1.4581983), (0., 1.458519 ), (0., 1.4585532), (0., 1.4586612),
       (0., 1.4587331), (0., 1.4588459), (0., 1.4588803), (0., 1.4588996),
       (0., 1.4591683), (0., 1.4592733), (0., 1.4592787), (0., 1.4593512),
       (0., 1.4595342), (0., 1.4595937), (0., 1.4596289), (0., 1.4596488),
       (0., 1.4597967), (0., 1.4598023), (0., 1.4598745), (0., 1.4600661),
       (0., 1.4605912), (0., 1.4607686), (0., 1.460929 ), (0., 1.4609754),
       (0., 1.4609956), (0., 1.461061 ), (0., 1.4611092), (0., 1.4611926),
       (0., 1.4612081), (0., 1.4612633), (0., 1.4612875), (0., 1.4613044),
       (0., 1.4613477), (0., 1.461429 ), (0., 1.4615949), (0., 1.4617299),
       (0., 1.4617589), (0., 1.4617903), (0., 1.4618368), (0., 1.4618926),
       (0., 1.4619144), (0., 1.4621452), (0., 1.4622583), (0., 1.462388 ),
       (0., 1.4625735), (0., 1.4625779), (0., 1.4626232), (0., 1.4626459),
       (0., 1.4628577), (0., 1.4628682), (0., 1.463027 ), (0., 1.4630783),
       (0., 1.4633873), (0., 1.4634203), (0., 1.4634839), (0., 1.4635688),
       (0., 1.463659 ), (0., 1.4637529), (0., 1.4638159), (0., 1.4639304),
       (0., 1.4639645), (0., 1.463967 ), (0., 1.4639947), (0., 1.4640007),
       (0., 1.4641378), (0., 1.4642347), (0., 1.4646178), (0., 1.4647257),
       (0., 1.4648622), (0., 1.4649669), (0., 1.4664866), (0., 1.467755 ),
       (0., 1.4678806), (0., 1.4679115), (0., 1.4683076), (0., 1.469594 ),
       (0., 1.4702688), (0., 1.4723334), (0., 1.4771432), (0., 1.4848655),
       (0., 1.4860941), (0., 1.5012491), (0., 1.5071145), (0., 1.5082701),
       (0., 1.5127857), (0., 1.5132164), (0., 1.5133883), (0., 1.513545 ),
       (0., 1.5145272), (0., 1.5149686), (0., 1.5163702), (0., 1.5165503),
       (0., 1.5165517), (0., 1.5169094), (0., 1.5169494), (0., 1.5171264),
       (0., 1.5171983), (0., 1.517687 ), (0., 1.5177091), (0., 1.5177419),
       (0., 1.5178136), (0., 1.5183011), (0., 1.5183048), (0., 1.5183948),
       (0., 1.5185784), (0., 1.5187088), (0., 1.518865 ), (0., 1.5190117),
       (0., 1.5190618), (0., 1.5190716), (0., 1.5192131), (0., 1.519286 ),
       (0., 1.5193129), (0., 1.5196877), (0., 1.5198832), (0., 1.5199205),
       (0., 1.5199208), (0., 1.5199636), (0., 1.5199708), (0., 1.5200375),
       (0., 1.5200506), (0., 1.5201725), (0., 1.5202183), (0., 1.5203507),
       (0., 1.5204431), (0., 1.5204511), (0., 1.5205214), (0., 1.5205604),
       (0., 1.5208777), (0., 1.5209239), (0., 1.5209893), (0., 1.5211481),
       (0., 1.5211784), (0., 1.521288 ), (0., 1.5213231), (0., 1.5213704),
       (0., 1.521394 ), (0., 1.5214432), (0., 1.5216045), (0., 1.5217193),
       (0., 1.521892 ), (0., 1.521987 ), (0., 1.5220102), (0., 1.5220324),
       (0., 1.5220363), (0., 1.5223116), (0., 1.5224891), (0., 1.522529 ),
       (0., 1.5228385), (0., 1.5228647), (0., 1.5229056), (0., 1.5230196),
       (0., 1.5230968), (0., 1.5231594), (0., 1.5232351), (0., 1.5233021),
       (0., 1.5234082), (0., 1.5234603), (0., 1.5234859), (0., 1.5235804),
       (0., 1.5236319), (0., 1.5238731), (0., 1.5239748), (0., 1.523999 ),
       (0., 1.5240278), (0., 1.5240567), (0., 1.5243591), (0., 1.5246491),
       (0., 1.5247612), (0., 1.5247998), (0., 1.5248909), (0., 1.525037 ),
       (0., 1.5254736), (0., 1.5255147), (0., 1.5255258), (0., 1.5256536),
       (0., 1.5256541), (0., 1.5256717), (0., 1.5259317), (0., 1.5259693),
       (0., 1.5260215), (0., 1.5261593), (0., 1.5265143), (0., 1.5265242),
       (0., 1.5265317), (0., 1.5267112), (0., 1.5267673), (0., 1.5267762),
       (0., 1.5269319), (0., 1.5269753), (0., 1.527008 ), (0., 1.5270264),
       (0., 1.5271641), (0., 1.5272508), (0., 1.5272589), (0., 1.5274975),
       (0., 1.5275078), (0., 1.527887 ), (0., 1.5285726), (0., 1.5288026),
       (0., 1.5289363), (0., 1.5293899), (0., 1.5297079), (0., 1.5297222),
       (0., 1.5303484), (0., 1.5314583), (0., 1.5329928), (0., 1.5704817)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(7.7790747, 8.341208 ), (6.9906125, 7.5366445),
       (6.874795 , 7.133666 ), (6.75459  , 7.3732634),
       (6.5028467, 6.815096 ), (6.3951902, 6.9639106),
       (6.3691688, 7.0664506), (6.191809 , 7.793743 ),
       (6.0944047, 7.0424156), (6.0479455, 7.2284713),
       (5.9676633, 6.44417  ), (5.713599 , 5.9920444),
       (5.6732917, 8.57802  ), (5.6307096, 5.6849427),
       (5.40927  , 8.398654 ), (5.3436756, 8.608907 ),
       (5.0600176, 7.029936 ), (4.971425 , 7.3199167),
       (4.426995 , 4.502487 ), (4.395125 , 7.953719 ),
       (4.0359373, 7.899302 ), (4.0139637, 4.031118 ),
       (3.9831102, 4.2731147), (3.9215086, 4.33124  ),
       (3.9180543, 4.7497535), (3.6455057, 8.576509 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.490296 , 10.005388), (9.472032 ,  9.511488),
       (9.0964155, 10.772538), (8.722751 ,  8.75718 ),
       (8.416341 ,  8.53141 ), (7.605195 ,  7.712362)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.313116431236267), (0.0, 1.320103645324707), (0.0, 1.3228771686553955), (0.0, 1.323798656463623), (0.0, 1.3252521753311157), (0.0, 1.3265902996063232), (0.0, 1.3272911310195923), (0.0, 1.3276993036270142), (0.0, 1.3279788494110107), (0.0, 1.3286484479904175), (0.0, 1.3286775350570679), (0.0, 1.3289902210235596), (0.0, 1.3290585279464722), (0.0, 1.329172134399414), (0.0, 1.329483985900879), (0.0, 1.3296375274658203), (0.0, 1.3297297954559326), (0.0, 1.3298828601837158), (0.0, 1.3299288749694824), (0.0, 1.3300883769989014), (0.0, 1.330151915550232), (0.0, 1.3301770687103271), (0.0, 1.3302199840545654), (0.0, 1.3303496837615967), (0.0, 1.3304510116577148), (0.0, 1.3305609226226807), (0.0, 1.3305855989456177), (0.0, 1.331066608428955), (0.0, 1.3311525583267212), (0.0, 1.3311688899993896), (0.0, 1.331322431564331), (0.0, 1.3314778804779053), (0.0, 1.3315097093582153), (0.0, 1.3318051099777222), (0.0, 1.3318485021591187), (0.0, 1.3322583436965942), (0.0, 1.3323737382888794), (0.0, 1.332537293434143), (0.0, 1.3328031301498413), (0.0, 1.3328502178192139), (0.0, 1.3330906629562378), (0.0, 1.3331689834594727), (0.0, 1.3331800699234009), (0.0, 1.333465576171875), (0.0, 1.333511471748352), (0.0, 1.3336241245269775), (0.0, 1.3337944746017456), (0.0, 1.333993673324585), (0.0, 1.3340367078781128), (0.0, 1.334219217300415), (0.0, 1.334380865097046), (0.0, 1.3343883752822876), (0.0, 1.3344279527664185), (0.0, 1.3346222639083862), (0.0, 1.3346247673034668), (0.0, 1.3346292972564697), (0.0, 1.3346344232559204), (0.0, 1.3347429037094116), (0.0, 1.334883451461792), (0.0, 1.3349034786224365), (0.0, 1.3349558115005493), (0.0, 1.334965467453003), (0.0, 1.3350982666015625), (0.0, 1.3351943492889404), (0.0, 1.33523428440094), (0.0, 1.3352702856063843), (0.0, 1.335479497909546), (0.0, 1.3356369733810425), (0.0, 1.3356809616088867), (0.0, 1.335780143737793), (0.0, 1.3358049392700195), (0.0, 1.3358120918273926), (0.0, 1.335866928100586), (0.0, 1.3358975648880005), (0.0, 1.3359642028808594), (0.0, 1.3360008001327515), (0.0, 1.3361001014709473), (0.0, 1.3361027240753174), (0.0, 1.3363269567489624), (0.0, 1.3363789319992065), (0.0, 1.3363823890686035), (0.0, 1.3364310264587402), (0.0, 1.3364508152008057), (0.0, 1.3366745710372925), (0.0, 1.3367066383361816), (0.0, 1.3367429971694946), (0.0, 1.3367656469345093), (0.0, 1.3368315696716309), (0.0, 1.336861491203308), (0.0, 1.336902379989624), (0.0, 1.3369189500808716), (0.0, 1.3369492292404175), (0.0, 1.3370369672775269), (0.0, 1.337048053741455), (0.0, 1.3373664617538452), (0.0, 1.3374345302581787), (0.0, 1.3374994993209839), (0.0, 1.3377563953399658), (0.0, 1.3378223180770874), (0.0, 1.3378725051879883), (0.0, 1.3378825187683105), (0.0, 1.3379578590393066), (0.0, 1.338171124458313), (0.0, 1.3381786346435547), (0.0, 1.3382313251495361), (0.0, 1.3382835388183594), (0.0, 1.338549256324768), (0.0, 1.3392184972763062), (0.0, 1.339543104171753), (0.0, 1.3396310806274414), (0.0, 1.339728593826294), (0.0, 1.3401107788085938), (0.0, 1.3406182527542114), (0.0, 1.3406716585159302), (0.0, 1.340696930885315), (0.0, 1.340785026550293), (0.0, 1.3418048620224), (0.0, 1.3418203592300415), (0.0, 1.3421152830123901), (0.0, 1.3428250551223755), (0.0, 1.3429198265075684), (0.0, 1.343553066253662), (0.0, 1.3440157175064087), (0.0, 1.3488388061523438), (0.0, 1.3728152513504028), (0.0, 1.378316044807434), (0.0, 1.3863587379455566), (0.0, 1.4413914680480957), (0.0, 1.4504995346069336), (0.0, 1.4508579969406128), (0.0, 1.4515199661254883), (0.0, 1.4518698453903198), (0.0, 1.4521595239639282), (0.0, 1.4526300430297852), (0.0, 1.4534547328948975), (0.0, 1.453505277633667), (0.0, 1.4540375471115112), (0.0, 1.4542086124420166), (0.0, 1.4543161392211914), (0.0, 1.4543784856796265), (0.0, 1.4545875787734985), (0.0, 1.4545942544937134), (0.0, 1.4548593759536743), (0.0, 1.45491623878479), (0.0, 1.4549907445907593), (0.0, 1.455219030380249), (0.0, 1.4553313255310059), (0.0, 1.4553550481796265), (0.0, 1.455422043800354), (0.0, 1.4555178880691528), (0.0, 1.455651879310608), (0.0, 1.4557137489318848), (0.0, 1.4557822942733765), (0.0, 1.4559051990509033), (0.0, 1.455924391746521), (0.0, 1.4562158584594727), (0.0, 1.4562427997589111), (0.0, 1.4562605619430542), (0.0, 1.4563703536987305), (0.0, 1.4563769102096558), (0.0, 1.4563976526260376), (0.0, 1.4565401077270508), (0.0, 1.4565880298614502), (0.0, 1.4567029476165771), (0.0, 1.4567945003509521), (0.0, 1.4568612575531006), (0.0, 1.4571365118026733), (0.0, 1.4571657180786133), (0.0, 1.457231879234314), (0.0, 1.4574357271194458), (0.0, 1.4577468633651733), (0.0, 1.4580862522125244), (0.0, 1.4581983089447021), (0.0, 1.4585189819335938), (0.0, 1.4585531949996948), (0.0, 1.4586611986160278), (0.0, 1.458733081817627), (0.0, 1.458845853805542), (0.0, 1.4588803052902222), (0.0, 1.4588996171951294), (0.0, 1.4591683149337769), (0.0, 1.459273338317871), (0.0, 1.4592787027359009), (0.0, 1.4593511819839478), (0.0, 1.4595341682434082), (0.0, 1.459593653678894), (0.0, 1.459628939628601), (0.0, 1.459648847579956), (0.0, 1.459796667098999), (0.0, 1.459802269935608), (0.0, 1.4598745107650757), (0.0, 1.4600660800933838), (0.0, 1.460591197013855), (0.0, 1.4607685804367065), (0.0, 1.460929036140442), (0.0, 1.4609754085540771), (0.0, 1.4609955549240112), (0.0, 1.4610610008239746), (0.0, 1.4611091613769531), (0.0, 1.4611926078796387), (0.0, 1.4612081050872803), (0.0, 1.4612632989883423), (0.0, 1.461287498474121), (0.0, 1.4613044261932373), (0.0, 1.4613476991653442), (0.0, 1.4614289999008179), (0.0, 1.4615949392318726), (0.0, 1.461729884147644), (0.0, 1.4617588520050049), (0.0, 1.4617903232574463), (0.0, 1.461836814880371), (0.0, 1.4618926048278809), (0.0, 1.4619144201278687), (0.0, 1.462145209312439), (0.0, 1.4622583389282227), (0.0, 1.462388038635254), (0.0, 1.462573528289795), (0.0, 1.4625779390335083), (0.0, 1.4626232385635376), (0.0, 1.4626458883285522), (0.0, 1.462857723236084), (0.0, 1.4628682136535645), (0.0, 1.463027000427246), (0.0, 1.463078260421753), (0.0, 1.4633872509002686), (0.0, 1.4634202718734741), (0.0, 1.4634839296340942), (0.0, 1.4635688066482544), (0.0, 1.4636590480804443), (0.0, 1.4637528657913208), (0.0, 1.4638159275054932), (0.0, 1.463930368423462), (0.0, 1.4639644622802734), (0.0, 1.463966965675354), (0.0, 1.4639947414398193), (0.0, 1.4640007019042969), (0.0, 1.4641377925872803), (0.0, 1.464234709739685), (0.0, 1.4646178483963013), (0.0, 1.4647257328033447), (0.0, 1.4648622274398804), (0.0, 1.464966893196106), (0.0, 1.4664865732192993), (0.0, 1.4677549600601196), (0.0, 1.4678806066513062), (0.0, 1.4679114818572998), (0.0, 1.468307614326477), (0.0, 1.4695940017700195), (0.0, 1.4702688455581665), (0.0, 1.4723334312438965), (0.0, 1.4771431684494019), (0.0, 1.4848655462265015), (0.0, 1.4860941171646118), (0.0, 1.501249074935913), (0.0, 1.5071145296096802), (0.0, 1.5082701444625854), (0.0, 1.5127856731414795), (0.0, 1.5132163763046265), (0.0, 1.5133882761001587), (0.0, 1.513545036315918), (0.0, 1.5145272016525269), (0.0, 1.5149686336517334), (0.0, 1.516370177268982), (0.0, 1.5165503025054932), (0.0, 1.5165517330169678), (0.0, 1.5169093608856201), (0.0, 1.5169494152069092), (0.0, 1.517126441001892), (0.0, 1.5171983242034912), (0.0, 1.5176869630813599), (0.0, 1.5177091360092163), (0.0, 1.5177419185638428), (0.0, 1.5178135633468628), (0.0, 1.5183011293411255), (0.0, 1.5183048248291016), (0.0, 1.5183948278427124), (0.0, 1.5185784101486206), (0.0, 1.5187088251113892), (0.0, 1.5188649892807007), (0.0, 1.5190117359161377), (0.0, 1.519061803817749), (0.0, 1.5190715789794922), (0.0, 1.519213080406189), (0.0, 1.519286036491394), (0.0, 1.519312858581543), (0.0, 1.5196876525878906), (0.0, 1.519883155822754), (0.0, 1.5199204683303833), (0.0, 1.519920825958252), (0.0, 1.5199636220932007), (0.0, 1.5199707746505737), (0.0, 1.5200375318527222), (0.0, 1.5200506448745728), (0.0, 1.5201724767684937), (0.0, 1.5202182531356812), (0.0, 1.520350694656372), (0.0, 1.520443081855774), (0.0, 1.5204510688781738), (0.0, 1.5205214023590088), (0.0, 1.520560383796692), (0.0, 1.520877718925476), (0.0, 1.5209238529205322), (0.0, 1.5209892988204956), (0.0, 1.5211480855941772), (0.0, 1.5211783647537231), (0.0, 1.5212880373001099), (0.0, 1.5213230848312378), (0.0, 1.5213704109191895), (0.0, 1.5213940143585205), (0.0, 1.521443247795105), (0.0, 1.5216045379638672), (0.0, 1.5217193365097046), (0.0, 1.5218919515609741), (0.0, 1.521986961364746), (0.0, 1.5220102071762085), (0.0, 1.522032380104065), (0.0, 1.5220363140106201), (0.0, 1.5223115682601929), (0.0, 1.522489070892334), (0.0, 1.5225290060043335), (0.0, 1.5228384733200073), (0.0, 1.5228646993637085), (0.0, 1.5229055881500244), (0.0, 1.523019552230835), (0.0, 1.5230967998504639), (0.0, 1.523159384727478), (0.0, 1.5232350826263428), (0.0, 1.5233020782470703), (0.0, 1.5234081745147705), (0.0, 1.5234602689743042), (0.0, 1.5234858989715576), (0.0, 1.5235804319381714), (0.0, 1.5236319303512573), (0.0, 1.5238730907440186), (0.0, 1.5239747762680054), (0.0, 1.5239989757537842), (0.0, 1.5240278244018555), (0.0, 1.5240566730499268), (0.0, 1.524359107017517), (0.0, 1.5246491432189941), (0.0, 1.5247611999511719), (0.0, 1.5247998237609863), (0.0, 1.5248908996582031), (0.0, 1.5250370502471924), (0.0, 1.5254735946655273), (0.0, 1.5255147218704224), (0.0, 1.5255258083343506), (0.0, 1.525653600692749), (0.0, 1.5256540775299072), (0.0, 1.5256717205047607), (0.0, 1.525931715965271), (0.0, 1.5259692668914795), (0.0, 1.5260214805603027), (0.0, 1.5261592864990234), (0.0, 1.5265142917633057), (0.0, 1.5265241861343384), (0.0, 1.52653169631958), (0.0, 1.5267112255096436), (0.0, 1.5267672538757324), (0.0, 1.5267761945724487), (0.0, 1.526931881904602), (0.0, 1.5269752740859985), (0.0, 1.527008056640625), (0.0, 1.5270264148712158), (0.0, 1.527164101600647), (0.0, 1.5272507667541504), (0.0, 1.5272588729858398), (0.0, 1.5274975299835205), (0.0, 1.5275077819824219), (0.0, 1.527886986732483), (0.0, 1.5285725593566895), (0.0, 1.5288026332855225), (0.0, 1.5289362668991089), (0.0, 1.5293898582458496), (0.0, 1.529707908630371), (0.0, 1.5297222137451172), (0.0, 1.5303484201431274), (0.0, 1.5314582586288452), (0.0, 1.5329928398132324), (0.0, 1.5704816579818726)], [(7.779074668884277, 8.341208457946777), (6.990612506866455, 7.536644458770752), (6.874794960021973, 7.133666038513184), (6.754590034484863, 7.373263359069824), (6.502846717834473, 6.815095901489258), (6.395190238952637, 6.9639105796813965), (6.369168758392334, 7.066450595855713), (6.191809177398682, 7.793743133544922), (6.094404697418213, 7.042415618896484), (6.047945499420166, 7.228471279144287), (5.967663288116455, 6.444169998168945), (5.71359920501709, 5.992044448852539), (5.6732916831970215, 8.578020095825195), (5.630709648132324, 5.684942722320557), (5.4092698097229, 8.398653984069824), (5.34367561340332, 8.608906745910645), (5.0600175857543945, 7.029935836791992), (4.9714250564575195, 7.319916725158691), (4.426994800567627, 4.5024871826171875), (4.395124912261963, 7.953719139099121), (4.035937309265137, 7.899302005767822), (4.01396369934082, 4.031117916107178), (3.983110189437866, 4.2731146812438965), (3.921508550643921, 4.331240177154541), (3.9180543422698975, 4.749753475189209), (3.645505666732788, 8.576509475708008)], [(9.490296363830566, 10.005388259887695), (9.472031593322754, 9.51148796081543), (9.096415519714355, 10.772538185119629), (8.722750663757324, 8.757180213928223), (8.416340827941895, 8.531410217285156), (7.605195045471191, 7.712361812591553)]]
[array([[0.        , 1.31311643],
       [0.        , 1.32010365],
       [0.        , 1.32287717],
       [0.        , 1.32379866],
       [0.        , 1.32525218],
       [0.        , 1.3265903 ],
       [0.        , 1.32729113],
       [0.        , 1.3276993 ],
       [0.        , 1.32797885],
       [0.        , 1.32864845],
       [0.        , 1.32867754],
       [0.        , 1.32899022],
       [0.        , 1.32905853],
       [0.        , 1.32917213],
       [0.        , 1.32948399],
       [0.        , 1.32963753],
       [0.        , 1.3297298 ],
       [0.        , 1.32988286],
       [0.        , 1.32992887],
       [0.        , 1.33008838],
       [0.        , 1.33015192],
       [0.        , 1.33017707],
       [0.        , 1.33021998],
       [0.        , 1.33034968],
       [0.        , 1.33045101],
       [0.        , 1.33056092],
       [0.        , 1.3305856 ],
       [0.        , 1.33106661],
       [0.        , 1.33115256],
       [0.        , 1.33116889],
       [0.        , 1.33132243],
       [0.        , 1.33147788],
       [0.        , 1.33150971],
       [0.        , 1.33180511],
       [0.        , 1.3318485 ],
       [0.        , 1.33225834],
       [0.        , 1.33237374],
       [0.        , 1.33253729],
       [0.        , 1.33280313],
       [0.        , 1.33285022],
       [0.        , 1.33309066],
       [0.        , 1.33316898],
       [0.        , 1.33318007],
       [0.        , 1.33346558],
       [0.        , 1.33351147],
       [0.        , 1.33362412],
       [0.        , 1.33379447],
       [0.        , 1.33399367],
       [0.        , 1.33403671],
       [0.        , 1.33421922],
       [0.        , 1.33438087],
       [0.        , 1.33438838],
       [0.        , 1.33442795],
       [0.        , 1.33462226],
       [0.        , 1.33462477],
       [0.        , 1.3346293 ],
       [0.        , 1.33463442],
       [0.        , 1.3347429 ],
       [0.        , 1.33488345],
       [0.        , 1.33490348],
       [0.        , 1.33495581],
       [0.        , 1.33496547],
       [0.        , 1.33509827],
       [0.        , 1.33519435],
       [0.        , 1.33523428],
       [0.        , 1.33527029],
       [0.        , 1.3354795 ],
       [0.        , 1.33563697],
       [0.        , 1.33568096],
       [0.        , 1.33578014],
       [0.        , 1.33580494],
       [0.        , 1.33581209],
       [0.        , 1.33586693],
       [0.        , 1.33589756],
       [0.        , 1.3359642 ],
       [0.        , 1.3360008 ],
       [0.        , 1.3361001 ],
       [0.        , 1.33610272],
       [0.        , 1.33632696],
       [0.        , 1.33637893],
       [0.        , 1.33638239],
       [0.        , 1.33643103],
       [0.        , 1.33645082],
       [0.        , 1.33667457],
       [0.        , 1.33670664],
       [0.        , 1.336743  ],
       [0.        , 1.33676565],
       [0.        , 1.33683157],
       [0.        , 1.33686149],
       [0.        , 1.33690238],
       [0.        , 1.33691895],
       [0.        , 1.33694923],
       [0.        , 1.33703697],
       [0.        , 1.33704805],
       [0.        , 1.33736646],
       [0.        , 1.33743453],
       [0.        , 1.3374995 ],
       [0.        , 1.3377564 ],
       [0.        , 1.33782232],
       [0.        , 1.33787251],
       [0.        , 1.33788252],
       [0.        , 1.33795786],
       [0.        , 1.33817112],
       [0.        , 1.33817863],
       [0.        , 1.33823133],
       [0.        , 1.33828354],
       [0.        , 1.33854926],
       [0.        , 1.3392185 ],
       [0.        , 1.3395431 ],
       [0.        , 1.33963108],
       [0.        , 1.33972859],
       [0.        , 1.34011078],
       [0.        , 1.34061825],
       [0.        , 1.34067166],
       [0.        , 1.34069693],
       [0.        , 1.34078503],
       [0.        , 1.34180486],
       [0.        , 1.34182036],
       [0.        , 1.34211528],
       [0.        , 1.34282506],
       [0.        , 1.34291983],
       [0.        , 1.34355307],
       [0.        , 1.34401572],
       [0.        , 1.34883881],
       [0.        , 1.37281525],
       [0.        , 1.37831604],
       [0.        , 1.38635874],
       [0.        , 1.44139147],
       [0.        , 1.45049953],
       [0.        , 1.450858  ],
       [0.        , 1.45151997],
       [0.        , 1.45186985],
       [0.        , 1.45215952],
       [0.        , 1.45263004],
       [0.        , 1.45345473],
       [0.        , 1.45350528],
       [0.        , 1.45403755],
       [0.        , 1.45420861],
       [0.        , 1.45431614],
       [0.        , 1.45437849],
       [0.        , 1.45458758],
       [0.        , 1.45459425],
       [0.        , 1.45485938],
       [0.        , 1.45491624],
       [0.        , 1.45499074],
       [0.        , 1.45521903],
       [0.        , 1.45533133],
       [0.        , 1.45535505],
       [0.        , 1.45542204],
       [0.        , 1.45551789],
       [0.        , 1.45565188],
       [0.        , 1.45571375],
       [0.        , 1.45578229],
       [0.        , 1.4559052 ],
       [0.        , 1.45592439],
       [0.        , 1.45621586],
       [0.        , 1.4562428 ],
       [0.        , 1.45626056],
       [0.        , 1.45637035],
       [0.        , 1.45637691],
       [0.        , 1.45639765],
       [0.        , 1.45654011],
       [0.        , 1.45658803],
       [0.        , 1.45670295],
       [0.        , 1.4567945 ],
       [0.        , 1.45686126],
       [0.        , 1.45713651],
       [0.        , 1.45716572],
       [0.        , 1.45723188],
       [0.        , 1.45743573],
       [0.        , 1.45774686],
       [0.        , 1.45808625],
       [0.        , 1.45819831],
       [0.        , 1.45851898],
       [0.        , 1.45855319],
       [0.        , 1.4586612 ],
       [0.        , 1.45873308],
       [0.        , 1.45884585],
       [0.        , 1.45888031],
       [0.        , 1.45889962],
       [0.        , 1.45916831],
       [0.        , 1.45927334],
       [0.        , 1.4592787 ],
       [0.        , 1.45935118],
       [0.        , 1.45953417],
       [0.        , 1.45959365],
       [0.        , 1.45962894],
       [0.        , 1.45964885],
       [0.        , 1.45979667],
       [0.        , 1.45980227],
       [0.        , 1.45987451],
       [0.        , 1.46006608],
       [0.        , 1.4605912 ],
       [0.        , 1.46076858],
       [0.        , 1.46092904],
       [0.        , 1.46097541],
       [0.        , 1.46099555],
       [0.        , 1.461061  ],
       [0.        , 1.46110916],
       [0.        , 1.46119261],
       [0.        , 1.46120811],
       [0.        , 1.4612633 ],
       [0.        , 1.4612875 ],
       [0.        , 1.46130443],
       [0.        , 1.4613477 ],
       [0.        , 1.461429  ],
       [0.        , 1.46159494],
       [0.        , 1.46172988],
       [0.        , 1.46175885],
       [0.        , 1.46179032],
       [0.        , 1.46183681],
       [0.        , 1.4618926 ],
       [0.        , 1.46191442],
       [0.        , 1.46214521],
       [0.        , 1.46225834],
       [0.        , 1.46238804],
       [0.        , 1.46257353],
       [0.        , 1.46257794],
       [0.        , 1.46262324],
       [0.        , 1.46264589],
       [0.        , 1.46285772],
       [0.        , 1.46286821],
       [0.        , 1.463027  ],
       [0.        , 1.46307826],
       [0.        , 1.46338725],
       [0.        , 1.46342027],
       [0.        , 1.46348393],
       [0.        , 1.46356881],
       [0.        , 1.46365905],
       [0.        , 1.46375287],
       [0.        , 1.46381593],
       [0.        , 1.46393037],
       [0.        , 1.46396446],
       [0.        , 1.46396697],
       [0.        , 1.46399474],
       [0.        , 1.4640007 ],
       [0.        , 1.46413779],
       [0.        , 1.46423471],
       [0.        , 1.46461785],
       [0.        , 1.46472573],
       [0.        , 1.46486223],
       [0.        , 1.46496689],
       [0.        , 1.46648657],
       [0.        , 1.46775496],
       [0.        , 1.46788061],
       [0.        , 1.46791148],
       [0.        , 1.46830761],
       [0.        , 1.469594  ],
       [0.        , 1.47026885],
       [0.        , 1.47233343],
       [0.        , 1.47714317],
       [0.        , 1.48486555],
       [0.        , 1.48609412],
       [0.        , 1.50124907],
       [0.        , 1.50711453],
       [0.        , 1.50827014],
       [0.        , 1.51278567],
       [0.        , 1.51321638],
       [0.        , 1.51338828],
       [0.        , 1.51354504],
       [0.        , 1.5145272 ],
       [0.        , 1.51496863],
       [0.        , 1.51637018],
       [0.        , 1.5165503 ],
       [0.        , 1.51655173],
       [0.        , 1.51690936],
       [0.        , 1.51694942],
       [0.        , 1.51712644],
       [0.        , 1.51719832],
       [0.        , 1.51768696],
       [0.        , 1.51770914],
       [0.        , 1.51774192],
       [0.        , 1.51781356],
       [0.        , 1.51830113],
       [0.        , 1.51830482],
       [0.        , 1.51839483],
       [0.        , 1.51857841],
       [0.        , 1.51870883],
       [0.        , 1.51886499],
       [0.        , 1.51901174],
       [0.        , 1.5190618 ],
       [0.        , 1.51907158],
       [0.        , 1.51921308],
       [0.        , 1.51928604],
       [0.        , 1.51931286],
       [0.        , 1.51968765],
       [0.        , 1.51988316],
       [0.        , 1.51992047],
       [0.        , 1.51992083],
       [0.        , 1.51996362],
       [0.        , 1.51997077],
       [0.        , 1.52003753],
       [0.        , 1.52005064],
       [0.        , 1.52017248],
       [0.        , 1.52021825],
       [0.        , 1.52035069],
       [0.        , 1.52044308],
       [0.        , 1.52045107],
       [0.        , 1.5205214 ],
       [0.        , 1.52056038],
       [0.        , 1.52087772],
       [0.        , 1.52092385],
       [0.        , 1.5209893 ],
       [0.        , 1.52114809],
       [0.        , 1.52117836],
       [0.        , 1.52128804],
       [0.        , 1.52132308],
       [0.        , 1.52137041],
       [0.        , 1.52139401],
       [0.        , 1.52144325],
       [0.        , 1.52160454],
       [0.        , 1.52171934],
       [0.        , 1.52189195],
       [0.        , 1.52198696],
       [0.        , 1.52201021],
       [0.        , 1.52203238],
       [0.        , 1.52203631],
       [0.        , 1.52231157],
       [0.        , 1.52248907],
       [0.        , 1.52252901],
       [0.        , 1.52283847],
       [0.        , 1.5228647 ],
       [0.        , 1.52290559],
       [0.        , 1.52301955],
       [0.        , 1.5230968 ],
       [0.        , 1.52315938],
       [0.        , 1.52323508],
       [0.        , 1.52330208],
       [0.        , 1.52340817],
       [0.        , 1.52346027],
       [0.        , 1.5234859 ],
       [0.        , 1.52358043],
       [0.        , 1.52363193],
       [0.        , 1.52387309],
       [0.        , 1.52397478],
       [0.        , 1.52399898],
       [0.        , 1.52402782],
       [0.        , 1.52405667],
       [0.        , 1.52435911],
       [0.        , 1.52464914],
       [0.        , 1.5247612 ],
       [0.        , 1.52479982],
       [0.        , 1.5248909 ],
       [0.        , 1.52503705],
       [0.        , 1.52547359],
       [0.        , 1.52551472],
       [0.        , 1.52552581],
       [0.        , 1.5256536 ],
       [0.        , 1.52565408],
       [0.        , 1.52567172],
       [0.        , 1.52593172],
       [0.        , 1.52596927],
       [0.        , 1.52602148],
       [0.        , 1.52615929],
       [0.        , 1.52651429],
       [0.        , 1.52652419],
       [0.        , 1.5265317 ],
       [0.        , 1.52671123],
       [0.        , 1.52676725],
       [0.        , 1.52677619],
       [0.        , 1.52693188],
       [0.        , 1.52697527],
       [0.        , 1.52700806],
       [0.        , 1.52702641],
       [0.        , 1.5271641 ],
       [0.        , 1.52725077],
       [0.        , 1.52725887],
       [0.        , 1.52749753],
       [0.        , 1.52750778],
       [0.        , 1.52788699],
       [0.        , 1.52857256],
       [0.        , 1.52880263],
       [0.        , 1.52893627],
       [0.        , 1.52938986],
       [0.        , 1.52970791],
       [0.        , 1.52972221],
       [0.        , 1.53034842],
       [0.        , 1.53145826],
       [0.        , 1.53299284],
       [0.        , 1.57048166]]), array([[7.77907467, 8.34120846],
       [6.99061251, 7.53664446],
       [6.87479496, 7.13366604],
       [6.75459003, 7.37326336],
       [6.50284672, 6.8150959 ],
       [6.39519024, 6.96391058],
       [6.36916876, 7.0664506 ],
       [6.19180918, 7.79374313],
       [6.0944047 , 7.04241562],
       [6.0479455 , 7.22847128],
       [5.96766329, 6.44417   ],
       [5.71359921, 5.99204445],
       [5.67329168, 8.5780201 ],
       [5.63070965, 5.68494272],
       [5.40926981, 8.39865398],
       [5.34367561, 8.60890675],
       [5.06001759, 7.02993584],
       [4.97142506, 7.31991673],
       [4.4269948 , 4.50248718],
       [4.39512491, 7.95371914],
       [4.03593731, 7.89930201],
       [4.0139637 , 4.03111792],
       [3.98311019, 4.27311468],
       [3.92150855, 4.33124018],
       [3.91805434, 4.74975348],
       [3.64550567, 8.57650948]]), array([[ 9.49029636, 10.00538826],
       [ 9.47203159,  9.51148796],
       [ 9.09641552, 10.77253819],
       [ 8.72275066,  8.75718021],
       [ 8.41634083,  8.53141022],
       [ 7.60519505,  7.71236181]])]2024-03-06 17:54:10.222072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KEK ph vector generated, counter: 123
2024-03-06 17:54:14.178630: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:14.222078: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:15.252893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KG6 ph vector generated, counter: 124
2024-03-06 17:54:19.047752: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:19.097114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:20.074511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KGB ph vector generated, counter: 125
2024-03-06 17:54:23.293279: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:23.335994: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:24.425539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KGJ ph vector generated, counter: 126
2024-03-06 17:54:27.925200: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:27.982659: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:28.881812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KHS ph vector generated, counter: 127
2024-03-06 17:54:31.975802: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:32.018847: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:32.929877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KJZ ph vector generated, counter: 128
2024-03-06 17:54:36.054555: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:36.098086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:36.981037: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KK0 ph vector generated, counter: 129
2024-03-06 17:54:40.216892: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:40.260248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:41.277076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KLZ ph vector generated, counter: 130
2024-03-06 17:54:44.660849: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:44.703983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:45.602715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KM0 ph vector generated, counter: 131
2024-03-06 17:54:48.887913: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:48.930999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:49.930727: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KM1 ph vector generated, counter: 132
2024-03-06 17:54:53.120815: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:53.185948: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:54.075578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KM2 ph vector generated, counter: 133
2024-03-06 17:54:57.214602: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:54:57.257803: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:54:58.364254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KM3 ph vector generated, counter: 134
2024-03-06 17:55:01.702656: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:01.745874: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:02.673411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KM4 ph vector generated, counter: 135
2024-03-06 17:55:05.883051: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:05.925628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:06.798367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KM5 ph vector generated, counter: 136
2024-03-06 17:55:09.990745: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:10.033886: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:10.879575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KM6 ph vector generated, counter: 137
2024-03-06 17:55:14.243352: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:14.286212: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:15.736577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KMH ph vector generated, counter: 138
2024-03-06 17:55:19.652158: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:19.695068: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:20.548395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KOF ph vector generated, counter: 139
2024-03-06 17:55:23.681827: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:23.725326: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:24.597355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KPS ph vector generated, counter: 140
2024-03-06 17:55:27.743445: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:27.807656: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:28.708285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KQP ph vector generated, counter: 141
2024-03-06 17:55:31.837816: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:31.880920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:32.979808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.2581688), (0., 1.275205 ), (0., 1.3023279), (0., 1.308753 ),
       (0., 1.3115038), (0., 1.3120539), (0., 1.3130236), (0., 1.3155355),
       (0., 1.3157853), (0., 1.3157995), (0., 1.317014 ), (0., 1.3187168),
       (0., 1.3191097), (0., 1.3192691), (0., 1.3199567), (0., 1.3205199),
       (0., 1.3212689), (0., 1.3213398), (0., 1.3213476), (0., 1.3218824),
       (0., 1.3224224), (0., 1.3227023), (0., 1.3229148), (0., 1.3233246),
       (0., 1.3234348), (0., 1.3241227), (0., 1.3242877), (0., 1.3252878),
       (0., 1.3252968), (0., 1.3253719), (0., 1.3254926), (0., 1.3255192),
       (0., 1.3255383), (0., 1.3258297), (0., 1.3258878), (0., 1.3259732),
       (0., 1.3260084), (0., 1.3264953), (0., 1.3268211), (0., 1.3270097),
       (0., 1.327317 ), (0., 1.3275383), (0., 1.3276311), (0., 1.327844 ),
       (0., 1.3280416), (0., 1.328093 ), (0., 1.328139 ), (0., 1.3282175),
       (0., 1.328219 ), (0., 1.3283608), (0., 1.3283745), (0., 1.3284576),
       (0., 1.328538 ), (0., 1.328657 ), (0., 1.3290166), (0., 1.3294847),
       (0., 1.3296098), (0., 1.3300633), (0., 1.3303902), (0., 1.3304094),
       (0., 1.3304983), (0., 1.3308548), (0., 1.3309841), (0., 1.3311099),
       (0., 1.3311803), (0., 1.3312209), (0., 1.331271 ), (0., 1.3317682),
       (0., 1.3318397), (0., 1.3319077), (0., 1.3324751), (0., 1.3326193),
       (0., 1.3327061), (0., 1.3332947), (0., 1.3333875), (0., 1.3334045),
       (0., 1.3334725), (0., 1.3338734), (0., 1.3338926), (0., 1.3340648),
       (0., 1.3344713), (0., 1.3347303), (0., 1.3349857), (0., 1.3350716),
       (0., 1.335172 ), (0., 1.3356497), (0., 1.3358148), (0., 1.3358554),
       (0., 1.3358573), (0., 1.3362055), (0., 1.3362322), (0., 1.3363928),
       (0., 1.3365796), (0., 1.3371531), (0., 1.3373817), (0., 1.337607 ),
       (0., 1.33772  ), (0., 1.337804 ), (0., 1.3386084), (0., 1.3388855),
       (0., 1.3389105), (0., 1.3396382), (0., 1.3401154), (0., 1.3407394),
       (0., 1.3409575), (0., 1.3411069), (0., 1.3413094), (0., 1.3425368),
       (0., 1.3438003), (0., 1.3438445), (0., 1.3439903), (0., 1.3446876),
       (0., 1.3453363), (0., 1.3456484), (0., 1.3463506), (0., 1.3473895),
       (0., 1.347507 ), (0., 1.3485864), (0., 1.3487879), (0., 1.3511469),
       (0., 1.352874 ), (0., 1.3550045), (0., 1.4217573), (0., 1.4304864),
       (0., 1.4326602), (0., 1.4328363), (0., 1.4341867), (0., 1.4344852),
       (0., 1.4367813), (0., 1.4394976), (0., 1.4396293), (0., 1.4408473),
       (0., 1.4415758), (0., 1.4424546), (0., 1.4430318), (0., 1.4448608),
       (0., 1.4450105), (0., 1.4454405), (0., 1.4455982), (0., 1.446133 ),
       (0., 1.4461596), (0., 1.4462801), (0., 1.4482379), (0., 1.4482849),
       (0., 1.4485774), (0., 1.4486849), (0., 1.44952  ), (0., 1.4497218),
       (0., 1.4502105), (0., 1.4503154), (0., 1.4507924), (0., 1.4517471),
       (0., 1.4531595), (0., 1.4534171), (0., 1.4536219), (0., 1.4538683),
       (0., 1.4538758), (0., 1.4540766), (0., 1.4545231), (0., 1.454713 ),
       (0., 1.4548204), (0., 1.4552296), (0., 1.4553376), (0., 1.4557809),
       (0., 1.4558481), (0., 1.4563615), (0., 1.4564025), (0., 1.4568545),
       (0., 1.4570817), (0., 1.4572197), (0., 1.4573092), (0., 1.4577026),
       (0., 1.4581472), (0., 1.4583035), (0., 1.4588523), (0., 1.4588907),
       (0., 1.458923 ), (0., 1.4591608), (0., 1.459559 ), (0., 1.4598086),
       (0., 1.4598141), (0., 1.46082  ), (0., 1.4608215), (0., 1.46177  ),
       (0., 1.4619654), (0., 1.4621799), (0., 1.4622364), (0., 1.4628158),
       (0., 1.4629323), (0., 1.4629426), (0., 1.4633043), (0., 1.4633694),
       (0., 1.4640007), (0., 1.4645408), (0., 1.464624 ), (0., 1.464787 ),
       (0., 1.4648483), (0., 1.4650126), (0., 1.4650215), (0., 1.466087 ),
       (0., 1.4665576), (0., 1.4668115), (0., 1.467373 ), (0., 1.46753  ),
       (0., 1.4676571), (0., 1.4681892), (0., 1.4682609), (0., 1.4682657),
       (0., 1.4688137), (0., 1.4692717), (0., 1.4696944), (0., 1.4706845),
       (0., 1.4707834), (0., 1.4707854), (0., 1.4709157), (0., 1.4711593),
       (0., 1.4712315), (0., 1.4717368), (0., 1.4720187), (0., 1.4722524),
       (0., 1.4724159), (0., 1.472965 ), (0., 1.474178 ), (0., 1.4748253),
       (0., 1.4754432), (0., 1.4759246), (0., 1.4774077), (0., 1.4781097),
       (0., 1.4782434), (0., 1.4785173), (0., 1.478625 ), (0., 1.4793612),
       (0., 1.4798089), (0., 1.4807371), (0., 1.4808404), (0., 1.4808571),
       (0., 1.4809738), (0., 1.4813819), (0., 1.4864006), (0., 1.4866723),
       (0., 1.4872832), (0., 1.4873705), (0., 1.4908305), (0., 1.4930671),
       (0., 1.4951394), (0., 1.4968908), (0., 1.497415 ), (0., 1.4975377),
       (0., 1.4975724), (0., 1.4993399), (0., 1.5007261), (0., 1.5045819),
       (0., 1.5048039), (0., 1.5050527), (0., 1.5056509), (0., 1.5062976),
       (0., 1.5087276), (0., 1.5112418), (0., 1.5119765), (0., 1.5127552),
       (0., 1.5135933), (0., 1.5136541), (0., 1.5137906), (0., 1.5148301),
       (0., 1.5148606), (0., 1.5151088), (0., 1.5159253), (0., 1.5160546),
       (0., 1.5161432), (0., 1.5163072), (0., 1.5168194), (0., 1.5171685),
       (0., 1.5183727), (0., 1.5188297), (0., 1.5191861), (0., 1.5192035),
       (0., 1.5194532), (0., 1.5200644), (0., 1.5203665), (0., 1.5210171),
       (0., 1.5213642), (0., 1.5214453), (0., 1.5222174), (0., 1.5225929),
       (0., 1.522649 ), (0., 1.5228477), (0., 1.5233399), (0., 1.5234116),
       (0., 1.5237393), (0., 1.5238044), (0., 1.5243217), (0., 1.5244135),
       (0., 1.524449 ), (0., 1.5248809), (0., 1.5262038), (0., 1.526284 ),
       (0., 1.5268457), (0., 1.5271747), (0., 1.5272824), (0., 1.5274081),
       (0., 1.5274161), (0., 1.5284019), (0., 1.528581 ), (0., 1.5285861),
       (0., 1.5287933), (0., 1.5291146), (0., 1.5291914), (0., 1.5295609),
       (0., 1.529844 ), (0., 1.5298915), (0., 1.53001  ), (0., 1.5300511),
       (0., 1.5301422), (0., 1.5307183), (0., 1.5308818), (0., 1.532101 ),
       (0., 1.5321294), (0., 1.5331907), (0., 1.533351 ), (0., 1.5334835),
       (0., 1.5334911), (0., 1.533625 ), (0., 1.5338582), (0., 1.5345157),
       (0., 1.5351675), (0., 1.5357723), (0., 1.5359277), (0., 1.5363561),
       (0., 1.5365057), (0., 1.5369669), (0., 1.5374473), (0., 1.5375191),
       (0., 1.538091 ), (0., 1.5392392), (0., 1.53951  ), (0., 1.539544 ),
       (0., 1.5397495), (0., 1.5398648), (0., 1.54048  ), (0., 1.5404913),
       (0., 1.5404929), (0., 1.540856 ), (0., 1.5411814), (0., 1.5415579),
       (0., 1.5417914), (0., 1.5424868), (0., 1.5429226), (0., 1.5432515),
       (0., 1.54383  ), (0., 1.5440843), (0., 1.544361 ), (0., 1.5446618),
       (0., 1.546345 ), (0., 1.5467296), (0., 1.5469457), (0., 1.5478648),
       (0., 1.5485831), (0., 1.5492673), (0., 1.5493628), (0., 1.5508567),
       (0., 1.5516853), (0., 1.5548137), (0., 1.5551244), (0., 1.5553927),
       (0., 1.5570511), (0., 1.5583198), (0., 1.5594189), (0., 1.5601807)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(10.087112 , 10.235357 ), ( 8.285557 ,  8.432308 ),
       ( 7.723968 ,  8.441851 ), ( 7.332167 ,  7.466554 ),
       ( 7.01706  ,  7.153068 ), ( 6.8853807,  8.08218  ),
       ( 6.4804177,  8.09373  ), ( 6.113044 ,  7.2349095),
       ( 5.997071 ,  6.300265 ), ( 5.930322 ,  6.1280518),
       ( 5.9189734,  6.1136494), ( 5.8538885,  6.2140255),
       ( 5.809943 ,  6.278651 ), ( 5.7534704,  5.9930305),
       ( 5.699696 ,  6.1272564), ( 5.6455293,  8.250582 ),
       ( 5.528985 ,  5.6021285), ( 5.496971 ,  6.7610407),
       ( 5.460646 ,  7.411784 ), ( 5.3995457,  6.617547 ),
       ( 5.3984156,  6.283242 ), ( 5.2025924,  8.7035   ),
       ( 5.1262493,  5.174884 ), ( 5.082752 ,  5.3520117),
       ( 5.0684023,  5.1901655), ( 5.0473223,  5.619585 ),
       ( 4.9723315,  5.4836793), ( 4.919522 ,  5.314597 ),
       ( 4.8310323,  4.9734206), ( 4.744786 ,  6.1480803),
       ( 4.5471244,  4.824486 ), ( 4.4258337,  4.434796 ),
       ( 4.4045963,  5.272364 ), ( 4.379074 ,  4.6940837),
       ( 4.276764 ,  4.563056 ), ( 4.195336 ,  4.4399767),
       ( 4.164677 ,  4.8276615), ( 4.1310205,  4.396753 ),
       ( 4.0865297,  4.1259065), ( 4.048115 ,  4.4842277),
       ( 4.047485 ,  4.6898584), ( 3.9827776,  4.4909554),
       ( 3.9512146,  4.4745145), ( 3.9396892,  4.844705 ),
       ( 3.9364522,  6.9050126), ( 3.8905923,  8.665237 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(11.346749 , 11.707258 ), ( 8.740345 ,  9.475493 ),
       ( 5.4848905,  5.516909 ), ( 4.7438073,  5.1053605)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.2581688165664673), (0.0, 1.2752050161361694), (0.0, 1.3023278713226318), (0.0, 1.3087530136108398), (0.0, 1.3115037679672241), (0.0, 1.312053918838501), (0.0, 1.313023567199707), (0.0, 1.315535545349121), (0.0, 1.31578528881073), (0.0, 1.3157994747161865), (0.0, 1.3170139789581299), (0.0, 1.3187167644500732), (0.0, 1.3191096782684326), (0.0, 1.319269061088562), (0.0, 1.319956660270691), (0.0, 1.3205199241638184), (0.0, 1.321268916130066), (0.0, 1.3213398456573486), (0.0, 1.3213475942611694), (0.0, 1.3218823671340942), (0.0, 1.3224223852157593), (0.0, 1.3227022886276245), (0.0, 1.3229148387908936), (0.0, 1.3233245611190796), (0.0, 1.323434829711914), (0.0, 1.324122667312622), (0.0, 1.3242876529693604), (0.0, 1.3252878189086914), (0.0, 1.3252967596054077), (0.0, 1.3253718614578247), (0.0, 1.3254926204681396), (0.0, 1.3255192041397095), (0.0, 1.3255382776260376), (0.0, 1.3258297443389893), (0.0, 1.3258877992630005), (0.0, 1.3259731531143188), (0.0, 1.3260084390640259), (0.0, 1.3264952898025513), (0.0, 1.3268210887908936), (0.0, 1.327009677886963), (0.0, 1.3273169994354248), (0.0, 1.327538251876831), (0.0, 1.3276311159133911), (0.0, 1.3278440237045288), (0.0, 1.3280415534973145), (0.0, 1.3280930519104004), (0.0, 1.3281389474868774), (0.0, 1.3282175064086914), (0.0, 1.3282190561294556), (0.0, 1.3283607959747314), (0.0, 1.3283745050430298), (0.0, 1.3284575939178467), (0.0, 1.328537940979004), (0.0, 1.3286570310592651), (0.0, 1.3290165662765503), (0.0, 1.3294847011566162), (0.0, 1.329609751701355), (0.0, 1.3300633430480957), (0.0, 1.330390214920044), (0.0, 1.3304094076156616), (0.0, 1.3304983377456665), (0.0, 1.3308547735214233), (0.0, 1.330984115600586), (0.0, 1.331109881401062), (0.0, 1.3311803340911865), (0.0, 1.3312208652496338), (0.0, 1.3312710523605347), (0.0, 1.3317681550979614), (0.0, 1.331839680671692), (0.0, 1.3319077491760254), (0.0, 1.3324750661849976), (0.0, 1.332619309425354), (0.0, 1.332706093788147), (0.0, 1.3332947492599487), (0.0, 1.3333874940872192), (0.0, 1.333404541015625), (0.0, 1.333472490310669), (0.0, 1.3338733911514282), (0.0, 1.333892583847046), (0.0, 1.3340648412704468), (0.0, 1.334471344947815), (0.0, 1.3347302675247192), (0.0, 1.3349857330322266), (0.0, 1.3350715637207031), (0.0, 1.3351720571517944), (0.0, 1.3356497287750244), (0.0, 1.3358148336410522), (0.0, 1.3358553647994995), (0.0, 1.3358572721481323), (0.0, 1.3362054824829102), (0.0, 1.3362321853637695), (0.0, 1.3363927602767944), (0.0, 1.3365795612335205), (0.0, 1.3371530771255493), (0.0, 1.3373817205429077), (0.0, 1.3376070261001587), (0.0, 1.3377200365066528), (0.0, 1.3378039598464966), (0.0, 1.3386083841323853), (0.0, 1.3388855457305908), (0.0, 1.338910460472107), (0.0, 1.3396382331848145), (0.0, 1.3401154279708862), (0.0, 1.340739369392395), (0.0, 1.340957522392273), (0.0, 1.34110689163208), (0.0, 1.3413094282150269), (0.0, 1.3425368070602417), (0.0, 1.3438003063201904), (0.0, 1.3438445329666138), (0.0, 1.3439903259277344), (0.0, 1.344687581062317), (0.0, 1.3453363180160522), (0.0, 1.3456484079360962), (0.0, 1.3463505506515503), (0.0, 1.3473894596099854), (0.0, 1.3475069999694824), (0.0, 1.3485864400863647), (0.0, 1.3487879037857056), (0.0, 1.351146936416626), (0.0, 1.3528740406036377), (0.0, 1.3550045490264893), (0.0, 1.4217573404312134), (0.0, 1.4304864406585693), (0.0, 1.4326602220535278), (0.0, 1.4328362941741943), (0.0, 1.4341866970062256), (0.0, 1.4344851970672607), (0.0, 1.4367812871932983), (0.0, 1.4394975900650024), (0.0, 1.439629316329956), (0.0, 1.4408472776412964), (0.0, 1.4415757656097412), (0.0, 1.4424545764923096), (0.0, 1.4430317878723145), (0.0, 1.444860816001892), (0.0, 1.4450105428695679), (0.0, 1.4454405307769775), (0.0, 1.4455982446670532), (0.0, 1.446133017539978), (0.0, 1.4461596012115479), (0.0, 1.4462801218032837), (0.0, 1.4482378959655762), (0.0, 1.4482848644256592), (0.0, 1.4485774040222168), (0.0, 1.4486849308013916), (0.0, 1.4495199918746948), (0.0, 1.4497218132019043), (0.0, 1.450210452079773), (0.0, 1.4503153562545776), (0.0, 1.4507924318313599), (0.0, 1.4517470598220825), (0.0, 1.4531594514846802), (0.0, 1.4534170627593994), (0.0, 1.4536218643188477), (0.0, 1.4538682699203491), (0.0, 1.4538757801055908), (0.0, 1.4540766477584839), (0.0, 1.4545230865478516), (0.0, 1.454712986946106), (0.0, 1.4548203945159912), (0.0, 1.455229640007019), (0.0, 1.455337643623352), (0.0, 1.4557808637619019), (0.0, 1.4558480978012085), (0.0, 1.4563615322113037), (0.0, 1.4564025402069092), (0.0, 1.4568544626235962), (0.0, 1.45708167552948), (0.0, 1.4572197198867798), (0.0, 1.4573092460632324), (0.0, 1.45770263671875), (0.0, 1.4581471681594849), (0.0, 1.458303451538086), (0.0, 1.4588522911071777), (0.0, 1.458890676498413), (0.0, 1.4589229822158813), (0.0, 1.4591608047485352), (0.0, 1.4595589637756348), (0.0, 1.459808588027954), (0.0, 1.4598140716552734), (0.0, 1.460819959640503), (0.0, 1.460821509361267), (0.0, 1.4617700576782227), (0.0, 1.4619654417037964), (0.0, 1.4621798992156982), (0.0, 1.4622364044189453), (0.0, 1.462815761566162), (0.0, 1.4629323482513428), (0.0, 1.4629426002502441), (0.0, 1.4633042812347412), (0.0, 1.463369369506836), (0.0, 1.4640007019042969), (0.0, 1.4645408391952515), (0.0, 1.464624047279358), (0.0, 1.4647870063781738), (0.0, 1.464848279953003), (0.0, 1.465012550354004), (0.0, 1.4650214910507202), (0.0, 1.466086983680725), (0.0, 1.4665576219558716), (0.0, 1.4668115377426147), (0.0, 1.467373013496399), (0.0, 1.4675300121307373), (0.0, 1.4676570892333984), (0.0, 1.4681892395019531), (0.0, 1.4682608842849731), (0.0, 1.4682656526565552), (0.0, 1.4688136577606201), (0.0, 1.4692716598510742), (0.0, 1.4696943759918213), (0.0, 1.47068452835083), (0.0, 1.4707833528518677), (0.0, 1.47078537940979), (0.0, 1.470915675163269), (0.0, 1.4711593389511108), (0.0, 1.471231460571289), (0.0, 1.4717367887496948), (0.0, 1.4720187187194824), (0.0, 1.472252368927002), (0.0, 1.4724159240722656), (0.0, 1.4729650020599365), (0.0, 1.4741779565811157), (0.0, 1.4748252630233765), (0.0, 1.4754432439804077), (0.0, 1.4759246110916138), (0.0, 1.477407693862915), (0.0, 1.4781097173690796), (0.0, 1.478243350982666), (0.0, 1.4785172939300537), (0.0, 1.4786250591278076), (0.0, 1.4793611764907837), (0.0, 1.4798089265823364), (0.0, 1.4807370901107788), (0.0, 1.4808404445648193), (0.0, 1.4808571338653564), (0.0, 1.4809738397598267), (0.0, 1.481381893157959), (0.0, 1.4864006042480469), (0.0, 1.486672282218933), (0.0, 1.4872832298278809), (0.0, 1.487370491027832), (0.0, 1.4908305406570435), (0.0, 1.4930671453475952), (0.0, 1.4951393604278564), (0.0, 1.4968907833099365), (0.0, 1.4974149465560913), (0.0, 1.4975377321243286), (0.0, 1.497572422027588), (0.0, 1.4993399381637573), (0.0, 1.5007261037826538), (0.0, 1.5045819282531738), (0.0, 1.5048038959503174), (0.0, 1.5050526857376099), (0.0, 1.5056508779525757), (0.0, 1.5062975883483887), (0.0, 1.5087275505065918), (0.0, 1.5112417936325073), (0.0, 1.5119764804840088), (0.0, 1.5127551555633545), (0.0, 1.513593316078186), (0.0, 1.513654112815857), (0.0, 1.5137906074523926), (0.0, 1.5148301124572754), (0.0, 1.5148606300354004), (0.0, 1.5151088237762451), (0.0, 1.5159252882003784), (0.0, 1.516054630279541), (0.0, 1.5161432027816772), (0.0, 1.5163072347640991), (0.0, 1.5168193578720093), (0.0, 1.5171685218811035), (0.0, 1.518372654914856), (0.0, 1.5188297033309937), (0.0, 1.5191861391067505), (0.0, 1.519203543663025), (0.0, 1.5194531679153442), (0.0, 1.520064353942871), (0.0, 1.5203665494918823), (0.0, 1.521017074584961), (0.0, 1.5213642120361328), (0.0, 1.5214452743530273), (0.0, 1.5222173929214478), (0.0, 1.5225929021835327), (0.0, 1.5226490497589111), (0.0, 1.5228476524353027), (0.0, 1.523339867591858), (0.0, 1.5234116315841675), (0.0, 1.5237393379211426), (0.0, 1.5238044261932373), (0.0, 1.5243216753005981), (0.0, 1.5244134664535522), (0.0, 1.5244489908218384), (0.0, 1.5248808860778809), (0.0, 1.5262037515640259), (0.0, 1.5262839794158936), (0.0, 1.5268456935882568), (0.0, 1.527174711227417), (0.0, 1.5272823572158813), (0.0, 1.5274081230163574), (0.0, 1.5274161100387573), (0.0, 1.5284018516540527), (0.0, 1.5285810232162476), (0.0, 1.5285861492156982), (0.0, 1.5287933349609375), (0.0, 1.5291146039962769), (0.0, 1.5291913747787476), (0.0, 1.529560923576355), (0.0, 1.529844045639038), (0.0, 1.5298914909362793), (0.0, 1.5300099849700928), (0.0, 1.5300511121749878), (0.0, 1.5301421880722046), (0.0, 1.5307183265686035), (0.0, 1.5308817625045776), (0.0, 1.532101035118103), (0.0, 1.5321294069290161), (0.0, 1.5331907272338867), (0.0, 1.533350944519043), (0.0, 1.5334835052490234), (0.0, 1.5334911346435547), (0.0, 1.5336250066757202), (0.0, 1.5338581800460815), (0.0, 1.5345157384872437), (0.0, 1.5351674556732178), (0.0, 1.5357723236083984), (0.0, 1.535927653312683), (0.0, 1.5363560914993286), (0.0, 1.5365056991577148), (0.0, 1.5369669198989868), (0.0, 1.5374473333358765), (0.0, 1.537519097328186), (0.0, 1.5380909442901611), (0.0, 1.5392391681671143), (0.0, 1.5395100116729736), (0.0, 1.5395439863204956), (0.0, 1.5397495031356812), (0.0, 1.5398647785186768), (0.0, 1.5404800176620483), (0.0, 1.5404913425445557), (0.0, 1.5404928922653198), (0.0, 1.5408560037612915), (0.0, 1.5411814451217651), (0.0, 1.5415579080581665), (0.0, 1.5417914390563965), (0.0, 1.5424867868423462), (0.0, 1.5429226160049438), (0.0, 1.5432515144348145), (0.0, 1.5438300371170044), (0.0, 1.5440843105316162), (0.0, 1.5443609952926636), (0.0, 1.5446617603302002), (0.0, 1.5463449954986572), (0.0, 1.546729564666748), (0.0, 1.5469456911087036), (0.0, 1.5478647947311401), (0.0, 1.5485831499099731), (0.0, 1.549267292022705), (0.0, 1.5493627786636353), (0.0, 1.5508567094802856), (0.0, 1.5516853332519531), (0.0, 1.5548137426376343), (0.0, 1.5551244020462036), (0.0, 1.5553927421569824), (0.0, 1.5570510625839233), (0.0, 1.5583198070526123), (0.0, 1.5594189167022705), (0.0, 1.5601806640625)], [(10.087112426757812, 10.235357284545898), (8.28555679321289, 8.432308197021484), (7.723968029022217, 8.441850662231445), (7.332167148590088, 7.466554164886475), (7.017059803009033, 7.1530680656433105), (6.885380744934082, 8.08218002319336), (6.480417728424072, 8.093729972839355), (6.113043785095215, 7.234909534454346), (5.997070789337158, 6.300264835357666), (5.930322170257568, 6.1280517578125), (5.918973445892334, 6.113649368286133), (5.853888511657715, 6.214025497436523), (5.809943199157715, 6.278651237487793), (5.753470420837402, 5.993030548095703), (5.699696063995361, 6.127256393432617), (5.645529270172119, 8.250581741333008), (5.528985023498535, 5.602128505706787), (5.496971130371094, 6.761040687561035), (5.460646152496338, 7.4117841720581055), (5.399545669555664, 6.617547035217285), (5.398415565490723, 6.283242225646973), (5.202592372894287, 8.703499794006348), (5.126249313354492, 5.174883842468262), (5.082752227783203, 5.352011680603027), (5.068402290344238, 5.1901655197143555), (5.0473222732543945, 5.619585037231445), (4.972331523895264, 5.483679294586182), (4.919521808624268, 5.314597129821777), (4.8310322761535645, 4.9734206199646), (4.744785785675049, 6.148080348968506), (4.54712438583374, 4.824485778808594), (4.425833702087402, 4.43479585647583), (4.404596328735352, 5.272364139556885), (4.3790740966796875, 4.6940836906433105), (4.276763916015625, 4.563055992126465), (4.195335865020752, 4.439976692199707), (4.164677143096924, 4.827661514282227), (4.131020545959473, 4.396752834320068), (4.086529731750488, 4.125906467437744), (4.048114776611328, 4.484227657318115), (4.047484874725342, 4.689858436584473), (3.9827775955200195, 4.490955352783203), (3.951214551925659, 4.474514484405518), (3.9396891593933105, 4.844705104827881), (3.9364521503448486, 6.905012607574463), (3.890592336654663, 8.665237426757812)], [(11.346749305725098, 11.707258224487305), (8.740345001220703, 9.475493431091309), (5.484890460968018, 5.516909122467041), (4.743807315826416, 5.105360507965088)]]
[array([[0.        , 1.25816882],
       [0.        , 1.27520502],
       [0.        , 1.30232787],
       [0.        , 1.30875301],
       [0.        , 1.31150377],
       [0.        , 1.31205392],
       [0.        , 1.31302357],
       [0.        , 1.31553555],
       [0.        , 1.31578529],
       [0.        , 1.31579947],
       [0.        , 1.31701398],
       [0.        , 1.31871676],
       [0.        , 1.31910968],
       [0.        , 1.31926906],
       [0.        , 1.31995666],
       [0.        , 1.32051992],
       [0.        , 1.32126892],
       [0.        , 1.32133985],
       [0.        , 1.32134759],
       [0.        , 1.32188237],
       [0.        , 1.32242239],
       [0.        , 1.32270229],
       [0.        , 1.32291484],
       [0.        , 1.32332456],
       [0.        , 1.32343483],
       [0.        , 1.32412267],
       [0.        , 1.32428765],
       [0.        , 1.32528782],
       [0.        , 1.32529676],
       [0.        , 1.32537186],
       [0.        , 1.32549262],
       [0.        , 1.3255192 ],
       [0.        , 1.32553828],
       [0.        , 1.32582974],
       [0.        , 1.3258878 ],
       [0.        , 1.32597315],
       [0.        , 1.32600844],
       [0.        , 1.32649529],
       [0.        , 1.32682109],
       [0.        , 1.32700968],
       [0.        , 1.327317  ],
       [0.        , 1.32753825],
       [0.        , 1.32763112],
       [0.        , 1.32784402],
       [0.        , 1.32804155],
       [0.        , 1.32809305],
       [0.        , 1.32813895],
       [0.        , 1.32821751],
       [0.        , 1.32821906],
       [0.        , 1.3283608 ],
       [0.        , 1.32837451],
       [0.        , 1.32845759],
       [0.        , 1.32853794],
       [0.        , 1.32865703],
       [0.        , 1.32901657],
       [0.        , 1.3294847 ],
       [0.        , 1.32960975],
       [0.        , 1.33006334],
       [0.        , 1.33039021],
       [0.        , 1.33040941],
       [0.        , 1.33049834],
       [0.        , 1.33085477],
       [0.        , 1.33098412],
       [0.        , 1.33110988],
       [0.        , 1.33118033],
       [0.        , 1.33122087],
       [0.        , 1.33127105],
       [0.        , 1.33176816],
       [0.        , 1.33183968],
       [0.        , 1.33190775],
       [0.        , 1.33247507],
       [0.        , 1.33261931],
       [0.        , 1.33270609],
       [0.        , 1.33329475],
       [0.        , 1.33338749],
       [0.        , 1.33340454],
       [0.        , 1.33347249],
       [0.        , 1.33387339],
       [0.        , 1.33389258],
       [0.        , 1.33406484],
       [0.        , 1.33447134],
       [0.        , 1.33473027],
       [0.        , 1.33498573],
       [0.        , 1.33507156],
       [0.        , 1.33517206],
       [0.        , 1.33564973],
       [0.        , 1.33581483],
       [0.        , 1.33585536],
       [0.        , 1.33585727],
       [0.        , 1.33620548],
       [0.        , 1.33623219],
       [0.        , 1.33639276],
       [0.        , 1.33657956],
       [0.        , 1.33715308],
       [0.        , 1.33738172],
       [0.        , 1.33760703],
       [0.        , 1.33772004],
       [0.        , 1.33780396],
       [0.        , 1.33860838],
       [0.        , 1.33888555],
       [0.        , 1.33891046],
       [0.        , 1.33963823],
       [0.        , 1.34011543],
       [0.        , 1.34073937],
       [0.        , 1.34095752],
       [0.        , 1.34110689],
       [0.        , 1.34130943],
       [0.        , 1.34253681],
       [0.        , 1.34380031],
       [0.        , 1.34384453],
       [0.        , 1.34399033],
       [0.        , 1.34468758],
       [0.        , 1.34533632],
       [0.        , 1.34564841],
       [0.        , 1.34635055],
       [0.        , 1.34738946],
       [0.        , 1.347507  ],
       [0.        , 1.34858644],
       [0.        , 1.3487879 ],
       [0.        , 1.35114694],
       [0.        , 1.35287404],
       [0.        , 1.35500455],
       [0.        , 1.42175734],
       [0.        , 1.43048644],
       [0.        , 1.43266022],
       [0.        , 1.43283629],
       [0.        , 1.4341867 ],
       [0.        , 1.4344852 ],
       [0.        , 1.43678129],
       [0.        , 1.43949759],
       [0.        , 1.43962932],
       [0.        , 1.44084728],
       [0.        , 1.44157577],
       [0.        , 1.44245458],
       [0.        , 1.44303179],
       [0.        , 1.44486082],
       [0.        , 1.44501054],
       [0.        , 1.44544053],
       [0.        , 1.44559824],
       [0.        , 1.44613302],
       [0.        , 1.4461596 ],
       [0.        , 1.44628012],
       [0.        , 1.4482379 ],
       [0.        , 1.44828486],
       [0.        , 1.4485774 ],
       [0.        , 1.44868493],
       [0.        , 1.44951999],
       [0.        , 1.44972181],
       [0.        , 1.45021045],
       [0.        , 1.45031536],
       [0.        , 1.45079243],
       [0.        , 1.45174706],
       [0.        , 1.45315945],
       [0.        , 1.45341706],
       [0.        , 1.45362186],
       [0.        , 1.45386827],
       [0.        , 1.45387578],
       [0.        , 1.45407665],
       [0.        , 1.45452309],
       [0.        , 1.45471299],
       [0.        , 1.45482039],
       [0.        , 1.45522964],
       [0.        , 1.45533764],
       [0.        , 1.45578086],
       [0.        , 1.4558481 ],
       [0.        , 1.45636153],
       [0.        , 1.45640254],
       [0.        , 1.45685446],
       [0.        , 1.45708168],
       [0.        , 1.45721972],
       [0.        , 1.45730925],
       [0.        , 1.45770264],
       [0.        , 1.45814717],
       [0.        , 1.45830345],
       [0.        , 1.45885229],
       [0.        , 1.45889068],
       [0.        , 1.45892298],
       [0.        , 1.4591608 ],
       [0.        , 1.45955896],
       [0.        , 1.45980859],
       [0.        , 1.45981407],
       [0.        , 1.46081996],
       [0.        , 1.46082151],
       [0.        , 1.46177006],
       [0.        , 1.46196544],
       [0.        , 1.4621799 ],
       [0.        , 1.4622364 ],
       [0.        , 1.46281576],
       [0.        , 1.46293235],
       [0.        , 1.4629426 ],
       [0.        , 1.46330428],
       [0.        , 1.46336937],
       [0.        , 1.4640007 ],
       [0.        , 1.46454084],
       [0.        , 1.46462405],
       [0.        , 1.46478701],
       [0.        , 1.46484828],
       [0.        , 1.46501255],
       [0.        , 1.46502149],
       [0.        , 1.46608698],
       [0.        , 1.46655762],
       [0.        , 1.46681154],
       [0.        , 1.46737301],
       [0.        , 1.46753001],
       [0.        , 1.46765709],
       [0.        , 1.46818924],
       [0.        , 1.46826088],
       [0.        , 1.46826565],
       [0.        , 1.46881366],
       [0.        , 1.46927166],
       [0.        , 1.46969438],
       [0.        , 1.47068453],
       [0.        , 1.47078335],
       [0.        , 1.47078538],
       [0.        , 1.47091568],
       [0.        , 1.47115934],
       [0.        , 1.47123146],
       [0.        , 1.47173679],
       [0.        , 1.47201872],
       [0.        , 1.47225237],
       [0.        , 1.47241592],
       [0.        , 1.472965  ],
       [0.        , 1.47417796],
       [0.        , 1.47482526],
       [0.        , 1.47544324],
       [0.        , 1.47592461],
       [0.        , 1.47740769],
       [0.        , 1.47810972],
       [0.        , 1.47824335],
       [0.        , 1.47851729],
       [0.        , 1.47862506],
       [0.        , 1.47936118],
       [0.        , 1.47980893],
       [0.        , 1.48073709],
       [0.        , 1.48084044],
       [0.        , 1.48085713],
       [0.        , 1.48097384],
       [0.        , 1.48138189],
       [0.        , 1.4864006 ],
       [0.        , 1.48667228],
       [0.        , 1.48728323],
       [0.        , 1.48737049],
       [0.        , 1.49083054],
       [0.        , 1.49306715],
       [0.        , 1.49513936],
       [0.        , 1.49689078],
       [0.        , 1.49741495],
       [0.        , 1.49753773],
       [0.        , 1.49757242],
       [0.        , 1.49933994],
       [0.        , 1.5007261 ],
       [0.        , 1.50458193],
       [0.        , 1.5048039 ],
       [0.        , 1.50505269],
       [0.        , 1.50565088],
       [0.        , 1.50629759],
       [0.        , 1.50872755],
       [0.        , 1.51124179],
       [0.        , 1.51197648],
       [0.        , 1.51275516],
       [0.        , 1.51359332],
       [0.        , 1.51365411],
       [0.        , 1.51379061],
       [0.        , 1.51483011],
       [0.        , 1.51486063],
       [0.        , 1.51510882],
       [0.        , 1.51592529],
       [0.        , 1.51605463],
       [0.        , 1.5161432 ],
       [0.        , 1.51630723],
       [0.        , 1.51681936],
       [0.        , 1.51716852],
       [0.        , 1.51837265],
       [0.        , 1.5188297 ],
       [0.        , 1.51918614],
       [0.        , 1.51920354],
       [0.        , 1.51945317],
       [0.        , 1.52006435],
       [0.        , 1.52036655],
       [0.        , 1.52101707],
       [0.        , 1.52136421],
       [0.        , 1.52144527],
       [0.        , 1.52221739],
       [0.        , 1.5225929 ],
       [0.        , 1.52264905],
       [0.        , 1.52284765],
       [0.        , 1.52333987],
       [0.        , 1.52341163],
       [0.        , 1.52373934],
       [0.        , 1.52380443],
       [0.        , 1.52432168],
       [0.        , 1.52441347],
       [0.        , 1.52444899],
       [0.        , 1.52488089],
       [0.        , 1.52620375],
       [0.        , 1.52628398],
       [0.        , 1.52684569],
       [0.        , 1.52717471],
       [0.        , 1.52728236],
       [0.        , 1.52740812],
       [0.        , 1.52741611],
       [0.        , 1.52840185],
       [0.        , 1.52858102],
       [0.        , 1.52858615],
       [0.        , 1.52879333],
       [0.        , 1.5291146 ],
       [0.        , 1.52919137],
       [0.        , 1.52956092],
       [0.        , 1.52984405],
       [0.        , 1.52989149],
       [0.        , 1.53000998],
       [0.        , 1.53005111],
       [0.        , 1.53014219],
       [0.        , 1.53071833],
       [0.        , 1.53088176],
       [0.        , 1.53210104],
       [0.        , 1.53212941],
       [0.        , 1.53319073],
       [0.        , 1.53335094],
       [0.        , 1.53348351],
       [0.        , 1.53349113],
       [0.        , 1.53362501],
       [0.        , 1.53385818],
       [0.        , 1.53451574],
       [0.        , 1.53516746],
       [0.        , 1.53577232],
       [0.        , 1.53592765],
       [0.        , 1.53635609],
       [0.        , 1.5365057 ],
       [0.        , 1.53696692],
       [0.        , 1.53744733],
       [0.        , 1.5375191 ],
       [0.        , 1.53809094],
       [0.        , 1.53923917],
       [0.        , 1.53951001],
       [0.        , 1.53954399],
       [0.        , 1.5397495 ],
       [0.        , 1.53986478],
       [0.        , 1.54048002],
       [0.        , 1.54049134],
       [0.        , 1.54049289],
       [0.        , 1.540856  ],
       [0.        , 1.54118145],
       [0.        , 1.54155791],
       [0.        , 1.54179144],
       [0.        , 1.54248679],
       [0.        , 1.54292262],
       [0.        , 1.54325151],
       [0.        , 1.54383004],
       [0.        , 1.54408431],
       [0.        , 1.544361  ],
       [0.        , 1.54466176],
       [0.        , 1.546345  ],
       [0.        , 1.54672956],
       [0.        , 1.54694569],
       [0.        , 1.54786479],
       [0.        , 1.54858315],
       [0.        , 1.54926729],
       [0.        , 1.54936278],
       [0.        , 1.55085671],
       [0.        , 1.55168533],
       [0.        , 1.55481374],
       [0.        , 1.5551244 ],
       [0.        , 1.55539274],
       [0.        , 1.55705106],
       [0.        , 1.55831981],
       [0.        , 1.55941892],
       [0.        , 1.56018066]]), array([[10.08711243, 10.23535728],
       [ 8.28555679,  8.4323082 ],
       [ 7.72396803,  8.44185066],
       [ 7.33216715,  7.46655416],
       [ 7.0170598 ,  7.15306807],
       [ 6.88538074,  8.08218002],
       [ 6.48041773,  8.09372997],
       [ 6.11304379,  7.23490953],
       [ 5.99707079,  6.30026484],
       [ 5.93032217,  6.12805176],
       [ 5.91897345,  6.11364937],
       [ 5.85388851,  6.2140255 ],
       [ 5.8099432 ,  6.27865124],
       [ 5.75347042,  5.99303055],
       [ 5.69969606,  6.12725639],
       [ 5.64552927,  8.25058174],
       [ 5.52898502,  5.60212851],
       [ 5.49697113,  6.76104069],
       [ 5.46064615,  7.41178417],
       [ 5.39954567,  6.61754704],
       [ 5.39841557,  6.28324223],
       [ 5.20259237,  8.70349979],
       [ 5.12624931,  5.17488384],
       [ 5.08275223,  5.35201168],
       [ 5.06840229,  5.19016552],
       [ 5.04732227,  5.61958504],
       [ 4.97233152,  5.48367929],
       [ 4.91952181,  5.31459713],
       [ 4.83103228,  4.97342062],
       [ 4.74478579,  6.14808035],
       [ 4.54712439,  4.82448578],
       [ 4.4258337 ,  4.43479586],
       [ 4.40459633,  5.27236414],
       [ 4.3790741 ,  4.69408369],
       [ 4.27676392,  4.56305599],
       [ 4.19533587,  4.43997669],
       [ 4.16467714,  4.82766151],
       [ 4.13102055,  4.39675283],
       [ 4.08652973,  4.12590647],
       [ 4.04811478,  4.48422766],
       [ 4.04748487,  4.68985844],
       [ 3.9827776 ,  4.49095535],
       [ 3.95121455,  4.47451448],
       [ 3.93968916,  4.8447051 ],
       [ 3.93645215,  6.90501261],
       [ 3.89059234,  8.66523743]]), array([[11.34674931, 11.70725822],
       [ 8.740345  ,  9.47549343],
       [ 5.48489046,  5.51690912],
       [ 4.74380732,  5.10536051]])]2024-03-06 17:55:37.113279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KQU ph vector generated, counter: 142
2024-03-06 17:55:41.126267: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:41.169520: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:42.039952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KS1 ph vector generated, counter: 143
2024-03-06 17:55:45.365553: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:45.408840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:46.451159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KSF ph vector generated, counter: 144
2024-03-06 17:55:49.460470: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:49.515977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:50.579680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KSQ ph vector generated, counter: 145
2024-03-06 17:55:53.684038: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:55:53.727274: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:55:54.599174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 0.825487 ), (0., 0.9718665), (0., 1.1548285), (0., 1.1788377),
       (0., 1.2608722), (0., 1.3185337), (0., 1.3256897), (0., 1.3264949),
       (0., 1.3270551), (0., 1.3271542), (0., 1.3274012), (0., 1.3274041),
       (0., 1.3274705), (0., 1.3275341), (0., 1.327585 ), (0., 1.3277746),
       (0., 1.3278248), (0., 1.3278263), (0., 1.3278773), (0., 1.327915 ),
       (0., 1.3279334), (0., 1.3280319), (0., 1.3280423), (0., 1.3281376),
       (0., 1.3281636), (0., 1.3282278), (0., 1.3282592), (0., 1.3282677),
       (0., 1.3282707), (0., 1.328348 ), (0., 1.3283725), (0., 1.3284154),
       (0., 1.3284402), (0., 1.3284477), (0., 1.3284564), (0., 1.3285127),
       (0., 1.3285645), (0., 1.3286299), (0., 1.32865  ), (0., 1.3286514),
       (0., 1.328662 ), (0., 1.3286669), (0., 1.3287381), (0., 1.3287504),
       (0., 1.3287644), (0., 1.3288078), (0., 1.3289208), (0., 1.3289758),
       (0., 1.3290303), (0., 1.3290678), (0., 1.3290801), (0., 1.3291082),
       (0., 1.3291333), (0., 1.3292115), (0., 1.3293085), (0., 1.3293438),
       (0., 1.3294168), (0., 1.3295664), (0., 1.3296094), (0., 1.3297294),
       (0., 1.3297857), (0., 1.3298725), (0., 1.3298885), (0., 1.329905 ),
       (0., 1.3301146), (0., 1.3303562), (0., 1.3303915), (0., 1.3305176),
       (0., 1.330771 ), (0., 1.3307939), (0., 1.330895 ), (0., 1.3310224),
       (0., 1.3312153), (0., 1.3319111), (0., 1.3322   ), (0., 1.3322557),
       (0., 1.3346993), (0., 1.339074 ), (0., 1.377427 ), (0., 1.381348 ),
       (0., 1.4072621), (0., 1.4075582), (0., 1.4341056), (0., 1.4462842),
       (0., 1.4486444), (0., 1.4486815), (0., 1.4496948), (0., 1.4503282),
       (0., 1.4503609), (0., 1.4509116), (0., 1.4509821), (0., 1.4511144),
       (0., 1.4513947), (0., 1.4527123), (0., 1.4532806), (0., 1.4534584),
       (0., 1.4536455), (0., 1.4537197), (0., 1.4540173), (0., 1.454253 ),
       (0., 1.4545207), (0., 1.4546132), (0., 1.455787 ), (0., 1.4560497),
       (0., 1.4563359), (0., 1.4563751), (0., 1.4565064), (0., 1.456565 ),
       (0., 1.456635 ), (0., 1.4566523), (0., 1.4566996), (0., 1.4568655),
       (0., 1.457039 ), (0., 1.4571238), (0., 1.457263 ), (0., 1.457314 ),
       (0., 1.457354 ), (0., 1.457388 ), (0., 1.4577221), (0., 1.457791 ),
       (0., 1.4579755), (0., 1.4579837), (0., 1.4581095), (0., 1.4581186),
       (0., 1.4581811), (0., 1.4581867), (0., 1.4582517), (0., 1.4582574),
       (0., 1.4583467), (0., 1.4583733), (0., 1.458384 ), (0., 1.45851  ),
       (0., 1.4585108), (0., 1.4585893), (0., 1.4586624), (0., 1.4588135),
       (0., 1.4589047), (0., 1.4589094), (0., 1.4589312), (0., 1.4592909),
       (0., 1.4593184), (0., 1.4593385), (0., 1.459735 ), (0., 1.4599323),
       (0., 1.4602237), (0., 1.4603442), (0., 1.4603842), (0., 1.4605277),
       (0., 1.4605873), (0., 1.4612409), (0., 1.4614199), (0., 1.4616692),
       (0., 1.4616878), (0., 1.4625138), (0., 1.4644939), (0., 1.4657685),
       (0., 1.4666575), (0., 1.4667759), (0., 1.4670061), (0., 1.4675026),
       (0., 1.5143999), (0., 1.5144198), (0., 1.5153886), (0., 1.5156097),
       (0., 1.5158327), (0., 1.5160826), (0., 1.5162092), (0., 1.5165312),
       (0., 1.5166572), (0., 1.5168871), (0., 1.5182503), (0., 1.5198319),
       (0., 1.5211755), (0., 1.5216285), (0., 1.5218211), (0., 1.5222486),
       (0., 1.5225487), (0., 1.5228174), (0., 1.5229073), (0., 1.5229336),
       (0., 1.5229686), (0., 1.5230391), (0., 1.5230744), (0., 1.523248 ),
       (0., 1.523473 ), (0., 1.5235797), (0., 1.5235847), (0., 1.5236635),
       (0., 1.5236636), (0., 1.5237536), (0., 1.5238998), (0., 1.5240765),
       (0., 1.5240822), (0., 1.5241518), (0., 1.5241699), (0., 1.5242392),
       (0., 1.5242819), (0., 1.5243341), (0., 1.5244517), (0., 1.5244824),
       (0., 1.5245649), (0., 1.524824 ), (0., 1.525087 ), (0., 1.5251311),
       (0., 1.525177 ), (0., 1.5253184), (0., 1.5253766), (0., 1.5254734),
       (0., 1.5256604), (0., 1.5257777), (0., 1.5258455), (0., 1.5261434),
       (0., 1.5263383), (0., 1.5263658), (0., 1.526538 ), (0., 1.526554 ),
       (0., 1.5267155), (0., 1.5269175), (0., 1.5270219), (0., 1.5270842),
       (0., 1.527163 ), (0., 1.5276505), (0., 1.5277082), (0., 1.5278567),
       (0., 1.5281457), (0., 1.5283782), (0., 1.5286274), (0., 1.5286716),
       (0., 1.5294057), (0., 1.5299481), (0., 1.5301011), (0., 1.53011  ),
       (0., 1.5314978), (0., 1.53768  ), (0., 1.5921471), (0., 1.608501 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(7.1898837, 7.480197 ), (5.803499 , 5.979853 ),
       (5.200275 , 5.236191 ), (4.99271  , 5.5452194),
       (4.943008 , 5.0778527), (4.444429 , 4.5645146),
       (4.335855 , 4.3783097), (4.321227 , 4.4272013),
       (4.3030453, 4.406846 ), (4.188307 , 4.311279 ),
       (4.1878858, 4.5542827), (4.179015 , 4.201472 ),
       (4.1434345, 4.175793 ), (4.1306243, 4.5149903),
       (4.116207 , 4.42331  ), (4.090945 , 4.531174 ),
       (4.083864 , 4.2683578), (4.082156 , 4.5371284),
       (4.080911 , 4.624409 ), (4.076969 , 4.439896 ),
       (4.06594  , 4.368909 ), (4.0559573, 4.5864277),
       (4.048538 , 4.6332645), (4.0456004, 4.6684475),
       (4.038998 , 4.214436 ), (4.0274644, 6.090992 ),
       (4.0219355, 4.422209 ), (4.021681 , 4.373593 ),
       (4.01181  , 4.587589 ), (4.0066576, 8.270079 ),
       (3.994297 , 4.7803445), (3.9663239, 4.522124 ),
       (3.961237 , 4.691766 ), (3.9586604, 4.091778 ),
       (3.9421098, 4.6622663), (3.9198701, 4.225891 ),
       (3.900976 , 4.873947 ), (3.8890948, 4.273861 ),
       (3.844121 , 4.1566277), (3.8073761, 4.8733397),
       (3.7595341, 4.1973658), (3.7532203, 4.62023  ),
       (3.751947 , 8.002538 ), (3.0592446, 3.6208305),
       (2.6881394, 2.7334564), (1.6586014, 2.1698284)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.599377 , 9.771576), (9.5890255, 9.729695),
       (9.403804 , 9.916773), (8.213834 , 8.241265)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 0.8254870176315308), (0.0, 0.9718664884567261), (0.0, 1.1548285484313965), (0.0, 1.1788376569747925), (0.0, 1.2608722448349), (0.0, 1.3185336589813232), (0.0, 1.325689673423767), (0.0, 1.3264949321746826), (0.0, 1.3270550966262817), (0.0, 1.3271541595458984), (0.0, 1.3274011611938477), (0.0, 1.3274041414260864), (0.0, 1.3274705410003662), (0.0, 1.3275340795516968), (0.0, 1.327584981918335), (0.0, 1.3277746438980103), (0.0, 1.3278248310089111), (0.0, 1.3278262615203857), (0.0, 1.3278772830963135), (0.0, 1.3279149532318115), (0.0, 1.327933430671692), (0.0, 1.3280318975448608), (0.0, 1.3280422687530518), (0.0, 1.3281376361846924), (0.0, 1.3281636238098145), (0.0, 1.3282277584075928), (0.0, 1.3282592296600342), (0.0, 1.3282676935195923), (0.0, 1.328270673751831), (0.0, 1.3283480405807495), (0.0, 1.3283724784851074), (0.0, 1.3284153938293457), (0.0, 1.3284401893615723), (0.0, 1.328447699546814), (0.0, 1.3284564018249512), (0.0, 1.3285126686096191), (0.0, 1.3285645246505737), (0.0, 1.3286298513412476), (0.0, 1.3286499977111816), (0.0, 1.3286514282226562), (0.0, 1.3286620378494263), (0.0, 1.3286669254302979), (0.0, 1.3287380933761597), (0.0, 1.3287503719329834), (0.0, 1.3287644386291504), (0.0, 1.3288078308105469), (0.0, 1.328920841217041), (0.0, 1.328975796699524), (0.0, 1.3290302753448486), (0.0, 1.3290678262710571), (0.0, 1.3290801048278809), (0.0, 1.3291082382202148), (0.0, 1.3291332721710205), (0.0, 1.3292114734649658), (0.0, 1.3293085098266602), (0.0, 1.3293437957763672), (0.0, 1.3294167518615723), (0.0, 1.3295663595199585), (0.0, 1.3296093940734863), (0.0, 1.329729437828064), (0.0, 1.329785704612732), (0.0, 1.329872488975525), (0.0, 1.3298884630203247), (0.0, 1.3299050331115723), (0.0, 1.3301146030426025), (0.0, 1.330356240272522), (0.0, 1.330391526222229), (0.0, 1.3305176496505737), (0.0, 1.3307709693908691), (0.0, 1.330793857574463), (0.0, 1.330894947052002), (0.0, 1.3310223817825317), (0.0, 1.331215262413025), (0.0, 1.3319110870361328), (0.0, 1.332200050354004), (0.0, 1.3322557210922241), (0.0, 1.334699273109436), (0.0, 1.3390740156173706), (0.0, 1.3774269819259644), (0.0, 1.3813480138778687), (0.0, 1.4072620868682861), (0.0, 1.4075582027435303), (0.0, 1.434105634689331), (0.0, 1.4462841749191284), (0.0, 1.4486443996429443), (0.0, 1.4486814737319946), (0.0, 1.4496947526931763), (0.0, 1.4503282308578491), (0.0, 1.450360894203186), (0.0, 1.4509116411209106), (0.0, 1.4509820938110352), (0.0, 1.4511144161224365), (0.0, 1.4513946771621704), (0.0, 1.4527122974395752), (0.0, 1.4532805681228638), (0.0, 1.4534584283828735), (0.0, 1.4536454677581787), (0.0, 1.4537197351455688), (0.0, 1.4540172815322876), (0.0, 1.4542529582977295), (0.0, 1.4545207023620605), (0.0, 1.454613208770752), (0.0, 1.455786943435669), (0.0, 1.4560496807098389), (0.0, 1.4563359022140503), (0.0, 1.4563751220703125), (0.0, 1.456506371498108), (0.0, 1.456565022468567), (0.0, 1.4566349983215332), (0.0, 1.456652283668518), (0.0, 1.4566996097564697), (0.0, 1.4568655490875244), (0.0, 1.4570389986038208), (0.0, 1.4571237564086914), (0.0, 1.4572629928588867), (0.0, 1.4573140144348145), (0.0, 1.457353949546814), (0.0, 1.4573880434036255), (0.0, 1.4577220678329468), (0.0, 1.4577909708023071), (0.0, 1.4579755067825317), (0.0, 1.4579837322235107), (0.0, 1.4581094980239868), (0.0, 1.4581185579299927), (0.0, 1.4581811428070068), (0.0, 1.4581867456436157), (0.0, 1.458251714706421), (0.0, 1.4582574367523193), (0.0, 1.4583467245101929), (0.0, 1.4583733081817627), (0.0, 1.4583840370178223), (0.0, 1.4585100412368774), (0.0, 1.4585107564926147), (0.0, 1.4585893154144287), (0.0, 1.4586623907089233), (0.0, 1.4588135480880737), (0.0, 1.45890474319458), (0.0, 1.4589093923568726), (0.0, 1.4589312076568604), (0.0, 1.459290862083435), (0.0, 1.4593183994293213), (0.0, 1.4593385457992554), (0.0, 1.4597350358963013), (0.0, 1.4599323272705078), (0.0, 1.46022367477417), (0.0, 1.4603441953659058), (0.0, 1.4603842496871948), (0.0, 1.4605276584625244), (0.0, 1.4605872631072998), (0.0, 1.4612408876419067), (0.0, 1.461419939994812), (0.0, 1.4616692066192627), (0.0, 1.4616878032684326), (0.0, 1.46251380443573), (0.0, 1.4644938707351685), (0.0, 1.4657684564590454), (0.0, 1.4666575193405151), (0.0, 1.466775894165039), (0.0, 1.4670060873031616), (0.0, 1.4675025939941406), (0.0, 1.5143998861312866), (0.0, 1.5144197940826416), (0.0, 1.5153886079788208), (0.0, 1.5156097412109375), (0.0, 1.5158326625823975), (0.0, 1.5160826444625854), (0.0, 1.5162092447280884), (0.0, 1.516531229019165), (0.0, 1.5166572332382202), (0.0, 1.5168870687484741), (0.0, 1.5182503461837769), (0.0, 1.519831895828247), (0.0, 1.521175503730774), (0.0, 1.521628499031067), (0.0, 1.521821141242981), (0.0, 1.52224862575531), (0.0, 1.5225486755371094), (0.0, 1.5228173732757568), (0.0, 1.5229072570800781), (0.0, 1.5229336023330688), (0.0, 1.5229686498641968), (0.0, 1.5230391025543213), (0.0, 1.5230743885040283), (0.0, 1.5232479572296143), (0.0, 1.5234730243682861), (0.0, 1.523579716682434), (0.0, 1.5235847234725952), (0.0, 1.5236635208129883), (0.0, 1.5236636400222778), (0.0, 1.5237536430358887), (0.0, 1.523899793624878), (0.0, 1.5240764617919922), (0.0, 1.5240821838378906), (0.0, 1.5241518020629883), (0.0, 1.524169921875), (0.0, 1.524239182472229), (0.0, 1.5242818593978882), (0.0, 1.5243340730667114), (0.0, 1.524451732635498), (0.0, 1.5244823694229126), (0.0, 1.5245648622512817), (0.0, 1.5248240232467651), (0.0, 1.5250869989395142), (0.0, 1.525131106376648), (0.0, 1.525177001953125), (0.0, 1.5253183841705322), (0.0, 1.525376558303833), (0.0, 1.5254733562469482), (0.0, 1.5256603956222534), (0.0, 1.5257776975631714), (0.0, 1.5258455276489258), (0.0, 1.5261434316635132), (0.0, 1.5263383388519287), (0.0, 1.5263657569885254), (0.0, 1.5265380144119263), (0.0, 1.526553988456726), (0.0, 1.5267155170440674), (0.0, 1.5269174575805664), (0.0, 1.527021884918213), (0.0, 1.527084231376648), (0.0, 1.527163028717041), (0.0, 1.5276504755020142), (0.0, 1.5277081727981567), (0.0, 1.527856707572937), (0.0, 1.528145670890808), (0.0, 1.5283782482147217), (0.0, 1.5286273956298828), (0.0, 1.5286716222763062), (0.0, 1.5294057130813599), (0.0, 1.529948115348816), (0.0, 1.5301010608673096), (0.0, 1.5301100015640259), (0.0, 1.531497836112976), (0.0, 1.5376800298690796), (0.0, 1.5921471118927002), (0.0, 1.6085009574890137)], [(7.189883708953857, 7.480196952819824), (5.803499221801758, 5.97985315322876), (5.20027494430542, 5.2361907958984375), (4.992710113525391, 5.545219421386719), (4.943007946014404, 5.077852725982666), (4.44442892074585, 4.564514636993408), (4.335855007171631, 4.378309726715088), (4.321227073669434, 4.427201271057129), (4.303045272827148, 4.406846046447754), (4.18830680847168, 4.311278820037842), (4.187885761260986, 4.5542826652526855), (4.179015159606934, 4.20147180557251), (4.143434524536133, 4.175793170928955), (4.130624294281006, 4.514990329742432), (4.116207122802734, 4.423309803009033), (4.090944766998291, 4.531174182891846), (4.083864212036133, 4.268357753753662), (4.082156181335449, 4.537128448486328), (4.080911159515381, 4.624409198760986), (4.076969146728516, 4.439896106719971), (4.065939903259277, 4.368908882141113), (4.055957317352295, 4.586427688598633), (4.0485382080078125, 4.633264541625977), (4.045600414276123, 4.668447494506836), (4.038998126983643, 4.214436054229736), (4.027464389801025, 6.090991973876953), (4.02193546295166, 4.422208786010742), (4.02168083190918, 4.373592853546143), (4.011809825897217, 4.587588787078857), (4.006657600402832, 8.270078659057617), (3.9942970275878906, 4.780344486236572), (3.9663238525390625, 4.52212381362915), (3.9612369537353516, 4.691765785217285), (3.958660364151001, 4.091777801513672), (3.9421098232269287, 4.662266254425049), (3.919870138168335, 4.22589111328125), (3.9009759426116943, 4.8739471435546875), (3.889094829559326, 4.273860931396484), (3.844120979309082, 4.156627655029297), (3.8073761463165283, 4.873339653015137), (3.7595341205596924, 4.197365760803223), (3.753220319747925, 4.620230197906494), (3.7519469261169434, 8.002537727355957), (3.0592446327209473, 3.620830535888672), (2.6881394386291504, 2.7334563732147217), (1.6586014032363892, 2.169828414916992)], [(9.599376678466797, 9.771575927734375), (9.589025497436523, 9.729695320129395), (9.403803825378418, 9.916772842407227), (8.213833808898926, 8.241265296936035)]]
[array([[0.        , 0.82548702],
       [0.        , 0.97186649],
       [0.        , 1.15482855],
       [0.        , 1.17883766],
       [0.        , 1.26087224],
       [0.        , 1.31853366],
       [0.        , 1.32568967],
       [0.        , 1.32649493],
       [0.        , 1.3270551 ],
       [0.        , 1.32715416],
       [0.        , 1.32740116],
       [0.        , 1.32740414],
       [0.        , 1.32747054],
       [0.        , 1.32753408],
       [0.        , 1.32758498],
       [0.        , 1.32777464],
       [0.        , 1.32782483],
       [0.        , 1.32782626],
       [0.        , 1.32787728],
       [0.        , 1.32791495],
       [0.        , 1.32793343],
       [0.        , 1.3280319 ],
       [0.        , 1.32804227],
       [0.        , 1.32813764],
       [0.        , 1.32816362],
       [0.        , 1.32822776],
       [0.        , 1.32825923],
       [0.        , 1.32826769],
       [0.        , 1.32827067],
       [0.        , 1.32834804],
       [0.        , 1.32837248],
       [0.        , 1.32841539],
       [0.        , 1.32844019],
       [0.        , 1.3284477 ],
       [0.        , 1.3284564 ],
       [0.        , 1.32851267],
       [0.        , 1.32856452],
       [0.        , 1.32862985],
       [0.        , 1.32865   ],
       [0.        , 1.32865143],
       [0.        , 1.32866204],
       [0.        , 1.32866693],
       [0.        , 1.32873809],
       [0.        , 1.32875037],
       [0.        , 1.32876444],
       [0.        , 1.32880783],
       [0.        , 1.32892084],
       [0.        , 1.3289758 ],
       [0.        , 1.32903028],
       [0.        , 1.32906783],
       [0.        , 1.3290801 ],
       [0.        , 1.32910824],
       [0.        , 1.32913327],
       [0.        , 1.32921147],
       [0.        , 1.32930851],
       [0.        , 1.3293438 ],
       [0.        , 1.32941675],
       [0.        , 1.32956636],
       [0.        , 1.32960939],
       [0.        , 1.32972944],
       [0.        , 1.3297857 ],
       [0.        , 1.32987249],
       [0.        , 1.32988846],
       [0.        , 1.32990503],
       [0.        , 1.3301146 ],
       [0.        , 1.33035624],
       [0.        , 1.33039153],
       [0.        , 1.33051765],
       [0.        , 1.33077097],
       [0.        , 1.33079386],
       [0.        , 1.33089495],
       [0.        , 1.33102238],
       [0.        , 1.33121526],
       [0.        , 1.33191109],
       [0.        , 1.33220005],
       [0.        , 1.33225572],
       [0.        , 1.33469927],
       [0.        , 1.33907402],
       [0.        , 1.37742698],
       [0.        , 1.38134801],
       [0.        , 1.40726209],
       [0.        , 1.4075582 ],
       [0.        , 1.43410563],
       [0.        , 1.44628417],
       [0.        , 1.4486444 ],
       [0.        , 1.44868147],
       [0.        , 1.44969475],
       [0.        , 1.45032823],
       [0.        , 1.45036089],
       [0.        , 1.45091164],
       [0.        , 1.45098209],
       [0.        , 1.45111442],
       [0.        , 1.45139468],
       [0.        , 1.4527123 ],
       [0.        , 1.45328057],
       [0.        , 1.45345843],
       [0.        , 1.45364547],
       [0.        , 1.45371974],
       [0.        , 1.45401728],
       [0.        , 1.45425296],
       [0.        , 1.4545207 ],
       [0.        , 1.45461321],
       [0.        , 1.45578694],
       [0.        , 1.45604968],
       [0.        , 1.4563359 ],
       [0.        , 1.45637512],
       [0.        , 1.45650637],
       [0.        , 1.45656502],
       [0.        , 1.456635  ],
       [0.        , 1.45665228],
       [0.        , 1.45669961],
       [0.        , 1.45686555],
       [0.        , 1.457039  ],
       [0.        , 1.45712376],
       [0.        , 1.45726299],
       [0.        , 1.45731401],
       [0.        , 1.45735395],
       [0.        , 1.45738804],
       [0.        , 1.45772207],
       [0.        , 1.45779097],
       [0.        , 1.45797551],
       [0.        , 1.45798373],
       [0.        , 1.4581095 ],
       [0.        , 1.45811856],
       [0.        , 1.45818114],
       [0.        , 1.45818675],
       [0.        , 1.45825171],
       [0.        , 1.45825744],
       [0.        , 1.45834672],
       [0.        , 1.45837331],
       [0.        , 1.45838404],
       [0.        , 1.45851004],
       [0.        , 1.45851076],
       [0.        , 1.45858932],
       [0.        , 1.45866239],
       [0.        , 1.45881355],
       [0.        , 1.45890474],
       [0.        , 1.45890939],
       [0.        , 1.45893121],
       [0.        , 1.45929086],
       [0.        , 1.4593184 ],
       [0.        , 1.45933855],
       [0.        , 1.45973504],
       [0.        , 1.45993233],
       [0.        , 1.46022367],
       [0.        , 1.4603442 ],
       [0.        , 1.46038425],
       [0.        , 1.46052766],
       [0.        , 1.46058726],
       [0.        , 1.46124089],
       [0.        , 1.46141994],
       [0.        , 1.46166921],
       [0.        , 1.4616878 ],
       [0.        , 1.4625138 ],
       [0.        , 1.46449387],
       [0.        , 1.46576846],
       [0.        , 1.46665752],
       [0.        , 1.46677589],
       [0.        , 1.46700609],
       [0.        , 1.46750259],
       [0.        , 1.51439989],
       [0.        , 1.51441979],
       [0.        , 1.51538861],
       [0.        , 1.51560974],
       [0.        , 1.51583266],
       [0.        , 1.51608264],
       [0.        , 1.51620924],
       [0.        , 1.51653123],
       [0.        , 1.51665723],
       [0.        , 1.51688707],
       [0.        , 1.51825035],
       [0.        , 1.5198319 ],
       [0.        , 1.5211755 ],
       [0.        , 1.5216285 ],
       [0.        , 1.52182114],
       [0.        , 1.52224863],
       [0.        , 1.52254868],
       [0.        , 1.52281737],
       [0.        , 1.52290726],
       [0.        , 1.5229336 ],
       [0.        , 1.52296865],
       [0.        , 1.5230391 ],
       [0.        , 1.52307439],
       [0.        , 1.52324796],
       [0.        , 1.52347302],
       [0.        , 1.52357972],
       [0.        , 1.52358472],
       [0.        , 1.52366352],
       [0.        , 1.52366364],
       [0.        , 1.52375364],
       [0.        , 1.52389979],
       [0.        , 1.52407646],
       [0.        , 1.52408218],
       [0.        , 1.5241518 ],
       [0.        , 1.52416992],
       [0.        , 1.52423918],
       [0.        , 1.52428186],
       [0.        , 1.52433407],
       [0.        , 1.52445173],
       [0.        , 1.52448237],
       [0.        , 1.52456486],
       [0.        , 1.52482402],
       [0.        , 1.525087  ],
       [0.        , 1.52513111],
       [0.        , 1.525177  ],
       [0.        , 1.52531838],
       [0.        , 1.52537656],
       [0.        , 1.52547336],
       [0.        , 1.5256604 ],
       [0.        , 1.5257777 ],
       [0.        , 1.52584553],
       [0.        , 1.52614343],
       [0.        , 1.52633834],
       [0.        , 1.52636576],
       [0.        , 1.52653801],
       [0.        , 1.52655399],
       [0.        , 1.52671552],
       [0.        , 1.52691746],
       [0.        , 1.52702188],
       [0.        , 1.52708423],
       [0.        , 1.52716303],
       [0.        , 1.52765048],
       [0.        , 1.52770817],
       [0.        , 1.52785671],
       [0.        , 1.52814567],
       [0.        , 1.52837825],
       [0.        , 1.5286274 ],
       [0.        , 1.52867162],
       [0.        , 1.52940571],
       [0.        , 1.52994812],
       [0.        , 1.53010106],
       [0.        , 1.53011   ],
       [0.        , 1.53149784],
       [0.        , 1.53768003],
       [0.        , 1.59214711],
       [0.        , 1.60850096]]), array([[7.18988371, 7.48019695],
       [5.80349922, 5.97985315],
       [5.20027494, 5.2361908 ],
       [4.99271011, 5.54521942],
       [4.94300795, 5.07785273],
       [4.44442892, 4.56451464],
       [4.33585501, 4.37830973],
       [4.32122707, 4.42720127],
       [4.30304527, 4.40684605],
       [4.18830681, 4.31127882],
       [4.18788576, 4.55428267],
       [4.17901516, 4.20147181],
       [4.14343452, 4.17579317],
       [4.13062429, 4.51499033],
       [4.11620712, 4.4233098 ],
       [4.09094477, 4.53117418],
       [4.08386421, 4.26835775],
       [4.08215618, 4.53712845],
       [4.08091116, 4.6244092 ],
       [4.07696915, 4.43989611],
       [4.0659399 , 4.36890888],
       [4.05595732, 4.58642769],
       [4.04853821, 4.63326454],
       [4.04560041, 4.66844749],
       [4.03899813, 4.21443605],
       [4.02746439, 6.09099197],
       [4.02193546, 4.42220879],
       [4.02168083, 4.37359285],
       [4.01180983, 4.58758879],
       [4.0066576 , 8.27007866],
       [3.99429703, 4.78034449],
       [3.96632385, 4.52212381],
       [3.96123695, 4.69176579],
       [3.95866036, 4.0917778 ],
       [3.94210982, 4.66226625],
       [3.91987014, 4.22589111],
       [3.90097594, 4.87394714],
       [3.88909483, 4.27386093],
       [3.84412098, 4.15662766],
       [3.80737615, 4.87333965],
       [3.75953412, 4.19736576],
       [3.75322032, 4.6202302 ],
       [3.75194693, 8.00253773],
       [3.05924463, 3.62083054],
       [2.68813944, 2.73345637],
       [1.6586014 , 2.16982841]]), array([[9.59937668, 9.77157593],
       [9.5890255 , 9.72969532],
       [9.40380383, 9.91677284],
       [8.21383381, 8.2412653 ]])]2024-03-06 17:55:57.818040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KTC ph vector generated, counter: 146
2024-03-06 17:56:01.299907: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:01.342828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:02.326875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KU0 ph vector generated, counter: 147
2024-03-06 17:56:05.537168: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:05.580058: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:06.459911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KVA ph vector generated, counter: 148
2024-03-06 17:56:09.667101: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:09.710069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:10.799289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KX2 ph vector generated, counter: 149
2024-03-06 17:56:14.402504: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:14.445642: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:15.456768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KX3 ph vector generated, counter: 150
2024-03-06 17:56:18.696406: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:18.739463: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:19.700814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
terminate called after throwing an instance of 'thrust::system::detail::bad_alloc'
  what():  std::bad_alloc: cudaErrorMemoryAllocation: out of memory
calculate_ph/run_ph_h2.sh: line 26: 3662359 Aborted                 (core dumped) python calculate_ph/ph_functions_h2.py ${input_file} ${threshold} ${intermediate_file} ${output_file} --diagrams_png ${diagrams_png} --landscape_png ${landscape_png}
6KXB ph vector generated, counter: 151
2024-03-06 17:56:24.410316: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:24.453634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:25.325692: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KXY ph vector generated, counter: 152
2024-03-06 17:56:28.689320: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:28.731549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:29.636665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KY0 ph vector generated, counter: 153
2024-03-06 17:56:32.867587: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:32.927363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:33.842915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KY1 ph vector generated, counter: 154
2024-03-06 17:56:36.974297: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:37.016949: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:38.155793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KYE ph vector generated, counter: 155
2024-03-06 17:56:41.829669: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:41.872937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:42.844079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KYQ ph vector generated, counter: 156
2024-03-06 17:56:46.138577: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:46.182150: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:47.223860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KYR ph vector generated, counter: 157
2024-03-06 17:56:50.426488: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:50.469673: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:51.363193: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KYZ ph vector generated, counter: 158
2024-03-06 17:56:54.625568: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:54.668398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:56:55.818732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KZ0 ph vector generated, counter: 159
2024-03-06 17:56:59.420656: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:56:59.463714: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:00.372999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3123052), (0., 1.3124491), (0., 1.3133143), (0., 1.3134708),
       (0., 1.3143722), (0., 1.3148425), (0., 1.3148457), (0., 1.315553 ),
       (0., 1.3160281), (0., 1.318004 ), (0., 1.3189062), (0., 1.3200284),
       (0., 1.3203609), (0., 1.3208807), (0., 1.3223771), (0., 1.3226086),
       (0., 1.3229111), (0., 1.3237909), (0., 1.323886 ), (0., 1.3243191),
       (0., 1.3246355), (0., 1.3250815), (0., 1.3251549), (0., 1.3253428),
       (0., 1.3253851), (0., 1.3255379), (0., 1.325839 ), (0., 1.3259696),
       (0., 1.3261309), (0., 1.3261386), (0., 1.326166 ), (0., 1.3262221),
       (0., 1.3263232), (0., 1.3263791), (0., 1.326443 ), (0., 1.3267475),
       (0., 1.3269194), (0., 1.3269725), (0., 1.3271173), (0., 1.3271719),
       (0., 1.3273395), (0., 1.3273804), (0., 1.3275782), (0., 1.3277534),
       (0., 1.3281406), (0., 1.3283323), (0., 1.3286912), (0., 1.3288836),
       (0., 1.3289809), (0., 1.3290468), (0., 1.329128 ), (0., 1.3292141),
       (0., 1.329421 ), (0., 1.3294624), (0., 1.3296509), (0., 1.3299949),
       (0., 1.3301708), (0., 1.3302324), (0., 1.3303096), (0., 1.3305029),
       (0., 1.3307179), (0., 1.3309505), (0., 1.3309603), (0., 1.3310567),
       (0., 1.3312851), (0., 1.3315368), (0., 1.3316733), (0., 1.3319898),
       (0., 1.3321393), (0., 1.3324585), (0., 1.3327154), (0., 1.3328941),
       (0., 1.3330973), (0., 1.3331218), (0., 1.3337325), (0., 1.3341619),
       (0., 1.3343844), (0., 1.3345274), (0., 1.334597 ), (0., 1.3346015),
       (0., 1.3347012), (0., 1.3347709), (0., 1.3348715), (0., 1.3350105),
       (0., 1.3351536), (0., 1.3354478), (0., 1.3358735), (0., 1.3360955),
       (0., 1.3363731), (0., 1.3369883), (0., 1.3371885), (0., 1.3372767),
       (0., 1.3374178), (0., 1.3382933), (0., 1.3383976), (0., 1.3386608),
       (0., 1.3387418), (0., 1.3389056), (0., 1.3392894), (0., 1.3393102),
       (0., 1.3395749), (0., 1.3444879), (0., 1.3452098), (0., 1.3459306),
       (0., 1.3504188), (0., 1.3506173), (0., 1.3622079), (0., 1.3771181),
       (0., 1.4016086), (0., 1.4045093), (0., 1.4249275), (0., 1.4350133),
       (0., 1.4373035), (0., 1.437972 ), (0., 1.4387281), (0., 1.4395759),
       (0., 1.4397794), (0., 1.4423853), (0., 1.4430637), (0., 1.4436477),
       (0., 1.4440796), (0., 1.444327 ), (0., 1.4448552), (0., 1.4454875),
       (0., 1.4468095), (0., 1.4468927), (0., 1.4478717), (0., 1.4480284),
       (0., 1.4481319), (0., 1.4483628), (0., 1.4483843), (0., 1.4485987),
       (0., 1.4489739), (0., 1.4491425), (0., 1.4498194), (0., 1.4498264),
       (0., 1.4498913), (0., 1.4499767), (0., 1.4501009), (0., 1.4511876),
       (0., 1.4515868), (0., 1.4518939), (0., 1.4520512), (0., 1.4521816),
       (0., 1.4526669), (0., 1.4531283), (0., 1.4531612), (0., 1.4532372),
       (0., 1.4535224), (0., 1.4536282), (0., 1.453841 ), (0., 1.453895 ),
       (0., 1.4540552), (0., 1.4550813), (0., 1.4551045), (0., 1.4553634),
       (0., 1.4554951), (0., 1.455927 ), (0., 1.4564925), (0., 1.4566643),
       (0., 1.457437 ), (0., 1.4575064), (0., 1.458115 ), (0., 1.4584526),
       (0., 1.4584839), (0., 1.4585882), (0., 1.458853 ), (0., 1.4592803),
       (0., 1.4593507), (0., 1.459561 ), (0., 1.4600718), (0., 1.4603107),
       (0., 1.4606606), (0., 1.4610502), (0., 1.4614261), (0., 1.4617403),
       (0., 1.4618186), (0., 1.4623196), (0., 1.4623634), (0., 1.4626675),
       (0., 1.4629202), (0., 1.4631857), (0., 1.4633523), (0., 1.4634799),
       (0., 1.4641045), (0., 1.4641765), (0., 1.4643867), (0., 1.4644076),
       (0., 1.4650205), (0., 1.4651746), (0., 1.4652909), (0., 1.4656672),
       (0., 1.4657353), (0., 1.4659307), (0., 1.4663833), (0., 1.4664032),
       (0., 1.4668772), (0., 1.4673266), (0., 1.4676412), (0., 1.4678062),
       (0., 1.4687731), (0., 1.4688432), (0., 1.4688619), (0., 1.4689276),
       (0., 1.4689716), (0., 1.4690535), (0., 1.4698303), (0., 1.4708017),
       (0., 1.470958 ), (0., 1.4711409), (0., 1.4712566), (0., 1.4730772),
       (0., 1.4740345), (0., 1.4752609), (0., 1.4752972), (0., 1.475326 ),
       (0., 1.4765131), (0., 1.4766436), (0., 1.4769067), (0., 1.4772513),
       (0., 1.4774383), (0., 1.4792465), (0., 1.48504  ), (0., 1.4903281),
       (0., 1.4913603), (0., 1.4969705), (0., 1.497202 ), (0., 1.4976866),
       (0., 1.4981891), (0., 1.5004591), (0., 1.5009236), (0., 1.5026714),
       (0., 1.5028901), (0., 1.5037419), (0., 1.5040863), (0., 1.504808 ),
       (0., 1.5053148), (0., 1.5059208), (0., 1.506302 ), (0., 1.506523 ),
       (0., 1.5072498), (0., 1.5091728), (0., 1.5101671), (0., 1.5103372),
       (0., 1.511694 ), (0., 1.5118984), (0., 1.512021 ), (0., 1.5120603),
       (0., 1.5123241), (0., 1.512473 ), (0., 1.512487 ), (0., 1.5126394),
       (0., 1.5127515), (0., 1.5131536), (0., 1.5131718), (0., 1.5138885),
       (0., 1.5151433), (0., 1.515221 ), (0., 1.5152647), (0., 1.5155823),
       (0., 1.5156492), (0., 1.5158747), (0., 1.5159347), (0., 1.5159624),
       (0., 1.5162144), (0., 1.5164089), (0., 1.5164106), (0., 1.5167148),
       (0., 1.5168817), (0., 1.5169315), (0., 1.5173421), (0., 1.5173631),
       (0., 1.5174205), (0., 1.5174606), (0., 1.5179007), (0., 1.5183338),
       (0., 1.5186247), (0., 1.5186292), (0., 1.5189288), (0., 1.5190669),
       (0., 1.5190787), (0., 1.5191901), (0., 1.5200242), (0., 1.5201168),
       (0., 1.5205489), (0., 1.5207332), (0., 1.5214161), (0., 1.5219892),
       (0., 1.5224017), (0., 1.5226971), (0., 1.5231409), (0., 1.5234205),
       (0., 1.523527 ), (0., 1.5241821), (0., 1.5241824), (0., 1.5243785),
       (0., 1.5244235), (0., 1.52443  ), (0., 1.5245519), (0., 1.5246272),
       (0., 1.5250578), (0., 1.5251063), (0., 1.5253202), (0., 1.5253719),
       (0., 1.5255797), (0., 1.5261532), (0., 1.5265512), (0., 1.526763 ),
       (0., 1.5267805), (0., 1.5269997), (0., 1.5270958), (0., 1.5271379),
       (0., 1.5272366), (0., 1.5272514), (0., 1.5274297), (0., 1.5282927),
       (0., 1.5293314), (0., 1.5296353), (0., 1.5298871), (0., 1.5304201),
       (0., 1.5304435), (0., 1.5306426), (0., 1.5307126), (0., 1.530716 ),
       (0., 1.5307966), (0., 1.5311618), (0., 1.5312115), (0., 1.5317546),
       (0., 1.5318574), (0., 1.5326359), (0., 1.5334947), (0., 1.5343477),
       (0., 1.5354204), (0., 1.5360166), (0., 1.537366 ), (0., 1.5374348),
       (0., 1.5376588), (0., 1.5383127), (0., 1.5385818), (0., 1.5389296),
       (0., 1.5444901), (0., 1.5555209), (0., 1.5586779), (0., 1.564909 ),
       (0., 1.5681262), (0., 1.5817988), (0., 1.5997763), (0., 1.6093866),
       (0., 1.6271557), (0., 1.6294628), (0., 1.6408471), (0., 1.6478351),
       (0., 1.6493664), (0., 1.6509216), (0., 1.6521245), (0., 1.6603733),
       (0., 1.6618947), (0., 1.6620843), (0., 1.6624154), (0., 1.6695478),
       (0., 1.6698337), (0., 1.6890031), (0., 1.6971222), (0., 1.7026424),
       (0., 1.704306 ), (0., 1.7095134), (0., 1.7120613), (0., 1.7234459),
       (0., 1.727594 ), (0., 1.727601 ), (0., 1.7284786), (0., 1.730239 ),
       (0., 1.7337278), (0., 1.7475054), (0., 1.7593436), (0., 1.7608   ),
       (0., 1.7980485), (0., 1.8017216), (0., 1.807277 ), (0., 3.4888723)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(7.020028 , 7.2484775), (6.8540854, 7.699747 ),
       (6.7613716, 7.435916 ), (6.676591 , 7.8260517),
       (6.5842495, 7.6707563), (5.96133  , 6.3537545),
       (5.929892 , 6.3295236), (5.8280306, 7.7136254),
       (5.6311398, 5.8812156), (5.4339604, 7.2694173),
       (4.455738 , 4.483072 ), (4.443725 , 4.5066366),
       (4.3992953, 4.4851756), (4.369644 , 5.213726 ),
       (4.3422236, 4.4305716), (4.271922 , 4.376243 ),
       (4.193242 , 4.1983666), (4.1563163, 4.3469176),
       (4.1187177, 4.118832 ), (4.1179156, 4.423073 ),
       (4.084989 , 4.4668875), (4.082254 , 4.5701094),
       (4.052875 , 4.3094873), (4.050188 , 4.5888166),
       (4.048324 , 4.4759426), (4.045442 , 4.472403 ),
       (4.04119  , 4.3606834), (4.014745 , 4.1965756),
       (4.0116143, 4.300312 ), (4.011526 , 4.6532593),
       (4.007164 , 4.42112  ), (3.9973865, 6.5346713),
       (3.9666395, 4.71363  ), (3.9564264, 4.5600953),
       (3.9405744, 4.4001594), (3.9388409, 3.9787996),
       (3.9272301, 4.3023353), (3.9246418, 4.8319926),
       (3.9163523, 8.180882 ), (3.904796 , 4.407762 ),
       (3.8863602, 3.900272 ), (3.8863528, 4.5550947),
       (3.8786974, 8.424916 ), (3.8694735, 7.2891183),
       (3.7625651, 5.3047214), (3.7178514, 4.949905 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(8.677411 , 12.201406 ), (8.440157 ,  8.495678 ),
       (7.941259 ,  7.9795337), (7.82007  ,  7.9066114),
       (5.37418  ,  5.4398723), (5.0728045,  5.1898336)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.312305212020874), (0.0, 1.3124490976333618), (0.0, 1.3133143186569214), (0.0, 1.3134708404541016), (0.0, 1.314372181892395), (0.0, 1.3148424625396729), (0.0, 1.3148456811904907), (0.0, 1.3155529499053955), (0.0, 1.316028118133545), (0.0, 1.3180040121078491), (0.0, 1.3189061880111694), (0.0, 1.3200284242630005), (0.0, 1.3203608989715576), (0.0, 1.320880651473999), (0.0, 1.32237708568573), (0.0, 1.3226085901260376), (0.0, 1.3229111433029175), (0.0, 1.3237909078598022), (0.0, 1.3238860368728638), (0.0, 1.3243191242218018), (0.0, 1.3246355056762695), (0.0, 1.325081467628479), (0.0, 1.3251549005508423), (0.0, 1.3253427743911743), (0.0, 1.3253850936889648), (0.0, 1.325537919998169), (0.0, 1.3258390426635742), (0.0, 1.3259695768356323), (0.0, 1.3261308670043945), (0.0, 1.3261386156082153), (0.0, 1.326166033744812), (0.0, 1.3262220621109009), (0.0, 1.32632315158844), (0.0, 1.3263790607452393), (0.0, 1.3264429569244385), (0.0, 1.3267475366592407), (0.0, 1.326919436454773), (0.0, 1.326972484588623), (0.0, 1.3271173238754272), (0.0, 1.3271719217300415), (0.0, 1.32733952999115), (0.0, 1.3273804187774658), (0.0, 1.3275781869888306), (0.0, 1.3277534246444702), (0.0, 1.3281406164169312), (0.0, 1.3283323049545288), (0.0, 1.3286912441253662), (0.0, 1.3288836479187012), (0.0, 1.3289809226989746), (0.0, 1.3290468454360962), (0.0, 1.3291280269622803), (0.0, 1.329214096069336), (0.0, 1.329421043395996), (0.0, 1.3294624090194702), (0.0, 1.32965087890625), (0.0, 1.3299949169158936), (0.0, 1.330170750617981), (0.0, 1.3302323818206787), (0.0, 1.3303096294403076), (0.0, 1.3305028676986694), (0.0, 1.330717921257019), (0.0, 1.3309504985809326), (0.0, 1.3309602737426758), (0.0, 1.3310567140579224), (0.0, 1.3312851190567017), (0.0, 1.3315367698669434), (0.0, 1.331673264503479), (0.0, 1.3319897651672363), (0.0, 1.332139253616333), (0.0, 1.33245849609375), (0.0, 1.332715392112732), (0.0, 1.3328940868377686), (0.0, 1.3330973386764526), (0.0, 1.3331217765808105), (0.0, 1.3337324857711792), (0.0, 1.3341618776321411), (0.0, 1.3343844413757324), (0.0, 1.3345273733139038), (0.0, 1.3345969915390015), (0.0, 1.3346015214920044), (0.0, 1.3347011804580688), (0.0, 1.334770917892456), (0.0, 1.334871530532837), (0.0, 1.3350105285644531), (0.0, 1.335153579711914), (0.0, 1.3354477882385254), (0.0, 1.3358734846115112), (0.0, 1.3360954523086548), (0.0, 1.3363730907440186), (0.0, 1.3369883298873901), (0.0, 1.337188482284546), (0.0, 1.3372766971588135), (0.0, 1.3374178409576416), (0.0, 1.3382933139801025), (0.0, 1.3383976221084595), (0.0, 1.3386608362197876), (0.0, 1.3387417793273926), (0.0, 1.3389055728912354), (0.0, 1.3392894268035889), (0.0, 1.3393101692199707), (0.0, 1.339574933052063), (0.0, 1.3444879055023193), (0.0, 1.3452098369598389), (0.0, 1.345930576324463), (0.0, 1.3504188060760498), (0.0, 1.3506172895431519), (0.0, 1.3622078895568848), (0.0, 1.3771181106567383), (0.0, 1.4016085863113403), (0.0, 1.4045093059539795), (0.0, 1.4249274730682373), (0.0, 1.4350132942199707), (0.0, 1.4373035430908203), (0.0, 1.4379719495773315), (0.0, 1.4387280941009521), (0.0, 1.4395759105682373), (0.0, 1.4397794008255005), (0.0, 1.4423853158950806), (0.0, 1.443063735961914), (0.0, 1.4436477422714233), (0.0, 1.4440796375274658), (0.0, 1.4443269968032837), (0.0, 1.4448552131652832), (0.0, 1.4454874992370605), (0.0, 1.4468095302581787), (0.0, 1.4468927383422852), (0.0, 1.4478716850280762), (0.0, 1.4480284452438354), (0.0, 1.4481319189071655), (0.0, 1.4483628273010254), (0.0, 1.4483842849731445), (0.0, 1.4485987424850464), (0.0, 1.4489738941192627), (0.0, 1.4491424560546875), (0.0, 1.4498194456100464), (0.0, 1.4498263597488403), (0.0, 1.4498913288116455), (0.0, 1.4499766826629639), (0.0, 1.4501008987426758), (0.0, 1.4511876106262207), (0.0, 1.4515868425369263), (0.0, 1.451893925666809), (0.0, 1.4520511627197266), (0.0, 1.4521815776824951), (0.0, 1.4526668787002563), (0.0, 1.4531283378601074), (0.0, 1.4531612396240234), (0.0, 1.4532371759414673), (0.0, 1.4535224437713623), (0.0, 1.4536281824111938), (0.0, 1.453840970993042), (0.0, 1.4538949728012085), (0.0, 1.4540551900863647), (0.0, 1.4550813436508179), (0.0, 1.4551044702529907), (0.0, 1.455363392829895), (0.0, 1.4554951190948486), (0.0, 1.4559270143508911), (0.0, 1.45649254322052), (0.0, 1.4566643238067627), (0.0, 1.4574370384216309), (0.0, 1.4575064182281494), (0.0, 1.4581149816513062), (0.0, 1.458452582359314), (0.0, 1.4584839344024658), (0.0, 1.4585882425308228), (0.0, 1.458853006362915), (0.0, 1.459280252456665), (0.0, 1.4593507051467896), (0.0, 1.4595609903335571), (0.0, 1.4600718021392822), (0.0, 1.460310697555542), (0.0, 1.4606605768203735), (0.0, 1.4610501527786255), (0.0, 1.4614261388778687), (0.0, 1.461740255355835), (0.0, 1.4618185758590698), (0.0, 1.4623196125030518), (0.0, 1.462363362312317), (0.0, 1.462667465209961), (0.0, 1.4629201889038086), (0.0, 1.4631856679916382), (0.0, 1.4633523225784302), (0.0, 1.4634798765182495), (0.0, 1.4641045331954956), (0.0, 1.4641765356063843), (0.0, 1.4643867015838623), (0.0, 1.4644075632095337), (0.0, 1.4650205373764038), (0.0, 1.4651745557785034), (0.0, 1.465290904045105), (0.0, 1.4656672477722168), (0.0, 1.4657353162765503), (0.0, 1.465930700302124), (0.0, 1.4663833379745483), (0.0, 1.4664032459259033), (0.0, 1.4668772220611572), (0.0, 1.4673266410827637), (0.0, 1.4676412343978882), (0.0, 1.4678062200546265), (0.0, 1.4687731266021729), (0.0, 1.4688432216644287), (0.0, 1.4688619375228882), (0.0, 1.4689276218414307), (0.0, 1.468971610069275), (0.0, 1.4690535068511963), (0.0, 1.4698302745819092), (0.0, 1.4708017110824585), (0.0, 1.4709579944610596), (0.0, 1.4711408615112305), (0.0, 1.4712566137313843), (0.0, 1.4730771780014038), (0.0, 1.4740345478057861), (0.0, 1.475260853767395), (0.0, 1.475297212600708), (0.0, 1.4753259420394897), (0.0, 1.476513147354126), (0.0, 1.4766435623168945), (0.0, 1.476906657218933), (0.0, 1.4772512912750244), (0.0, 1.4774383306503296), (0.0, 1.4792464971542358), (0.0, 1.4850399494171143), (0.0, 1.490328073501587), (0.0, 1.4913603067398071), (0.0, 1.496970534324646), (0.0, 1.4972020387649536), (0.0, 1.4976866245269775), (0.0, 1.498189091682434), (0.0, 1.50045907497406), (0.0, 1.5009236335754395), (0.0, 1.5026713609695435), (0.0, 1.5028901100158691), (0.0, 1.5037418603897095), (0.0, 1.5040862560272217), (0.0, 1.504807949066162), (0.0, 1.505314826965332), (0.0, 1.5059207677841187), (0.0, 1.506301999092102), (0.0, 1.5065230131149292), (0.0, 1.5072498321533203), (0.0, 1.509172797203064), (0.0, 1.510167121887207), (0.0, 1.510337233543396), (0.0, 1.5116939544677734), (0.0, 1.511898398399353), (0.0, 1.5120209455490112), (0.0, 1.512060284614563), (0.0, 1.5123240947723389), (0.0, 1.5124729871749878), (0.0, 1.5124870538711548), (0.0, 1.5126394033432007), (0.0, 1.5127514600753784), (0.0, 1.5131535530090332), (0.0, 1.5131717920303345), (0.0, 1.5138884782791138), (0.0, 1.5151432752609253), (0.0, 1.5152209997177124), (0.0, 1.5152647495269775), (0.0, 1.5155823230743408), (0.0, 1.5156491994857788), (0.0, 1.5158747434616089), (0.0, 1.515934705734253), (0.0, 1.5159623622894287), (0.0, 1.516214370727539), (0.0, 1.516408920288086), (0.0, 1.5164105892181396), (0.0, 1.5167148113250732), (0.0, 1.5168817043304443), (0.0, 1.5169315338134766), (0.0, 1.5173420906066895), (0.0, 1.5173630714416504), (0.0, 1.5174205303192139), (0.0, 1.517460584640503), (0.0, 1.5179007053375244), (0.0, 1.5183337926864624), (0.0, 1.5186246633529663), (0.0, 1.5186291933059692), (0.0, 1.5189287662506104), (0.0, 1.5190669298171997), (0.0, 1.5190787315368652), (0.0, 1.5191900730133057), (0.0, 1.5200241804122925), (0.0, 1.5201168060302734), (0.0, 1.520548939704895), (0.0, 1.5207332372665405), (0.0, 1.5214160680770874), (0.0, 1.5219892263412476), (0.0, 1.5224016904830933), (0.0, 1.5226970911026), (0.0, 1.5231409072875977), (0.0, 1.5234204530715942), (0.0, 1.5235270261764526), (0.0, 1.5241820812225342), (0.0, 1.5241824388504028), (0.0, 1.5243785381317139), (0.0, 1.5244234800338745), (0.0, 1.5244300365447998), (0.0, 1.5245518684387207), (0.0, 1.5246272087097168), (0.0, 1.5250577926635742), (0.0, 1.5251063108444214), (0.0, 1.5253201723098755), (0.0, 1.5253719091415405), (0.0, 1.5255796909332275), (0.0, 1.5261532068252563), (0.0, 1.5265512466430664), (0.0, 1.5267629623413086), (0.0, 1.5267804861068726), (0.0, 1.5269997119903564), (0.0, 1.5270957946777344), (0.0, 1.5271378755569458), (0.0, 1.5272365808486938), (0.0, 1.5272513628005981), (0.0, 1.5274296998977661), (0.0, 1.5282926559448242), (0.0, 1.5293314456939697), (0.0, 1.5296353101730347), (0.0, 1.529887080192566), (0.0, 1.5304200649261475), (0.0, 1.530443549156189), (0.0, 1.5306426286697388), (0.0, 1.530712604522705), (0.0, 1.5307159423828125), (0.0, 1.5307966470718384), (0.0, 1.5311617851257324), (0.0, 1.531211495399475), (0.0, 1.5317546129226685), (0.0, 1.5318573713302612), (0.0, 1.5326359272003174), (0.0, 1.5334947109222412), (0.0, 1.534347653388977), (0.0, 1.5354204177856445), (0.0, 1.536016583442688), (0.0, 1.5373660326004028), (0.0, 1.5374348163604736), (0.0, 1.5376588106155396), (0.0, 1.5383126735687256), (0.0, 1.5385818481445312), (0.0, 1.5389295816421509), (0.0, 1.544490098953247), (0.0, 1.5555208921432495), (0.0, 1.5586779117584229), (0.0, 1.5649089813232422), (0.0, 1.5681262016296387), (0.0, 1.581798791885376), (0.0, 1.599776268005371), (0.0, 1.6093865633010864), (0.0, 1.6271556615829468), (0.0, 1.6294628381729126), (0.0, 1.640847086906433), (0.0, 1.6478351354599), (0.0, 1.6493663787841797), (0.0, 1.6509215831756592), (0.0, 1.6521245241165161), (0.0, 1.660373330116272), (0.0, 1.661894679069519), (0.0, 1.6620843410491943), (0.0, 1.6624153852462769), (0.0, 1.6695477962493896), (0.0, 1.6698336601257324), (0.0, 1.6890031099319458), (0.0, 1.6971222162246704), (0.0, 1.7026424407958984), (0.0, 1.7043060064315796), (0.0, 1.7095134258270264), (0.0, 1.7120612859725952), (0.0, 1.7234458923339844), (0.0, 1.727594017982483), (0.0, 1.7276010513305664), (0.0, 1.7284785509109497), (0.0, 1.7302390336990356), (0.0, 1.7337278127670288), (0.0, 1.7475054264068604), (0.0, 1.7593436241149902), (0.0, 1.7608000040054321), (0.0, 1.798048496246338), (0.0, 1.8017215728759766), (0.0, 1.807276964187622), (0.0, 3.4888722896575928)], [(7.020028114318848, 7.248477458953857), (6.854085445404053, 7.699747085571289), (6.761371612548828, 7.435915946960449), (6.676590919494629, 7.826051712036133), (6.584249496459961, 7.6707563400268555), (5.961329936981201, 6.35375452041626), (5.929892063140869, 6.32952356338501), (5.828030586242676, 7.713625431060791), (5.631139755249023, 5.881215572357178), (5.433960437774658, 7.2694172859191895), (4.455738067626953, 4.483071804046631), (4.443725109100342, 4.506636619567871), (4.399295330047607, 4.485175609588623), (4.3696441650390625, 5.213726043701172), (4.342223644256592, 4.430571556091309), (4.2719221115112305, 4.3762431144714355), (4.193242073059082, 4.198366641998291), (4.15631628036499, 4.346917629241943), (4.118717670440674, 4.118832111358643), (4.117915630340576, 4.423072814941406), (4.084989070892334, 4.466887474060059), (4.082253932952881, 4.5701093673706055), (4.05287504196167, 4.309487342834473), (4.050188064575195, 4.5888166427612305), (4.048324108123779, 4.475942611694336), (4.0454421043396, 4.472403049468994), (4.041190147399902, 4.360683441162109), (4.014745235443115, 4.19657564163208), (4.0116143226623535, 4.300312042236328), (4.011526107788086, 4.65325927734375), (4.007164001464844, 4.4211201667785645), (3.9973864555358887, 6.534671306610107), (3.966639518737793, 4.713630199432373), (3.9564263820648193, 4.560095310211182), (3.9405744075775146, 4.4001593589782715), (3.938840866088867, 3.97879958152771), (3.9272301197052, 4.302335262298584), (3.9246418476104736, 4.8319926261901855), (3.9163522720336914, 8.180882453918457), (3.9047958850860596, 4.407762050628662), (3.8863601684570312, 3.9002718925476074), (3.886352777481079, 4.5550947189331055), (3.878697395324707, 8.42491626739502), (3.869473457336426, 7.28911828994751), (3.7625651359558105, 5.304721355438232), (3.717851400375366, 4.949904918670654)], [(8.677411079406738, 12.20140552520752), (8.440156936645508, 8.495677947998047), (7.941258907318115, 7.979533672332764), (7.820069789886475, 7.906611442565918), (5.374179840087891, 5.4398722648620605), (5.0728044509887695, 5.189833641052246)]]
[array([[0.        , 1.31230521],
       [0.        , 1.3124491 ],
       [0.        , 1.31331432],
       [0.        , 1.31347084],
       [0.        , 1.31437218],
       [0.        , 1.31484246],
       [0.        , 1.31484568],
       [0.        , 1.31555295],
       [0.        , 1.31602812],
       [0.        , 1.31800401],
       [0.        , 1.31890619],
       [0.        , 1.32002842],
       [0.        , 1.3203609 ],
       [0.        , 1.32088065],
       [0.        , 1.32237709],
       [0.        , 1.32260859],
       [0.        , 1.32291114],
       [0.        , 1.32379091],
       [0.        , 1.32388604],
       [0.        , 1.32431912],
       [0.        , 1.32463551],
       [0.        , 1.32508147],
       [0.        , 1.3251549 ],
       [0.        , 1.32534277],
       [0.        , 1.32538509],
       [0.        , 1.32553792],
       [0.        , 1.32583904],
       [0.        , 1.32596958],
       [0.        , 1.32613087],
       [0.        , 1.32613862],
       [0.        , 1.32616603],
       [0.        , 1.32622206],
       [0.        , 1.32632315],
       [0.        , 1.32637906],
       [0.        , 1.32644296],
       [0.        , 1.32674754],
       [0.        , 1.32691944],
       [0.        , 1.32697248],
       [0.        , 1.32711732],
       [0.        , 1.32717192],
       [0.        , 1.32733953],
       [0.        , 1.32738042],
       [0.        , 1.32757819],
       [0.        , 1.32775342],
       [0.        , 1.32814062],
       [0.        , 1.3283323 ],
       [0.        , 1.32869124],
       [0.        , 1.32888365],
       [0.        , 1.32898092],
       [0.        , 1.32904685],
       [0.        , 1.32912803],
       [0.        , 1.3292141 ],
       [0.        , 1.32942104],
       [0.        , 1.32946241],
       [0.        , 1.32965088],
       [0.        , 1.32999492],
       [0.        , 1.33017075],
       [0.        , 1.33023238],
       [0.        , 1.33030963],
       [0.        , 1.33050287],
       [0.        , 1.33071792],
       [0.        , 1.3309505 ],
       [0.        , 1.33096027],
       [0.        , 1.33105671],
       [0.        , 1.33128512],
       [0.        , 1.33153677],
       [0.        , 1.33167326],
       [0.        , 1.33198977],
       [0.        , 1.33213925],
       [0.        , 1.3324585 ],
       [0.        , 1.33271539],
       [0.        , 1.33289409],
       [0.        , 1.33309734],
       [0.        , 1.33312178],
       [0.        , 1.33373249],
       [0.        , 1.33416188],
       [0.        , 1.33438444],
       [0.        , 1.33452737],
       [0.        , 1.33459699],
       [0.        , 1.33460152],
       [0.        , 1.33470118],
       [0.        , 1.33477092],
       [0.        , 1.33487153],
       [0.        , 1.33501053],
       [0.        , 1.33515358],
       [0.        , 1.33544779],
       [0.        , 1.33587348],
       [0.        , 1.33609545],
       [0.        , 1.33637309],
       [0.        , 1.33698833],
       [0.        , 1.33718848],
       [0.        , 1.3372767 ],
       [0.        , 1.33741784],
       [0.        , 1.33829331],
       [0.        , 1.33839762],
       [0.        , 1.33866084],
       [0.        , 1.33874178],
       [0.        , 1.33890557],
       [0.        , 1.33928943],
       [0.        , 1.33931017],
       [0.        , 1.33957493],
       [0.        , 1.34448791],
       [0.        , 1.34520984],
       [0.        , 1.34593058],
       [0.        , 1.35041881],
       [0.        , 1.35061729],
       [0.        , 1.36220789],
       [0.        , 1.37711811],
       [0.        , 1.40160859],
       [0.        , 1.40450931],
       [0.        , 1.42492747],
       [0.        , 1.43501329],
       [0.        , 1.43730354],
       [0.        , 1.43797195],
       [0.        , 1.43872809],
       [0.        , 1.43957591],
       [0.        , 1.4397794 ],
       [0.        , 1.44238532],
       [0.        , 1.44306374],
       [0.        , 1.44364774],
       [0.        , 1.44407964],
       [0.        , 1.444327  ],
       [0.        , 1.44485521],
       [0.        , 1.4454875 ],
       [0.        , 1.44680953],
       [0.        , 1.44689274],
       [0.        , 1.44787169],
       [0.        , 1.44802845],
       [0.        , 1.44813192],
       [0.        , 1.44836283],
       [0.        , 1.44838428],
       [0.        , 1.44859874],
       [0.        , 1.44897389],
       [0.        , 1.44914246],
       [0.        , 1.44981945],
       [0.        , 1.44982636],
       [0.        , 1.44989133],
       [0.        , 1.44997668],
       [0.        , 1.4501009 ],
       [0.        , 1.45118761],
       [0.        , 1.45158684],
       [0.        , 1.45189393],
       [0.        , 1.45205116],
       [0.        , 1.45218158],
       [0.        , 1.45266688],
       [0.        , 1.45312834],
       [0.        , 1.45316124],
       [0.        , 1.45323718],
       [0.        , 1.45352244],
       [0.        , 1.45362818],
       [0.        , 1.45384097],
       [0.        , 1.45389497],
       [0.        , 1.45405519],
       [0.        , 1.45508134],
       [0.        , 1.45510447],
       [0.        , 1.45536339],
       [0.        , 1.45549512],
       [0.        , 1.45592701],
       [0.        , 1.45649254],
       [0.        , 1.45666432],
       [0.        , 1.45743704],
       [0.        , 1.45750642],
       [0.        , 1.45811498],
       [0.        , 1.45845258],
       [0.        , 1.45848393],
       [0.        , 1.45858824],
       [0.        , 1.45885301],
       [0.        , 1.45928025],
       [0.        , 1.45935071],
       [0.        , 1.45956099],
       [0.        , 1.4600718 ],
       [0.        , 1.4603107 ],
       [0.        , 1.46066058],
       [0.        , 1.46105015],
       [0.        , 1.46142614],
       [0.        , 1.46174026],
       [0.        , 1.46181858],
       [0.        , 1.46231961],
       [0.        , 1.46236336],
       [0.        , 1.46266747],
       [0.        , 1.46292019],
       [0.        , 1.46318567],
       [0.        , 1.46335232],
       [0.        , 1.46347988],
       [0.        , 1.46410453],
       [0.        , 1.46417654],
       [0.        , 1.4643867 ],
       [0.        , 1.46440756],
       [0.        , 1.46502054],
       [0.        , 1.46517456],
       [0.        , 1.4652909 ],
       [0.        , 1.46566725],
       [0.        , 1.46573532],
       [0.        , 1.4659307 ],
       [0.        , 1.46638334],
       [0.        , 1.46640325],
       [0.        , 1.46687722],
       [0.        , 1.46732664],
       [0.        , 1.46764123],
       [0.        , 1.46780622],
       [0.        , 1.46877313],
       [0.        , 1.46884322],
       [0.        , 1.46886194],
       [0.        , 1.46892762],
       [0.        , 1.46897161],
       [0.        , 1.46905351],
       [0.        , 1.46983027],
       [0.        , 1.47080171],
       [0.        , 1.47095799],
       [0.        , 1.47114086],
       [0.        , 1.47125661],
       [0.        , 1.47307718],
       [0.        , 1.47403455],
       [0.        , 1.47526085],
       [0.        , 1.47529721],
       [0.        , 1.47532594],
       [0.        , 1.47651315],
       [0.        , 1.47664356],
       [0.        , 1.47690666],
       [0.        , 1.47725129],
       [0.        , 1.47743833],
       [0.        , 1.4792465 ],
       [0.        , 1.48503995],
       [0.        , 1.49032807],
       [0.        , 1.49136031],
       [0.        , 1.49697053],
       [0.        , 1.49720204],
       [0.        , 1.49768662],
       [0.        , 1.49818909],
       [0.        , 1.50045907],
       [0.        , 1.50092363],
       [0.        , 1.50267136],
       [0.        , 1.50289011],
       [0.        , 1.50374186],
       [0.        , 1.50408626],
       [0.        , 1.50480795],
       [0.        , 1.50531483],
       [0.        , 1.50592077],
       [0.        , 1.506302  ],
       [0.        , 1.50652301],
       [0.        , 1.50724983],
       [0.        , 1.5091728 ],
       [0.        , 1.51016712],
       [0.        , 1.51033723],
       [0.        , 1.51169395],
       [0.        , 1.5118984 ],
       [0.        , 1.51202095],
       [0.        , 1.51206028],
       [0.        , 1.51232409],
       [0.        , 1.51247299],
       [0.        , 1.51248705],
       [0.        , 1.5126394 ],
       [0.        , 1.51275146],
       [0.        , 1.51315355],
       [0.        , 1.51317179],
       [0.        , 1.51388848],
       [0.        , 1.51514328],
       [0.        , 1.515221  ],
       [0.        , 1.51526475],
       [0.        , 1.51558232],
       [0.        , 1.5156492 ],
       [0.        , 1.51587474],
       [0.        , 1.51593471],
       [0.        , 1.51596236],
       [0.        , 1.51621437],
       [0.        , 1.51640892],
       [0.        , 1.51641059],
       [0.        , 1.51671481],
       [0.        , 1.5168817 ],
       [0.        , 1.51693153],
       [0.        , 1.51734209],
       [0.        , 1.51736307],
       [0.        , 1.51742053],
       [0.        , 1.51746058],
       [0.        , 1.51790071],
       [0.        , 1.51833379],
       [0.        , 1.51862466],
       [0.        , 1.51862919],
       [0.        , 1.51892877],
       [0.        , 1.51906693],
       [0.        , 1.51907873],
       [0.        , 1.51919007],
       [0.        , 1.52002418],
       [0.        , 1.52011681],
       [0.        , 1.52054894],
       [0.        , 1.52073324],
       [0.        , 1.52141607],
       [0.        , 1.52198923],
       [0.        , 1.52240169],
       [0.        , 1.52269709],
       [0.        , 1.52314091],
       [0.        , 1.52342045],
       [0.        , 1.52352703],
       [0.        , 1.52418208],
       [0.        , 1.52418244],
       [0.        , 1.52437854],
       [0.        , 1.52442348],
       [0.        , 1.52443004],
       [0.        , 1.52455187],
       [0.        , 1.52462721],
       [0.        , 1.52505779],
       [0.        , 1.52510631],
       [0.        , 1.52532017],
       [0.        , 1.52537191],
       [0.        , 1.52557969],
       [0.        , 1.52615321],
       [0.        , 1.52655125],
       [0.        , 1.52676296],
       [0.        , 1.52678049],
       [0.        , 1.52699971],
       [0.        , 1.52709579],
       [0.        , 1.52713788],
       [0.        , 1.52723658],
       [0.        , 1.52725136],
       [0.        , 1.5274297 ],
       [0.        , 1.52829266],
       [0.        , 1.52933145],
       [0.        , 1.52963531],
       [0.        , 1.52988708],
       [0.        , 1.53042006],
       [0.        , 1.53044355],
       [0.        , 1.53064263],
       [0.        , 1.5307126 ],
       [0.        , 1.53071594],
       [0.        , 1.53079665],
       [0.        , 1.53116179],
       [0.        , 1.5312115 ],
       [0.        , 1.53175461],
       [0.        , 1.53185737],
       [0.        , 1.53263593],
       [0.        , 1.53349471],
       [0.        , 1.53434765],
       [0.        , 1.53542042],
       [0.        , 1.53601658],
       [0.        , 1.53736603],
       [0.        , 1.53743482],
       [0.        , 1.53765881],
       [0.        , 1.53831267],
       [0.        , 1.53858185],
       [0.        , 1.53892958],
       [0.        , 1.5444901 ],
       [0.        , 1.55552089],
       [0.        , 1.55867791],
       [0.        , 1.56490898],
       [0.        , 1.5681262 ],
       [0.        , 1.58179879],
       [0.        , 1.59977627],
       [0.        , 1.60938656],
       [0.        , 1.62715566],
       [0.        , 1.62946284],
       [0.        , 1.64084709],
       [0.        , 1.64783514],
       [0.        , 1.64936638],
       [0.        , 1.65092158],
       [0.        , 1.65212452],
       [0.        , 1.66037333],
       [0.        , 1.66189468],
       [0.        , 1.66208434],
       [0.        , 1.66241539],
       [0.        , 1.6695478 ],
       [0.        , 1.66983366],
       [0.        , 1.68900311],
       [0.        , 1.69712222],
       [0.        , 1.70264244],
       [0.        , 1.70430601],
       [0.        , 1.70951343],
       [0.        , 1.71206129],
       [0.        , 1.72344589],
       [0.        , 1.72759402],
       [0.        , 1.72760105],
       [0.        , 1.72847855],
       [0.        , 1.73023903],
       [0.        , 1.73372781],
       [0.        , 1.74750543],
       [0.        , 1.75934362],
       [0.        , 1.7608    ],
       [0.        , 1.7980485 ],
       [0.        , 1.80172157],
       [0.        , 1.80727696],
       [0.        , 3.48887229]]), array([[7.02002811, 7.24847746],
       [6.85408545, 7.69974709],
       [6.76137161, 7.43591595],
       [6.67659092, 7.82605171],
       [6.5842495 , 7.67075634],
       [5.96132994, 6.35375452],
       [5.92989206, 6.32952356],
       [5.82803059, 7.71362543],
       [5.63113976, 5.88121557],
       [5.43396044, 7.26941729],
       [4.45573807, 4.4830718 ],
       [4.44372511, 4.50663662],
       [4.39929533, 4.48517561],
       [4.36964417, 5.21372604],
       [4.34222364, 4.43057156],
       [4.27192211, 4.37624311],
       [4.19324207, 4.19836664],
       [4.15631628, 4.34691763],
       [4.11871767, 4.11883211],
       [4.11791563, 4.42307281],
       [4.08498907, 4.46688747],
       [4.08225393, 4.57010937],
       [4.05287504, 4.30948734],
       [4.05018806, 4.58881664],
       [4.04832411, 4.47594261],
       [4.0454421 , 4.47240305],
       [4.04119015, 4.36068344],
       [4.01474524, 4.19657564],
       [4.01161432, 4.30031204],
       [4.01152611, 4.65325928],
       [4.007164  , 4.42112017],
       [3.99738646, 6.53467131],
       [3.96663952, 4.7136302 ],
       [3.95642638, 4.56009531],
       [3.94057441, 4.40015936],
       [3.93884087, 3.97879958],
       [3.92723012, 4.30233526],
       [3.92464185, 4.83199263],
       [3.91635227, 8.18088245],
       [3.90479589, 4.40776205],
       [3.88636017, 3.90027189],
       [3.88635278, 4.55509472],
       [3.8786974 , 8.42491627],
       [3.86947346, 7.28911829],
       [3.76256514, 5.30472136],
       [3.7178514 , 4.94990492]]), array([[ 8.67741108, 12.20140553],
       [ 8.44015694,  8.49567795],
       [ 7.94125891,  7.97953367],
       [ 7.82006979,  7.90661144],
       [ 5.37417984,  5.43987226],
       [ 5.07280445,  5.18983364]])]2024-03-06 17:57:04.665665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6KZ1 ph vector generated, counter: 160
2024-03-06 17:57:08.669177: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:08.711749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:09.809378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KZG ph vector generated, counter: 161
2024-03-06 17:57:12.726194: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:12.807476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:13.771568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6KZQ ph vector generated, counter: 162
2024-03-06 17:57:16.944174: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:16.987988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:17.970470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L03 ph vector generated, counter: 163
2024-03-06 17:57:21.049061: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:21.091933: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:22.042685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L04 ph vector generated, counter: 164
2024-03-06 17:57:25.704135: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:25.746589: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:26.644288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L05 ph vector generated, counter: 165
2024-03-06 17:57:30.023289: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:30.065801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:31.105035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L11 ph vector generated, counter: 166
2024-03-06 17:57:34.251903: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:34.296144: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:35.157201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L14 ph vector generated, counter: 167
2024-03-06 17:57:38.294071: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:38.337243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:39.212295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L16 ph vector generated, counter: 168
2024-03-06 17:57:42.276345: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:42.319393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:43.412164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L1E ph vector generated, counter: 169
2024-03-06 17:57:46.663366: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:46.706302: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:47.584746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3198038), (0., 1.3224474), (0., 1.3225832), (0., 1.3231494),
       (0., 1.3233415), (0., 1.3241671), (0., 1.3241684), (0., 1.3244245),
       (0., 1.3260338), (0., 1.3268571), (0., 1.3271362), (0., 1.3271419),
       (0., 1.3271723), (0., 1.3274848), (0., 1.3277053), (0., 1.3277555),
       (0., 1.3278682), (0., 1.3279922), (0., 1.3283112), (0., 1.3286142),
       (0., 1.3287834), (0., 1.3287932), (0., 1.329059 ), (0., 1.3290986),
       (0., 1.3292495), (0., 1.329297 ), (0., 1.3293983), (0., 1.3294835),
       (0., 1.3294908), (0., 1.3295499), (0., 1.3296924), (0., 1.3304789),
       (0., 1.3305061), (0., 1.3307852), (0., 1.33101  ), (0., 1.3311273),
       (0., 1.3313173), (0., 1.3315421), (0., 1.332007 ), (0., 1.3321462),
       (0., 1.3322647), (0., 1.3323574), (0., 1.3325074), (0., 1.3330432),
       (0., 1.3331542), (0., 1.3331887), (0., 1.3332499), (0., 1.3334167),
       (0., 1.3336322), (0., 1.3337144), (0., 1.3339081), (0., 1.3339747),
       (0., 1.3340067), (0., 1.3341044), (0., 1.3344009), (0., 1.3344344),
       (0., 1.3345649), (0., 1.3348013), (0., 1.3349878), (0., 1.3352332),
       (0., 1.3363558), (0., 1.336433 ), (0., 1.3366182), (0., 1.3375391),
       (0., 1.3375429), (0., 1.3377454), (0., 1.3381231), (0., 1.338175 ),
       (0., 1.3385502), (0., 1.3404497), (0., 1.3408005), (0., 1.3411006),
       (0., 1.4260118), (0., 1.4434242), (0., 1.44625  ), (0., 1.4489807),
       (0., 1.449293 ), (0., 1.4495075), (0., 1.4496056), (0., 1.4508699),
       (0., 1.4515   ), (0., 1.4515544), (0., 1.4517611), (0., 1.4518082),
       (0., 1.4523205), (0., 1.45235  ), (0., 1.452568 ), (0., 1.4527023),
       (0., 1.4531068), (0., 1.4533298), (0., 1.4536245), (0., 1.4537575),
       (0., 1.4538269), (0., 1.4541589), (0., 1.4545664), (0., 1.4550849),
       (0., 1.4553982), (0., 1.4554949), (0., 1.4555044), (0., 1.4557265),
       (0., 1.4560593), (0., 1.4561387), (0., 1.4565896), (0., 1.456743 ),
       (0., 1.4568737), (0., 1.4570847), (0., 1.457137 ), (0., 1.4573901),
       (0., 1.4574482), (0., 1.4576311), (0., 1.4580578), (0., 1.4584364),
       (0., 1.4584408), (0., 1.4585434), (0., 1.4585506), (0., 1.4589809),
       (0., 1.4590017), (0., 1.4590948), (0., 1.4600142), (0., 1.460215 ),
       (0., 1.4602257), (0., 1.4604735), (0., 1.4604763), (0., 1.4606717),
       (0., 1.4607975), (0., 1.4608275), (0., 1.460873 ), (0., 1.4609066),
       (0., 1.4609247), (0., 1.4609624), (0., 1.4612502), (0., 1.461749 ),
       (0., 1.4617869), (0., 1.461803 ), (0., 1.4619483), (0., 1.4623893),
       (0., 1.462412 ), (0., 1.4633197), (0., 1.4635463), (0., 1.4641818),
       (0., 1.4642537), (0., 1.4647682), (0., 1.4654188), (0., 1.4658726),
       (0., 1.4662555), (0., 1.4676596), (0., 1.4831821), (0., 1.4866375),
       (0., 1.491194 ), (0., 1.50681  ), (0., 1.5119087), (0., 1.5135912),
       (0., 1.5146224), (0., 1.5149118), (0., 1.5155351), (0., 1.5160117),
       (0., 1.5164843), (0., 1.5166934), (0., 1.5170728), (0., 1.5172354),
       (0., 1.5176353), (0., 1.5176431), (0., 1.5177531), (0., 1.5178887),
       (0., 1.5184567), (0., 1.5185022), (0., 1.5186939), (0., 1.5187442),
       (0., 1.5191379), (0., 1.5194335), (0., 1.5197443), (0., 1.5197577),
       (0., 1.5197632), (0., 1.5198805), (0., 1.5198903), (0., 1.5199112),
       (0., 1.5203501), (0., 1.5204937), (0., 1.5207056), (0., 1.5207543),
       (0., 1.5208544), (0., 1.5211083), (0., 1.5211191), (0., 1.5211208),
       (0., 1.5214081), (0., 1.5215002), (0., 1.5215236), (0., 1.5219687),
       (0., 1.5221703), (0., 1.5221947), (0., 1.522322 ), (0., 1.5227058),
       (0., 1.5230279), (0., 1.5232728), (0., 1.523457 ), (0., 1.5236882),
       (0., 1.5238503), (0., 1.524135 ), (0., 1.5241568), (0., 1.5241834),
       (0., 1.5241944), (0., 1.5243683), (0., 1.5245869), (0., 1.5247635),
       (0., 1.5249283), (0., 1.5249629), (0., 1.5250597), (0., 1.525351 ),
       (0., 1.5255898), (0., 1.5257069), (0., 1.5260588), (0., 1.5263045),
       (0., 1.5263658), (0., 1.5267228), (0., 1.5270003), (0., 1.5272063),
       (0., 1.5278689), (0., 1.5281034), (0., 1.5281384), (0., 1.5281622),
       (0., 1.5286096), (0., 1.5287039), (0., 1.5289654), (0., 1.5291624),
       (0., 1.5649091), (0., 1.5652786), (0., 7.1935735)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.190138 , 8.835079 ), (7.731052 , 9.026575 ),
       (7.3673434, 7.8861046), (7.0282507, 7.103432 ),
       (6.53073  , 7.227433 ), (6.2472367, 7.0702863),
       (4.436872 , 6.685507 ), (4.370472 , 4.3781104),
       (4.2225575, 4.3168306), (4.1665454, 4.5289392),
       (4.121494 , 4.3185954), (4.1018248, 4.5189166),
       (4.0821357, 4.4817157), (4.065698 , 4.2035475),
       (4.061163 , 4.720511 ), (4.0535808, 8.804406 ),
       (4.049986 , 4.585734 ), (4.045889 , 4.531935 ),
       (4.0447893, 4.0866456), (4.020815 , 4.262888 ),
       (4.0099525, 4.6694407), (3.9886544, 4.524222 ),
       (3.9882226, 4.1839895), (3.984707 , 4.767802 ),
       (3.942183 , 4.5371556), (3.936519 , 4.677641 ),
       (3.9271781, 4.37115  ), (3.9251413, 4.007829 ),
       (3.9072945, 8.682136 ), (3.895254 , 4.096762 ),
       (3.8848186, 6.5665126), (3.843868 , 4.7798986)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.378537, 10.535568), (8.747511,  9.071481)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3198038339614868), (0.0, 1.322447419166565), (0.0, 1.3225831985473633), (0.0, 1.3231494426727295), (0.0, 1.3233414888381958), (0.0, 1.3241671323776245), (0.0, 1.3241684436798096), (0.0, 1.3244245052337646), (0.0, 1.3260338306427002), (0.0, 1.326857089996338), (0.0, 1.3271361589431763), (0.0, 1.3271418809890747), (0.0, 1.3271722793579102), (0.0, 1.3274848461151123), (0.0, 1.3277052640914917), (0.0, 1.3277554512023926), (0.0, 1.3278682231903076), (0.0, 1.3279922008514404), (0.0, 1.3283112049102783), (0.0, 1.3286142349243164), (0.0, 1.328783392906189), (0.0, 1.3287931680679321), (0.0, 1.3290590047836304), (0.0, 1.3290985822677612), (0.0, 1.3292495012283325), (0.0, 1.3292969465255737), (0.0, 1.329398274421692), (0.0, 1.3294835090637207), (0.0, 1.3294907808303833), (0.0, 1.3295499086380005), (0.0, 1.3296923637390137), (0.0, 1.3304789066314697), (0.0, 1.3305060863494873), (0.0, 1.3307851552963257), (0.0, 1.3310099840164185), (0.0, 1.3311272859573364), (0.0, 1.3313173055648804), (0.0, 1.3315421342849731), (0.0, 1.3320070505142212), (0.0, 1.332146167755127), (0.0, 1.3322646617889404), (0.0, 1.332357406616211), (0.0, 1.3325073719024658), (0.0, 1.3330432176589966), (0.0, 1.3331542015075684), (0.0, 1.3331886529922485), (0.0, 1.3332499265670776), (0.0, 1.3334167003631592), (0.0, 1.333632230758667), (0.0, 1.3337143659591675), (0.0, 1.3339080810546875), (0.0, 1.3339747190475464), (0.0, 1.334006667137146), (0.0, 1.3341044187545776), (0.0, 1.3344008922576904), (0.0, 1.3344343900680542), (0.0, 1.3345649242401123), (0.0, 1.3348013162612915), (0.0, 1.334987759590149), (0.0, 1.335233211517334), (0.0, 1.3363558053970337), (0.0, 1.3364330530166626), (0.0, 1.336618185043335), (0.0, 1.3375390768051147), (0.0, 1.3375428915023804), (0.0, 1.3377454280853271), (0.0, 1.338123083114624), (0.0, 1.3381750583648682), (0.0, 1.3385502099990845), (0.0, 1.3404496908187866), (0.0, 1.3408005237579346), (0.0, 1.3411005735397339), (0.0, 1.4260118007659912), (0.0, 1.4434242248535156), (0.0, 1.4462499618530273), (0.0, 1.448980689048767), (0.0, 1.4492930173873901), (0.0, 1.449507474899292), (0.0, 1.4496055841445923), (0.0, 1.4508699178695679), (0.0, 1.4515000581741333), (0.0, 1.4515544176101685), (0.0, 1.4517611265182495), (0.0, 1.451808214187622), (0.0, 1.4523204565048218), (0.0, 1.4523500204086304), (0.0, 1.4525680541992188), (0.0, 1.452702283859253), (0.0, 1.4531067609786987), (0.0, 1.4533298015594482), (0.0, 1.4536244869232178), (0.0, 1.4537575244903564), (0.0, 1.453826904296875), (0.0, 1.454158902168274), (0.0, 1.4545663595199585), (0.0, 1.4550849199295044), (0.0, 1.4553982019424438), (0.0, 1.4554948806762695), (0.0, 1.4555044174194336), (0.0, 1.4557265043258667), (0.0, 1.4560593366622925), (0.0, 1.4561387300491333), (0.0, 1.4565895795822144), (0.0, 1.4567430019378662), (0.0, 1.4568736553192139), (0.0, 1.4570846557617188), (0.0, 1.4571369886398315), (0.0, 1.4573900699615479), (0.0, 1.4574482440948486), (0.0, 1.4576311111450195), (0.0, 1.4580577611923218), (0.0, 1.458436369895935), (0.0, 1.4584407806396484), (0.0, 1.4585434198379517), (0.0, 1.4585505723953247), (0.0, 1.458980917930603), (0.0, 1.4590016603469849), (0.0, 1.459094762802124), (0.0, 1.4600142240524292), (0.0, 1.4602149724960327), (0.0, 1.4602257013320923), (0.0, 1.4604735374450684), (0.0, 1.460476279258728), (0.0, 1.4606716632843018), (0.0, 1.4607975482940674), (0.0, 1.4608274698257446), (0.0, 1.460873007774353), (0.0, 1.4609066247940063), (0.0, 1.460924744606018), (0.0, 1.4609624147415161), (0.0, 1.4612501859664917), (0.0, 1.4617489576339722), (0.0, 1.4617868661880493), (0.0, 1.4618029594421387), (0.0, 1.461948275566101), (0.0, 1.462389349937439), (0.0, 1.4624119997024536), (0.0, 1.4633196592330933), (0.0, 1.4635462760925293), (0.0, 1.4641817808151245), (0.0, 1.4642536640167236), (0.0, 1.4647681713104248), (0.0, 1.465418815612793), (0.0, 1.4658726453781128), (0.0, 1.46625554561615), (0.0, 1.467659592628479), (0.0, 1.4831820726394653), (0.0, 1.4866374731063843), (0.0, 1.4911940097808838), (0.0, 1.506809949874878), (0.0, 1.5119086503982544), (0.0, 1.5135911703109741), (0.0, 1.514622449874878), (0.0, 1.5149117708206177), (0.0, 1.5155351161956787), (0.0, 1.5160117149353027), (0.0, 1.516484260559082), (0.0, 1.516693353652954), (0.0, 1.5170727968215942), (0.0, 1.5172353982925415), (0.0, 1.5176353454589844), (0.0, 1.5176430940628052), (0.0, 1.5177531242370605), (0.0, 1.5178886651992798), (0.0, 1.5184566974639893), (0.0, 1.5185022354125977), (0.0, 1.5186939239501953), (0.0, 1.5187442302703857), (0.0, 1.5191378593444824), (0.0, 1.5194334983825684), (0.0, 1.5197442770004272), (0.0, 1.5197577476501465), (0.0, 1.5197632312774658), (0.0, 1.5198805332183838), (0.0, 1.519890308380127), (0.0, 1.5199111700057983), (0.0, 1.5203500986099243), (0.0, 1.520493745803833), (0.0, 1.5207055807113647), (0.0, 1.520754337310791), (0.0, 1.5208543539047241), (0.0, 1.5211082696914673), (0.0, 1.5211191177368164), (0.0, 1.5211207866668701), (0.0, 1.5214080810546875), (0.0, 1.5215002298355103), (0.0, 1.5215235948562622), (0.0, 1.5219687223434448), (0.0, 1.5221703052520752), (0.0, 1.522194743156433), (0.0, 1.5223220586776733), (0.0, 1.5227057933807373), (0.0, 1.5230278968811035), (0.0, 1.5232727527618408), (0.0, 1.5234570503234863), (0.0, 1.5236881971359253), (0.0, 1.5238503217697144), (0.0, 1.5241349935531616), (0.0, 1.5241568088531494), (0.0, 1.5241833925247192), (0.0, 1.524194359779358), (0.0, 1.5243682861328125), (0.0, 1.5245869159698486), (0.0, 1.5247634649276733), (0.0, 1.524928331375122), (0.0, 1.5249629020690918), (0.0, 1.525059700012207), (0.0, 1.5253510475158691), (0.0, 1.5255898237228394), (0.0, 1.5257068872451782), (0.0, 1.5260587930679321), (0.0, 1.5263044834136963), (0.0, 1.5263657569885254), (0.0, 1.52672278881073), (0.0, 1.5270003080368042), (0.0, 1.527206301689148), (0.0, 1.5278688669204712), (0.0, 1.5281033515930176), (0.0, 1.5281383991241455), (0.0, 1.5281622409820557), (0.0, 1.5286096334457397), (0.0, 1.5287039279937744), (0.0, 1.5289653539657593), (0.0, 1.5291624069213867), (0.0, 1.5649091005325317), (0.0, 1.5652786493301392), (0.0, 7.193573474884033)], [(8.19013786315918, 8.835079193115234), (7.731051921844482, 9.026575088500977), (7.367343425750732, 7.886104583740234), (7.028250694274902, 7.1034321784973145), (6.5307297706604, 7.227433204650879), (6.247236728668213, 7.070286273956299), (4.4368720054626465, 6.685506820678711), (4.370471954345703, 4.378110408782959), (4.222557544708252, 4.316830635070801), (4.166545391082764, 4.528939247131348), (4.121493816375732, 4.3185954093933105), (4.101824760437012, 4.518916606903076), (4.0821356773376465, 4.481715679168701), (4.065698146820068, 4.203547477722168), (4.061162948608398, 4.720510959625244), (4.0535807609558105, 8.80440616607666), (4.049985885620117, 4.585733890533447), (4.045888900756836, 4.531935214996338), (4.0447893142700195, 4.086645603179932), (4.020814895629883, 4.262887954711914), (4.009952545166016, 4.669440746307373), (3.988654375076294, 4.524221897125244), (3.988222599029541, 4.183989524841309), (3.9847071170806885, 4.767801761627197), (3.942183017730713, 4.537155628204346), (3.936518907546997, 4.677640914916992), (3.927178144454956, 4.371150016784668), (3.9251413345336914, 4.007829189300537), (3.907294511795044, 8.682135581970215), (3.895253896713257, 4.096762180328369), (3.8848185539245605, 6.566512584686279), (3.8438680171966553, 4.779898643493652)], [(9.37853717803955, 10.535568237304688), (8.74751091003418, 9.071480751037598)]]
[array([[0.        , 1.31980383],
       [0.        , 1.32244742],
       [0.        , 1.3225832 ],
       [0.        , 1.32314944],
       [0.        , 1.32334149],
       [0.        , 1.32416713],
       [0.        , 1.32416844],
       [0.        , 1.32442451],
       [0.        , 1.32603383],
       [0.        , 1.32685709],
       [0.        , 1.32713616],
       [0.        , 1.32714188],
       [0.        , 1.32717228],
       [0.        , 1.32748485],
       [0.        , 1.32770526],
       [0.        , 1.32775545],
       [0.        , 1.32786822],
       [0.        , 1.3279922 ],
       [0.        , 1.3283112 ],
       [0.        , 1.32861423],
       [0.        , 1.32878339],
       [0.        , 1.32879317],
       [0.        , 1.329059  ],
       [0.        , 1.32909858],
       [0.        , 1.3292495 ],
       [0.        , 1.32929695],
       [0.        , 1.32939827],
       [0.        , 1.32948351],
       [0.        , 1.32949078],
       [0.        , 1.32954991],
       [0.        , 1.32969236],
       [0.        , 1.33047891],
       [0.        , 1.33050609],
       [0.        , 1.33078516],
       [0.        , 1.33100998],
       [0.        , 1.33112729],
       [0.        , 1.33131731],
       [0.        , 1.33154213],
       [0.        , 1.33200705],
       [0.        , 1.33214617],
       [0.        , 1.33226466],
       [0.        , 1.33235741],
       [0.        , 1.33250737],
       [0.        , 1.33304322],
       [0.        , 1.3331542 ],
       [0.        , 1.33318865],
       [0.        , 1.33324993],
       [0.        , 1.3334167 ],
       [0.        , 1.33363223],
       [0.        , 1.33371437],
       [0.        , 1.33390808],
       [0.        , 1.33397472],
       [0.        , 1.33400667],
       [0.        , 1.33410442],
       [0.        , 1.33440089],
       [0.        , 1.33443439],
       [0.        , 1.33456492],
       [0.        , 1.33480132],
       [0.        , 1.33498776],
       [0.        , 1.33523321],
       [0.        , 1.33635581],
       [0.        , 1.33643305],
       [0.        , 1.33661819],
       [0.        , 1.33753908],
       [0.        , 1.33754289],
       [0.        , 1.33774543],
       [0.        , 1.33812308],
       [0.        , 1.33817506],
       [0.        , 1.33855021],
       [0.        , 1.34044969],
       [0.        , 1.34080052],
       [0.        , 1.34110057],
       [0.        , 1.4260118 ],
       [0.        , 1.44342422],
       [0.        , 1.44624996],
       [0.        , 1.44898069],
       [0.        , 1.44929302],
       [0.        , 1.44950747],
       [0.        , 1.44960558],
       [0.        , 1.45086992],
       [0.        , 1.45150006],
       [0.        , 1.45155442],
       [0.        , 1.45176113],
       [0.        , 1.45180821],
       [0.        , 1.45232046],
       [0.        , 1.45235002],
       [0.        , 1.45256805],
       [0.        , 1.45270228],
       [0.        , 1.45310676],
       [0.        , 1.4533298 ],
       [0.        , 1.45362449],
       [0.        , 1.45375752],
       [0.        , 1.4538269 ],
       [0.        , 1.4541589 ],
       [0.        , 1.45456636],
       [0.        , 1.45508492],
       [0.        , 1.4553982 ],
       [0.        , 1.45549488],
       [0.        , 1.45550442],
       [0.        , 1.4557265 ],
       [0.        , 1.45605934],
       [0.        , 1.45613873],
       [0.        , 1.45658958],
       [0.        , 1.456743  ],
       [0.        , 1.45687366],
       [0.        , 1.45708466],
       [0.        , 1.45713699],
       [0.        , 1.45739007],
       [0.        , 1.45744824],
       [0.        , 1.45763111],
       [0.        , 1.45805776],
       [0.        , 1.45843637],
       [0.        , 1.45844078],
       [0.        , 1.45854342],
       [0.        , 1.45855057],
       [0.        , 1.45898092],
       [0.        , 1.45900166],
       [0.        , 1.45909476],
       [0.        , 1.46001422],
       [0.        , 1.46021497],
       [0.        , 1.4602257 ],
       [0.        , 1.46047354],
       [0.        , 1.46047628],
       [0.        , 1.46067166],
       [0.        , 1.46079755],
       [0.        , 1.46082747],
       [0.        , 1.46087301],
       [0.        , 1.46090662],
       [0.        , 1.46092474],
       [0.        , 1.46096241],
       [0.        , 1.46125019],
       [0.        , 1.46174896],
       [0.        , 1.46178687],
       [0.        , 1.46180296],
       [0.        , 1.46194828],
       [0.        , 1.46238935],
       [0.        , 1.462412  ],
       [0.        , 1.46331966],
       [0.        , 1.46354628],
       [0.        , 1.46418178],
       [0.        , 1.46425366],
       [0.        , 1.46476817],
       [0.        , 1.46541882],
       [0.        , 1.46587265],
       [0.        , 1.46625555],
       [0.        , 1.46765959],
       [0.        , 1.48318207],
       [0.        , 1.48663747],
       [0.        , 1.49119401],
       [0.        , 1.50680995],
       [0.        , 1.51190865],
       [0.        , 1.51359117],
       [0.        , 1.51462245],
       [0.        , 1.51491177],
       [0.        , 1.51553512],
       [0.        , 1.51601171],
       [0.        , 1.51648426],
       [0.        , 1.51669335],
       [0.        , 1.5170728 ],
       [0.        , 1.5172354 ],
       [0.        , 1.51763535],
       [0.        , 1.51764309],
       [0.        , 1.51775312],
       [0.        , 1.51788867],
       [0.        , 1.5184567 ],
       [0.        , 1.51850224],
       [0.        , 1.51869392],
       [0.        , 1.51874423],
       [0.        , 1.51913786],
       [0.        , 1.5194335 ],
       [0.        , 1.51974428],
       [0.        , 1.51975775],
       [0.        , 1.51976323],
       [0.        , 1.51988053],
       [0.        , 1.51989031],
       [0.        , 1.51991117],
       [0.        , 1.5203501 ],
       [0.        , 1.52049375],
       [0.        , 1.52070558],
       [0.        , 1.52075434],
       [0.        , 1.52085435],
       [0.        , 1.52110827],
       [0.        , 1.52111912],
       [0.        , 1.52112079],
       [0.        , 1.52140808],
       [0.        , 1.52150023],
       [0.        , 1.52152359],
       [0.        , 1.52196872],
       [0.        , 1.52217031],
       [0.        , 1.52219474],
       [0.        , 1.52232206],
       [0.        , 1.52270579],
       [0.        , 1.5230279 ],
       [0.        , 1.52327275],
       [0.        , 1.52345705],
       [0.        , 1.5236882 ],
       [0.        , 1.52385032],
       [0.        , 1.52413499],
       [0.        , 1.52415681],
       [0.        , 1.52418339],
       [0.        , 1.52419436],
       [0.        , 1.52436829],
       [0.        , 1.52458692],
       [0.        , 1.52476346],
       [0.        , 1.52492833],
       [0.        , 1.5249629 ],
       [0.        , 1.5250597 ],
       [0.        , 1.52535105],
       [0.        , 1.52558982],
       [0.        , 1.52570689],
       [0.        , 1.52605879],
       [0.        , 1.52630448],
       [0.        , 1.52636576],
       [0.        , 1.52672279],
       [0.        , 1.52700031],
       [0.        , 1.5272063 ],
       [0.        , 1.52786887],
       [0.        , 1.52810335],
       [0.        , 1.5281384 ],
       [0.        , 1.52816224],
       [0.        , 1.52860963],
       [0.        , 1.52870393],
       [0.        , 1.52896535],
       [0.        , 1.52916241],
       [0.        , 1.5649091 ],
       [0.        , 1.56527865],
       [0.        , 7.19357347]]), array([[8.19013786, 8.83507919],
       [7.73105192, 9.02657509],
       [7.36734343, 7.88610458],
       [7.02825069, 7.10343218],
       [6.53072977, 7.2274332 ],
       [6.24723673, 7.07028627],
       [4.43687201, 6.68550682],
       [4.37047195, 4.37811041],
       [4.22255754, 4.31683064],
       [4.16654539, 4.52893925],
       [4.12149382, 4.31859541],
       [4.10182476, 4.51891661],
       [4.08213568, 4.48171568],
       [4.06569815, 4.20354748],
       [4.06116295, 4.72051096],
       [4.05358076, 8.80440617],
       [4.04998589, 4.58573389],
       [4.0458889 , 4.53193521],
       [4.04478931, 4.0866456 ],
       [4.0208149 , 4.26288795],
       [4.00995255, 4.66944075],
       [3.98865438, 4.5242219 ],
       [3.9882226 , 4.18398952],
       [3.98470712, 4.76780176],
       [3.94218302, 4.53715563],
       [3.93651891, 4.67764091],
       [3.92717814, 4.37115002],
       [3.92514133, 4.00782919],
       [3.90729451, 8.68213558],
       [3.8952539 , 4.09676218],
       [3.88481855, 6.56651258],
       [3.84386802, 4.77989864]]), array([[ 9.37853718, 10.53556824],
       [ 8.74751091,  9.07148075]])]2024-03-06 17:57:50.720286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6L1F ph vector generated, counter: 170
2024-03-06 17:57:54.015979: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:57:54.063264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:57:55.031975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.319458 ), (0., 1.3199824), (0., 1.3206704), (0., 1.3208755),
       (0., 1.3212676), (0., 1.3214424), (0., 1.3217356), (0., 1.3217485),
       (0., 1.3217863), (0., 1.3218101), (0., 1.3220357), (0., 1.3221722),
       (0., 1.3225724), (0., 1.3226706), (0., 1.3228145), (0., 1.3229127),
       (0., 1.3231772), (0., 1.3231971), (0., 1.323307 ), (0., 1.3235868),
       (0., 1.3236034), (0., 1.3237135), (0., 1.3237377), (0., 1.3240004),
       (0., 1.3241625), (0., 1.3242952), (0., 1.3242979), (0., 1.3243091),
       (0., 1.3243454), (0., 1.3243531), (0., 1.3243678), (0., 1.3243817),
       (0., 1.3243842), (0., 1.324404 ), (0., 1.3244172), (0., 1.3244324),
       (0., 1.3244962), (0., 1.3245307), (0., 1.3245751), (0., 1.324638 ),
       (0., 1.3250256), (0., 1.3250487), (0., 1.3251852), (0., 1.3253171),
       (0., 1.3255022), (0., 1.3256001), (0., 1.3256748), (0., 1.325771 ),
       (0., 1.3257716), (0., 1.3258964), (0., 1.3259152), (0., 1.3259673),
       (0., 1.3259974), (0., 1.3260068), (0., 1.3260152), (0., 1.3260659),
       (0., 1.3260669), (0., 1.3260847), (0., 1.3260925), (0., 1.3260944),
       (0., 1.3261877), (0., 1.3262408), (0., 1.326277 ), (0., 1.3262963),
       (0., 1.3264302), (0., 1.3265165), (0., 1.3265208), (0., 1.3268064),
       (0., 1.3268399), (0., 1.326982 ), (0., 1.3270526), (0., 1.3270557),
       (0., 1.327137 ), (0., 1.3271989), (0., 1.327217 ), (0., 1.3272862),
       (0., 1.327347 ), (0., 1.3273482), (0., 1.3274564), (0., 1.3275263),
       (0., 1.3275672), (0., 1.3275738), (0., 1.3275789), (0., 1.3275816),
       (0., 1.3276454), (0., 1.3278457), (0., 1.3278764), (0., 1.3279871),
       (0., 1.3280888), (0., 1.3281448), (0., 1.3291012), (0., 1.3292046),
       (0., 1.3294257), (0., 1.3294373), (0., 1.3297238), (0., 1.3301198),
       (0., 1.3312954), (0., 1.3317935), (0., 1.3326172), (0., 1.3337878),
       (0., 1.3609079), (0., 1.4410189), (0., 1.4421998), (0., 1.4437196),
       (0., 1.4440709), (0., 1.4445313), (0., 1.445    ), (0., 1.4450293),
       (0., 1.4450649), (0., 1.4450859), (0., 1.4457209), (0., 1.4457273),
       (0., 1.4457681), (0., 1.445993 ), (0., 1.4461677), (0., 1.4463309),
       (0., 1.4463768), (0., 1.446612 ), (0., 1.446623 ), (0., 1.4467763),
       (0., 1.4471232), (0., 1.4472202), (0., 1.4474365), (0., 1.4475892),
       (0., 1.4477024), (0., 1.4478931), (0., 1.4479122), (0., 1.4482758),
       (0., 1.4483787), (0., 1.4486427), (0., 1.4488437), (0., 1.4488541),
       (0., 1.4489273), (0., 1.4489733), (0., 1.4490443), (0., 1.4490904),
       (0., 1.4493102), (0., 1.4493109), (0., 1.449312 ), (0., 1.4495467),
       (0., 1.4496536), (0., 1.4498544), (0., 1.4501514), (0., 1.450324 ),
       (0., 1.4505297), (0., 1.4505949), (0., 1.4506038), (0., 1.450706 ),
       (0., 1.4511466), (0., 1.4511892), (0., 1.4512204), (0., 1.4515322),
       (0., 1.4517417), (0., 1.4520713), (0., 1.4521368), (0., 1.452142 ),
       (0., 1.4524909), (0., 1.4524945), (0., 1.4527624), (0., 1.4527987),
       (0., 1.4528184), (0., 1.4530214), (0., 1.4531271), (0., 1.4531726),
       (0., 1.4533097), (0., 1.4534253), (0., 1.4536412), (0., 1.4536738),
       (0., 1.453953 ), (0., 1.4540379), (0., 1.4541631), (0., 1.4541779),
       (0., 1.4543247), (0., 1.4550093), (0., 1.4550657), (0., 1.4554026),
       (0., 1.4556155), (0., 1.4557173), (0., 1.4557866), (0., 1.456974 ),
       (0., 1.4574432), (0., 1.4577912), (0., 1.4582307), (0., 1.4582934),
       (0., 1.4587092), (0., 1.458856 ), (0., 1.4589413), (0., 1.4590713),
       (0., 1.4591606), (0., 1.4594811), (0., 1.4595438), (0., 1.4597377),
       (0., 1.4600617), (0., 1.4600892), (0., 1.4603066), (0., 1.4603388),
       (0., 1.4605563), (0., 1.4618295), (0., 1.462653 ), (0., 1.4631715),
       (0., 1.4633245), (0., 1.4730339), (0., 1.4738445), (0., 1.5089164),
       (0., 1.5089438), (0., 1.5089654), (0., 1.5097913), (0., 1.5099609),
       (0., 1.5100278), (0., 1.5118622), (0., 1.5119532), (0., 1.5119834),
       (0., 1.5122083), (0., 1.5123458), (0., 1.5138195), (0., 1.5140249),
       (0., 1.5141413), (0., 1.5142428), (0., 1.5145786), (0., 1.515613 ),
       (0., 1.5159067), (0., 1.5162734), (0., 1.5163174), (0., 1.516525 ),
       (0., 1.5165793), (0., 1.5168614), (0., 1.5170784), (0., 1.5183563),
       (0., 1.5185242), (0., 1.5188289), (0., 1.5188528), (0., 1.5190552),
       (0., 1.519084 ), (0., 1.5191115), (0., 1.5192746), (0., 1.519528 ),
       (0., 1.5196674), (0., 1.5197264), (0., 1.5199463), (0., 1.5203376),
       (0., 1.5204362), (0., 1.5205886), (0., 1.5206431), (0., 1.5206738),
       (0., 1.5206746), (0., 1.5209783), (0., 1.5210694), (0., 1.5211242),
       (0., 1.5212758), (0., 1.5214874), (0., 1.521545 ), (0., 1.5217661),
       (0., 1.5218875), (0., 1.5219315), (0., 1.5223938), (0., 1.5224522),
       (0., 1.5226257), (0., 1.5226568), (0., 1.5226772), (0., 1.523094 ),
       (0., 1.5232229), (0., 1.5234311), (0., 1.5236325), (0., 1.5240226),
       (0., 1.5240831), (0., 1.5243554), (0., 1.5247465), (0., 1.5247577),
       (0., 1.5248715), (0., 1.5249307), (0., 1.5249354), (0., 1.5250654),
       (0., 1.5251311), (0., 1.5252463), (0., 1.5254091), (0., 1.5261077),
       (0., 1.5262443), (0., 1.5262954), (0., 1.5264982), (0., 1.5267969),
       (0., 1.5269305), (0., 1.5271316), (0., 1.5283904), (0., 1.5285952),
       (0., 1.5289631), (0., 1.529459 ), (0., 1.5295273), (0., 1.5297214),
       (0., 1.5297469), (0., 1.5303003), (0., 1.5308027), (0., 1.5311264),
       (0., 1.5313959), (0., 1.531461 ), (0., 1.5316353), (0., 1.5318425),
       (0., 1.5318741), (0., 1.5320864), (0., 1.5327845), (0., 1.5329207),
       (0., 1.5346036), (0., 1.535814 ), (0., 1.5378035), (0., 1.539536 ),
       (0., 1.5449953)], dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(9.222014 , 9.348008 ), (7.8897758, 8.182048 ),
       (7.8327575, 8.687351 ), (7.405425 , 7.4644203),
       (6.671001 , 8.025321 ), (5.7010307, 5.7482505),
       (5.5597587, 5.963574 ), (5.491776 , 6.4446583),
       (5.434302 , 7.648978 ), (4.725589 , 4.8466797),
       (4.370456 , 4.576037 ), (4.324441 , 4.6467156),
       (4.285597 , 4.76581  ), (4.262283 , 4.402128 ),
       (4.2491093, 4.3750677), (4.227562 , 4.2439156),
       (4.218007 , 4.7630687), (4.206203 , 4.2439156),
       (4.1839976, 4.741951 ), (4.168101 , 4.3982344),
       (4.143858 , 4.541594 ), (4.128914 , 4.6914725),
       (4.118832 , 4.1488485), (4.118093 , 4.34528  ),
       (4.094073 , 4.3766556), (4.090932 , 4.4591737),
       (4.067721 , 4.715913 ), (4.055542 , 4.5302377),
       (4.046618 , 4.3153067), (4.0396767, 4.499901 ),
       (4.0251493, 4.3029466), (4.0189457, 4.14228  ),
       (4.014563 , 4.252084 ), (4.0127544, 4.3050027),
       (4.007906 , 4.2981915), (4.001438 , 4.5331583),
       (3.9952593, 4.6789966), (3.9932094, 4.768332 ),
       (3.9748774, 4.3710413), (3.9736264, 4.6001825),
       (3.9695702, 4.4118123), (3.9649215, 4.380718 ),
       (3.9648507, 4.4503603), (3.9642332, 4.656466 ),
       (3.9607923, 5.0862017), (3.9606028, 4.468331 ),
       (3.9422681, 8.724551 ), (3.9418216, 4.35286  ),
       (3.9342055, 4.1743345), (3.9302235, 9.95261  ),
       (3.927756 , 4.035401 ), (3.9268944, 4.8928733),
       (3.891598 , 4.7144647), (3.8876796, 4.2470207),
       (3.8553922, 4.8370414), (3.81076  , 4.726598 ),
       (3.7963626, 4.7929015), (3.7469487, 4.1954527)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(10.287312, 12.019882), ( 8.188061,  8.222312)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3194580078125), (0.0, 1.3199824094772339), (0.0, 1.3206703662872314), (0.0, 1.3208755254745483), (0.0, 1.3212676048278809), (0.0, 1.3214423656463623), (0.0, 1.3217356204986572), (0.0, 1.3217484951019287), (0.0, 1.3217862844467163), (0.0, 1.3218101263046265), (0.0, 1.3220356702804565), (0.0, 1.3221721649169922), (0.0, 1.3225723505020142), (0.0, 1.322670578956604), (0.0, 1.3228144645690918), (0.0, 1.3229126930236816), (0.0, 1.3231772184371948), (0.0, 1.3231971263885498), (0.0, 1.3233070373535156), (0.0, 1.3235868215560913), (0.0, 1.3236033916473389), (0.0, 1.3237135410308838), (0.0, 1.3237377405166626), (0.0, 1.324000358581543), (0.0, 1.324162483215332), (0.0, 1.324295163154602), (0.0, 1.3242979049682617), (0.0, 1.3243091106414795), (0.0, 1.324345350265503), (0.0, 1.3243530988693237), (0.0, 1.3243677616119385), (0.0, 1.324381709098816), (0.0, 1.3243842124938965), (0.0, 1.324404001235962), (0.0, 1.324417233467102), (0.0, 1.324432373046875), (0.0, 1.3244961500167847), (0.0, 1.3245307207107544), (0.0, 1.3245750665664673), (0.0, 1.32463800907135), (0.0, 1.3250255584716797), (0.0, 1.3250486850738525), (0.0, 1.3251851797103882), (0.0, 1.325317144393921), (0.0, 1.3255021572113037), (0.0, 1.3256001472473145), (0.0, 1.3256747722625732), (0.0, 1.3257709741592407), (0.0, 1.3257715702056885), (0.0, 1.3258963823318481), (0.0, 1.3259152173995972), (0.0, 1.3259673118591309), (0.0, 1.3259973526000977), (0.0, 1.3260067701339722), (0.0, 1.3260152339935303), (0.0, 1.3260658979415894), (0.0, 1.3260668516159058), (0.0, 1.3260847330093384), (0.0, 1.3260924816131592), (0.0, 1.326094388961792), (0.0, 1.3261877298355103), (0.0, 1.3262407779693604), (0.0, 1.3262770175933838), (0.0, 1.326296329498291), (0.0, 1.3264302015304565), (0.0, 1.3265165090560913), (0.0, 1.3265208005905151), (0.0, 1.3268064260482788), (0.0, 1.3268399238586426), (0.0, 1.326982021331787), (0.0, 1.3270525932312012), (0.0, 1.3270556926727295), (0.0, 1.3271369934082031), (0.0, 1.32719886302948), (0.0, 1.3272169828414917), (0.0, 1.3272862434387207), (0.0, 1.3273470401763916), (0.0, 1.327348232269287), (0.0, 1.3274563550949097), (0.0, 1.327526330947876), (0.0, 1.327567219734192), (0.0, 1.3275737762451172), (0.0, 1.3275789022445679), (0.0, 1.3275816440582275), (0.0, 1.3276454210281372), (0.0, 1.3278456926345825), (0.0, 1.3278764486312866), (0.0, 1.3279870748519897), (0.0, 1.3280887603759766), (0.0, 1.3281447887420654), (0.0, 1.3291012048721313), (0.0, 1.3292045593261719), (0.0, 1.3294256925582886), (0.0, 1.329437255859375), (0.0, 1.329723834991455), (0.0, 1.3301198482513428), (0.0, 1.331295371055603), (0.0, 1.3317935466766357), (0.0, 1.332617163658142), (0.0, 1.3337877988815308), (0.0, 1.3609079122543335), (0.0, 1.4410189390182495), (0.0, 1.4421998262405396), (0.0, 1.4437196254730225), (0.0, 1.4440709352493286), (0.0, 1.4445313215255737), (0.0, 1.4450000524520874), (0.0, 1.4450292587280273), (0.0, 1.445064902305603), (0.0, 1.445085883140564), (0.0, 1.445720911026001), (0.0, 1.4457273483276367), (0.0, 1.445768117904663), (0.0, 1.4459929466247559), (0.0, 1.4461677074432373), (0.0, 1.4463309049606323), (0.0, 1.4463768005371094), (0.0, 1.446612000465393), (0.0, 1.4466229677200317), (0.0, 1.446776270866394), (0.0, 1.4471231698989868), (0.0, 1.4472202062606812), (0.0, 1.4474364519119263), (0.0, 1.4475891590118408), (0.0, 1.447702407836914), (0.0, 1.4478931427001953), (0.0, 1.4479122161865234), (0.0, 1.4482758045196533), (0.0, 1.4483786821365356), (0.0, 1.4486427307128906), (0.0, 1.4488437175750732), (0.0, 1.4488540887832642), (0.0, 1.4489272832870483), (0.0, 1.448973298072815), (0.0, 1.4490443468093872), (0.0, 1.4490903615951538), (0.0, 1.4493101835250854), (0.0, 1.4493108987808228), (0.0, 1.4493119716644287), (0.0, 1.4495466947555542), (0.0, 1.4496536254882812), (0.0, 1.4498543739318848), (0.0, 1.4501514434814453), (0.0, 1.4503240585327148), (0.0, 1.45052969455719), (0.0, 1.4505949020385742), (0.0, 1.4506038427352905), (0.0, 1.4507060050964355), (0.0, 1.4511466026306152), (0.0, 1.4511891603469849), (0.0, 1.4512203931808472), (0.0, 1.451532244682312), (0.0, 1.4517416954040527), (0.0, 1.4520713090896606), (0.0, 1.452136754989624), (0.0, 1.4521420001983643), (0.0, 1.4524909257888794), (0.0, 1.452494502067566), (0.0, 1.4527623653411865), (0.0, 1.4527987241744995), (0.0, 1.4528183937072754), (0.0, 1.4530214071273804), (0.0, 1.453127145767212), (0.0, 1.4531725645065308), (0.0, 1.4533096551895142), (0.0, 1.4534252882003784), (0.0, 1.4536411762237549), (0.0, 1.4536738395690918), (0.0, 1.4539530277252197), (0.0, 1.4540379047393799), (0.0, 1.4541630744934082), (0.0, 1.4541778564453125), (0.0, 1.454324722290039), (0.0, 1.4550093412399292), (0.0, 1.4550657272338867), (0.0, 1.4554026126861572), (0.0, 1.455615520477295), (0.0, 1.4557173252105713), (0.0, 1.4557865858078003), (0.0, 1.4569740295410156), (0.0, 1.4574432373046875), (0.0, 1.4577912092208862), (0.0, 1.45823073387146), (0.0, 1.4582934379577637), (0.0, 1.4587092399597168), (0.0, 1.4588559865951538), (0.0, 1.4589413404464722), (0.0, 1.4590712785720825), (0.0, 1.459160566329956), (0.0, 1.459481120109558), (0.0, 1.4595438241958618), (0.0, 1.4597376585006714), (0.0, 1.4600616693496704), (0.0, 1.4600892066955566), (0.0, 1.4603066444396973), (0.0, 1.460338830947876), (0.0, 1.4605562686920166), (0.0, 1.4618295431137085), (0.0, 1.4626530408859253), (0.0, 1.4631714820861816), (0.0, 1.4633245468139648), (0.0, 1.4730339050292969), (0.0, 1.4738445281982422), (0.0, 1.5089163780212402), (0.0, 1.508943796157837), (0.0, 1.5089653730392456), (0.0, 1.5097912549972534), (0.0, 1.5099608898162842), (0.0, 1.5100277662277222), (0.0, 1.5118621587753296), (0.0, 1.5119532346725464), (0.0, 1.5119833946228027), (0.0, 1.512208342552185), (0.0, 1.512345790863037), (0.0, 1.5138194561004639), (0.0, 1.5140248537063599), (0.0, 1.514141321182251), (0.0, 1.5142427682876587), (0.0, 1.5145785808563232), (0.0, 1.5156129598617554), (0.0, 1.5159066915512085), (0.0, 1.5162733793258667), (0.0, 1.516317367553711), (0.0, 1.5165250301361084), (0.0, 1.516579270362854), (0.0, 1.5168614387512207), (0.0, 1.5170783996582031), (0.0, 1.5183563232421875), (0.0, 1.518524169921875), (0.0, 1.5188288688659668), (0.0, 1.5188528299331665), (0.0, 1.5190552473068237), (0.0, 1.5190839767456055), (0.0, 1.5191115140914917), (0.0, 1.5192745923995972), (0.0, 1.5195280313491821), (0.0, 1.519667387008667), (0.0, 1.5197263956069946), (0.0, 1.5199463367462158), (0.0, 1.5203375816345215), (0.0, 1.52043616771698), (0.0, 1.5205886363983154), (0.0, 1.5206431150436401), (0.0, 1.5206737518310547), (0.0, 1.5206745862960815), (0.0, 1.520978331565857), (0.0, 1.5210694074630737), (0.0, 1.521124243736267), (0.0, 1.5212757587432861), (0.0, 1.5214873552322388), (0.0, 1.5215450525283813), (0.0, 1.5217660665512085), (0.0, 1.5218875408172607), (0.0, 1.521931529045105), (0.0, 1.522393822669983), (0.0, 1.5224522352218628), (0.0, 1.5226256847381592), (0.0, 1.522656798362732), (0.0, 1.5226771831512451), (0.0, 1.5230940580368042), (0.0, 1.5232229232788086), (0.0, 1.5234310626983643), (0.0, 1.523632526397705), (0.0, 1.5240225791931152), (0.0, 1.524083137512207), (0.0, 1.524355411529541), (0.0, 1.5247465372085571), (0.0, 1.524757742881775), (0.0, 1.5248714685440063), (0.0, 1.524930715560913), (0.0, 1.5249353647232056), (0.0, 1.5250654220581055), (0.0, 1.525131106376648), (0.0, 1.525246262550354), (0.0, 1.5254091024398804), (0.0, 1.526107668876648), (0.0, 1.5262442827224731), (0.0, 1.5262954235076904), (0.0, 1.5264981985092163), (0.0, 1.5267969369888306), (0.0, 1.5269304513931274), (0.0, 1.5271315574645996), (0.0, 1.5283904075622559), (0.0, 1.528595209121704), (0.0, 1.5289630889892578), (0.0, 1.529458999633789), (0.0, 1.5295273065567017), (0.0, 1.5297213792800903), (0.0, 1.5297468900680542), (0.0, 1.530300259590149), (0.0, 1.5308027267456055), (0.0, 1.5311263799667358), (0.0, 1.5313959121704102), (0.0, 1.5314610004425049), (0.0, 1.5316352844238281), (0.0, 1.5318424701690674), (0.0, 1.5318740606307983), (0.0, 1.5320863723754883), (0.0, 1.5327844619750977), (0.0, 1.5329207181930542), (0.0, 1.5346035957336426), (0.0, 1.5358140468597412), (0.0, 1.5378035306930542), (0.0, 1.5395359992980957), (0.0, 1.5449953079223633)], [(9.222014427185059, 9.348008155822754), (7.88977575302124, 8.182047843933105), (7.832757472991943, 8.68735122680664), (7.405425071716309, 7.464420318603516), (6.671000957489014, 8.025321006774902), (5.701030731201172, 5.748250484466553), (5.55975866317749, 5.963573932647705), (5.491775989532471, 6.444658279418945), (5.434301853179932, 7.648978233337402), (4.725588798522949, 4.8466796875), (4.370456218719482, 4.5760369300842285), (4.324440956115723, 4.6467156410217285), (4.28559684753418, 4.765810012817383), (4.262282848358154, 4.402128219604492), (4.249109268188477, 4.375067710876465), (4.227561950683594, 4.243915557861328), (4.2180070877075195, 4.763068675994873), (4.206202983856201, 4.243915557861328), (4.183997631072998, 4.741950988769531), (4.168100833892822, 4.3982343673706055), (4.143857955932617, 4.5415940284729), (4.128913879394531, 4.69147253036499), (4.118832111358643, 4.148848533630371), (4.118093013763428, 4.345280170440674), (4.0940728187561035, 4.376655578613281), (4.0909318923950195, 4.459173679351807), (4.067720890045166, 4.715912818908691), (4.0555419921875, 4.530237674713135), (4.0466179847717285, 4.315306663513184), (4.039676666259766, 4.499900817871094), (4.025149345397949, 4.3029465675354), (4.018945693969727, 4.142280101776123), (4.014563083648682, 4.252083778381348), (4.012754440307617, 4.305002689361572), (4.007905960083008, 4.298191547393799), (4.001438140869141, 4.533158302307129), (3.9952592849731445, 4.678996562957764), (3.9932093620300293, 4.768332004547119), (3.97487735748291, 4.371041297912598), (3.9736263751983643, 4.60018253326416), (3.9695701599121094, 4.4118123054504395), (3.964921474456787, 4.380718231201172), (3.964850664138794, 4.450360298156738), (3.964233160018921, 4.656466007232666), (3.960792303085327, 5.0862016677856445), (3.9606027603149414, 4.4683308601379395), (3.942268133163452, 8.7245512008667), (3.941821575164795, 4.352859973907471), (3.9342055320739746, 4.174334526062012), (3.9302234649658203, 9.95261001586914), (3.9277560710906982, 4.035400867462158), (3.926894426345825, 4.892873287200928), (3.8915979862213135, 4.7144646644592285), (3.8876795768737793, 4.247020721435547), (3.8553922176361084, 4.83704137802124), (3.810760021209717, 4.726597785949707), (3.7963626384735107, 4.792901515960693), (3.746948719024658, 4.195452690124512)], [(10.287311553955078, 12.019882202148438), (8.188060760498047, 8.222311973571777)]]
[array([[0.        , 1.31945801],
       [0.        , 1.31998241],
       [0.        , 1.32067037],
       [0.        , 1.32087553],
       [0.        , 1.3212676 ],
       [0.        , 1.32144237],
       [0.        , 1.32173562],
       [0.        , 1.3217485 ],
       [0.        , 1.32178628],
       [0.        , 1.32181013],
       [0.        , 1.32203567],
       [0.        , 1.32217216],
       [0.        , 1.32257235],
       [0.        , 1.32267058],
       [0.        , 1.32281446],
       [0.        , 1.32291269],
       [0.        , 1.32317722],
       [0.        , 1.32319713],
       [0.        , 1.32330704],
       [0.        , 1.32358682],
       [0.        , 1.32360339],
       [0.        , 1.32371354],
       [0.        , 1.32373774],
       [0.        , 1.32400036],
       [0.        , 1.32416248],
       [0.        , 1.32429516],
       [0.        , 1.3242979 ],
       [0.        , 1.32430911],
       [0.        , 1.32434535],
       [0.        , 1.3243531 ],
       [0.        , 1.32436776],
       [0.        , 1.32438171],
       [0.        , 1.32438421],
       [0.        , 1.324404  ],
       [0.        , 1.32441723],
       [0.        , 1.32443237],
       [0.        , 1.32449615],
       [0.        , 1.32453072],
       [0.        , 1.32457507],
       [0.        , 1.32463801],
       [0.        , 1.32502556],
       [0.        , 1.32504869],
       [0.        , 1.32518518],
       [0.        , 1.32531714],
       [0.        , 1.32550216],
       [0.        , 1.32560015],
       [0.        , 1.32567477],
       [0.        , 1.32577097],
       [0.        , 1.32577157],
       [0.        , 1.32589638],
       [0.        , 1.32591522],
       [0.        , 1.32596731],
       [0.        , 1.32599735],
       [0.        , 1.32600677],
       [0.        , 1.32601523],
       [0.        , 1.3260659 ],
       [0.        , 1.32606685],
       [0.        , 1.32608473],
       [0.        , 1.32609248],
       [0.        , 1.32609439],
       [0.        , 1.32618773],
       [0.        , 1.32624078],
       [0.        , 1.32627702],
       [0.        , 1.32629633],
       [0.        , 1.3264302 ],
       [0.        , 1.32651651],
       [0.        , 1.3265208 ],
       [0.        , 1.32680643],
       [0.        , 1.32683992],
       [0.        , 1.32698202],
       [0.        , 1.32705259],
       [0.        , 1.32705569],
       [0.        , 1.32713699],
       [0.        , 1.32719886],
       [0.        , 1.32721698],
       [0.        , 1.32728624],
       [0.        , 1.32734704],
       [0.        , 1.32734823],
       [0.        , 1.32745636],
       [0.        , 1.32752633],
       [0.        , 1.32756722],
       [0.        , 1.32757378],
       [0.        , 1.3275789 ],
       [0.        , 1.32758164],
       [0.        , 1.32764542],
       [0.        , 1.32784569],
       [0.        , 1.32787645],
       [0.        , 1.32798707],
       [0.        , 1.32808876],
       [0.        , 1.32814479],
       [0.        , 1.3291012 ],
       [0.        , 1.32920456],
       [0.        , 1.32942569],
       [0.        , 1.32943726],
       [0.        , 1.32972383],
       [0.        , 1.33011985],
       [0.        , 1.33129537],
       [0.        , 1.33179355],
       [0.        , 1.33261716],
       [0.        , 1.3337878 ],
       [0.        , 1.36090791],
       [0.        , 1.44101894],
       [0.        , 1.44219983],
       [0.        , 1.44371963],
       [0.        , 1.44407094],
       [0.        , 1.44453132],
       [0.        , 1.44500005],
       [0.        , 1.44502926],
       [0.        , 1.4450649 ],
       [0.        , 1.44508588],
       [0.        , 1.44572091],
       [0.        , 1.44572735],
       [0.        , 1.44576812],
       [0.        , 1.44599295],
       [0.        , 1.44616771],
       [0.        , 1.4463309 ],
       [0.        , 1.4463768 ],
       [0.        , 1.446612  ],
       [0.        , 1.44662297],
       [0.        , 1.44677627],
       [0.        , 1.44712317],
       [0.        , 1.44722021],
       [0.        , 1.44743645],
       [0.        , 1.44758916],
       [0.        , 1.44770241],
       [0.        , 1.44789314],
       [0.        , 1.44791222],
       [0.        , 1.4482758 ],
       [0.        , 1.44837868],
       [0.        , 1.44864273],
       [0.        , 1.44884372],
       [0.        , 1.44885409],
       [0.        , 1.44892728],
       [0.        , 1.4489733 ],
       [0.        , 1.44904435],
       [0.        , 1.44909036],
       [0.        , 1.44931018],
       [0.        , 1.4493109 ],
       [0.        , 1.44931197],
       [0.        , 1.44954669],
       [0.        , 1.44965363],
       [0.        , 1.44985437],
       [0.        , 1.45015144],
       [0.        , 1.45032406],
       [0.        , 1.45052969],
       [0.        , 1.4505949 ],
       [0.        , 1.45060384],
       [0.        , 1.45070601],
       [0.        , 1.4511466 ],
       [0.        , 1.45118916],
       [0.        , 1.45122039],
       [0.        , 1.45153224],
       [0.        , 1.4517417 ],
       [0.        , 1.45207131],
       [0.        , 1.45213675],
       [0.        , 1.452142  ],
       [0.        , 1.45249093],
       [0.        , 1.4524945 ],
       [0.        , 1.45276237],
       [0.        , 1.45279872],
       [0.        , 1.45281839],
       [0.        , 1.45302141],
       [0.        , 1.45312715],
       [0.        , 1.45317256],
       [0.        , 1.45330966],
       [0.        , 1.45342529],
       [0.        , 1.45364118],
       [0.        , 1.45367384],
       [0.        , 1.45395303],
       [0.        , 1.4540379 ],
       [0.        , 1.45416307],
       [0.        , 1.45417786],
       [0.        , 1.45432472],
       [0.        , 1.45500934],
       [0.        , 1.45506573],
       [0.        , 1.45540261],
       [0.        , 1.45561552],
       [0.        , 1.45571733],
       [0.        , 1.45578659],
       [0.        , 1.45697403],
       [0.        , 1.45744324],
       [0.        , 1.45779121],
       [0.        , 1.45823073],
       [0.        , 1.45829344],
       [0.        , 1.45870924],
       [0.        , 1.45885599],
       [0.        , 1.45894134],
       [0.        , 1.45907128],
       [0.        , 1.45916057],
       [0.        , 1.45948112],
       [0.        , 1.45954382],
       [0.        , 1.45973766],
       [0.        , 1.46006167],
       [0.        , 1.46008921],
       [0.        , 1.46030664],
       [0.        , 1.46033883],
       [0.        , 1.46055627],
       [0.        , 1.46182954],
       [0.        , 1.46265304],
       [0.        , 1.46317148],
       [0.        , 1.46332455],
       [0.        , 1.47303391],
       [0.        , 1.47384453],
       [0.        , 1.50891638],
       [0.        , 1.5089438 ],
       [0.        , 1.50896537],
       [0.        , 1.50979125],
       [0.        , 1.50996089],
       [0.        , 1.51002777],
       [0.        , 1.51186216],
       [0.        , 1.51195323],
       [0.        , 1.51198339],
       [0.        , 1.51220834],
       [0.        , 1.51234579],
       [0.        , 1.51381946],
       [0.        , 1.51402485],
       [0.        , 1.51414132],
       [0.        , 1.51424277],
       [0.        , 1.51457858],
       [0.        , 1.51561296],
       [0.        , 1.51590669],
       [0.        , 1.51627338],
       [0.        , 1.51631737],
       [0.        , 1.51652503],
       [0.        , 1.51657927],
       [0.        , 1.51686144],
       [0.        , 1.5170784 ],
       [0.        , 1.51835632],
       [0.        , 1.51852417],
       [0.        , 1.51882887],
       [0.        , 1.51885283],
       [0.        , 1.51905525],
       [0.        , 1.51908398],
       [0.        , 1.51911151],
       [0.        , 1.51927459],
       [0.        , 1.51952803],
       [0.        , 1.51966739],
       [0.        , 1.5197264 ],
       [0.        , 1.51994634],
       [0.        , 1.52033758],
       [0.        , 1.52043617],
       [0.        , 1.52058864],
       [0.        , 1.52064312],
       [0.        , 1.52067375],
       [0.        , 1.52067459],
       [0.        , 1.52097833],
       [0.        , 1.52106941],
       [0.        , 1.52112424],
       [0.        , 1.52127576],
       [0.        , 1.52148736],
       [0.        , 1.52154505],
       [0.        , 1.52176607],
       [0.        , 1.52188754],
       [0.        , 1.52193153],
       [0.        , 1.52239382],
       [0.        , 1.52245224],
       [0.        , 1.52262568],
       [0.        , 1.5226568 ],
       [0.        , 1.52267718],
       [0.        , 1.52309406],
       [0.        , 1.52322292],
       [0.        , 1.52343106],
       [0.        , 1.52363253],
       [0.        , 1.52402258],
       [0.        , 1.52408314],
       [0.        , 1.52435541],
       [0.        , 1.52474654],
       [0.        , 1.52475774],
       [0.        , 1.52487147],
       [0.        , 1.52493072],
       [0.        , 1.52493536],
       [0.        , 1.52506542],
       [0.        , 1.52513111],
       [0.        , 1.52524626],
       [0.        , 1.5254091 ],
       [0.        , 1.52610767],
       [0.        , 1.52624428],
       [0.        , 1.52629542],
       [0.        , 1.5264982 ],
       [0.        , 1.52679694],
       [0.        , 1.52693045],
       [0.        , 1.52713156],
       [0.        , 1.52839041],
       [0.        , 1.52859521],
       [0.        , 1.52896309],
       [0.        , 1.529459  ],
       [0.        , 1.52952731],
       [0.        , 1.52972138],
       [0.        , 1.52974689],
       [0.        , 1.53030026],
       [0.        , 1.53080273],
       [0.        , 1.53112638],
       [0.        , 1.53139591],
       [0.        , 1.531461  ],
       [0.        , 1.53163528],
       [0.        , 1.53184247],
       [0.        , 1.53187406],
       [0.        , 1.53208637],
       [0.        , 1.53278446],
       [0.        , 1.53292072],
       [0.        , 1.5346036 ],
       [0.        , 1.53581405],
       [0.        , 1.53780353],
       [0.        , 1.539536  ],
       [0.        , 1.54499531]]), array([[9.22201443, 9.34800816],
       [7.88977575, 8.18204784],
       [7.83275747, 8.68735123],
       [7.40542507, 7.46442032],
       [6.67100096, 8.02532101],
       [5.70103073, 5.74825048],
       [5.55975866, 5.96357393],
       [5.49177599, 6.44465828],
       [5.43430185, 7.64897823],
       [4.7255888 , 4.84667969],
       [4.37045622, 4.57603693],
       [4.32444096, 4.64671564],
       [4.28559685, 4.76581001],
       [4.26228285, 4.40212822],
       [4.24910927, 4.37506771],
       [4.22756195, 4.24391556],
       [4.21800709, 4.76306868],
       [4.20620298, 4.24391556],
       [4.18399763, 4.74195099],
       [4.16810083, 4.39823437],
       [4.14385796, 4.54159403],
       [4.12891388, 4.69147253],
       [4.11883211, 4.14884853],
       [4.11809301, 4.34528017],
       [4.09407282, 4.37665558],
       [4.09093189, 4.45917368],
       [4.06772089, 4.71591282],
       [4.05554199, 4.53023767],
       [4.04661798, 4.31530666],
       [4.03967667, 4.49990082],
       [4.02514935, 4.30294657],
       [4.01894569, 4.1422801 ],
       [4.01456308, 4.25208378],
       [4.01275444, 4.30500269],
       [4.00790596, 4.29819155],
       [4.00143814, 4.5331583 ],
       [3.99525928, 4.67899656],
       [3.99320936, 4.768332  ],
       [3.97487736, 4.3710413 ],
       [3.97362638, 4.60018253],
       [3.96957016, 4.41181231],
       [3.96492147, 4.38071823],
       [3.96485066, 4.4503603 ],
       [3.96423316, 4.65646601],
       [3.9607923 , 5.08620167],
       [3.96060276, 4.46833086],
       [3.94226813, 8.7245512 ],
       [3.94182158, 4.35285997],
       [3.93420553, 4.17433453],
       [3.93022346, 9.95261002],
       [3.92775607, 4.03540087],
       [3.92689443, 4.89287329],
       [3.89159799, 4.71446466],
       [3.88767958, 4.24702072],
       [3.85539222, 4.83704138],
       [3.81076002, 4.72659779],
       [3.79636264, 4.79290152],
       [3.74694872, 4.19545269]]), array([[10.28731155, 12.0198822 ],
       [ 8.18806076,  8.22231197]])]2024-03-06 17:57:58.736761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6L1R ph vector generated, counter: 171
2024-03-06 17:58:02.356263: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:02.399816: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:03.308368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L1S ph vector generated, counter: 172
2024-03-06 17:58:06.452610: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:06.495502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:07.406842: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L1Y ph vector generated, counter: 173
2024-03-06 17:58:10.572368: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:10.615251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:11.480695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L1Z ph vector generated, counter: 174
2024-03-06 17:58:14.637016: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:14.679568: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:15.564574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L21 ph vector generated, counter: 175
2024-03-06 17:58:18.788988: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:18.831306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:19.980883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L22 ph vector generated, counter: 176
2024-03-06 17:58:23.123995: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:23.167085: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:24.138362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L23 ph vector generated, counter: 177
2024-03-06 17:58:27.701973: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:27.744550: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:28.651490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L24 ph vector generated, counter: 178
2024-03-06 17:58:31.649833: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:31.692433: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:32.570421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3281012), (0., 1.3288006), (0., 1.3299569), (0., 1.3304443),
       (0., 1.331178 ), (0., 1.3314277), (0., 1.3314514), (0., 1.3314651),
       (0., 1.3316202), (0., 1.331749 ), (0., 1.3317657), (0., 1.3318129),
       (0., 1.331939 ), (0., 1.3321213), (0., 1.332206 ), (0., 1.3322605),
       (0., 1.3325078), (0., 1.3325883), (0., 1.3328265), (0., 1.3331596),
       (0., 1.3333124), (0., 1.3334535), (0., 1.3335192), (0., 1.3337377),
       (0., 1.3338575), (0., 1.3341016), (0., 1.3341395), (0., 1.334159 ),
       (0., 1.3345726), (0., 1.3346341), (0., 1.334762 ), (0., 1.3347787),
       (0., 1.3350314), (0., 1.3351616), (0., 1.3353968), (0., 1.3354158),
       (0., 1.3354481), (0., 1.3354928), (0., 1.3354939), (0., 1.3355026),
       (0., 1.3357822), (0., 1.3363074), (0., 1.3363843), (0., 1.3363919),
       (0., 1.3364356), (0., 1.3365659), (0., 1.3366044), (0., 1.336622 ),
       (0., 1.3366377), (0., 1.3370197), (0., 1.337079 ), (0., 1.337242 ),
       (0., 1.3372537), (0., 1.3373618), (0., 1.3374926), (0., 1.3376821),
       (0., 1.3378319), (0., 1.3384321), (0., 1.3384557), (0., 1.3385347),
       (0., 1.3387325), (0., 1.3393495), (0., 1.3393551), (0., 1.3411183),
       (0., 1.3417163), (0., 1.3421912), (0., 1.3422612), (0., 1.4456424),
       (0., 1.4519764), (0., 1.454422 ), (0., 1.4545994), (0., 1.4546801),
       (0., 1.455143 ), (0., 1.4551626), (0., 1.4558929), (0., 1.456232 ),
       (0., 1.4562918), (0., 1.4569712), (0., 1.4571797), (0., 1.4575275),
       (0., 1.4576509), (0., 1.4578696), (0., 1.4579084), (0., 1.4580672),
       (0., 1.4580938), (0., 1.4581968), (0., 1.4582087), (0., 1.4582365),
       (0., 1.458582 ), (0., 1.4586897), (0., 1.4588026), (0., 1.4589502),
       (0., 1.4590825), (0., 1.459656 ), (0., 1.4597607), (0., 1.4597765),
       (0., 1.4598032), (0., 1.4598768), (0., 1.4598821), (0., 1.4599019),
       (0., 1.4604194), (0., 1.4604506), (0., 1.4604882), (0., 1.4605038),
       (0., 1.4606031), (0., 1.4607629), (0., 1.460763 ), (0., 1.4610889),
       (0., 1.461226 ), (0., 1.4612615), (0., 1.4613727), (0., 1.4614111),
       (0., 1.4614731), (0., 1.4618841), (0., 1.461928 ), (0., 1.4623482),
       (0., 1.4624736), (0., 1.4628272), (0., 1.4630249), (0., 1.4630834),
       (0., 1.4631127), (0., 1.4631394), (0., 1.4631947), (0., 1.4633324),
       (0., 1.4634778), (0., 1.46363  ), (0., 1.4636674), (0., 1.4637514),
       (0., 1.4647   ), (0., 1.4649597), (0., 1.4650395), (0., 1.46701  ),
       (0., 1.4679784), (0., 1.4708933), (0., 1.4724852), (0., 1.5100178),
       (0., 1.5133566), (0., 1.5150667), (0., 1.5176141), (0., 1.5185274),
       (0., 1.5187812), (0., 1.5191616), (0., 1.5192001), (0., 1.5194757),
       (0., 1.5203289), (0., 1.5204895), (0., 1.5207696), (0., 1.5209543),
       (0., 1.5214083), (0., 1.5216696), (0., 1.5216959), (0., 1.5218277),
       (0., 1.5218738), (0., 1.5221019), (0., 1.522229 ), (0., 1.5223402),
       (0., 1.5224866), (0., 1.5226995), (0., 1.5231547), (0., 1.5231625),
       (0., 1.5232549), (0., 1.523308 ), (0., 1.5233941), (0., 1.5234996),
       (0., 1.5235974), (0., 1.5236037), (0., 1.5237092), (0., 1.5239682),
       (0., 1.524123 ), (0., 1.5241425), (0., 1.5242239), (0., 1.5243292),
       (0., 1.5246001), (0., 1.5250202), (0., 1.5251878), (0., 1.5252804),
       (0., 1.525681 ), (0., 1.5258373), (0., 1.5259391), (0., 1.5260551),
       (0., 1.5260588), (0., 1.5260779), (0., 1.5261452), (0., 1.5261884),
       (0., 1.5264508), (0., 1.5264906), (0., 1.5265088), (0., 1.5265123),
       (0., 1.52654  ), (0., 1.5265437), (0., 1.5267938), (0., 1.5269152),
       (0., 1.5273358), (0., 1.5273732), (0., 1.527889 ), (0., 1.5279086),
       (0., 1.5279865), (0., 1.5285583), (0., 1.5304796), (0., 1.5306562),
       (0., 1.5306828), (0., 1.5316644), (0., 1.5332762)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(7.9265394, 8.265917 ), (7.804293 , 7.9778743),
       (7.2426   , 8.29456  ), (5.354081 , 5.4344244),
       (4.17453  , 8.5399475)], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([], dtype=float64)}
Result dictionary stored.
[[(0.0, 1.3281011581420898), (0.0, 1.3288005590438843), (0.0, 1.3299568891525269), (0.0, 1.3304443359375), (0.0, 1.3311779499053955), (0.0, 1.3314276933670044), (0.0, 1.331451416015625), (0.0, 1.3314651250839233), (0.0, 1.331620216369629), (0.0, 1.3317489624023438), (0.0, 1.3317656517028809), (0.0, 1.331812858581543), (0.0, 1.3319389820098877), (0.0, 1.3321212530136108), (0.0, 1.3322060108184814), (0.0, 1.3322604894638062), (0.0, 1.332507848739624), (0.0, 1.3325883150100708), (0.0, 1.3328264951705933), (0.0, 1.3331595659255981), (0.0, 1.3333123922348022), (0.0, 1.3334535360336304), (0.0, 1.3335192203521729), (0.0, 1.3337377309799194), (0.0, 1.333857536315918), (0.0, 1.3341015577316284), (0.0, 1.3341394662857056), (0.0, 1.334159016609192), (0.0, 1.3345725536346436), (0.0, 1.3346340656280518), (0.0, 1.3347619771957397), (0.0, 1.3347786664962769), (0.0, 1.3350313901901245), (0.0, 1.335161566734314), (0.0, 1.3353967666625977), (0.0, 1.3354158401489258), (0.0, 1.335448145866394), (0.0, 1.3354928493499756), (0.0, 1.3354939222335815), (0.0, 1.3355026245117188), (0.0, 1.3357821702957153), (0.0, 1.336307406425476), (0.0, 1.3363842964172363), (0.0, 1.3363919258117676), (0.0, 1.3364355564117432), (0.0, 1.3365658521652222), (0.0, 1.336604356765747), (0.0, 1.3366219997406006), (0.0, 1.3366377353668213), (0.0, 1.337019681930542), (0.0, 1.3370790481567383), (0.0, 1.3372420072555542), (0.0, 1.3372536897659302), (0.0, 1.3373618125915527), (0.0, 1.33749258518219), (0.0, 1.3376821279525757), (0.0, 1.3378318548202515), (0.0, 1.3384320735931396), (0.0, 1.3384556770324707), (0.0, 1.3385347127914429), (0.0, 1.3387324810028076), (0.0, 1.3393495082855225), (0.0, 1.3393551111221313), (0.0, 1.341118335723877), (0.0, 1.3417162895202637), (0.0, 1.342191219329834), (0.0, 1.3422611951828003), (0.0, 1.445642352104187), (0.0, 1.4519764184951782), (0.0, 1.4544219970703125), (0.0, 1.454599380493164), (0.0, 1.45468008518219), (0.0, 1.4551429748535156), (0.0, 1.4551626443862915), (0.0, 1.4558929204940796), (0.0, 1.456231951713562), (0.0, 1.4562917947769165), (0.0, 1.4569711685180664), (0.0, 1.4571796655654907), (0.0, 1.4575275182724), (0.0, 1.457650899887085), (0.0, 1.4578696489334106), (0.0, 1.4579083919525146), (0.0, 1.4580671787261963), (0.0, 1.4580937623977661), (0.0, 1.458196759223938), (0.0, 1.458208680152893), (0.0, 1.4582364559173584), (0.0, 1.4585820436477661), (0.0, 1.4586896896362305), (0.0, 1.458802580833435), (0.0, 1.458950161933899), (0.0, 1.4590824842453003), (0.0, 1.459656000137329), (0.0, 1.4597606658935547), (0.0, 1.459776520729065), (0.0, 1.4598032236099243), (0.0, 1.4598767757415771), (0.0, 1.459882140159607), (0.0, 1.4599019289016724), (0.0, 1.4604194164276123), (0.0, 1.4604506492614746), (0.0, 1.460488200187683), (0.0, 1.4605038166046143), (0.0, 1.46060311794281), (0.0, 1.460762858390808), (0.0, 1.4607629776000977), (0.0, 1.4610888957977295), (0.0, 1.461225986480713), (0.0, 1.461261510848999), (0.0, 1.46137273311615), (0.0, 1.4614111185073853), (0.0, 1.4614731073379517), (0.0, 1.4618841409683228), (0.0, 1.4619280099868774), (0.0, 1.462348222732544), (0.0, 1.4624736309051514), (0.0, 1.462827205657959), (0.0, 1.4630248546600342), (0.0, 1.4630833864212036), (0.0, 1.463112711906433), (0.0, 1.4631394147872925), (0.0, 1.463194727897644), (0.0, 1.4633324146270752), (0.0, 1.4634778499603271), (0.0, 1.463629961013794), (0.0, 1.463667392730713), (0.0, 1.4637514352798462), (0.0, 1.4646999835968018), (0.0, 1.464959740638733), (0.0, 1.4650394916534424), (0.0, 1.4670100212097168), (0.0, 1.4679783582687378), (0.0, 1.4708932638168335), (0.0, 1.4724851846694946), (0.0, 1.5100177526474), (0.0, 1.5133565664291382), (0.0, 1.5150667428970337), (0.0, 1.5176141262054443), (0.0, 1.5185273885726929), (0.0, 1.5187811851501465), (0.0, 1.519161581993103), (0.0, 1.519200086593628), (0.0, 1.5194756984710693), (0.0, 1.5203288793563843), (0.0, 1.5204894542694092), (0.0, 1.5207695960998535), (0.0, 1.5209542512893677), (0.0, 1.5214083194732666), (0.0, 1.521669626235962), (0.0, 1.521695852279663), (0.0, 1.5218276977539062), (0.0, 1.5218738317489624), (0.0, 1.522101879119873), (0.0, 1.5222289562225342), (0.0, 1.522340178489685), (0.0, 1.5224865674972534), (0.0, 1.5226994752883911), (0.0, 1.5231547355651855), (0.0, 1.5231624841690063), (0.0, 1.5232548713684082), (0.0, 1.5233080387115479), (0.0, 1.5233941078186035), (0.0, 1.523499608039856), (0.0, 1.5235973596572876), (0.0, 1.5236036777496338), (0.0, 1.5237091779708862), (0.0, 1.52396821975708), (0.0, 1.524122953414917), (0.0, 1.5241425037384033), (0.0, 1.5242239236831665), (0.0, 1.5243291854858398), (0.0, 1.5246001482009888), (0.0, 1.5250202417373657), (0.0, 1.5251878499984741), (0.0, 1.5252803564071655), (0.0, 1.5256810188293457), (0.0, 1.5258373022079468), (0.0, 1.5259391069412231), (0.0, 1.526055097579956), (0.0, 1.5260587930679321), (0.0, 1.5260778665542603), (0.0, 1.5261452198028564), (0.0, 1.5261883735656738), (0.0, 1.526450753211975), (0.0, 1.526490569114685), (0.0, 1.5265088081359863), (0.0, 1.5265122652053833), (0.0, 1.5265400409698486), (0.0, 1.5265437364578247), (0.0, 1.5267938375473022), (0.0, 1.526915192604065), (0.0, 1.5273357629776), (0.0, 1.527373194694519), (0.0, 1.5278890132904053), (0.0, 1.5279085636138916), (0.0, 1.5279865264892578), (0.0, 1.5285582542419434), (0.0, 1.5304795503616333), (0.0, 1.5306562185287476), (0.0, 1.5306828022003174), (0.0, 1.5316643714904785), (0.0, 1.5332762002944946)], [(7.926539421081543, 8.26591682434082), (7.804293155670166, 7.977874279022217), (7.242599964141846, 8.294560432434082), (5.354081153869629, 5.43442440032959), (4.174530029296875, 8.539947509765625)], []]
[array([[0.        , 1.32810116],
       [0.        , 1.32880056],
       [0.        , 1.32995689],
       [0.        , 1.33044434],
       [0.        , 1.33117795],
       [0.        , 1.33142769],
       [0.        , 1.33145142],
       [0.        , 1.33146513],
       [0.        , 1.33162022],
       [0.        , 1.33174896],
       [0.        , 1.33176565],
       [0.        , 1.33181286],
       [0.        , 1.33193898],
       [0.        , 1.33212125],
       [0.        , 1.33220601],
       [0.        , 1.33226049],
       [0.        , 1.33250785],
       [0.        , 1.33258832],
       [0.        , 1.3328265 ],
       [0.        , 1.33315957],
       [0.        , 1.33331239],
       [0.        , 1.33345354],
       [0.        , 1.33351922],
       [0.        , 1.33373773],
       [0.        , 1.33385754],
       [0.        , 1.33410156],
       [0.        , 1.33413947],
       [0.        , 1.33415902],
       [0.        , 1.33457255],
       [0.        , 1.33463407],
       [0.        , 1.33476198],
       [0.        , 1.33477867],
       [0.        , 1.33503139],
       [0.        , 1.33516157],
       [0.        , 1.33539677],
       [0.        , 1.33541584],
       [0.        , 1.33544815],
       [0.        , 1.33549285],
       [0.        , 1.33549392],
       [0.        , 1.33550262],
       [0.        , 1.33578217],
       [0.        , 1.33630741],
       [0.        , 1.3363843 ],
       [0.        , 1.33639193],
       [0.        , 1.33643556],
       [0.        , 1.33656585],
       [0.        , 1.33660436],
       [0.        , 1.336622  ],
       [0.        , 1.33663774],
       [0.        , 1.33701968],
       [0.        , 1.33707905],
       [0.        , 1.33724201],
       [0.        , 1.33725369],
       [0.        , 1.33736181],
       [0.        , 1.33749259],
       [0.        , 1.33768213],
       [0.        , 1.33783185],
       [0.        , 1.33843207],
       [0.        , 1.33845568],
       [0.        , 1.33853471],
       [0.        , 1.33873248],
       [0.        , 1.33934951],
       [0.        , 1.33935511],
       [0.        , 1.34111834],
       [0.        , 1.34171629],
       [0.        , 1.34219122],
       [0.        , 1.3422612 ],
       [0.        , 1.44564235],
       [0.        , 1.45197642],
       [0.        , 1.454422  ],
       [0.        , 1.45459938],
       [0.        , 1.45468009],
       [0.        , 1.45514297],
       [0.        , 1.45516264],
       [0.        , 1.45589292],
       [0.        , 1.45623195],
       [0.        , 1.45629179],
       [0.        , 1.45697117],
       [0.        , 1.45717967],
       [0.        , 1.45752752],
       [0.        , 1.4576509 ],
       [0.        , 1.45786965],
       [0.        , 1.45790839],
       [0.        , 1.45806718],
       [0.        , 1.45809376],
       [0.        , 1.45819676],
       [0.        , 1.45820868],
       [0.        , 1.45823646],
       [0.        , 1.45858204],
       [0.        , 1.45868969],
       [0.        , 1.45880258],
       [0.        , 1.45895016],
       [0.        , 1.45908248],
       [0.        , 1.459656  ],
       [0.        , 1.45976067],
       [0.        , 1.45977652],
       [0.        , 1.45980322],
       [0.        , 1.45987678],
       [0.        , 1.45988214],
       [0.        , 1.45990193],
       [0.        , 1.46041942],
       [0.        , 1.46045065],
       [0.        , 1.4604882 ],
       [0.        , 1.46050382],
       [0.        , 1.46060312],
       [0.        , 1.46076286],
       [0.        , 1.46076298],
       [0.        , 1.4610889 ],
       [0.        , 1.46122599],
       [0.        , 1.46126151],
       [0.        , 1.46137273],
       [0.        , 1.46141112],
       [0.        , 1.46147311],
       [0.        , 1.46188414],
       [0.        , 1.46192801],
       [0.        , 1.46234822],
       [0.        , 1.46247363],
       [0.        , 1.46282721],
       [0.        , 1.46302485],
       [0.        , 1.46308339],
       [0.        , 1.46311271],
       [0.        , 1.46313941],
       [0.        , 1.46319473],
       [0.        , 1.46333241],
       [0.        , 1.46347785],
       [0.        , 1.46362996],
       [0.        , 1.46366739],
       [0.        , 1.46375144],
       [0.        , 1.46469998],
       [0.        , 1.46495974],
       [0.        , 1.46503949],
       [0.        , 1.46701002],
       [0.        , 1.46797836],
       [0.        , 1.47089326],
       [0.        , 1.47248518],
       [0.        , 1.51001775],
       [0.        , 1.51335657],
       [0.        , 1.51506674],
       [0.        , 1.51761413],
       [0.        , 1.51852739],
       [0.        , 1.51878119],
       [0.        , 1.51916158],
       [0.        , 1.51920009],
       [0.        , 1.5194757 ],
       [0.        , 1.52032888],
       [0.        , 1.52048945],
       [0.        , 1.5207696 ],
       [0.        , 1.52095425],
       [0.        , 1.52140832],
       [0.        , 1.52166963],
       [0.        , 1.52169585],
       [0.        , 1.5218277 ],
       [0.        , 1.52187383],
       [0.        , 1.52210188],
       [0.        , 1.52222896],
       [0.        , 1.52234018],
       [0.        , 1.52248657],
       [0.        , 1.52269948],
       [0.        , 1.52315474],
       [0.        , 1.52316248],
       [0.        , 1.52325487],
       [0.        , 1.52330804],
       [0.        , 1.52339411],
       [0.        , 1.52349961],
       [0.        , 1.52359736],
       [0.        , 1.52360368],
       [0.        , 1.52370918],
       [0.        , 1.52396822],
       [0.        , 1.52412295],
       [0.        , 1.5241425 ],
       [0.        , 1.52422392],
       [0.        , 1.52432919],
       [0.        , 1.52460015],
       [0.        , 1.52502024],
       [0.        , 1.52518785],
       [0.        , 1.52528036],
       [0.        , 1.52568102],
       [0.        , 1.5258373 ],
       [0.        , 1.52593911],
       [0.        , 1.5260551 ],
       [0.        , 1.52605879],
       [0.        , 1.52607787],
       [0.        , 1.52614522],
       [0.        , 1.52618837],
       [0.        , 1.52645075],
       [0.        , 1.52649057],
       [0.        , 1.52650881],
       [0.        , 1.52651227],
       [0.        , 1.52654004],
       [0.        , 1.52654374],
       [0.        , 1.52679384],
       [0.        , 1.52691519],
       [0.        , 1.52733576],
       [0.        , 1.52737319],
       [0.        , 1.52788901],
       [0.        , 1.52790856],
       [0.        , 1.52798653],
       [0.        , 1.52855825],
       [0.        , 1.53047955],
       [0.        , 1.53065622],
       [0.        , 1.5306828 ],
       [0.        , 1.53166437],
       [0.        , 1.5332762 ]]), array([[7.92653942, 8.26591682],
       [7.80429316, 7.97787428],
       [7.24259996, 8.29456043],
       [5.35408115, 5.4344244 ],
       [4.17453003, 8.53994751]]), array([], dtype=float64)]
Traceback (most recent call last):
  File "/gpfs/gibbs/pi/gerstein/as4272/KnotFun/calculate_ph/ph_functions_h2.py", line 152, in <module>
    main()
  File "/gpfs/gibbs/pi/gerstein/as4272/KnotFun/calculate_ph/ph_functions_h2.py", line 131, in main
    vectors = get_perslayer(result_dict_serializable)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/gibbs/pi/gerstein/as4272/KnotFun/calculate_ph/ph_functions_h2.py", line 84, in get_perslayer
    diagrams_scaled = scaler.fit_transform(diagrams_reshaped)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/as4272/.conda/envs/ph/lib/python3.11/site-packages/sklearn/utils/_set_output.py", line 157, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/as4272/.conda/envs/ph/lib/python3.11/site-packages/sklearn/base.py", line 916, in fit_transform
    return self.fit(X, **fit_params).transform(X)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/as4272/.conda/envs/ph/lib/python3.11/site-packages/gudhi/representations/preprocessing.py", line 152, in fit
    P = np.concatenate(X,0)
        ^^^^^^^^^^^^^^^^^^^
ValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 2 has 1 dimension(s)
6L34 ph vector generated, counter: 179
2024-03-06 17:58:36.283676: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:36.326603: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:37.362211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L3R ph vector generated, counter: 180
2024-03-06 17:58:40.748936: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:40.792361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:41.713608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L57 ph vector generated, counter: 181
2024-03-06 17:58:45.462344: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:45.506607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:46.364849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L59 ph vector generated, counter: 182
2024-03-06 17:58:49.583819: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:49.627139: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:50.514967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L5V ph vector generated, counter: 183
2024-03-06 17:58:53.667421: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:53.711329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:54.696174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L5W ph vector generated, counter: 184
2024-03-06 17:58:57.901174: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:58:57.951407: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:58:58.837993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L5X ph vector generated, counter: 185
2024-03-06 17:59:01.933703: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:01.976672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:02.920872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L5Y ph vector generated, counter: 186
2024-03-06 17:59:06.157623: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:06.200418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:07.319668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L65 ph vector generated, counter: 187
2024-03-06 17:59:10.873641: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:10.916115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:12.121494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L66 ph vector generated, counter: 188
2024-03-06 17:59:15.862085: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:15.905243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:16.955420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L6E ph vector generated, counter: 189
2024-03-06 17:59:20.533635: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:20.577180: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:21.532189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L6K ph vector generated, counter: 190
2024-03-06 17:59:24.631440: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:24.674659: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:25.838712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L71 ph vector generated, counter: 191
2024-03-06 17:59:29.405179: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:29.448094: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:30.403480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L7L ph vector generated, counter: 192
2024-03-06 17:59:33.703973: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:33.747078: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:34.697534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L7S ph vector generated, counter: 193
2024-03-06 17:59:38.386836: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:38.430682: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:39.341153: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L7T ph vector generated, counter: 194
2024-03-06 17:59:42.749611: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:42.792771: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:43.892285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L89 ph vector generated, counter: 195
2024-03-06 17:59:46.977387: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:47.020520: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:47.919057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6L8B ph vector generated, counter: 196
2024-03-06 17:59:51.328339: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:51.370909: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:52.234598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LB4 ph vector generated, counter: 197
2024-03-06 17:59:55.445941: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:55.488467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 17:59:56.465052: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LB5 ph vector generated, counter: 198
2024-03-06 17:59:59.625905: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 17:59:59.668799: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:00.579824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LB6 ph vector generated, counter: 199
2024-03-06 18:00:03.874586: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:03.917339: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:04.929669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LBX ph vector generated, counter: 200
2024-03-06 18:00:08.216047: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:08.258694: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:09.129957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LCA ph vector generated, counter: 201
2024-03-06 18:00:12.559142: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:12.603925: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:13.563849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3116808), (0., 1.3187834), (0., 1.3238949), (0., 1.3248837),
       (0., 1.3254569), (0., 1.3258159), (0., 1.3260177), (0., 1.3267953),
       (0., 1.3274436), (0., 1.3275876), (0., 1.3276657), (0., 1.3277053),
       (0., 1.3278514), (0., 1.3280047), (0., 1.3280395), (0., 1.3281828),
       (0., 1.3281838), (0., 1.3282398), (0., 1.3282425), (0., 1.3286046),
       (0., 1.328636 ), (0., 1.3286372), (0., 1.3286839), (0., 1.3287231),
       (0., 1.3288916), (0., 1.3293531), (0., 1.3295329), (0., 1.329674 ),
       (0., 1.3298562), (0., 1.3299136), (0., 1.3301044), (0., 1.3303158),
       (0., 1.3303369), (0., 1.3303702), (0., 1.330393 ), (0., 1.3303994),
       (0., 1.3304111), (0., 1.3306941), (0., 1.3307656), (0., 1.330823 ),
       (0., 1.3308297), (0., 1.3309861), (0., 1.3312316), (0., 1.331296 ),
       (0., 1.3313925), (0., 1.3314162), (0., 1.3315408), (0., 1.3317919),
       (0., 1.3319627), (0., 1.3319739), (0., 1.3320193), (0., 1.3321141),
       (0., 1.3322762), (0., 1.3323795), (0., 1.3325671), (0., 1.332768 ),
       (0., 1.3327906), (0., 1.3330604), (0., 1.3334554), (0., 1.3337793),
       (0., 1.3338664), (0., 1.3339218), (0., 1.3339945), (0., 1.33432  ),
       (0., 1.3343489), (0., 1.3344084), (0., 1.3345422), (0., 1.3345672),
       (0., 1.3346888), (0., 1.3347198), (0., 1.3347243), (0., 1.3352892),
       (0., 1.3352934), (0., 1.3354032), (0., 1.3357795), (0., 1.3358234),
       (0., 1.3358473), (0., 1.3360037), (0., 1.3364539), (0., 1.3366619),
       (0., 1.3366804), (0., 1.3370355), (0., 1.3372972), (0., 1.3376633),
       (0., 1.3383781), (0., 1.3430514), (0., 1.3507043), (0., 1.351655 ),
       (0., 1.3551159), (0., 1.3618087), (0., 1.3690591), (0., 1.3700429),
       (0., 1.3763058), (0., 1.3793063), (0., 1.3858149), (0., 1.4172512),
       (0., 1.41815  ), (0., 1.4186218), (0., 1.4217603), (0., 1.4247204),
       (0., 1.4253074), (0., 1.440738 ), (0., 1.4433191), (0., 1.4460546),
       (0., 1.4465421), (0., 1.4483386), (0., 1.4491159), (0., 1.4504303),
       (0., 1.4510074), (0., 1.4516182), (0., 1.4520091), (0., 1.452304 ),
       (0., 1.4524108), (0., 1.4525928), (0., 1.4527946), (0., 1.4530643),
       (0., 1.4531859), (0., 1.4535209), (0., 1.4536707), (0., 1.4539732),
       (0., 1.4539925), (0., 1.4543236), (0., 1.4543318), (0., 1.4546887),
       (0., 1.4547209), (0., 1.4548045), (0., 1.4548517), (0., 1.4549367),
       (0., 1.4550054), (0., 1.4551387), (0., 1.4551626), (0., 1.4553087),
       (0., 1.4553844), (0., 1.4556953), (0., 1.4558148), (0., 1.4560013),
       (0., 1.4560506), (0., 1.4560734), (0., 1.4561434), (0., 1.4562265),
       (0., 1.4562768), (0., 1.4568865), (0., 1.4569759), (0., 1.4570348),
       (0., 1.4570647), (0., 1.4571352), (0., 1.4574533), (0., 1.4577454),
       (0., 1.4578454), (0., 1.4578952), (0., 1.4578974), (0., 1.4581728),
       (0., 1.458478 ), (0., 1.4586041), (0., 1.4586143), (0., 1.4586272),
       (0., 1.4587914), (0., 1.4588332), (0., 1.4588579), (0., 1.4589102),
       (0., 1.45895  ), (0., 1.4589857), (0., 1.4589915), (0., 1.4590795),
       (0., 1.4593041), (0., 1.4593126), (0., 1.4593916), (0., 1.4593984),
       (0., 1.4594502), (0., 1.4594827), (0., 1.4595096), (0., 1.4597477),
       (0., 1.4597652), (0., 1.4597687), (0., 1.4604228), (0., 1.4606909),
       (0., 1.4610025), (0., 1.4613562), (0., 1.4616264), (0., 1.4617817),
       (0., 1.4621572), (0., 1.4626639), (0., 1.4631596), (0., 1.4633961),
       (0., 1.464138 ), (0., 1.4642737), (0., 1.4643079), (0., 1.4655179),
       (0., 1.466043 ), (0., 1.4669459), (0., 1.4745167), (0., 1.4776316),
       (0., 1.4783363), (0., 1.4800292), (0., 1.4831697), (0., 1.484236 ),
       (0., 1.497772 ), (0., 1.5027881), (0., 1.5093527), (0., 1.5116024),
       (0., 1.5116037), (0., 1.5134507), (0., 1.5137607), (0., 1.513783 ),
       (0., 1.5147377), (0., 1.5169477), (0., 1.5178533), (0., 1.5179125),
       (0., 1.5185828), (0., 1.5186164), (0., 1.5187923), (0., 1.5190344),
       (0., 1.5191243), (0., 1.5193416), (0., 1.5194807), (0., 1.5197545),
       (0., 1.5197594), (0., 1.5198   ), (0., 1.5198188), (0., 1.5199009),
       (0., 1.5199226), (0., 1.520013 ), (0., 1.5201211), (0., 1.5201781),
       (0., 1.5202197), (0., 1.5204612), (0., 1.5205954), (0., 1.5209202),
       (0., 1.5209591), (0., 1.5211709), (0., 1.5212685), (0., 1.5213199),
       (0., 1.5213584), (0., 1.5213617), (0., 1.5213721), (0., 1.5216289),
       (0., 1.5217847), (0., 1.5218701), (0., 1.5218748), (0., 1.5218874),
       (0., 1.5219244), (0., 1.5219517), (0., 1.5220306), (0., 1.5223734),
       (0., 1.5226524), (0., 1.5227726), (0., 1.5228444), (0., 1.5229063),
       (0., 1.522962 ), (0., 1.5229828), (0., 1.523104 ), (0., 1.5231786),
       (0., 1.5232491), (0., 1.5234598), (0., 1.5236088), (0., 1.5236188),
       (0., 1.5236933), (0., 1.5237814), (0., 1.5238636), (0., 1.5239234),
       (0., 1.5239434), (0., 1.524195 ), (0., 1.5243053), (0., 1.5243871),
       (0., 1.524421 ), (0., 1.5246699), (0., 1.5248826), (0., 1.5252079),
       (0., 1.5252101), (0., 1.5252469), (0., 1.525454 ), (0., 1.5254666),
       (0., 1.5255669), (0., 1.5256648), (0., 1.5260825), (0., 1.5261483),
       (0., 1.5261642), (0., 1.5262202), (0., 1.5264081), (0., 1.5265435),
       (0., 1.5265769), (0., 1.5272946), (0., 1.5275708), (0., 1.5285115),
       (0., 1.5289601), (0., 1.5316476), (0., 1.5316958), (0., 1.5317677),
       (0., 1.5358905), (0., 1.5396872), (0., 1.5415883), (0., 1.5423884),
       (0., 1.5444539), (0., 1.5452813), (0., 1.5462828), (0., 1.5537899),
       (0., 1.553897 ), (0., 1.5558829), (0., 1.5664154), (0., 1.5684037),
       (0., 1.5817064), (0., 1.6008371), (0., 1.6117957), (0., 1.6182245),
       (0., 1.6260864)], dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.068433 , 8.436396 ), (7.848899 , 8.124763 ),
       (7.6451683, 7.701595 ), (7.2027116, 7.38322  ),
       (7.0342093, 7.9504876), (6.950106 , 7.0691957),
       (6.615368 , 7.1212935), (6.572866 , 6.666815 ),
       (6.480821 , 8.436199 ), (6.452334 , 7.3285484),
       (5.7355237, 5.8247457), (5.0677423, 5.1834455),
       (4.5360923, 4.6357274), (4.3308034, 4.3858986),
       (4.2438073, 4.3287063), (4.242945 , 4.4366183),
       (4.13354  , 4.3394995), (4.1312118, 4.410377 ),
       (4.1084385, 4.38648  ), (4.104498 , 4.3388057),
       (4.100746 , 4.3606396), (4.097322 , 4.3616023),
       (4.0787616, 4.554985 ), (4.070277 , 4.3844943),
       (4.0662947, 4.360803 ), (4.0518627, 4.4462876),
       (4.0426183, 4.496938 ), (4.030448 , 4.2144365),
       (4.020581 , 4.411439 ), (4.0174108, 4.156985 ),
       (3.9894347, 4.700926 ), (3.970622 , 4.3979435),
       (3.94666  , 8.8879795), (3.9171243, 4.654715 ),
       (3.9024196, 7.5552974), (3.900052 , 4.604367 ),
       (3.8780258, 4.351381 ), (3.8776786, 4.2554173),
       (3.8765442, 9.240597 ), (3.8735702, 5.084564 ),
       (3.8640447, 4.6127067), (3.790534 , 7.106596 ),
       (3.3449469, 3.530942 ), (3.03148  , 3.1445558)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.534109, 11.6352825), (8.445368,  8.496831 )],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.311680793762207), (0.0, 1.3187834024429321), (0.0, 1.3238948583602905), (0.0, 1.3248836994171143), (0.0, 1.3254568576812744), (0.0, 1.3258159160614014), (0.0, 1.3260177373886108), (0.0, 1.3267953395843506), (0.0, 1.3274435997009277), (0.0, 1.327587604522705), (0.0, 1.3276656866073608), (0.0, 1.3277052640914917), (0.0, 1.327851414680481), (0.0, 1.3280047178268433), (0.0, 1.328039526939392), (0.0, 1.3281828165054321), (0.0, 1.3281837701797485), (0.0, 1.3282397985458374), (0.0, 1.328242540359497), (0.0, 1.3286045789718628), (0.0, 1.3286360502243042), (0.0, 1.3286372423171997), (0.0, 1.328683853149414), (0.0, 1.3287230730056763), (0.0, 1.328891634941101), (0.0, 1.3293530941009521), (0.0, 1.3295328617095947), (0.0, 1.3296740055084229), (0.0, 1.3298561573028564), (0.0, 1.32991361618042), (0.0, 1.3301043510437012), (0.0, 1.3303158283233643), (0.0, 1.3303369283676147), (0.0, 1.3303701877593994), (0.0, 1.3303929567337036), (0.0, 1.3303993940353394), (0.0, 1.3304110765457153), (0.0, 1.3306940793991089), (0.0, 1.3307656049728394), (0.0, 1.3308229446411133), (0.0, 1.3308297395706177), (0.0, 1.3309861421585083), (0.0, 1.3312315940856934), (0.0, 1.3312959671020508), (0.0, 1.331392526626587), (0.0, 1.3314162492752075), (0.0, 1.331540822982788), (0.0, 1.331791877746582), (0.0, 1.3319627046585083), (0.0, 1.331973910331726), (0.0, 1.332019329071045), (0.0, 1.3321141004562378), (0.0, 1.3322762250900269), (0.0, 1.3323794603347778), (0.0, 1.3325670957565308), (0.0, 1.3327679634094238), (0.0, 1.3327906131744385), (0.0, 1.333060383796692), (0.0, 1.3334554433822632), (0.0, 1.3337793350219727), (0.0, 1.3338663578033447), (0.0, 1.3339217901229858), (0.0, 1.3339945077896118), (0.0, 1.3343199491500854), (0.0, 1.3343489170074463), (0.0, 1.3344084024429321), (0.0, 1.334542155265808), (0.0, 1.3345671892166138), (0.0, 1.3346887826919556), (0.0, 1.3347197771072388), (0.0, 1.3347243070602417), (0.0, 1.3352892398834229), (0.0, 1.3352934122085571), (0.0, 1.3354032039642334), (0.0, 1.3357795476913452), (0.0, 1.3358234167099), (0.0, 1.33584725856781), (0.0, 1.3360036611557007), (0.0, 1.336453914642334), (0.0, 1.3366619348526), (0.0, 1.3366804122924805), (0.0, 1.3370355367660522), (0.0, 1.3372972011566162), (0.0, 1.3376632928848267), (0.0, 1.3383780717849731), (0.0, 1.3430514335632324), (0.0, 1.350704312324524), (0.0, 1.3516550064086914), (0.0, 1.3551158905029297), (0.0, 1.3618086576461792), (0.0, 1.3690590858459473), (0.0, 1.3700429201126099), (0.0, 1.3763058185577393), (0.0, 1.3793063163757324), (0.0, 1.385814905166626), (0.0, 1.4172512292861938), (0.0, 1.4181499481201172), (0.0, 1.4186217784881592), (0.0, 1.4217603206634521), (0.0, 1.4247204065322876), (0.0, 1.4253073930740356), (0.0, 1.4407379627227783), (0.0, 1.4433190822601318), (0.0, 1.4460545778274536), (0.0, 1.4465421438217163), (0.0, 1.4483386278152466), (0.0, 1.4491158723831177), (0.0, 1.4504302740097046), (0.0, 1.45100736618042), (0.0, 1.4516181945800781), (0.0, 1.4520090818405151), (0.0, 1.4523040056228638), (0.0, 1.4524108171463013), (0.0, 1.4525928497314453), (0.0, 1.4527945518493652), (0.0, 1.4530643224716187), (0.0, 1.4531859159469604), (0.0, 1.4535208940505981), (0.0, 1.4536707401275635), (0.0, 1.4539731740951538), (0.0, 1.453992486000061), (0.0, 1.454323649406433), (0.0, 1.4543317556381226), (0.0, 1.4546886682510376), (0.0, 1.4547208547592163), (0.0, 1.454804539680481), (0.0, 1.454851746559143), (0.0, 1.4549367427825928), (0.0, 1.455005407333374), (0.0, 1.4551386833190918), (0.0, 1.4551626443862915), (0.0, 1.4553086757659912), (0.0, 1.455384373664856), (0.0, 1.4556952714920044), (0.0, 1.4558148384094238), (0.0, 1.4560012817382812), (0.0, 1.4560506343841553), (0.0, 1.4560734033584595), (0.0, 1.4561433792114258), (0.0, 1.4562264680862427), (0.0, 1.456276774406433), (0.0, 1.4568865299224854), (0.0, 1.4569759368896484), (0.0, 1.4570348262786865), (0.0, 1.4570647478103638), (0.0, 1.4571352005004883), (0.0, 1.4574532508850098), (0.0, 1.4577454328536987), (0.0, 1.4578454494476318), (0.0, 1.4578951597213745), (0.0, 1.457897424697876), (0.0, 1.4581727981567383), (0.0, 1.4584779739379883), (0.0, 1.458604097366333), (0.0, 1.4586143493652344), (0.0, 1.4586272239685059), (0.0, 1.4587913751602173), (0.0, 1.4588332176208496), (0.0, 1.4588578939437866), (0.0, 1.4589102268218994), (0.0, 1.4589500427246094), (0.0, 1.458985686302185), (0.0, 1.458991527557373), (0.0, 1.4590795040130615), (0.0, 1.4593040943145752), (0.0, 1.4593125581741333), (0.0, 1.4593915939331055), (0.0, 1.4593983888626099), (0.0, 1.4594502449035645), (0.0, 1.4594826698303223), (0.0, 1.4595096111297607), (0.0, 1.4597476720809937), (0.0, 1.4597651958465576), (0.0, 1.4597686529159546), (0.0, 1.4604227542877197), (0.0, 1.4606908559799194), (0.0, 1.4610024690628052), (0.0, 1.4613561630249023), (0.0, 1.461626410484314), (0.0, 1.4617817401885986), (0.0, 1.4621572494506836), (0.0, 1.4626638889312744), (0.0, 1.4631595611572266), (0.0, 1.4633960723876953), (0.0, 1.4641380310058594), (0.0, 1.4642736911773682), (0.0, 1.4643079042434692), (0.0, 1.4655178785324097), (0.0, 1.4660429954528809), (0.0, 1.4669458866119385), (0.0, 1.474516749382019), (0.0, 1.4776315689086914), (0.0, 1.4783363342285156), (0.0, 1.4800292253494263), (0.0, 1.483169674873352), (0.0, 1.4842360019683838), (0.0, 1.497771978378296), (0.0, 1.5027880668640137), (0.0, 1.509352684020996), (0.0, 1.5116024017333984), (0.0, 1.5116037130355835), (0.0, 1.5134507417678833), (0.0, 1.5137606859207153), (0.0, 1.5137829780578613), (0.0, 1.5147377252578735), (0.0, 1.5169477462768555), (0.0, 1.5178532600402832), (0.0, 1.51791250705719), (0.0, 1.518582820892334), (0.0, 1.5186164379119873), (0.0, 1.5187922716140747), (0.0, 1.5190343856811523), (0.0, 1.5191242694854736), (0.0, 1.5193415880203247), (0.0, 1.5194807052612305), (0.0, 1.5197545289993286), (0.0, 1.5197594165802002), (0.0, 1.5197999477386475), (0.0, 1.5198187828063965), (0.0, 1.519900918006897), (0.0, 1.5199226140975952), (0.0, 1.5200129747390747), (0.0, 1.5201210975646973), (0.0, 1.5201780796051025), (0.0, 1.5202196836471558), (0.0, 1.5204612016677856), (0.0, 1.5205954313278198), (0.0, 1.5209201574325562), (0.0, 1.5209591388702393), (0.0, 1.5211708545684814), (0.0, 1.5212684869766235), (0.0, 1.52131986618042), (0.0, 1.5213583707809448), (0.0, 1.5213617086410522), (0.0, 1.5213720798492432), (0.0, 1.5216288566589355), (0.0, 1.5217846632003784), (0.0, 1.5218701362609863), (0.0, 1.5218747854232788), (0.0, 1.5218874216079712), (0.0, 1.521924376487732), (0.0, 1.521951675415039), (0.0, 1.5220305919647217), (0.0, 1.5223734378814697), (0.0, 1.5226523876190186), (0.0, 1.5227725505828857), (0.0, 1.5228444337844849), (0.0, 1.5229063034057617), (0.0, 1.522961974143982), (0.0, 1.5229828357696533), (0.0, 1.523103952407837), (0.0, 1.5231785774230957), (0.0, 1.5232491493225098), (0.0, 1.523459792137146), (0.0, 1.5236088037490845), (0.0, 1.5236188173294067), (0.0, 1.523693323135376), (0.0, 1.523781418800354), (0.0, 1.5238635540008545), (0.0, 1.523923397064209), (0.0, 1.5239434242248535), (0.0, 1.5241949558258057), (0.0, 1.5243053436279297), (0.0, 1.5243871212005615), (0.0, 1.524420976638794), (0.0, 1.524669885635376), (0.0, 1.5248825550079346), (0.0, 1.5252078771591187), (0.0, 1.5252101421356201), (0.0, 1.5252468585968018), (0.0, 1.525454044342041), (0.0, 1.5254665613174438), (0.0, 1.5255669355392456), (0.0, 1.5256648063659668), (0.0, 1.5260825157165527), (0.0, 1.5261483192443848), (0.0, 1.526164174079895), (0.0, 1.5262202024459839), (0.0, 1.526408076286316), (0.0, 1.5265434980392456), (0.0, 1.5265768766403198), (0.0, 1.527294635772705), (0.0, 1.5275708436965942), (0.0, 1.5285115242004395), (0.0, 1.528960108757019), (0.0, 1.5316475629806519), (0.0, 1.53169584274292), (0.0, 1.531767725944519), (0.0, 1.5358904600143433), (0.0, 1.539687156677246), (0.0, 1.541588306427002), (0.0, 1.5423884391784668), (0.0, 1.5444538593292236), (0.0, 1.5452812910079956), (0.0, 1.5462827682495117), (0.0, 1.5537898540496826), (0.0, 1.5538970232009888), (0.0, 1.5558829307556152), (0.0, 1.5664154291152954), (0.0, 1.568403720855713), (0.0, 1.5817064046859741), (0.0, 1.6008371114730835), (0.0, 1.6117956638336182), (0.0, 1.6182245016098022), (0.0, 1.6260863542556763)], [(8.068432807922363, 8.436395645141602), (7.848898887634277, 8.124762535095215), (7.645168304443359, 7.701594829559326), (7.202711582183838, 7.383220195770264), (7.034209251403809, 7.9504876136779785), (6.950106143951416, 7.069195747375488), (6.615367889404297, 7.121293544769287), (6.572865962982178, 6.666814804077148), (6.480821132659912, 8.436199188232422), (6.452333927154541, 7.328548431396484), (5.735523700714111, 5.8247456550598145), (5.067742347717285, 5.183445453643799), (4.536092281341553, 4.635727405548096), (4.330803394317627, 4.385898590087891), (4.243807315826416, 4.32870626449585), (4.242945194244385, 4.436618328094482), (4.133540153503418, 4.339499473571777), (4.131211757659912, 4.410377025604248), (4.108438491821289, 4.38647985458374), (4.104497909545898, 4.338805675506592), (4.100746154785156, 4.360639572143555), (4.0973219871521, 4.361602306365967), (4.078761577606201, 4.554985046386719), (4.070277214050293, 4.384494304656982), (4.0662946701049805, 4.360803127288818), (4.051862716674805, 4.446287631988525), (4.042618274688721, 4.496938228607178), (4.030447959899902, 4.2144365310668945), (4.020580768585205, 4.411438941955566), (4.017410755157471, 4.156984806060791), (3.9894347190856934, 4.700925827026367), (3.9706220626831055, 4.397943496704102), (3.946660041809082, 8.887979507446289), (3.9171242713928223, 4.654715061187744), (3.9024195671081543, 7.555297374725342), (3.900052070617676, 4.604366779327393), (3.878025770187378, 4.351380825042725), (3.877678632736206, 4.255417346954346), (3.876544237136841, 9.240596771240234), (3.873570203781128, 5.084564208984375), (3.864044666290283, 4.612706661224365), (3.790534019470215, 7.106595993041992), (3.34494686126709, 3.530941963195801), (3.031480073928833, 3.1445558071136475)], [(9.534109115600586, 11.635282516479492), (8.445367813110352, 8.496830940246582)]]
[array([[0.        , 1.31168079],
       [0.        , 1.3187834 ],
       [0.        , 1.32389486],
       [0.        , 1.3248837 ],
       [0.        , 1.32545686],
       [0.        , 1.32581592],
       [0.        , 1.32601774],
       [0.        , 1.32679534],
       [0.        , 1.3274436 ],
       [0.        , 1.3275876 ],
       [0.        , 1.32766569],
       [0.        , 1.32770526],
       [0.        , 1.32785141],
       [0.        , 1.32800472],
       [0.        , 1.32803953],
       [0.        , 1.32818282],
       [0.        , 1.32818377],
       [0.        , 1.3282398 ],
       [0.        , 1.32824254],
       [0.        , 1.32860458],
       [0.        , 1.32863605],
       [0.        , 1.32863724],
       [0.        , 1.32868385],
       [0.        , 1.32872307],
       [0.        , 1.32889163],
       [0.        , 1.32935309],
       [0.        , 1.32953286],
       [0.        , 1.32967401],
       [0.        , 1.32985616],
       [0.        , 1.32991362],
       [0.        , 1.33010435],
       [0.        , 1.33031583],
       [0.        , 1.33033693],
       [0.        , 1.33037019],
       [0.        , 1.33039296],
       [0.        , 1.33039939],
       [0.        , 1.33041108],
       [0.        , 1.33069408],
       [0.        , 1.3307656 ],
       [0.        , 1.33082294],
       [0.        , 1.33082974],
       [0.        , 1.33098614],
       [0.        , 1.33123159],
       [0.        , 1.33129597],
       [0.        , 1.33139253],
       [0.        , 1.33141625],
       [0.        , 1.33154082],
       [0.        , 1.33179188],
       [0.        , 1.3319627 ],
       [0.        , 1.33197391],
       [0.        , 1.33201933],
       [0.        , 1.3321141 ],
       [0.        , 1.33227623],
       [0.        , 1.33237946],
       [0.        , 1.3325671 ],
       [0.        , 1.33276796],
       [0.        , 1.33279061],
       [0.        , 1.33306038],
       [0.        , 1.33345544],
       [0.        , 1.33377934],
       [0.        , 1.33386636],
       [0.        , 1.33392179],
       [0.        , 1.33399451],
       [0.        , 1.33431995],
       [0.        , 1.33434892],
       [0.        , 1.3344084 ],
       [0.        , 1.33454216],
       [0.        , 1.33456719],
       [0.        , 1.33468878],
       [0.        , 1.33471978],
       [0.        , 1.33472431],
       [0.        , 1.33528924],
       [0.        , 1.33529341],
       [0.        , 1.3354032 ],
       [0.        , 1.33577955],
       [0.        , 1.33582342],
       [0.        , 1.33584726],
       [0.        , 1.33600366],
       [0.        , 1.33645391],
       [0.        , 1.33666193],
       [0.        , 1.33668041],
       [0.        , 1.33703554],
       [0.        , 1.3372972 ],
       [0.        , 1.33766329],
       [0.        , 1.33837807],
       [0.        , 1.34305143],
       [0.        , 1.35070431],
       [0.        , 1.35165501],
       [0.        , 1.35511589],
       [0.        , 1.36180866],
       [0.        , 1.36905909],
       [0.        , 1.37004292],
       [0.        , 1.37630582],
       [0.        , 1.37930632],
       [0.        , 1.38581491],
       [0.        , 1.41725123],
       [0.        , 1.41814995],
       [0.        , 1.41862178],
       [0.        , 1.42176032],
       [0.        , 1.42472041],
       [0.        , 1.42530739],
       [0.        , 1.44073796],
       [0.        , 1.44331908],
       [0.        , 1.44605458],
       [0.        , 1.44654214],
       [0.        , 1.44833863],
       [0.        , 1.44911587],
       [0.        , 1.45043027],
       [0.        , 1.45100737],
       [0.        , 1.45161819],
       [0.        , 1.45200908],
       [0.        , 1.45230401],
       [0.        , 1.45241082],
       [0.        , 1.45259285],
       [0.        , 1.45279455],
       [0.        , 1.45306432],
       [0.        , 1.45318592],
       [0.        , 1.45352089],
       [0.        , 1.45367074],
       [0.        , 1.45397317],
       [0.        , 1.45399249],
       [0.        , 1.45432365],
       [0.        , 1.45433176],
       [0.        , 1.45468867],
       [0.        , 1.45472085],
       [0.        , 1.45480454],
       [0.        , 1.45485175],
       [0.        , 1.45493674],
       [0.        , 1.45500541],
       [0.        , 1.45513868],
       [0.        , 1.45516264],
       [0.        , 1.45530868],
       [0.        , 1.45538437],
       [0.        , 1.45569527],
       [0.        , 1.45581484],
       [0.        , 1.45600128],
       [0.        , 1.45605063],
       [0.        , 1.4560734 ],
       [0.        , 1.45614338],
       [0.        , 1.45622647],
       [0.        , 1.45627677],
       [0.        , 1.45688653],
       [0.        , 1.45697594],
       [0.        , 1.45703483],
       [0.        , 1.45706475],
       [0.        , 1.4571352 ],
       [0.        , 1.45745325],
       [0.        , 1.45774543],
       [0.        , 1.45784545],
       [0.        , 1.45789516],
       [0.        , 1.45789742],
       [0.        , 1.4581728 ],
       [0.        , 1.45847797],
       [0.        , 1.4586041 ],
       [0.        , 1.45861435],
       [0.        , 1.45862722],
       [0.        , 1.45879138],
       [0.        , 1.45883322],
       [0.        , 1.45885789],
       [0.        , 1.45891023],
       [0.        , 1.45895004],
       [0.        , 1.45898569],
       [0.        , 1.45899153],
       [0.        , 1.4590795 ],
       [0.        , 1.45930409],
       [0.        , 1.45931256],
       [0.        , 1.45939159],
       [0.        , 1.45939839],
       [0.        , 1.45945024],
       [0.        , 1.45948267],
       [0.        , 1.45950961],
       [0.        , 1.45974767],
       [0.        , 1.4597652 ],
       [0.        , 1.45976865],
       [0.        , 1.46042275],
       [0.        , 1.46069086],
       [0.        , 1.46100247],
       [0.        , 1.46135616],
       [0.        , 1.46162641],
       [0.        , 1.46178174],
       [0.        , 1.46215725],
       [0.        , 1.46266389],
       [0.        , 1.46315956],
       [0.        , 1.46339607],
       [0.        , 1.46413803],
       [0.        , 1.46427369],
       [0.        , 1.4643079 ],
       [0.        , 1.46551788],
       [0.        , 1.466043  ],
       [0.        , 1.46694589],
       [0.        , 1.47451675],
       [0.        , 1.47763157],
       [0.        , 1.47833633],
       [0.        , 1.48002923],
       [0.        , 1.48316967],
       [0.        , 1.484236  ],
       [0.        , 1.49777198],
       [0.        , 1.50278807],
       [0.        , 1.50935268],
       [0.        , 1.5116024 ],
       [0.        , 1.51160371],
       [0.        , 1.51345074],
       [0.        , 1.51376069],
       [0.        , 1.51378298],
       [0.        , 1.51473773],
       [0.        , 1.51694775],
       [0.        , 1.51785326],
       [0.        , 1.51791251],
       [0.        , 1.51858282],
       [0.        , 1.51861644],
       [0.        , 1.51879227],
       [0.        , 1.51903439],
       [0.        , 1.51912427],
       [0.        , 1.51934159],
       [0.        , 1.51948071],
       [0.        , 1.51975453],
       [0.        , 1.51975942],
       [0.        , 1.51979995],
       [0.        , 1.51981878],
       [0.        , 1.51990092],
       [0.        , 1.51992261],
       [0.        , 1.52001297],
       [0.        , 1.5201211 ],
       [0.        , 1.52017808],
       [0.        , 1.52021968],
       [0.        , 1.5204612 ],
       [0.        , 1.52059543],
       [0.        , 1.52092016],
       [0.        , 1.52095914],
       [0.        , 1.52117085],
       [0.        , 1.52126849],
       [0.        , 1.52131987],
       [0.        , 1.52135837],
       [0.        , 1.52136171],
       [0.        , 1.52137208],
       [0.        , 1.52162886],
       [0.        , 1.52178466],
       [0.        , 1.52187014],
       [0.        , 1.52187479],
       [0.        , 1.52188742],
       [0.        , 1.52192438],
       [0.        , 1.52195168],
       [0.        , 1.52203059],
       [0.        , 1.52237344],
       [0.        , 1.52265239],
       [0.        , 1.52277255],
       [0.        , 1.52284443],
       [0.        , 1.5229063 ],
       [0.        , 1.52296197],
       [0.        , 1.52298284],
       [0.        , 1.52310395],
       [0.        , 1.52317858],
       [0.        , 1.52324915],
       [0.        , 1.52345979],
       [0.        , 1.5236088 ],
       [0.        , 1.52361882],
       [0.        , 1.52369332],
       [0.        , 1.52378142],
       [0.        , 1.52386355],
       [0.        , 1.5239234 ],
       [0.        , 1.52394342],
       [0.        , 1.52419496],
       [0.        , 1.52430534],
       [0.        , 1.52438712],
       [0.        , 1.52442098],
       [0.        , 1.52466989],
       [0.        , 1.52488256],
       [0.        , 1.52520788],
       [0.        , 1.52521014],
       [0.        , 1.52524686],
       [0.        , 1.52545404],
       [0.        , 1.52546656],
       [0.        , 1.52556694],
       [0.        , 1.52566481],
       [0.        , 1.52608252],
       [0.        , 1.52614832],
       [0.        , 1.52616417],
       [0.        , 1.5262202 ],
       [0.        , 1.52640808],
       [0.        , 1.5265435 ],
       [0.        , 1.52657688],
       [0.        , 1.52729464],
       [0.        , 1.52757084],
       [0.        , 1.52851152],
       [0.        , 1.52896011],
       [0.        , 1.53164756],
       [0.        , 1.53169584],
       [0.        , 1.53176773],
       [0.        , 1.53589046],
       [0.        , 1.53968716],
       [0.        , 1.54158831],
       [0.        , 1.54238844],
       [0.        , 1.54445386],
       [0.        , 1.54528129],
       [0.        , 1.54628277],
       [0.        , 1.55378985],
       [0.        , 1.55389702],
       [0.        , 1.55588293],
       [0.        , 1.56641543],
       [0.        , 1.56840372],
       [0.        , 1.5817064 ],
       [0.        , 1.60083711],
       [0.        , 1.61179566],
       [0.        , 1.6182245 ],
       [0.        , 1.62608635]]), array([[8.06843281, 8.43639565],
       [7.84889889, 8.12476254],
       [7.6451683 , 7.70159483],
       [7.20271158, 7.3832202 ],
       [7.03420925, 7.95048761],
       [6.95010614, 7.06919575],
       [6.61536789, 7.12129354],
       [6.57286596, 6.6668148 ],
       [6.48082113, 8.43619919],
       [6.45233393, 7.32854843],
       [5.7355237 , 5.82474566],
       [5.06774235, 5.18344545],
       [4.53609228, 4.63572741],
       [4.33080339, 4.38589859],
       [4.24380732, 4.32870626],
       [4.24294519, 4.43661833],
       [4.13354015, 4.33949947],
       [4.13121176, 4.41037703],
       [4.10843849, 4.38647985],
       [4.10449791, 4.33880568],
       [4.10074615, 4.36063957],
       [4.09732199, 4.36160231],
       [4.07876158, 4.55498505],
       [4.07027721, 4.3844943 ],
       [4.06629467, 4.36080313],
       [4.05186272, 4.44628763],
       [4.04261827, 4.49693823],
       [4.03044796, 4.21443653],
       [4.02058077, 4.41143894],
       [4.01741076, 4.15698481],
       [3.98943472, 4.70092583],
       [3.97062206, 4.3979435 ],
       [3.94666004, 8.88797951],
       [3.91712427, 4.65471506],
       [3.90241957, 7.55529737],
       [3.90005207, 4.60436678],
       [3.87802577, 4.35138083],
       [3.87767863, 4.25541735],
       [3.87654424, 9.24059677],
       [3.8735702 , 5.08456421],
       [3.86404467, 4.61270666],
       [3.79053402, 7.10659599],
       [3.34494686, 3.53094196],
       [3.03148007, 3.14455581]]), array([[ 9.53410912, 11.63528252],
       [ 8.44536781,  8.49683094]])]2024-03-06 18:00:17.152304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LCB ph vector generated, counter: 202
2024-03-06 18:00:21.177423: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:21.223072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:22.440474: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LCW ph vector generated, counter: 203
2024-03-06 18:00:25.918279: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:25.962039: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:26.995553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LCX ph vector generated, counter: 204
2024-03-06 18:00:31.376228: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:31.419900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:32.390654: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LDV ph vector generated, counter: 205
2024-03-06 18:00:35.873865: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:35.916856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:37.068127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LDW ph vector generated, counter: 206
2024-03-06 18:00:40.593028: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:40.635667: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:41.524937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LDX ph vector generated, counter: 207
2024-03-06 18:00:45.013202: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:45.056044: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:46.002370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LDY ph vector generated, counter: 208
2024-03-06 18:00:49.332296: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:49.375617: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:50.317165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LFF ph vector generated, counter: 209
2024-03-06 18:00:53.635594: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:00:53.678885: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:00:54.828545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3250138), (0., 1.3259971), (0., 1.3262235), (0., 1.3270037),
       (0., 1.3274529), (0., 1.32757  ), (0., 1.3277942), (0., 1.328194 ),
       (0., 1.3283234), (0., 1.3283839), (0., 1.3284017), (0., 1.3284943),
       (0., 1.3286515), (0., 1.3293344), (0., 1.3293626), (0., 1.3297697),
       (0., 1.3297983), (0., 1.3300898), (0., 1.3301401), (0., 1.3303193),
       (0., 1.3304474), (0., 1.3305655), (0., 1.3305898), (0., 1.3306555),
       (0., 1.3307564), (0., 1.3310872), (0., 1.3312442), (0., 1.3312628),
       (0., 1.3313255), (0., 1.331815 ), (0., 1.3318665), (0., 1.3318893),
       (0., 1.3318951), (0., 1.332192 ), (0., 1.332218 ), (0., 1.33226  ),
       (0., 1.3323677), (0., 1.3324785), (0., 1.3325745), (0., 1.3327057),
       (0., 1.3327335), (0., 1.333053 ), (0., 1.3331549), (0., 1.3331859),
       (0., 1.3332299), (0., 1.3333027), (0., 1.3333322), (0., 1.3333421),
       (0., 1.3334523), (0., 1.3335195), (0., 1.3335865), (0., 1.3337479),
       (0., 1.3337713), (0., 1.3340453), (0., 1.3341105), (0., 1.3341159),
       (0., 1.3342257), (0., 1.3342841), (0., 1.3343692), (0., 1.3343834),
       (0., 1.334394 ), (0., 1.3344746), (0., 1.334651 ), (0., 1.3346823),
       (0., 1.3346953), (0., 1.3346957), (0., 1.3347138), (0., 1.3347772),
       (0., 1.3347906), (0., 1.3348038), (0., 1.3348464), (0., 1.334857 ),
       (0., 1.3349291), (0., 1.3349326), (0., 1.3349695), (0., 1.3350351),
       (0., 1.3350681), (0., 1.3350726), (0., 1.3351054), (0., 1.3351282),
       (0., 1.3355453), (0., 1.335565 ), (0., 1.3355709), (0., 1.3356017),
       (0., 1.3357259), (0., 1.3357451), (0., 1.3357673), (0., 1.3357989),
       (0., 1.3359526), (0., 1.3359561), (0., 1.3359812), (0., 1.3361433),
       (0., 1.3361808), (0., 1.3362112), (0., 1.3363163), (0., 1.336339 ),
       (0., 1.3365257), (0., 1.3365344), (0., 1.3365464), (0., 1.336702 ),
       (0., 1.3368522), (0., 1.3368671), (0., 1.3370678), (0., 1.3370823),
       (0., 1.3374112), (0., 1.3374227), (0., 1.3374233), (0., 1.3375115),
       (0., 1.3375522), (0., 1.3376005), (0., 1.3376493), (0., 1.3378975),
       (0., 1.3378997), (0., 1.3379158), (0., 1.337953 ), (0., 1.3379894),
       (0., 1.3382446), (0., 1.3388362), (0., 1.339055 ), (0., 1.3394662),
       (0., 1.3396751), (0., 1.3397219), (0., 1.340003 ), (0., 1.340265 ),
       (0., 1.3405604), (0., 1.3407592), (0., 1.3410896), (0., 1.3412387),
       (0., 1.3418065), (0., 1.4442717), (0., 1.4466028), (0., 1.4477094),
       (0., 1.4495839), (0., 1.4499531), (0., 1.4501626), (0., 1.4502314),
       (0., 1.4502984), (0., 1.450379 ), (0., 1.4508548), (0., 1.451299 ),
       (0., 1.4515886), (0., 1.4517169), (0., 1.4522117), (0., 1.4524063),
       (0., 1.4527773), (0., 1.4529799), (0., 1.4537317), (0., 1.4546568),
       (0., 1.4546987), (0., 1.4550754), (0., 1.4554768), (0., 1.4554944),
       (0., 1.4556079), (0., 1.4556586), (0., 1.4556817), (0., 1.455721 ),
       (0., 1.4561282), (0., 1.4562864), (0., 1.4563274), (0., 1.4563956),
       (0., 1.4565009), (0., 1.4565562), (0., 1.4565845), (0., 1.456802 ),
       (0., 1.4569187), (0., 1.4569846), (0., 1.4571004), (0., 1.4571508),
       (0., 1.4572185), (0., 1.4572732), (0., 1.4573568), (0., 1.45741  ),
       (0., 1.4574163), (0., 1.4574877), (0., 1.4575524), (0., 1.4577171),
       (0., 1.4578325), (0., 1.457952 ), (0., 1.4579676), (0., 1.4580162),
       (0., 1.4580559), (0., 1.4582486), (0., 1.4582962), (0., 1.4583911),
       (0., 1.4584053), (0., 1.4584185), (0., 1.4584558), (0., 1.4586449),
       (0., 1.4586548), (0., 1.4588463), (0., 1.4589748), (0., 1.4589899),
       (0., 1.4590136), (0., 1.4591681), (0., 1.4591957), (0., 1.4592065),
       (0., 1.4593536), (0., 1.4593818), (0., 1.4594898), (0., 1.4594957),
       (0., 1.4597967), (0., 1.4598926), (0., 1.4599048), (0., 1.4599279),
       (0., 1.4600353), (0., 1.460165 ), (0., 1.4602448), (0., 1.4602631),
       (0., 1.460343 ), (0., 1.4604131), (0., 1.4604986), (0., 1.4605051),
       (0., 1.4605552), (0., 1.4605848), (0., 1.4606919), (0., 1.4607689),
       (0., 1.4609612), (0., 1.4609764), (0., 1.4612879), (0., 1.4613552),
       (0., 1.4613721), (0., 1.4613725), (0., 1.4615076), (0., 1.461578 ),
       (0., 1.4615817), (0., 1.4616387), (0., 1.4617044), (0., 1.4618231),
       (0., 1.4619043), (0., 1.4620072), (0., 1.4620392), (0., 1.4620466),
       (0., 1.4622885), (0., 1.4623027), (0., 1.4623029), (0., 1.4624648),
       (0., 1.4625213), (0., 1.4630166), (0., 1.4631406), (0., 1.4631603),
       (0., 1.463161 ), (0., 1.463219 ), (0., 1.4632382), (0., 1.4633968),
       (0., 1.463551 ), (0., 1.4637752), (0., 1.4640619), (0., 1.4641074),
       (0., 1.4642551), (0., 1.4644719), (0., 1.4644927), (0., 1.4649532),
       (0., 1.4654499), (0., 1.4656335), (0., 1.4658933), (0., 1.4665716),
       (0., 1.4670907), (0., 1.467618 ), (0., 1.4802405), (0., 1.508205 ),
       (0., 1.5084467), (0., 1.5096081), (0., 1.5100428), (0., 1.5100676),
       (0., 1.5130446), (0., 1.5147129), (0., 1.5147134), (0., 1.5155482),
       (0., 1.5166637), (0., 1.5167638), (0., 1.5168705), (0., 1.5172279),
       (0., 1.5172498), (0., 1.5176803), (0., 1.5179662), (0., 1.5182468),
       (0., 1.5187663), (0., 1.5190346), (0., 1.5190566), (0., 1.5192454),
       (0., 1.5192559), (0., 1.5192959), (0., 1.5193512), (0., 1.5200391),
       (0., 1.5202247), (0., 1.5202405), (0., 1.5203352), (0., 1.5204337),
       (0., 1.5205727), (0., 1.520576 ), (0., 1.520624 ), (0., 1.5207031),
       (0., 1.5207796), (0., 1.5207916), (0., 1.5209118), (0., 1.5213836),
       (0., 1.52148  ), (0., 1.5215434), (0., 1.5215875), (0., 1.5216963),
       (0., 1.5217053), (0., 1.5218853), (0., 1.5220019), (0., 1.522104 ),
       (0., 1.5223575), (0., 1.5224988), (0., 1.522519 ), (0., 1.5225562),
       (0., 1.5225642), (0., 1.5225868), (0., 1.5226805), (0., 1.5227047),
       (0., 1.5227218), (0., 1.5228592), (0., 1.5229003), (0., 1.5229884),
       (0., 1.5231639), (0., 1.5234596), (0., 1.5234945), (0., 1.5235401),
       (0., 1.5235732), (0., 1.5236048), (0., 1.5237169), (0., 1.5237695),
       (0., 1.5237727), (0., 1.5238396), (0., 1.5238492), (0., 1.5238657),
       (0., 1.5240896), (0., 1.5240915), (0., 1.5241138), (0., 1.524144 ),
       (0., 1.524253 ), (0., 1.5243405), (0., 1.5243583), (0., 1.5243648),
       (0., 1.5243669), (0., 1.5244906), (0., 1.5245162), (0., 1.5246053),
       (0., 1.5246096), (0., 1.5246396), (0., 1.524668 ), (0., 1.5247383),
       (0., 1.524772 ), (0., 1.5248699), (0., 1.5250434), (0., 1.525113 ),
       (0., 1.5252326), (0., 1.5252633), (0., 1.5253521), (0., 1.5253594),
       (0., 1.5253692), (0., 1.5254207), (0., 1.5255241), (0., 1.5256497),
       (0., 1.5256625), (0., 1.5256748), (0., 1.5258474), (0., 1.5259902),
       (0., 1.5261879), (0., 1.526274 ), (0., 1.5263095), (0., 1.5263417),
       (0., 1.5263736), (0., 1.5264494), (0., 1.5265311), (0., 1.5267088),
       (0., 1.5267979), (0., 1.5268884), (0., 1.5271232), (0., 1.5271248),
       (0., 1.5272752), (0., 1.5273947), (0., 1.5275155), (0., 1.5277305),
       (0., 1.5280404), (0., 1.5280695), (0., 1.5282385), (0., 1.5284358),
       (0., 1.529348 ), (0., 1.5296804), (0., 1.5297168), (0., 1.5297717),
       (0., 1.530019 ), (0., 1.5309885), (0., 1.5312402), (0., 1.5317822),
       (0., 1.5320532)], dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.16299  , 8.213614 ), (7.642714 , 7.6925974),
       (7.3369174, 7.4983597), (7.264372 , 7.331686 ),
       (7.2434297, 7.6477532), (6.995112 , 7.9701877),
       (6.9887714, 7.099486 ), (6.7691875, 7.0237103),
       (6.654652 , 8.046748 ), (6.5993857, 6.7882576),
       (6.497461 , 7.0181694), (6.356469 , 6.4315753),
       (6.351122 , 6.640159 ), (6.146904 , 6.4286075),
       (5.5926642, 6.88148  ), (5.491719 , 5.6501703),
       (5.381369 , 7.8049245), (5.3709803, 5.547101 ),
       (5.316632 , 7.1161547), (5.2318316, 7.3553505),
       (5.2230015, 5.420889 ), (4.9157233, 6.079793 ),
       (4.820518 , 5.296545 ), (4.7348256, 9.118832 ),
       (4.6328244, 5.6907787), (4.5278106, 5.154904 ),
       (4.4538255, 4.7052207), (4.324946 , 7.4560766),
       (4.303322 , 4.342081 ), (4.2495613, 4.4451303),
       (4.2485332, 4.43868  ), (4.218501 , 4.450939 ),
       (4.1880765, 4.364355 ), (4.1540947, 4.2880454),
       (4.152496 , 4.452246 ), (4.0860295, 4.467741 ),
       (4.070974 , 4.379214 ), (4.0586042, 4.6749043),
       (4.039311 , 8.384691 ), (3.9752169, 4.8789377),
       (3.9479294, 4.5690236), (3.9436522, 4.5608463),
       (3.9365013, 4.7062607), (3.933797 , 4.926804 ),
       (3.8905842, 3.9999833), (3.8610392, 3.8623147),
       (3.8226016, 5.3710084), (3.7934868, 6.6364803),
       (3.7750468, 4.2528296), (3.7418187, 3.9870448),
       (3.617258 , 4.001448 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(10.835438 , 11.054527 ), (10.055568 , 11.35778  ),
       ( 9.505651 ,  9.810609 ), ( 9.505651 , 10.577381 ),
       ( 9.174609 ,  9.213647 ), ( 8.3751745,  8.623219 ),
       ( 7.3933654,  7.4034376), ( 5.7577834,  5.7670283)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3250137567520142), (0.0, 1.3259971141815186), (0.0, 1.3262234926223755), (0.0, 1.3270037174224854), (0.0, 1.3274528980255127), (0.0, 1.3275699615478516), (0.0, 1.3277941942214966), (0.0, 1.32819402217865), (0.0, 1.3283233642578125), (0.0, 1.3283839225769043), (0.0, 1.3284016847610474), (0.0, 1.3284943103790283), (0.0, 1.3286515474319458), (0.0, 1.3293343782424927), (0.0, 1.3293626308441162), (0.0, 1.3297697305679321), (0.0, 1.3297983407974243), (0.0, 1.330089807510376), (0.0, 1.3301401138305664), (0.0, 1.3303192853927612), (0.0, 1.3304474353790283), (0.0, 1.3305654525756836), (0.0, 1.330589771270752), (0.0, 1.3306554555892944), (0.0, 1.330756425857544), (0.0, 1.3310872316360474), (0.0, 1.3312442302703857), (0.0, 1.3312628269195557), (0.0, 1.3313255310058594), (0.0, 1.3318150043487549), (0.0, 1.3318665027618408), (0.0, 1.331889271736145), (0.0, 1.331895112991333), (0.0, 1.3321919441223145), (0.0, 1.332218050956726), (0.0, 1.332260012626648), (0.0, 1.3323676586151123), (0.0, 1.3324785232543945), (0.0, 1.332574486732483), (0.0, 1.3327057361602783), (0.0, 1.3327335119247437), (0.0, 1.3330529928207397), (0.0, 1.3331549167633057), (0.0, 1.3331859111785889), (0.0, 1.333229899406433), (0.0, 1.3333027362823486), (0.0, 1.3333321809768677), (0.0, 1.3333420753479004), (0.0, 1.3334523439407349), (0.0, 1.333519458770752), (0.0, 1.3335864543914795), (0.0, 1.3337478637695312), (0.0, 1.3337713479995728), (0.0, 1.3340452909469604), (0.0, 1.3341104984283447), (0.0, 1.3341158628463745), (0.0, 1.3342256546020508), (0.0, 1.3342840671539307), (0.0, 1.33436918258667), (0.0, 1.3343833684921265), (0.0, 1.3343939781188965), (0.0, 1.3344745635986328), (0.0, 1.334650993347168), (0.0, 1.3346823453903198), (0.0, 1.3346953392028809), (0.0, 1.3346956968307495), (0.0, 1.3347138166427612), (0.0, 1.3347772359848022), (0.0, 1.334790587425232), (0.0, 1.334803819656372), (0.0, 1.3348463773727417), (0.0, 1.3348569869995117), (0.0, 1.33492910861969), (0.0, 1.334932565689087), (0.0, 1.3349695205688477), (0.0, 1.3350350856781006), (0.0, 1.3350681066513062), (0.0, 1.335072636604309), (0.0, 1.3351054191589355), (0.0, 1.3351281881332397), (0.0, 1.335545301437378), (0.0, 1.3355649709701538), (0.0, 1.3355709314346313), (0.0, 1.3356016874313354), (0.0, 1.3357259035110474), (0.0, 1.335745096206665), (0.0, 1.3357672691345215), (0.0, 1.3357988595962524), (0.0, 1.335952639579773), (0.0, 1.33595609664917), (0.0, 1.3359812498092651), (0.0, 1.3361432552337646), (0.0, 1.3361808061599731), (0.0, 1.3362112045288086), (0.0, 1.3363163471221924), (0.0, 1.336338996887207), (0.0, 1.3365256786346436), (0.0, 1.3365343809127808), (0.0, 1.3365464210510254), (0.0, 1.3367019891738892), (0.0, 1.3368521928787231), (0.0, 1.336867094039917), (0.0, 1.3370678424835205), (0.0, 1.3370822668075562), (0.0, 1.3374111652374268), (0.0, 1.3374227285385132), (0.0, 1.337423324584961), (0.0, 1.3375115394592285), (0.0, 1.3375521898269653), (0.0, 1.3376004695892334), (0.0, 1.3376493453979492), (0.0, 1.337897539138794), (0.0, 1.3378996849060059), (0.0, 1.3379157781600952), (0.0, 1.337952971458435), (0.0, 1.3379894495010376), (0.0, 1.3382445573806763), (0.0, 1.3388361930847168), (0.0, 1.3390549421310425), (0.0, 1.3394662141799927), (0.0, 1.3396750688552856), (0.0, 1.339721918106079), (0.0, 1.3400030136108398), (0.0, 1.3402650356292725), (0.0, 1.3405604362487793), (0.0, 1.3407591581344604), (0.0, 1.3410896062850952), (0.0, 1.3412387371063232), (0.0, 1.3418065309524536), (0.0, 1.4442716836929321), (0.0, 1.4466028213500977), (0.0, 1.4477094411849976), (0.0, 1.449583888053894), (0.0, 1.4499530792236328), (0.0, 1.450162649154663), (0.0, 1.4502314329147339), (0.0, 1.4502984285354614), (0.0, 1.4503790140151978), (0.0, 1.450854778289795), (0.0, 1.4512989521026611), (0.0, 1.4515886306762695), (0.0, 1.4517168998718262), (0.0, 1.4522117376327515), (0.0, 1.4524062871932983), (0.0, 1.4527772665023804), (0.0, 1.4529799222946167), (0.0, 1.453731656074524), (0.0, 1.4546568393707275), (0.0, 1.4546986818313599), (0.0, 1.4550753831863403), (0.0, 1.4554767608642578), (0.0, 1.4554944038391113), (0.0, 1.4556078910827637), (0.0, 1.4556585550308228), (0.0, 1.4556816816329956), (0.0, 1.4557210206985474), (0.0, 1.4561282396316528), (0.0, 1.4562864303588867), (0.0, 1.4563274383544922), (0.0, 1.4563956260681152), (0.0, 1.4565008878707886), (0.0, 1.4565562009811401), (0.0, 1.4565844535827637), (0.0, 1.4568020105361938), (0.0, 1.456918716430664), (0.0, 1.4569846391677856), (0.0, 1.4571003913879395), (0.0, 1.4571508169174194), (0.0, 1.4572185277938843), (0.0, 1.457273244857788), (0.0, 1.4573568105697632), (0.0, 1.4574099779129028), (0.0, 1.457416296005249), (0.0, 1.45748770236969), (0.0, 1.457552433013916), (0.0, 1.4577170610427856), (0.0, 1.4578324556350708), (0.0, 1.4579520225524902), (0.0, 1.4579676389694214), (0.0, 1.4580161571502686), (0.0, 1.458055853843689), (0.0, 1.4582486152648926), (0.0, 1.4582961797714233), (0.0, 1.4583910703659058), (0.0, 1.4584052562713623), (0.0, 1.4584184885025024), (0.0, 1.4584558010101318), (0.0, 1.4586448669433594), (0.0, 1.458654761314392), (0.0, 1.4588463306427002), (0.0, 1.458974838256836), (0.0, 1.4589898586273193), (0.0, 1.45901358127594), (0.0, 1.4591680765151978), (0.0, 1.4591957330703735), (0.0, 1.459206461906433), (0.0, 1.4593535661697388), (0.0, 1.4593818187713623), (0.0, 1.4594898223876953), (0.0, 1.4594956636428833), (0.0, 1.459796667098999), (0.0, 1.4598926305770874), (0.0, 1.4599047899246216), (0.0, 1.4599279165267944), (0.0, 1.4600353240966797), (0.0, 1.460165023803711), (0.0, 1.4602447748184204), (0.0, 1.4602631330490112), (0.0, 1.4603430032730103), (0.0, 1.4604130983352661), (0.0, 1.460498571395874), (0.0, 1.4605051279067993), (0.0, 1.4605551958084106), (0.0, 1.4605847597122192), (0.0, 1.4606919288635254), (0.0, 1.4607689380645752), (0.0, 1.4609612226486206), (0.0, 1.4609763622283936), (0.0, 1.4612878561019897), (0.0, 1.461355209350586), (0.0, 1.4613721370697021), (0.0, 1.4613724946975708), (0.0, 1.4615075588226318), (0.0, 1.4615780115127563), (0.0, 1.4615817070007324), (0.0, 1.4616386890411377), (0.0, 1.4617043733596802), (0.0, 1.4618231058120728), (0.0, 1.4619042873382568), (0.0, 1.4620071649551392), (0.0, 1.4620392322540283), (0.0, 1.4620466232299805), (0.0, 1.462288498878479), (0.0, 1.4623026847839355), (0.0, 1.4623029232025146), (0.0, 1.4624648094177246), (0.0, 1.4625213146209717), (0.0, 1.4630166292190552), (0.0, 1.463140606880188), (0.0, 1.4631602764129639), (0.0, 1.4631609916687012), (0.0, 1.4632190465927124), (0.0, 1.46323823928833), (0.0, 1.4633967876434326), (0.0, 1.4635510444641113), (0.0, 1.4637751579284668), (0.0, 1.4640618562698364), (0.0, 1.4641073942184448), (0.0, 1.4642550945281982), (0.0, 1.4644719362258911), (0.0, 1.464492678642273), (0.0, 1.4649531841278076), (0.0, 1.4654499292373657), (0.0, 1.465633511543274), (0.0, 1.465893268585205), (0.0, 1.466571569442749), (0.0, 1.4670907258987427), (0.0, 1.4676179885864258), (0.0, 1.4802404642105103), (0.0, 1.5082050561904907), (0.0, 1.5084466934204102), (0.0, 1.5096081495285034), (0.0, 1.5100427865982056), (0.0, 1.5100675821304321), (0.0, 1.5130445957183838), (0.0, 1.514712929725647), (0.0, 1.5147134065628052), (0.0, 1.5155482292175293), (0.0, 1.516663670539856), (0.0, 1.5167638063430786), (0.0, 1.5168704986572266), (0.0, 1.5172278881072998), (0.0, 1.5172498226165771), (0.0, 1.517680287361145), (0.0, 1.5179661512374878), (0.0, 1.5182467699050903), (0.0, 1.5187662839889526), (0.0, 1.5190346240997314), (0.0, 1.5190565586090088), (0.0, 1.5192453861236572), (0.0, 1.5192558765411377), (0.0, 1.5192959308624268), (0.0, 1.5193512439727783), (0.0, 1.5200390815734863), (0.0, 1.520224690437317), (0.0, 1.5202405452728271), (0.0, 1.5203351974487305), (0.0, 1.5204336643218994), (0.0, 1.5205726623535156), (0.0, 1.520576000213623), (0.0, 1.520624041557312), (0.0, 1.5207030773162842), (0.0, 1.5207796096801758), (0.0, 1.5207916498184204), (0.0, 1.5209118127822876), (0.0, 1.5213836431503296), (0.0, 1.5214799642562866), (0.0, 1.5215433835983276), (0.0, 1.5215874910354614), (0.0, 1.5216963291168213), (0.0, 1.5217052698135376), (0.0, 1.5218852758407593), (0.0, 1.52200186252594), (0.0, 1.522104024887085), (0.0, 1.52235746383667), (0.0, 1.5224988460540771), (0.0, 1.5225189924240112), (0.0, 1.522556185722351), (0.0, 1.522564172744751), (0.0, 1.5225868225097656), (0.0, 1.5226805210113525), (0.0, 1.5227047204971313), (0.0, 1.522721767425537), (0.0, 1.5228592157363892), (0.0, 1.5229003429412842), (0.0, 1.5229884386062622), (0.0, 1.523163914680481), (0.0, 1.523459553718567), (0.0, 1.5234944820404053), (0.0, 1.5235401391983032), (0.0, 1.5235731601715088), (0.0, 1.5236047506332397), (0.0, 1.523716926574707), (0.0, 1.523769497871399), (0.0, 1.5237727165222168), (0.0, 1.5238395929336548), (0.0, 1.5238492488861084), (0.0, 1.5238656997680664), (0.0, 1.5240895748138428), (0.0, 1.5240914821624756), (0.0, 1.5241137742996216), (0.0, 1.5241440534591675), (0.0, 1.524253010749817), (0.0, 1.5243405103683472), (0.0, 1.5243582725524902), (0.0, 1.5243648290634155), (0.0, 1.524366855621338), (0.0, 1.5244905948638916), (0.0, 1.524516224861145), (0.0, 1.5246052742004395), (0.0, 1.5246095657348633), (0.0, 1.52463960647583), (0.0, 1.5246679782867432), (0.0, 1.5247383117675781), (0.0, 1.524772047996521), (0.0, 1.5248699188232422), (0.0, 1.5250433683395386), (0.0, 1.5251129865646362), (0.0, 1.5252325534820557), (0.0, 1.5252633094787598), (0.0, 1.525352120399475), (0.0, 1.5253593921661377), (0.0, 1.5253691673278809), (0.0, 1.5254206657409668), (0.0, 1.5255241394042969), (0.0, 1.5256496667861938), (0.0, 1.5256625413894653), (0.0, 1.525674819946289), (0.0, 1.5258474349975586), (0.0, 1.5259902477264404), (0.0, 1.5261878967285156), (0.0, 1.5262739658355713), (0.0, 1.5263094902038574), (0.0, 1.5263416767120361), (0.0, 1.5263736248016357), (0.0, 1.52644944190979), (0.0, 1.5265311002731323), (0.0, 1.5267088413238525), (0.0, 1.526797890663147), (0.0, 1.526888370513916), (0.0, 1.527123212814331), (0.0, 1.5271247625350952), (0.0, 1.5272752046585083), (0.0, 1.5273946523666382), (0.0, 1.5275155305862427), (0.0, 1.5277304649353027), (0.0, 1.5280404090881348), (0.0, 1.5280694961547852), (0.0, 1.5282385349273682), (0.0, 1.5284358263015747), (0.0, 1.5293480157852173), (0.0, 1.5296803712844849), (0.0, 1.5297168493270874), (0.0, 1.5297716856002808), (0.0, 1.5300190448760986), (0.0, 1.5309884548187256), (0.0, 1.5312402248382568), (0.0, 1.5317821502685547), (0.0, 1.5320532321929932)], [(8.162989616394043, 8.213614463806152), (7.642714023590088, 7.692597389221191), (7.336917400360107, 7.498359680175781), (7.264371871948242, 7.331686019897461), (7.243429660797119, 7.6477532386779785), (6.99511194229126, 7.970187664031982), (6.988771438598633, 7.099485874176025), (6.7691874504089355, 7.023710250854492), (6.654652118682861, 8.046748161315918), (6.599385738372803, 6.788257598876953), (6.497460842132568, 7.018169403076172), (6.35646915435791, 6.431575298309326), (6.35112190246582, 6.6401591300964355), (6.146903991699219, 6.42860746383667), (5.5926642417907715, 6.8814802169799805), (5.491718769073486, 5.65017032623291), (5.381369113922119, 7.804924488067627), (5.370980262756348, 5.547101020812988), (5.31663179397583, 7.116154670715332), (5.2318315505981445, 7.355350494384766), (5.223001480102539, 5.420888900756836), (4.9157233238220215, 6.0797929763793945), (4.8205180168151855, 5.296545028686523), (4.734825611114502, 9.118831634521484), (4.632824420928955, 5.690778732299805), (4.527810573577881, 5.154903888702393), (4.4538254737854, 4.705220699310303), (4.32494592666626, 7.456076622009277), (4.303321838378906, 4.342081069946289), (4.249561309814453, 4.445130348205566), (4.248533248901367, 4.438680171966553), (4.218501091003418, 4.450939178466797), (4.188076496124268, 4.364355087280273), (4.154094696044922, 4.288045406341553), (4.152495861053467, 4.452246189117432), (4.086029529571533, 4.467741012573242), (4.070973873138428, 4.379213809967041), (4.0586042404174805, 4.6749043464660645), (4.039310932159424, 8.38469123840332), (3.975216865539551, 4.878937721252441), (3.9479293823242188, 4.569023609161377), (3.9436521530151367, 4.560846328735352), (3.9365012645721436, 4.706260681152344), (3.9337968826293945, 4.926804065704346), (3.8905842304229736, 3.999983310699463), (3.861039161682129, 3.8623147010803223), (3.822601556777954, 5.371008396148682), (3.7934868335723877, 6.636480331420898), (3.7750468254089355, 4.252829551696777), (3.74181866645813, 3.9870448112487793), (3.617258071899414, 4.001448154449463)], [(10.835437774658203, 11.054527282714844), (10.055567741394043, 11.357780456542969), (9.505651473999023, 9.810608863830566), (9.505651473999023, 10.577381134033203), (9.174609184265137, 9.21364688873291), (8.375174522399902, 8.623218536376953), (7.393365383148193, 7.403437614440918), (5.75778341293335, 5.767028331756592)]]
[array([[0.        , 1.32501376],
       [0.        , 1.32599711],
       [0.        , 1.32622349],
       [0.        , 1.32700372],
       [0.        , 1.3274529 ],
       [0.        , 1.32756996],
       [0.        , 1.32779419],
       [0.        , 1.32819402],
       [0.        , 1.32832336],
       [0.        , 1.32838392],
       [0.        , 1.32840168],
       [0.        , 1.32849431],
       [0.        , 1.32865155],
       [0.        , 1.32933438],
       [0.        , 1.32936263],
       [0.        , 1.32976973],
       [0.        , 1.32979834],
       [0.        , 1.33008981],
       [0.        , 1.33014011],
       [0.        , 1.33031929],
       [0.        , 1.33044744],
       [0.        , 1.33056545],
       [0.        , 1.33058977],
       [0.        , 1.33065546],
       [0.        , 1.33075643],
       [0.        , 1.33108723],
       [0.        , 1.33124423],
       [0.        , 1.33126283],
       [0.        , 1.33132553],
       [0.        , 1.331815  ],
       [0.        , 1.3318665 ],
       [0.        , 1.33188927],
       [0.        , 1.33189511],
       [0.        , 1.33219194],
       [0.        , 1.33221805],
       [0.        , 1.33226001],
       [0.        , 1.33236766],
       [0.        , 1.33247852],
       [0.        , 1.33257449],
       [0.        , 1.33270574],
       [0.        , 1.33273351],
       [0.        , 1.33305299],
       [0.        , 1.33315492],
       [0.        , 1.33318591],
       [0.        , 1.3332299 ],
       [0.        , 1.33330274],
       [0.        , 1.33333218],
       [0.        , 1.33334208],
       [0.        , 1.33345234],
       [0.        , 1.33351946],
       [0.        , 1.33358645],
       [0.        , 1.33374786],
       [0.        , 1.33377135],
       [0.        , 1.33404529],
       [0.        , 1.3341105 ],
       [0.        , 1.33411586],
       [0.        , 1.33422565],
       [0.        , 1.33428407],
       [0.        , 1.33436918],
       [0.        , 1.33438337],
       [0.        , 1.33439398],
       [0.        , 1.33447456],
       [0.        , 1.33465099],
       [0.        , 1.33468235],
       [0.        , 1.33469534],
       [0.        , 1.3346957 ],
       [0.        , 1.33471382],
       [0.        , 1.33477724],
       [0.        , 1.33479059],
       [0.        , 1.33480382],
       [0.        , 1.33484638],
       [0.        , 1.33485699],
       [0.        , 1.33492911],
       [0.        , 1.33493257],
       [0.        , 1.33496952],
       [0.        , 1.33503509],
       [0.        , 1.33506811],
       [0.        , 1.33507264],
       [0.        , 1.33510542],
       [0.        , 1.33512819],
       [0.        , 1.3355453 ],
       [0.        , 1.33556497],
       [0.        , 1.33557093],
       [0.        , 1.33560169],
       [0.        , 1.3357259 ],
       [0.        , 1.3357451 ],
       [0.        , 1.33576727],
       [0.        , 1.33579886],
       [0.        , 1.33595264],
       [0.        , 1.3359561 ],
       [0.        , 1.33598125],
       [0.        , 1.33614326],
       [0.        , 1.33618081],
       [0.        , 1.3362112 ],
       [0.        , 1.33631635],
       [0.        , 1.336339  ],
       [0.        , 1.33652568],
       [0.        , 1.33653438],
       [0.        , 1.33654642],
       [0.        , 1.33670199],
       [0.        , 1.33685219],
       [0.        , 1.33686709],
       [0.        , 1.33706784],
       [0.        , 1.33708227],
       [0.        , 1.33741117],
       [0.        , 1.33742273],
       [0.        , 1.33742332],
       [0.        , 1.33751154],
       [0.        , 1.33755219],
       [0.        , 1.33760047],
       [0.        , 1.33764935],
       [0.        , 1.33789754],
       [0.        , 1.33789968],
       [0.        , 1.33791578],
       [0.        , 1.33795297],
       [0.        , 1.33798945],
       [0.        , 1.33824456],
       [0.        , 1.33883619],
       [0.        , 1.33905494],
       [0.        , 1.33946621],
       [0.        , 1.33967507],
       [0.        , 1.33972192],
       [0.        , 1.34000301],
       [0.        , 1.34026504],
       [0.        , 1.34056044],
       [0.        , 1.34075916],
       [0.        , 1.34108961],
       [0.        , 1.34123874],
       [0.        , 1.34180653],
       [0.        , 1.44427168],
       [0.        , 1.44660282],
       [0.        , 1.44770944],
       [0.        , 1.44958389],
       [0.        , 1.44995308],
       [0.        , 1.45016265],
       [0.        , 1.45023143],
       [0.        , 1.45029843],
       [0.        , 1.45037901],
       [0.        , 1.45085478],
       [0.        , 1.45129895],
       [0.        , 1.45158863],
       [0.        , 1.4517169 ],
       [0.        , 1.45221174],
       [0.        , 1.45240629],
       [0.        , 1.45277727],
       [0.        , 1.45297992],
       [0.        , 1.45373166],
       [0.        , 1.45465684],
       [0.        , 1.45469868],
       [0.        , 1.45507538],
       [0.        , 1.45547676],
       [0.        , 1.4554944 ],
       [0.        , 1.45560789],
       [0.        , 1.45565856],
       [0.        , 1.45568168],
       [0.        , 1.45572102],
       [0.        , 1.45612824],
       [0.        , 1.45628643],
       [0.        , 1.45632744],
       [0.        , 1.45639563],
       [0.        , 1.45650089],
       [0.        , 1.4565562 ],
       [0.        , 1.45658445],
       [0.        , 1.45680201],
       [0.        , 1.45691872],
       [0.        , 1.45698464],
       [0.        , 1.45710039],
       [0.        , 1.45715082],
       [0.        , 1.45721853],
       [0.        , 1.45727324],
       [0.        , 1.45735681],
       [0.        , 1.45740998],
       [0.        , 1.4574163 ],
       [0.        , 1.4574877 ],
       [0.        , 1.45755243],
       [0.        , 1.45771706],
       [0.        , 1.45783246],
       [0.        , 1.45795202],
       [0.        , 1.45796764],
       [0.        , 1.45801616],
       [0.        , 1.45805585],
       [0.        , 1.45824862],
       [0.        , 1.45829618],
       [0.        , 1.45839107],
       [0.        , 1.45840526],
       [0.        , 1.45841849],
       [0.        , 1.4584558 ],
       [0.        , 1.45864487],
       [0.        , 1.45865476],
       [0.        , 1.45884633],
       [0.        , 1.45897484],
       [0.        , 1.45898986],
       [0.        , 1.45901358],
       [0.        , 1.45916808],
       [0.        , 1.45919573],
       [0.        , 1.45920646],
       [0.        , 1.45935357],
       [0.        , 1.45938182],
       [0.        , 1.45948982],
       [0.        , 1.45949566],
       [0.        , 1.45979667],
       [0.        , 1.45989263],
       [0.        , 1.45990479],
       [0.        , 1.45992792],
       [0.        , 1.46003532],
       [0.        , 1.46016502],
       [0.        , 1.46024477],
       [0.        , 1.46026313],
       [0.        , 1.460343  ],
       [0.        , 1.4604131 ],
       [0.        , 1.46049857],
       [0.        , 1.46050513],
       [0.        , 1.4605552 ],
       [0.        , 1.46058476],
       [0.        , 1.46069193],
       [0.        , 1.46076894],
       [0.        , 1.46096122],
       [0.        , 1.46097636],
       [0.        , 1.46128786],
       [0.        , 1.46135521],
       [0.        , 1.46137214],
       [0.        , 1.46137249],
       [0.        , 1.46150756],
       [0.        , 1.46157801],
       [0.        , 1.46158171],
       [0.        , 1.46163869],
       [0.        , 1.46170437],
       [0.        , 1.46182311],
       [0.        , 1.46190429],
       [0.        , 1.46200716],
       [0.        , 1.46203923],
       [0.        , 1.46204662],
       [0.        , 1.4622885 ],
       [0.        , 1.46230268],
       [0.        , 1.46230292],
       [0.        , 1.46246481],
       [0.        , 1.46252131],
       [0.        , 1.46301663],
       [0.        , 1.46314061],
       [0.        , 1.46316028],
       [0.        , 1.46316099],
       [0.        , 1.46321905],
       [0.        , 1.46323824],
       [0.        , 1.46339679],
       [0.        , 1.46355104],
       [0.        , 1.46377516],
       [0.        , 1.46406186],
       [0.        , 1.46410739],
       [0.        , 1.46425509],
       [0.        , 1.46447194],
       [0.        , 1.46449268],
       [0.        , 1.46495318],
       [0.        , 1.46544993],
       [0.        , 1.46563351],
       [0.        , 1.46589327],
       [0.        , 1.46657157],
       [0.        , 1.46709073],
       [0.        , 1.46761799],
       [0.        , 1.48024046],
       [0.        , 1.50820506],
       [0.        , 1.50844669],
       [0.        , 1.50960815],
       [0.        , 1.51004279],
       [0.        , 1.51006758],
       [0.        , 1.5130446 ],
       [0.        , 1.51471293],
       [0.        , 1.51471341],
       [0.        , 1.51554823],
       [0.        , 1.51666367],
       [0.        , 1.51676381],
       [0.        , 1.5168705 ],
       [0.        , 1.51722789],
       [0.        , 1.51724982],
       [0.        , 1.51768029],
       [0.        , 1.51796615],
       [0.        , 1.51824677],
       [0.        , 1.51876628],
       [0.        , 1.51903462],
       [0.        , 1.51905656],
       [0.        , 1.51924539],
       [0.        , 1.51925588],
       [0.        , 1.51929593],
       [0.        , 1.51935124],
       [0.        , 1.52003908],
       [0.        , 1.52022469],
       [0.        , 1.52024055],
       [0.        , 1.5203352 ],
       [0.        , 1.52043366],
       [0.        , 1.52057266],
       [0.        , 1.520576  ],
       [0.        , 1.52062404],
       [0.        , 1.52070308],
       [0.        , 1.52077961],
       [0.        , 1.52079165],
       [0.        , 1.52091181],
       [0.        , 1.52138364],
       [0.        , 1.52147996],
       [0.        , 1.52154338],
       [0.        , 1.52158749],
       [0.        , 1.52169633],
       [0.        , 1.52170527],
       [0.        , 1.52188528],
       [0.        , 1.52200186],
       [0.        , 1.52210402],
       [0.        , 1.52235746],
       [0.        , 1.52249885],
       [0.        , 1.52251899],
       [0.        , 1.52255619],
       [0.        , 1.52256417],
       [0.        , 1.52258682],
       [0.        , 1.52268052],
       [0.        , 1.52270472],
       [0.        , 1.52272177],
       [0.        , 1.52285922],
       [0.        , 1.52290034],
       [0.        , 1.52298844],
       [0.        , 1.52316391],
       [0.        , 1.52345955],
       [0.        , 1.52349448],
       [0.        , 1.52354014],
       [0.        , 1.52357316],
       [0.        , 1.52360475],
       [0.        , 1.52371693],
       [0.        , 1.5237695 ],
       [0.        , 1.52377272],
       [0.        , 1.52383959],
       [0.        , 1.52384925],
       [0.        , 1.5238657 ],
       [0.        , 1.52408957],
       [0.        , 1.52409148],
       [0.        , 1.52411377],
       [0.        , 1.52414405],
       [0.        , 1.52425301],
       [0.        , 1.52434051],
       [0.        , 1.52435827],
       [0.        , 1.52436483],
       [0.        , 1.52436686],
       [0.        , 1.52449059],
       [0.        , 1.52451622],
       [0.        , 1.52460527],
       [0.        , 1.52460957],
       [0.        , 1.52463961],
       [0.        , 1.52466798],
       [0.        , 1.52473831],
       [0.        , 1.52477205],
       [0.        , 1.52486992],
       [0.        , 1.52504337],
       [0.        , 1.52511299],
       [0.        , 1.52523255],
       [0.        , 1.52526331],
       [0.        , 1.52535212],
       [0.        , 1.52535939],
       [0.        , 1.52536917],
       [0.        , 1.52542067],
       [0.        , 1.52552414],
       [0.        , 1.52564967],
       [0.        , 1.52566254],
       [0.        , 1.52567482],
       [0.        , 1.52584743],
       [0.        , 1.52599025],
       [0.        , 1.5261879 ],
       [0.        , 1.52627397],
       [0.        , 1.52630949],
       [0.        , 1.52634168],
       [0.        , 1.52637362],
       [0.        , 1.52644944],
       [0.        , 1.5265311 ],
       [0.        , 1.52670884],
       [0.        , 1.52679789],
       [0.        , 1.52688837],
       [0.        , 1.52712321],
       [0.        , 1.52712476],
       [0.        , 1.5272752 ],
       [0.        , 1.52739465],
       [0.        , 1.52751553],
       [0.        , 1.52773046],
       [0.        , 1.52804041],
       [0.        , 1.5280695 ],
       [0.        , 1.52823853],
       [0.        , 1.52843583],
       [0.        , 1.52934802],
       [0.        , 1.52968037],
       [0.        , 1.52971685],
       [0.        , 1.52977169],
       [0.        , 1.53001904],
       [0.        , 1.53098845],
       [0.        , 1.53124022],
       [0.        , 1.53178215],
       [0.        , 1.53205323]]), array([[8.16298962, 8.21361446],
       [7.64271402, 7.69259739],
       [7.3369174 , 7.49835968],
       [7.26437187, 7.33168602],
       [7.24342966, 7.64775324],
       [6.99511194, 7.97018766],
       [6.98877144, 7.09948587],
       [6.76918745, 7.02371025],
       [6.65465212, 8.04674816],
       [6.59938574, 6.7882576 ],
       [6.49746084, 7.0181694 ],
       [6.35646915, 6.4315753 ],
       [6.3511219 , 6.64015913],
       [6.14690399, 6.42860746],
       [5.59266424, 6.88148022],
       [5.49171877, 5.65017033],
       [5.38136911, 7.80492449],
       [5.37098026, 5.54710102],
       [5.31663179, 7.11615467],
       [5.23183155, 7.35535049],
       [5.22300148, 5.4208889 ],
       [4.91572332, 6.07979298],
       [4.82051802, 5.29654503],
       [4.73482561, 9.11883163],
       [4.63282442, 5.69077873],
       [4.52781057, 5.15490389],
       [4.45382547, 4.7052207 ],
       [4.32494593, 7.45607662],
       [4.30332184, 4.34208107],
       [4.24956131, 4.44513035],
       [4.24853325, 4.43868017],
       [4.21850109, 4.45093918],
       [4.1880765 , 4.36435509],
       [4.1540947 , 4.28804541],
       [4.15249586, 4.45224619],
       [4.08602953, 4.46774101],
       [4.07097387, 4.37921381],
       [4.05860424, 4.67490435],
       [4.03931093, 8.38469124],
       [3.97521687, 4.87893772],
       [3.94792938, 4.56902361],
       [3.94365215, 4.56084633],
       [3.93650126, 4.70626068],
       [3.93379688, 4.92680407],
       [3.89058423, 3.99998331],
       [3.86103916, 3.8623147 ],
       [3.82260156, 5.3710084 ],
       [3.79348683, 6.63648033],
       [3.77504683, 4.25282955],
       [3.74181867, 3.98704481],
       [3.61725807, 4.00144815]]), array([[10.83543777, 11.05452728],
       [10.05556774, 11.35778046],
       [ 9.50565147,  9.81060886],
       [ 9.50565147, 10.57738113],
       [ 9.17460918,  9.21364689],
       [ 8.37517452,  8.62321854],
       [ 7.39336538,  7.40343761],
       [ 5.75778341,  5.76702833]])]2024-03-06 18:00:59.272159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LFH ph vector generated, counter: 210
2024-03-06 18:01:03.433933: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:01:03.478214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:01:04.422914: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.2771547), (0., 1.3197972), (0., 1.3208427), (0., 1.3227365),
       (0., 1.3251982), (0., 1.3259003), (0., 1.3264396), (0., 1.3271563),
       (0., 1.3276347), (0., 1.3280256), (0., 1.3283634), (0., 1.3287013),
       (0., 1.3288901), (0., 1.3290719), (0., 1.3292122), (0., 1.3292373),
       (0., 1.3293756), (0., 1.329414 ), (0., 1.3294936), (0., 1.3297414),
       (0., 1.3297751), (0., 1.3297955), (0., 1.329906 ), (0., 1.3300097),
       (0., 1.3301814), (0., 1.3304081), (0., 1.3305439), (0., 1.3305854),
       (0., 1.3305945), (0., 1.3308523), (0., 1.3309276), (0., 1.3310703),
       (0., 1.3310854), (0., 1.3311005), (0., 1.3311167), (0., 1.3311625),
       (0., 1.3312045), (0., 1.3312465), (0., 1.3312626), (0., 1.33127  ),
       (0., 1.3313125), (0., 1.3316195), (0., 1.3318882), (0., 1.3318958),
       (0., 1.3319266), (0., 1.3319494), (0., 1.3321259), (0., 1.3321911),
       (0., 1.3322749), (0., 1.3324187), (0., 1.3324983), (0., 1.3328712),
       (0., 1.3329376), (0., 1.3330706), (0., 1.3331305), (0., 1.3331356),
       (0., 1.3331447), (0., 1.3331699), (0., 1.3333706), (0., 1.3335708),
       (0., 1.3336239), (0., 1.3337342), (0., 1.333807 ), (0., 1.3341519),
       (0., 1.3342602), (0., 1.3343613), (0., 1.3343644), (0., 1.3345703),
       (0., 1.3347046), (0., 1.3350757), (0., 1.3350922), (0., 1.3351104),
       (0., 1.3352028), (0., 1.3352728), (0., 1.3352814), (0., 1.3352892),
       (0., 1.3355619), (0., 1.3356436), (0., 1.3356742), (0., 1.3357997),
       (0., 1.3358071), (0., 1.3358338), (0., 1.3358833), (0., 1.3358967),
       (0., 1.3359727), (0., 1.3361819), (0., 1.3361906), (0., 1.3363166),
       (0., 1.3363237), (0., 1.3364732), (0., 1.3364764), (0., 1.3366854),
       (0., 1.3367069), (0., 1.3367579), (0., 1.3368717), (0., 1.3369972),
       (0., 1.3371091), (0., 1.3372936), (0., 1.3373969), (0., 1.3374141),
       (0., 1.3375772), (0., 1.3375778), (0., 1.3376402), (0., 1.3376797),
       (0., 1.3377012), (0., 1.3377899), (0., 1.3379172), (0., 1.3379513),
       (0., 1.3379782), (0., 1.3380488), (0., 1.3381692), (0., 1.3385022),
       (0., 1.3386217), (0., 1.338975 ), (0., 1.3393954), (0., 1.3394375),
       (0., 1.3395079), (0., 1.3400176), (0., 1.3400543), (0., 1.3403982),
       (0., 1.3407769), (0., 1.3410118), (0., 1.3412517), (0., 1.3454117),
       (0., 1.4468192), (0., 1.4482179), (0., 1.4482242), (0., 1.4497234),
       (0., 1.4508432), (0., 1.4510996), (0., 1.4517318), (0., 1.4522452),
       (0., 1.4528055), (0., 1.452848 ), (0., 1.4529092), (0., 1.4530259),
       (0., 1.4536825), (0., 1.4539139), (0., 1.454039 ), (0., 1.4541608),
       (0., 1.4547278), (0., 1.4547473), (0., 1.4548547), (0., 1.4548794),
       (0., 1.4554876), (0., 1.4557259), (0., 1.4559195), (0., 1.4560243),
       (0., 1.4562775), (0., 1.4563931), (0., 1.4564527), (0., 1.4565445),
       (0., 1.4565749), (0., 1.4567302), (0., 1.4568177), (0., 1.4568496),
       (0., 1.4568759), (0., 1.4568878), (0., 1.4569012), (0., 1.4571   ),
       (0., 1.4571717), (0., 1.4572353), (0., 1.4572405), (0., 1.4573421),
       (0., 1.4573826), (0., 1.4574493), (0., 1.4574805), (0., 1.4575206),
       (0., 1.4575316), (0., 1.457692 ), (0., 1.4578388), (0., 1.4578916),
       (0., 1.4579318), (0., 1.458024 ), (0., 1.458079 ), (0., 1.4580883),
       (0., 1.4581368), (0., 1.4582282), (0., 1.458645 ), (0., 1.459135 ),
       (0., 1.4591519), (0., 1.459383 ), (0., 1.459519 ), (0., 1.4595423),
       (0., 1.4596102), (0., 1.4596242), (0., 1.459698 ), (0., 1.4597106),
       (0., 1.4599161), (0., 1.4601575), (0., 1.4601943), (0., 1.4602954),
       (0., 1.4604756), (0., 1.460499 ), (0., 1.4605222), (0., 1.4605631),
       (0., 1.46057  ), (0., 1.4606882), (0., 1.4608133), (0., 1.4609549),
       (0., 1.4611926), (0., 1.4612697), (0., 1.4613065), (0., 1.4613451),
       (0., 1.4613866), (0., 1.4614031), (0., 1.4614594), (0., 1.4614725),
       (0., 1.4617693), (0., 1.4618033), (0., 1.4618118), (0., 1.4619714),
       (0., 1.4619882), (0., 1.4620699), (0., 1.4620724), (0., 1.4621301),
       (0., 1.4623508), (0., 1.4624109), (0., 1.4626511), (0., 1.46266  ),
       (0., 1.463035 ), (0., 1.4631217), (0., 1.4631741), (0., 1.46331  ),
       (0., 1.463351 ), (0., 1.4633833), (0., 1.4634104), (0., 1.4637485),
       (0., 1.4638401), (0., 1.4640498), (0., 1.4643334), (0., 1.4644434),
       (0., 1.4650645), (0., 1.4650824), (0., 1.4655302), (0., 1.4660789),
       (0., 1.466275 ), (0., 1.4665172), (0., 1.4673003), (0., 1.4678029),
       (0., 1.4680219), (0., 1.468443 ), (0., 1.4687748), (0., 1.4693372),
       (0., 1.4709623), (0., 1.4709826), (0., 1.4716176), (0., 1.4757417),
       (0., 1.5063289), (0., 1.5081663), (0., 1.513254 ), (0., 1.5142758),
       (0., 1.5148021), (0., 1.515497 ), (0., 1.5158389), (0., 1.5159272),
       (0., 1.5171577), (0., 1.5176303), (0., 1.517698 ), (0., 1.5179366),
       (0., 1.51798  ), (0., 1.5179988), (0., 1.5180131), (0., 1.5182137),
       (0., 1.518256 ), (0., 1.5185441), (0., 1.5185776), (0., 1.5186024),
       (0., 1.519012 ), (0., 1.5191214), (0., 1.519154 ), (0., 1.5192153),
       (0., 1.519652 ), (0., 1.5202191), (0., 1.520272 ), (0., 1.520445 ),
       (0., 1.5207465), (0., 1.5209004), (0., 1.5209937), (0., 1.5211182),
       (0., 1.5211297), (0., 1.5211385), (0., 1.5211842), (0., 1.5212305),
       (0., 1.5212971), (0., 1.5213139), (0., 1.5214185), (0., 1.5214311),
       (0., 1.5215862), (0., 1.5216395), (0., 1.5216589), (0., 1.521718 ),
       (0., 1.5217731), (0., 1.5217912), (0., 1.5217944), (0., 1.521835 ),
       (0., 1.5218875), (0., 1.5219474), (0., 1.5220997), (0., 1.5221592),
       (0., 1.5223087), (0., 1.5223517), (0., 1.5223547), (0., 1.5224319),
       (0., 1.5224872), (0., 1.5225117), (0., 1.522551 ), (0., 1.5225585),
       (0., 1.5226499), (0., 1.5227135), (0., 1.5227828), (0., 1.5227901),
       (0., 1.5228151), (0., 1.5228959), (0., 1.5229621), (0., 1.5231642),
       (0., 1.5233003), (0., 1.5233421), (0., 1.5233493), (0., 1.5234175),
       (0., 1.5235207), (0., 1.5236223), (0., 1.5238063), (0., 1.5239383),
       (0., 1.5241096), (0., 1.5241603), (0., 1.5242094), (0., 1.52424  ),
       (0., 1.5243055), (0., 1.524326 ), (0., 1.5245793), (0., 1.524611 ),
       (0., 1.5246203), (0., 1.5246724), (0., 1.5246974), (0., 1.5248424),
       (0., 1.5250132), (0., 1.525191 ), (0., 1.5254114), (0., 1.525464 ),
       (0., 1.5254674), (0., 1.5256244), (0., 1.5258157), (0., 1.5259122),
       (0., 1.5259167), (0., 1.5259517), (0., 1.5259792), (0., 1.5260307),
       (0., 1.5260946), (0., 1.5260955), (0., 1.5262598), (0., 1.5263095),
       (0., 1.5264169), (0., 1.5265589), (0., 1.5266522), (0., 1.5266598),
       (0., 1.5266795), (0., 1.5273324), (0., 1.5273542), (0., 1.5273972),
       (0., 1.5276769), (0., 1.5279844), (0., 1.5281026), (0., 1.5281309),
       (0., 1.5281374), (0., 1.5283782), (0., 1.5285045), (0., 1.5289303),
       (0., 1.5313481), (0., 1.5334702), (0., 1.5334764), (0., 1.5353928),
       (0., 1.5859251), (0., 1.5924246)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(7.806099 , 8.221949 ), (7.092136 , 7.666169 ),
       (7.0178795, 7.0567365), (6.8517265, 7.0760713),
       (6.8064246, 7.23667  ), (6.706962 , 7.3359866),
       (6.4619384, 7.060897 ), (6.4134417, 6.96823  ),
       (6.217612 , 7.150987 ), (6.16373  , 7.9541855),
       (6.1042004, 7.2440434), (6.062362 , 6.4210687),
       (5.927752 , 5.9595804), (5.7138443, 5.810759 ),
       (5.5451946, 8.612382 ), (5.279996 , 8.233778 ),
       (5.1716847, 5.203309 ), (5.1365895, 6.6547155),
       (5.135308 , 5.5183125), (4.9409895, 7.071132 ),
       (4.797234 , 8.330668 ), (4.631743 , 4.7842884),
       (4.441024 , 7.81334  ), (4.1593657, 8.013637 ),
       (3.9907153, 4.2318363), (3.9874372, 4.130065 ),
       (3.9811537, 4.020765 ), (3.9491904, 4.726527 ),
       (3.8913555, 4.190088 ), (3.735128 , 8.72198  ),
       (1.9087585, 1.9315381)], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.689834, 10.284567 ), (9.437744,  9.519922 ),
       (9.18441 , 10.488039 ), (8.26563 ,  8.310351 ),
       (8.262927,  8.3644705), (7.680275,  7.814806 )],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.2771546840667725), (0.0, 1.319797158241272), (0.0, 1.3208427429199219), (0.0, 1.3227365016937256), (0.0, 1.3251981735229492), (0.0, 1.3259003162384033), (0.0, 1.326439619064331), (0.0, 1.3271563053131104), (0.0, 1.3276346921920776), (0.0, 1.3280255794525146), (0.0, 1.3283634185791016), (0.0, 1.3287012577056885), (0.0, 1.328890085220337), (0.0, 1.3290718793869019), (0.0, 1.3292121887207031), (0.0, 1.3292373418807983), (0.0, 1.3293756246566772), (0.0, 1.3294140100479126), (0.0, 1.3294936418533325), (0.0, 1.329741358757019), (0.0, 1.329775094985962), (0.0, 1.329795479774475), (0.0, 1.3299059867858887), (0.0, 1.3300096988677979), (0.0, 1.330181360244751), (0.0, 1.3304080963134766), (0.0, 1.330543875694275), (0.0, 1.3305853605270386), (0.0, 1.330594539642334), (0.0, 1.3308522701263428), (0.0, 1.3309276103973389), (0.0, 1.3310703039169312), (0.0, 1.331085443496704), (0.0, 1.3311004638671875), (0.0, 1.3311166763305664), (0.0, 1.331162452697754), (0.0, 1.3312045335769653), (0.0, 1.3312464952468872), (0.0, 1.3312625885009766), (0.0, 1.3312699794769287), (0.0, 1.3313125371932983), (0.0, 1.3316195011138916), (0.0, 1.331888198852539), (0.0, 1.3318958282470703), (0.0, 1.3319265842437744), (0.0, 1.3319493532180786), (0.0, 1.3321259021759033), (0.0, 1.3321911096572876), (0.0, 1.3322749137878418), (0.0, 1.33241868019104), (0.0, 1.33249831199646), (0.0, 1.3328711986541748), (0.0, 1.3329375982284546), (0.0, 1.3330706357955933), (0.0, 1.3331304788589478), (0.0, 1.3331356048583984), (0.0, 1.3331446647644043), (0.0, 1.333169937133789), (0.0, 1.333370566368103), (0.0, 1.3335708379745483), (0.0, 1.3336238861083984), (0.0, 1.333734154701233), (0.0, 1.3338069915771484), (0.0, 1.3341518640518188), (0.0, 1.3342602252960205), (0.0, 1.3343613147735596), (0.0, 1.334364414215088), (0.0, 1.334570288658142), (0.0, 1.3347046375274658), (0.0, 1.3350757360458374), (0.0, 1.3350921869277954), (0.0, 1.3351104259490967), (0.0, 1.3352028131484985), (0.0, 1.3352727890014648), (0.0, 1.3352813720703125), (0.0, 1.3352892398834229), (0.0, 1.3355618715286255), (0.0, 1.3356436491012573), (0.0, 1.3356741666793823), (0.0, 1.3357996940612793), (0.0, 1.3358070850372314), (0.0, 1.3358337879180908), (0.0, 1.3358832597732544), (0.0, 1.3358967304229736), (0.0, 1.3359726667404175), (0.0, 1.336181879043579), (0.0, 1.3361905813217163), (0.0, 1.3363165855407715), (0.0, 1.3363237380981445), (0.0, 1.3364732265472412), (0.0, 1.336476445198059), (0.0, 1.3366854190826416), (0.0, 1.3367068767547607), (0.0, 1.3367578983306885), (0.0, 1.3368717432022095), (0.0, 1.336997151374817), (0.0, 1.337109088897705), (0.0, 1.3372936248779297), (0.0, 1.3373968601226807), (0.0, 1.3374141454696655), (0.0, 1.337577223777771), (0.0, 1.3375778198242188), (0.0, 1.3376401662826538), (0.0, 1.3376797437667847), (0.0, 1.3377012014389038), (0.0, 1.3377898931503296), (0.0, 1.3379172086715698), (0.0, 1.3379513025283813), (0.0, 1.3379782438278198), (0.0, 1.3380488157272339), (0.0, 1.3381692171096802), (0.0, 1.3385021686553955), (0.0, 1.338621735572815), (0.0, 1.338974952697754), (0.0, 1.3393954038619995), (0.0, 1.339437484741211), (0.0, 1.3395079374313354), (0.0, 1.340017557144165), (0.0, 1.3400542736053467), (0.0, 1.3403981924057007), (0.0, 1.3407769203186035), (0.0, 1.3410117626190186), (0.0, 1.3412517309188843), (0.0, 1.3454116582870483), (0.0, 1.4468191862106323), (0.0, 1.4482178688049316), (0.0, 1.4482241868972778), (0.0, 1.4497233629226685), (0.0, 1.4508432149887085), (0.0, 1.4510996341705322), (0.0, 1.45173180103302), (0.0, 1.4522452354431152), (0.0, 1.452805519104004), (0.0, 1.452847957611084), (0.0, 1.452909231185913), (0.0, 1.4530259370803833), (0.0, 1.453682541847229), (0.0, 1.453913927078247), (0.0, 1.4540389776229858), (0.0, 1.4541608095169067), (0.0, 1.4547277688980103), (0.0, 1.4547473192214966), (0.0, 1.4548547267913818), (0.0, 1.4548794031143188), (0.0, 1.455487608909607), (0.0, 1.455725908279419), (0.0, 1.4559195041656494), (0.0, 1.4560242891311646), (0.0, 1.4562774896621704), (0.0, 1.4563931226730347), (0.0, 1.45645272731781), (0.0, 1.4565445184707642), (0.0, 1.4565749168395996), (0.0, 1.4567302465438843), (0.0, 1.4568177461624146), (0.0, 1.4568495750427246), (0.0, 1.4568759202957153), (0.0, 1.4568878412246704), (0.0, 1.4569011926651), (0.0, 1.4571000337600708), (0.0, 1.4571716785430908), (0.0, 1.457235336303711), (0.0, 1.4572404623031616), (0.0, 1.4573421478271484), (0.0, 1.4573825597763062), (0.0, 1.4574493169784546), (0.0, 1.457480549812317), (0.0, 1.457520604133606), (0.0, 1.4575315713882446), (0.0, 1.45769202709198), (0.0, 1.457838773727417), (0.0, 1.457891583442688), (0.0, 1.4579317569732666), (0.0, 1.458024024963379), (0.0, 1.4580789804458618), (0.0, 1.4580882787704468), (0.0, 1.458136796951294), (0.0, 1.4582282304763794), (0.0, 1.458644986152649), (0.0, 1.4591350555419922), (0.0, 1.4591518640518188), (0.0, 1.4593830108642578), (0.0, 1.4595190286636353), (0.0, 1.4595422744750977), (0.0, 1.4596102237701416), (0.0, 1.459624171257019), (0.0, 1.459697961807251), (0.0, 1.4597105979919434), (0.0, 1.459916114807129), (0.0, 1.4601575136184692), (0.0, 1.4601943492889404), (0.0, 1.4602954387664795), (0.0, 1.4604755640029907), (0.0, 1.4604990482330322), (0.0, 1.460522174835205), (0.0, 1.460563063621521), (0.0, 1.460569977760315), (0.0, 1.4606882333755493), (0.0, 1.460813283920288), (0.0, 1.4609549045562744), (0.0, 1.4611926078796387), (0.0, 1.461269736289978), (0.0, 1.4613064527511597), (0.0, 1.4613450765609741), (0.0, 1.4613865613937378), (0.0, 1.4614031314849854), (0.0, 1.4614593982696533), (0.0, 1.461472511291504), (0.0, 1.4617693424224854), (0.0, 1.4618033170700073), (0.0, 1.4618117809295654), (0.0, 1.461971402168274), (0.0, 1.4619882106781006), (0.0, 1.4620698690414429), (0.0, 1.4620723724365234), (0.0, 1.462130069732666), (0.0, 1.462350845336914), (0.0, 1.4624109268188477), (0.0, 1.4626511335372925), (0.0, 1.4626599550247192), (0.0, 1.463034987449646), (0.0, 1.4631216526031494), (0.0, 1.4631741046905518), (0.0, 1.4633100032806396), (0.0, 1.4633510112762451), (0.0, 1.4633833169937134), (0.0, 1.4634103775024414), (0.0, 1.4637484550476074), (0.0, 1.463840126991272), (0.0, 1.4640498161315918), (0.0, 1.464333415031433), (0.0, 1.4644434452056885), (0.0, 1.465064525604248), (0.0, 1.4650824069976807), (0.0, 1.4655301570892334), (0.0, 1.4660788774490356), (0.0, 1.4662749767303467), (0.0, 1.4665172100067139), (0.0, 1.467300295829773), (0.0, 1.467802882194519), (0.0, 1.4680218696594238), (0.0, 1.4684430360794067), (0.0, 1.4687747955322266), (0.0, 1.4693372249603271), (0.0, 1.4709622859954834), (0.0, 1.470982551574707), (0.0, 1.471617579460144), (0.0, 1.4757417440414429), (0.0, 1.5063289403915405), (0.0, 1.5081663131713867), (0.0, 1.5132540464401245), (0.0, 1.5142757892608643), (0.0, 1.514802098274231), (0.0, 1.5154969692230225), (0.0, 1.515838861465454), (0.0, 1.5159271955490112), (0.0, 1.5171576738357544), (0.0, 1.5176303386688232), (0.0, 1.517698049545288), (0.0, 1.5179365873336792), (0.0, 1.5179799795150757), (0.0, 1.5179988145828247), (0.0, 1.5180131196975708), (0.0, 1.5182137489318848), (0.0, 1.5182559490203857), (0.0, 1.51854407787323), (0.0, 1.5185775756835938), (0.0, 1.5186023712158203), (0.0, 1.5190119743347168), (0.0, 1.5191214084625244), (0.0, 1.5191539525985718), (0.0, 1.5192153453826904), (0.0, 1.519652009010315), (0.0, 1.520219087600708), (0.0, 1.5202720165252686), (0.0, 1.5204449892044067), (0.0, 1.5207464694976807), (0.0, 1.5209003686904907), (0.0, 1.520993709564209), (0.0, 1.5211181640625), (0.0, 1.5211297273635864), (0.0, 1.5211385488510132), (0.0, 1.5211842060089111), (0.0, 1.5212304592132568), (0.0, 1.5212970972061157), (0.0, 1.5213139057159424), (0.0, 1.5214184522628784), (0.0, 1.5214310884475708), (0.0, 1.5215861797332764), (0.0, 1.5216394662857056), (0.0, 1.5216588973999023), (0.0, 1.5217180252075195), (0.0, 1.521773099899292), (0.0, 1.5217912197113037), (0.0, 1.5217944383621216), (0.0, 1.5218349695205688), (0.0, 1.5218875408172607), (0.0, 1.5219473838806152), (0.0, 1.5220997333526611), (0.0, 1.522159218788147), (0.0, 1.5223087072372437), (0.0, 1.5223517417907715), (0.0, 1.5223547220230103), (0.0, 1.5224318504333496), (0.0, 1.5224871635437012), (0.0, 1.5225117206573486), (0.0, 1.5225509405136108), (0.0, 1.5225584506988525), (0.0, 1.522649884223938), (0.0, 1.522713541984558), (0.0, 1.522782802581787), (0.0, 1.5227900743484497), (0.0, 1.5228151082992554), (0.0, 1.5228959321975708), (0.0, 1.5229620933532715), (0.0, 1.52316415309906), (0.0, 1.523300290107727), (0.0, 1.5233421325683594), (0.0, 1.5233492851257324), (0.0, 1.5234174728393555), (0.0, 1.5235207080841064), (0.0, 1.5236222743988037), (0.0, 1.5238063335418701), (0.0, 1.5239382982254028), (0.0, 1.5241096019744873), (0.0, 1.5241602659225464), (0.0, 1.5242093801498413), (0.0, 1.5242400169372559), (0.0, 1.5243054628372192), (0.0, 1.524325966835022), (0.0, 1.5245792865753174), (0.0, 1.524610996246338), (0.0, 1.5246202945709229), (0.0, 1.5246723890304565), (0.0, 1.5246974229812622), (0.0, 1.524842381477356), (0.0, 1.5250132083892822), (0.0, 1.5251909494400024), (0.0, 1.5254113674163818), (0.0, 1.5254640579223633), (0.0, 1.5254673957824707), (0.0, 1.525624394416809), (0.0, 1.525815725326538), (0.0, 1.5259121656417847), (0.0, 1.5259166955947876), (0.0, 1.5259517431259155), (0.0, 1.5259791612625122), (0.0, 1.5260306596755981), (0.0, 1.5260945558547974), (0.0, 1.5260955095291138), (0.0, 1.5262597799301147), (0.0, 1.5263094902038574), (0.0, 1.5264168977737427), (0.0, 1.5265588760375977), (0.0, 1.526652216911316), (0.0, 1.5266598463058472), (0.0, 1.526679515838623), (0.0, 1.5273324251174927), (0.0, 1.5273542404174805), (0.0, 1.5273971557617188), (0.0, 1.5276769399642944), (0.0, 1.527984380722046), (0.0, 1.5281026363372803), (0.0, 1.5281308889389038), (0.0, 1.528137445449829), (0.0, 1.5283782482147217), (0.0, 1.528504490852356), (0.0, 1.5289303064346313), (0.0, 1.5313481092453003), (0.0, 1.5334701538085938), (0.0, 1.5334763526916504), (0.0, 1.5353927612304688), (0.0, 1.5859251022338867), (0.0, 1.5924246311187744)], [(7.806098937988281, 8.221948623657227), (7.092135906219482, 7.666169166564941), (7.017879486083984, 7.056736469268799), (6.851726531982422, 7.076071262359619), (6.806424617767334, 7.236670017242432), (6.7069621086120605, 7.335986614227295), (6.461938381195068, 7.060896873474121), (6.4134416580200195, 6.9682297706604), (6.217611789703369, 7.150987148284912), (6.163730144500732, 7.954185485839844), (6.10420036315918, 7.244043350219727), (6.062362194061279, 6.4210686683654785), (5.9277520179748535, 5.959580421447754), (5.713844299316406, 5.8107590675354), (5.545194625854492, 8.612381935119629), (5.279995918273926, 8.23377799987793), (5.171684741973877, 5.203309059143066), (5.136589527130127, 6.654715538024902), (5.135307788848877, 5.518312454223633), (4.9409894943237305, 7.071132183074951), (4.797234058380127, 8.330668449401855), (4.63174295425415, 4.78428840637207), (4.441023826599121, 7.813340187072754), (4.159365653991699, 8.013636589050293), (3.990715265274048, 4.231836318969727), (3.9874372482299805, 4.130064964294434), (3.981153726577759, 4.0207648277282715), (3.949190378189087, 4.726527214050293), (3.891355514526367, 4.190087795257568), (3.7351279258728027, 8.721980094909668), (1.908758521080017, 1.9315381050109863)], [(9.689833641052246, 10.284566879272461), (9.437744140625, 9.519922256469727), (9.184410095214844, 10.488039016723633), (8.265629768371582, 8.310351371765137), (8.262927055358887, 8.364470481872559), (7.680274963378906, 7.81480598449707)]]
[array([[0.        , 1.27715468],
       [0.        , 1.31979716],
       [0.        , 1.32084274],
       [0.        , 1.3227365 ],
       [0.        , 1.32519817],
       [0.        , 1.32590032],
       [0.        , 1.32643962],
       [0.        , 1.32715631],
       [0.        , 1.32763469],
       [0.        , 1.32802558],
       [0.        , 1.32836342],
       [0.        , 1.32870126],
       [0.        , 1.32889009],
       [0.        , 1.32907188],
       [0.        , 1.32921219],
       [0.        , 1.32923734],
       [0.        , 1.32937562],
       [0.        , 1.32941401],
       [0.        , 1.32949364],
       [0.        , 1.32974136],
       [0.        , 1.32977509],
       [0.        , 1.32979548],
       [0.        , 1.32990599],
       [0.        , 1.3300097 ],
       [0.        , 1.33018136],
       [0.        , 1.3304081 ],
       [0.        , 1.33054388],
       [0.        , 1.33058536],
       [0.        , 1.33059454],
       [0.        , 1.33085227],
       [0.        , 1.33092761],
       [0.        , 1.3310703 ],
       [0.        , 1.33108544],
       [0.        , 1.33110046],
       [0.        , 1.33111668],
       [0.        , 1.33116245],
       [0.        , 1.33120453],
       [0.        , 1.3312465 ],
       [0.        , 1.33126259],
       [0.        , 1.33126998],
       [0.        , 1.33131254],
       [0.        , 1.3316195 ],
       [0.        , 1.3318882 ],
       [0.        , 1.33189583],
       [0.        , 1.33192658],
       [0.        , 1.33194935],
       [0.        , 1.3321259 ],
       [0.        , 1.33219111],
       [0.        , 1.33227491],
       [0.        , 1.33241868],
       [0.        , 1.33249831],
       [0.        , 1.3328712 ],
       [0.        , 1.3329376 ],
       [0.        , 1.33307064],
       [0.        , 1.33313048],
       [0.        , 1.3331356 ],
       [0.        , 1.33314466],
       [0.        , 1.33316994],
       [0.        , 1.33337057],
       [0.        , 1.33357084],
       [0.        , 1.33362389],
       [0.        , 1.33373415],
       [0.        , 1.33380699],
       [0.        , 1.33415186],
       [0.        , 1.33426023],
       [0.        , 1.33436131],
       [0.        , 1.33436441],
       [0.        , 1.33457029],
       [0.        , 1.33470464],
       [0.        , 1.33507574],
       [0.        , 1.33509219],
       [0.        , 1.33511043],
       [0.        , 1.33520281],
       [0.        , 1.33527279],
       [0.        , 1.33528137],
       [0.        , 1.33528924],
       [0.        , 1.33556187],
       [0.        , 1.33564365],
       [0.        , 1.33567417],
       [0.        , 1.33579969],
       [0.        , 1.33580709],
       [0.        , 1.33583379],
       [0.        , 1.33588326],
       [0.        , 1.33589673],
       [0.        , 1.33597267],
       [0.        , 1.33618188],
       [0.        , 1.33619058],
       [0.        , 1.33631659],
       [0.        , 1.33632374],
       [0.        , 1.33647323],
       [0.        , 1.33647645],
       [0.        , 1.33668542],
       [0.        , 1.33670688],
       [0.        , 1.3367579 ],
       [0.        , 1.33687174],
       [0.        , 1.33699715],
       [0.        , 1.33710909],
       [0.        , 1.33729362],
       [0.        , 1.33739686],
       [0.        , 1.33741415],
       [0.        , 1.33757722],
       [0.        , 1.33757782],
       [0.        , 1.33764017],
       [0.        , 1.33767974],
       [0.        , 1.3377012 ],
       [0.        , 1.33778989],
       [0.        , 1.33791721],
       [0.        , 1.3379513 ],
       [0.        , 1.33797824],
       [0.        , 1.33804882],
       [0.        , 1.33816922],
       [0.        , 1.33850217],
       [0.        , 1.33862174],
       [0.        , 1.33897495],
       [0.        , 1.3393954 ],
       [0.        , 1.33943748],
       [0.        , 1.33950794],
       [0.        , 1.34001756],
       [0.        , 1.34005427],
       [0.        , 1.34039819],
       [0.        , 1.34077692],
       [0.        , 1.34101176],
       [0.        , 1.34125173],
       [0.        , 1.34541166],
       [0.        , 1.44681919],
       [0.        , 1.44821787],
       [0.        , 1.44822419],
       [0.        , 1.44972336],
       [0.        , 1.45084321],
       [0.        , 1.45109963],
       [0.        , 1.4517318 ],
       [0.        , 1.45224524],
       [0.        , 1.45280552],
       [0.        , 1.45284796],
       [0.        , 1.45290923],
       [0.        , 1.45302594],
       [0.        , 1.45368254],
       [0.        , 1.45391393],
       [0.        , 1.45403898],
       [0.        , 1.45416081],
       [0.        , 1.45472777],
       [0.        , 1.45474732],
       [0.        , 1.45485473],
       [0.        , 1.4548794 ],
       [0.        , 1.45548761],
       [0.        , 1.45572591],
       [0.        , 1.4559195 ],
       [0.        , 1.45602429],
       [0.        , 1.45627749],
       [0.        , 1.45639312],
       [0.        , 1.45645273],
       [0.        , 1.45654452],
       [0.        , 1.45657492],
       [0.        , 1.45673025],
       [0.        , 1.45681775],
       [0.        , 1.45684958],
       [0.        , 1.45687592],
       [0.        , 1.45688784],
       [0.        , 1.45690119],
       [0.        , 1.45710003],
       [0.        , 1.45717168],
       [0.        , 1.45723534],
       [0.        , 1.45724046],
       [0.        , 1.45734215],
       [0.        , 1.45738256],
       [0.        , 1.45744932],
       [0.        , 1.45748055],
       [0.        , 1.4575206 ],
       [0.        , 1.45753157],
       [0.        , 1.45769203],
       [0.        , 1.45783877],
       [0.        , 1.45789158],
       [0.        , 1.45793176],
       [0.        , 1.45802402],
       [0.        , 1.45807898],
       [0.        , 1.45808828],
       [0.        , 1.4581368 ],
       [0.        , 1.45822823],
       [0.        , 1.45864499],
       [0.        , 1.45913506],
       [0.        , 1.45915186],
       [0.        , 1.45938301],
       [0.        , 1.45951903],
       [0.        , 1.45954227],
       [0.        , 1.45961022],
       [0.        , 1.45962417],
       [0.        , 1.45969796],
       [0.        , 1.4597106 ],
       [0.        , 1.45991611],
       [0.        , 1.46015751],
       [0.        , 1.46019435],
       [0.        , 1.46029544],
       [0.        , 1.46047556],
       [0.        , 1.46049905],
       [0.        , 1.46052217],
       [0.        , 1.46056306],
       [0.        , 1.46056998],
       [0.        , 1.46068823],
       [0.        , 1.46081328],
       [0.        , 1.4609549 ],
       [0.        , 1.46119261],
       [0.        , 1.46126974],
       [0.        , 1.46130645],
       [0.        , 1.46134508],
       [0.        , 1.46138656],
       [0.        , 1.46140313],
       [0.        , 1.4614594 ],
       [0.        , 1.46147251],
       [0.        , 1.46176934],
       [0.        , 1.46180332],
       [0.        , 1.46181178],
       [0.        , 1.4619714 ],
       [0.        , 1.46198821],
       [0.        , 1.46206987],
       [0.        , 1.46207237],
       [0.        , 1.46213007],
       [0.        , 1.46235085],
       [0.        , 1.46241093],
       [0.        , 1.46265113],
       [0.        , 1.46265996],
       [0.        , 1.46303499],
       [0.        , 1.46312165],
       [0.        , 1.4631741 ],
       [0.        , 1.46331   ],
       [0.        , 1.46335101],
       [0.        , 1.46338332],
       [0.        , 1.46341038],
       [0.        , 1.46374846],
       [0.        , 1.46384013],
       [0.        , 1.46404982],
       [0.        , 1.46433342],
       [0.        , 1.46444345],
       [0.        , 1.46506453],
       [0.        , 1.46508241],
       [0.        , 1.46553016],
       [0.        , 1.46607888],
       [0.        , 1.46627498],
       [0.        , 1.46651721],
       [0.        , 1.4673003 ],
       [0.        , 1.46780288],
       [0.        , 1.46802187],
       [0.        , 1.46844304],
       [0.        , 1.4687748 ],
       [0.        , 1.46933722],
       [0.        , 1.47096229],
       [0.        , 1.47098255],
       [0.        , 1.47161758],
       [0.        , 1.47574174],
       [0.        , 1.50632894],
       [0.        , 1.50816631],
       [0.        , 1.51325405],
       [0.        , 1.51427579],
       [0.        , 1.5148021 ],
       [0.        , 1.51549697],
       [0.        , 1.51583886],
       [0.        , 1.5159272 ],
       [0.        , 1.51715767],
       [0.        , 1.51763034],
       [0.        , 1.51769805],
       [0.        , 1.51793659],
       [0.        , 1.51797998],
       [0.        , 1.51799881],
       [0.        , 1.51801312],
       [0.        , 1.51821375],
       [0.        , 1.51825595],
       [0.        , 1.51854408],
       [0.        , 1.51857758],
       [0.        , 1.51860237],
       [0.        , 1.51901197],
       [0.        , 1.51912141],
       [0.        , 1.51915395],
       [0.        , 1.51921535],
       [0.        , 1.51965201],
       [0.        , 1.52021909],
       [0.        , 1.52027202],
       [0.        , 1.52044499],
       [0.        , 1.52074647],
       [0.        , 1.52090037],
       [0.        , 1.52099371],
       [0.        , 1.52111816],
       [0.        , 1.52112973],
       [0.        , 1.52113855],
       [0.        , 1.52118421],
       [0.        , 1.52123046],
       [0.        , 1.5212971 ],
       [0.        , 1.52131391],
       [0.        , 1.52141845],
       [0.        , 1.52143109],
       [0.        , 1.52158618],
       [0.        , 1.52163947],
       [0.        , 1.5216589 ],
       [0.        , 1.52171803],
       [0.        , 1.5217731 ],
       [0.        , 1.52179122],
       [0.        , 1.52179444],
       [0.        , 1.52183497],
       [0.        , 1.52188754],
       [0.        , 1.52194738],
       [0.        , 1.52209973],
       [0.        , 1.52215922],
       [0.        , 1.52230871],
       [0.        , 1.52235174],
       [0.        , 1.52235472],
       [0.        , 1.52243185],
       [0.        , 1.52248716],
       [0.        , 1.52251172],
       [0.        , 1.52255094],
       [0.        , 1.52255845],
       [0.        , 1.52264988],
       [0.        , 1.52271354],
       [0.        , 1.5227828 ],
       [0.        , 1.52279007],
       [0.        , 1.52281511],
       [0.        , 1.52289593],
       [0.        , 1.52296209],
       [0.        , 1.52316415],
       [0.        , 1.52330029],
       [0.        , 1.52334213],
       [0.        , 1.52334929],
       [0.        , 1.52341747],
       [0.        , 1.52352071],
       [0.        , 1.52362227],
       [0.        , 1.52380633],
       [0.        , 1.5239383 ],
       [0.        , 1.5241096 ],
       [0.        , 1.52416027],
       [0.        , 1.52420938],
       [0.        , 1.52424002],
       [0.        , 1.52430546],
       [0.        , 1.52432597],
       [0.        , 1.52457929],
       [0.        , 1.524611  ],
       [0.        , 1.52462029],
       [0.        , 1.52467239],
       [0.        , 1.52469742],
       [0.        , 1.52484238],
       [0.        , 1.52501321],
       [0.        , 1.52519095],
       [0.        , 1.52541137],
       [0.        , 1.52546406],
       [0.        , 1.5254674 ],
       [0.        , 1.52562439],
       [0.        , 1.52581573],
       [0.        , 1.52591217],
       [0.        , 1.5259167 ],
       [0.        , 1.52595174],
       [0.        , 1.52597916],
       [0.        , 1.52603066],
       [0.        , 1.52609456],
       [0.        , 1.52609551],
       [0.        , 1.52625978],
       [0.        , 1.52630949],
       [0.        , 1.5264169 ],
       [0.        , 1.52655888],
       [0.        , 1.52665222],
       [0.        , 1.52665985],
       [0.        , 1.52667952],
       [0.        , 1.52733243],
       [0.        , 1.52735424],
       [0.        , 1.52739716],
       [0.        , 1.52767694],
       [0.        , 1.52798438],
       [0.        , 1.52810264],
       [0.        , 1.52813089],
       [0.        , 1.52813745],
       [0.        , 1.52837825],
       [0.        , 1.52850449],
       [0.        , 1.52893031],
       [0.        , 1.53134811],
       [0.        , 1.53347015],
       [0.        , 1.53347635],
       [0.        , 1.53539276],
       [0.        , 1.5859251 ],
       [0.        , 1.59242463]]), array([[7.80609894, 8.22194862],
       [7.09213591, 7.66616917],
       [7.01787949, 7.05673647],
       [6.85172653, 7.07607126],
       [6.80642462, 7.23667002],
       [6.70696211, 7.33598661],
       [6.46193838, 7.06089687],
       [6.41344166, 6.96822977],
       [6.21761179, 7.15098715],
       [6.16373014, 7.95418549],
       [6.10420036, 7.24404335],
       [6.06236219, 6.42106867],
       [5.92775202, 5.95958042],
       [5.7138443 , 5.81075907],
       [5.54519463, 8.61238194],
       [5.27999592, 8.233778  ],
       [5.17168474, 5.20330906],
       [5.13658953, 6.65471554],
       [5.13530779, 5.51831245],
       [4.94098949, 7.07113218],
       [4.79723406, 8.33066845],
       [4.63174295, 4.78428841],
       [4.44102383, 7.81334019],
       [4.15936565, 8.01363659],
       [3.99071527, 4.23183632],
       [3.98743725, 4.13006496],
       [3.98115373, 4.02076483],
       [3.94919038, 4.72652721],
       [3.89135551, 4.1900878 ],
       [3.73512793, 8.72198009],
       [1.90875852, 1.93153811]]), array([[ 9.68983364, 10.28456688],
       [ 9.43774414,  9.51992226],
       [ 9.1844101 , 10.48803902],
       [ 8.26562977,  8.31035137],
       [ 8.26292706,  8.36447048],
       [ 7.68027496,  7.81480598]])]2024-03-06 18:01:08.862576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LG4 ph vector generated, counter: 211
2024-03-06 18:01:14.945598: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:01:14.988752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:01:16.128837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.2966449), (0., 1.2986784), (0., 1.3242   ), (0., 1.3248132),
       (0., 1.3254162), (0., 1.3256899), (0., 1.3257767), (0., 1.3265529),
       (0., 1.3268654), (0., 1.3269058), (0., 1.3275021), (0., 1.3276497),
       (0., 1.3286551), (0., 1.3287021), (0., 1.3289112), (0., 1.3289505),
       (0., 1.329157 ), (0., 1.3291621), (0., 1.3302699), (0., 1.3304775),
       (0., 1.3310153), (0., 1.331184 ), (0., 1.3313203), (0., 1.3313587),
       (0., 1.3314793), (0., 1.3315945), (0., 1.3316828), (0., 1.3316909),
       (0., 1.3318585), (0., 1.3319031), (0., 1.3320552), (0., 1.3322284),
       (0., 1.3322606), (0., 1.3323383), (0., 1.3324033), (0., 1.3325082),
       (0., 1.3326143), (0., 1.3326505), (0., 1.3327153), (0., 1.3330302),
       (0., 1.333192 ), (0., 1.3333105), (0., 1.3333693), (0., 1.3334693),
       (0., 1.333503 ), (0., 1.3335168), (0., 1.3335708), (0., 1.3336178),
       (0., 1.3337153), (0., 1.333735 ), (0., 1.3338071), (0., 1.3338614),
       (0., 1.3339252), (0., 1.3339511), (0., 1.3341072), (0., 1.3341833),
       (0., 1.334314 ), (0., 1.3343441), (0., 1.334429 ), (0., 1.3344626),
       (0., 1.3344891), (0., 1.3346347), (0., 1.3346574), (0., 1.3346614),
       (0., 1.3346955), (0., 1.3348037), (0., 1.3348752), (0., 1.3349483),
       (0., 1.334979 ), (0., 1.3349905), (0., 1.3350034), (0., 1.3350133),
       (0., 1.3350368), (0., 1.3351344), (0., 1.3353524), (0., 1.3353705),
       (0., 1.3354075), (0., 1.3354692), (0., 1.3354851), (0., 1.3354894),
       (0., 1.3356404), (0., 1.3358284), (0., 1.3362223), (0., 1.336381 ),
       (0., 1.3364224), (0., 1.336448 ), (0., 1.336531 ), (0., 1.3366126),
       (0., 1.3366646), (0., 1.3367642), (0., 1.3367932), (0., 1.336854 ),
       (0., 1.3369052), (0., 1.3369254), (0., 1.336929 ), (0., 1.3369498),
       (0., 1.3370472), (0., 1.3371364), (0., 1.337212 ), (0., 1.3373144),
       (0., 1.3373784), (0., 1.3373924), (0., 1.3376749), (0., 1.337678 ),
       (0., 1.3377097), (0., 1.3377557), (0., 1.3378338), (0., 1.3378768),
       (0., 1.3379858), (0., 1.3380154), (0., 1.3380325), (0., 1.3382587),
       (0., 1.3383048), (0., 1.339164 ), (0., 1.3393996), (0., 1.3395542),
       (0., 1.3397226), (0., 1.340438 ), (0., 1.3405988), (0., 1.3409585),
       (0., 1.3409851), (0., 1.3414768), (0., 1.342045 ), (0., 1.3477247),
       (0., 1.4430008), (0., 1.4478171), (0., 1.4486914), (0., 1.4517238),
       (0., 1.4521937), (0., 1.4529207), (0., 1.453129 ), (0., 1.4531392),
       (0., 1.4537002), (0., 1.4538469), (0., 1.4540044), (0., 1.4544487),
       (0., 1.4545203), (0., 1.4549903), (0., 1.4550834), (0., 1.455543 ),
       (0., 1.4556798), (0., 1.4557668), (0., 1.45585  ), (0., 1.4559656),
       (0., 1.4560044), (0., 1.4560412), (0., 1.4560732), (0., 1.4561764),
       (0., 1.4564734), (0., 1.4565912), (0., 1.4566001), (0., 1.4566398),
       (0., 1.4566586), (0., 1.4566956), (0., 1.4567525), (0., 1.4567668),
       (0., 1.4569728), (0., 1.4569895), (0., 1.4570856), (0., 1.4570867),
       (0., 1.4570961), (0., 1.4571012), (0., 1.4571693), (0., 1.4572967),
       (0., 1.4573536), (0., 1.4574566), (0., 1.4574593), (0., 1.4575561),
       (0., 1.4576099), (0., 1.4576265), (0., 1.4577739), (0., 1.457824 ),
       (0., 1.4578445), (0., 1.4580413), (0., 1.4580636), (0., 1.4580773),
       (0., 1.4581313), (0., 1.4582692), (0., 1.4583601), (0., 1.4585421),
       (0., 1.4585993), (0., 1.4586681), (0., 1.4587796), (0., 1.4589359),
       (0., 1.4589537), (0., 1.4590116), (0., 1.459145 ), (0., 1.4591491),
       (0., 1.4592371), (0., 1.4593749), (0., 1.4593773), (0., 1.4593899),
       (0., 1.4593927), (0., 1.4594045), (0., 1.4594287), (0., 1.4595351),
       (0., 1.4599959), (0., 1.4603956), (0., 1.4604814), (0., 1.4606165),
       (0., 1.4606694), (0., 1.4606735), (0., 1.460718 ), (0., 1.460898 ),
       (0., 1.4609661), (0., 1.4611806), (0., 1.4613626), (0., 1.4613959),
       (0., 1.4614   ), (0., 1.4614228), (0., 1.4616067), (0., 1.4616548),
       (0., 1.4617051), (0., 1.461822 ), (0., 1.4618798), (0., 1.4620116),
       (0., 1.4621751), (0., 1.4621898), (0., 1.4622697), (0., 1.4623334),
       (0., 1.4623504), (0., 1.4623787), (0., 1.4624346), (0., 1.4625316),
       (0., 1.4625417), (0., 1.4625546), (0., 1.4628946), (0., 1.4632415),
       (0., 1.4635339), (0., 1.4639826), (0., 1.4643627), (0., 1.4647144),
       (0., 1.4650118), (0., 1.4650393), (0., 1.4657316), (0., 1.4662272),
       (0., 1.4667462), (0., 1.4669117), (0., 1.467387 ), (0., 1.4675816),
       (0., 1.4678632), (0., 1.4679217), (0., 1.4689285), (0., 1.4695542),
       (0., 1.4695579), (0., 1.4697421), (0., 1.4705833), (0., 1.4710051),
       (0., 1.4722807), (0., 1.5073955), (0., 1.5138372), (0., 1.5147046),
       (0., 1.5151993), (0., 1.5160215), (0., 1.5167847), (0., 1.5171994),
       (0., 1.5173   ), (0., 1.5173335), (0., 1.517487 ), (0., 1.5175254),
       (0., 1.5176998), (0., 1.5182347), (0., 1.5183703), (0., 1.5184258),
       (0., 1.5189329), (0., 1.5191869), (0., 1.5193032), (0., 1.5194134),
       (0., 1.5197316), (0., 1.5197474), (0., 1.5198163), (0., 1.5198848),
       (0., 1.5199254), (0., 1.5202564), (0., 1.5202951), (0., 1.5203189),
       (0., 1.5203292), (0., 1.5203854), (0., 1.520406 ), (0., 1.5204175),
       (0., 1.5205132), (0., 1.5205857), (0., 1.5206566), (0., 1.5207227),
       (0., 1.5210749), (0., 1.5210761), (0., 1.5210767), (0., 1.5211399),
       (0., 1.5214214), (0., 1.5214413), (0., 1.521503 ), (0., 1.521548 ),
       (0., 1.5215821), (0., 1.521721 ), (0., 1.5217792), (0., 1.5218945),
       (0., 1.5220212), (0., 1.5221019), (0., 1.5221541), (0., 1.5221581),
       (0., 1.5222307), (0., 1.5222883), (0., 1.522599 ), (0., 1.5226021),
       (0., 1.5226492), (0., 1.5226603), (0., 1.5226758), (0., 1.5228691),
       (0., 1.5229492), (0., 1.5230106), (0., 1.5231167), (0., 1.5231265),
       (0., 1.523255 ), (0., 1.523521 ), (0., 1.5235732), (0., 1.5236301),
       (0., 1.5236653), (0., 1.5237529), (0., 1.5237534), (0., 1.5238258),
       (0., 1.5239002), (0., 1.5239122), (0., 1.5239769), (0., 1.5239785),
       (0., 1.524031 ), (0., 1.5240561), (0., 1.5240659), (0., 1.5241303),
       (0., 1.5241841), (0., 1.5242287), (0., 1.5245547), (0., 1.5247258),
       (0., 1.524793 ), (0., 1.5248708), (0., 1.5249193), (0., 1.5249213),
       (0., 1.5249376), (0., 1.5250087), (0., 1.5250987), (0., 1.5251504),
       (0., 1.5252978), (0., 1.5257692), (0., 1.525801 ), (0., 1.5258412),
       (0., 1.5258474), (0., 1.5258591), (0., 1.5261092), (0., 1.5262603),
       (0., 1.5265099), (0., 1.5266073), (0., 1.526619 ), (0., 1.526634 ),
       (0., 1.5266913), (0., 1.5269022), (0., 1.5269558), (0., 1.5270393),
       (0., 1.5272403), (0., 1.5272422), (0., 1.527317 ), (0., 1.5276352),
       (0., 1.5277727), (0., 1.5277954), (0., 1.5278201), (0., 1.5280646),
       (0., 1.5285057), (0., 1.5285311), (0., 1.5291967), (0., 1.5292988),
       (0., 1.5293378), (0., 1.5297278), (0., 1.5298723), (0., 1.5317886),
       (0., 1.5338304), (0., 1.5800028)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.523452 , 8.5552025), (8.00622  , 8.41697  ),
       (7.0866795, 7.63728  ), (6.8417783, 7.0737567),
       (6.828404 , 7.2055254), (6.6669803, 7.256851 ),
       (6.563147 , 6.9859796), (6.44313  , 7.0826607),
       (6.2963147, 7.824992 ), (6.075441 , 7.234186 ),
       (6.0725794, 7.050199 ), (5.9391313, 6.441025 ),
       (5.899519 , 6.055576 ), (5.673395 , 5.737764 ),
       (5.542389 , 8.6920595), (5.4857483, 5.559859 ),
       (5.425505 , 8.280467 ), (5.215893 , 8.4967375),
       (5.064194 , 6.7120914), (4.8814535, 7.06663  ),
       (4.4117136, 4.832673 ), (4.4073563, 7.889081 ),
       (4.397772 , 4.4625   ), (4.368496 , 7.880726 ),
       (4.0395727, 4.05634  ), (4.0382385, 4.26656  ),
       (3.9871259, 4.7942414), (3.8735356, 4.2981415),
       (3.676034 , 8.663405 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.593707 ,  9.594338 ), (9.490221 , 10.139176 ),
       (9.140612 , 10.591128 ), (8.337547 ,  8.350877 ),
       (7.6931987,  7.7896233)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.296644926071167), (0.0, 1.2986783981323242), (0.0, 1.3242000341415405), (0.0, 1.3248132467269897), (0.0, 1.3254162073135376), (0.0, 1.3256899118423462), (0.0, 1.3257766962051392), (0.0, 1.3265528678894043), (0.0, 1.3268654346466064), (0.0, 1.3269058465957642), (0.0, 1.3275021314620972), (0.0, 1.327649712562561), (0.0, 1.3286551237106323), (0.0, 1.3287020921707153), (0.0, 1.3289111852645874), (0.0, 1.3289505243301392), (0.0, 1.3291569948196411), (0.0, 1.3291621208190918), (0.0, 1.3302699327468872), (0.0, 1.3304774761199951), (0.0, 1.3310153484344482), (0.0, 1.3311840295791626), (0.0, 1.3313202857971191), (0.0, 1.3313586711883545), (0.0, 1.3314793109893799), (0.0, 1.331594467163086), (0.0, 1.331682801246643), (0.0, 1.3316909074783325), (0.0, 1.331858515739441), (0.0, 1.331903100013733), (0.0, 1.3320552110671997), (0.0, 1.332228422164917), (0.0, 1.3322606086730957), (0.0, 1.3323383331298828), (0.0, 1.332403302192688), (0.0, 1.3325082063674927), (0.0, 1.3326143026351929), (0.0, 1.3326505422592163), (0.0, 1.3327152729034424), (0.0, 1.3330302238464355), (0.0, 1.333191990852356), (0.0, 1.3333104848861694), (0.0, 1.333369255065918), (0.0, 1.333469271659851), (0.0, 1.333503007888794), (0.0, 1.3335168361663818), (0.0, 1.3335708379745483), (0.0, 1.3336178064346313), (0.0, 1.3337153196334839), (0.0, 1.3337349891662598), (0.0, 1.333807110786438), (0.0, 1.3338613510131836), (0.0, 1.3339252471923828), (0.0, 1.3339511156082153), (0.0, 1.3341071605682373), (0.0, 1.3341833353042603), (0.0, 1.334313988685608), (0.0, 1.3343441486358643), (0.0, 1.3344290256500244), (0.0, 1.3344626426696777), (0.0, 1.334489107131958), (0.0, 1.3346346616744995), (0.0, 1.3346574306488037), (0.0, 1.3346613645553589), (0.0, 1.3346954584121704), (0.0, 1.3348037004470825), (0.0, 1.334875226020813), (0.0, 1.3349483013153076), (0.0, 1.3349790573120117), (0.0, 1.3349905014038086), (0.0, 1.33500337600708), (0.0, 1.3350132703781128), (0.0, 1.3350367546081543), (0.0, 1.3351343870162964), (0.0, 1.3353524208068848), (0.0, 1.3353705406188965), (0.0, 1.3354074954986572), (0.0, 1.3354692459106445), (0.0, 1.3354851007461548), (0.0, 1.3354893922805786), (0.0, 1.3356404304504395), (0.0, 1.335828423500061), (0.0, 1.3362222909927368), (0.0, 1.336380958557129), (0.0, 1.3364224433898926), (0.0, 1.3364479541778564), (0.0, 1.3365310430526733), (0.0, 1.336612582206726), (0.0, 1.3366645574569702), (0.0, 1.3367642164230347), (0.0, 1.3367931842803955), (0.0, 1.3368539810180664), (0.0, 1.3369052410125732), (0.0, 1.3369253873825073), (0.0, 1.3369289636611938), (0.0, 1.3369498252868652), (0.0, 1.3370472192764282), (0.0, 1.3371363878250122), (0.0, 1.3372119665145874), (0.0, 1.3373143672943115), (0.0, 1.3373783826828003), (0.0, 1.3373924493789673), (0.0, 1.337674856185913), (0.0, 1.3376779556274414), (0.0, 1.337709665298462), (0.0, 1.3377556800842285), (0.0, 1.3378337621688843), (0.0, 1.337876796722412), (0.0, 1.3379857540130615), (0.0, 1.3380154371261597), (0.0, 1.3380324840545654), (0.0, 1.3382587432861328), (0.0, 1.3383047580718994), (0.0, 1.3391640186309814), (0.0, 1.3393995761871338), (0.0, 1.3395541906356812), (0.0, 1.3397226333618164), (0.0, 1.3404380083084106), (0.0, 1.3405988216400146), (0.0, 1.3409584760665894), (0.0, 1.3409850597381592), (0.0, 1.3414767980575562), (0.0, 1.3420449495315552), (0.0, 1.3477246761322021), (0.0, 1.4430007934570312), (0.0, 1.447817087173462), (0.0, 1.4486913681030273), (0.0, 1.4517238140106201), (0.0, 1.4521937370300293), (0.0, 1.45292067527771), (0.0, 1.4531290531158447), (0.0, 1.4531391859054565), (0.0, 1.4537001848220825), (0.0, 1.4538469314575195), (0.0, 1.4540044069290161), (0.0, 1.4544486999511719), (0.0, 1.454520344734192), (0.0, 1.454990267753601), (0.0, 1.4550833702087402), (0.0, 1.455543041229248), (0.0, 1.4556797742843628), (0.0, 1.4557667970657349), (0.0, 1.4558500051498413), (0.0, 1.4559656381607056), (0.0, 1.4560043811798096), (0.0, 1.4560412168502808), (0.0, 1.4560731649398804), (0.0, 1.4561764001846313), (0.0, 1.4564733505249023), (0.0, 1.456591248512268), (0.0, 1.4566000699996948), (0.0, 1.4566397666931152), (0.0, 1.4566586017608643), (0.0, 1.456695556640625), (0.0, 1.4567525386810303), (0.0, 1.4567668437957764), (0.0, 1.4569728374481201), (0.0, 1.4569895267486572), (0.0, 1.4570856094360352), (0.0, 1.4570866823196411), (0.0, 1.4570960998535156), (0.0, 1.4571012258529663), (0.0, 1.4571692943572998), (0.0, 1.4572967290878296), (0.0, 1.4573535919189453), (0.0, 1.4574565887451172), (0.0, 1.4574593305587769), (0.0, 1.457556128501892), (0.0, 1.4576098918914795), (0.0, 1.457626461982727), (0.0, 1.4577739238739014), (0.0, 1.4578239917755127), (0.0, 1.4578444957733154), (0.0, 1.4580413103103638), (0.0, 1.4580636024475098), (0.0, 1.458077311515808), (0.0, 1.4581313133239746), (0.0, 1.4582692384719849), (0.0, 1.4583600759506226), (0.0, 1.4585421085357666), (0.0, 1.458599328994751), (0.0, 1.4586681127548218), (0.0, 1.4587795734405518), (0.0, 1.4589358568191528), (0.0, 1.4589537382125854), (0.0, 1.4590115547180176), (0.0, 1.459144949913025), (0.0, 1.4591491222381592), (0.0, 1.4592370986938477), (0.0, 1.4593749046325684), (0.0, 1.4593772888183594), (0.0, 1.4593899250030518), (0.0, 1.4593926668167114), (0.0, 1.459404468536377), (0.0, 1.4594286680221558), (0.0, 1.4595351219177246), (0.0, 1.4599958658218384), (0.0, 1.4603955745697021), (0.0, 1.4604814052581787), (0.0, 1.4606164693832397), (0.0, 1.4606693983078003), (0.0, 1.460673451423645), (0.0, 1.460718035697937), (0.0, 1.4608980417251587), (0.0, 1.4609661102294922), (0.0, 1.461180567741394), (0.0, 1.461362600326538), (0.0, 1.4613958597183228), (0.0, 1.461400032043457), (0.0, 1.4614228010177612), (0.0, 1.461606740951538), (0.0, 1.461654782295227), (0.0, 1.4617050886154175), (0.0, 1.4618220329284668), (0.0, 1.461879849433899), (0.0, 1.4620115756988525), (0.0, 1.4621751308441162), (0.0, 1.462189793586731), (0.0, 1.46226966381073), (0.0, 1.4623334407806396), (0.0, 1.4623503684997559), (0.0, 1.462378740310669), (0.0, 1.4624346494674683), (0.0, 1.462531566619873), (0.0, 1.4625416994094849), (0.0, 1.4625545740127563), (0.0, 1.4628945589065552), (0.0, 1.463241457939148), (0.0, 1.463533878326416), (0.0, 1.4639825820922852), (0.0, 1.4643627405166626), (0.0, 1.4647144079208374), (0.0, 1.4650118350982666), (0.0, 1.4650392532348633), (0.0, 1.4657316207885742), (0.0, 1.4662271738052368), (0.0, 1.466746211051941), (0.0, 1.4669116735458374), (0.0, 1.4673869609832764), (0.0, 1.4675816297531128), (0.0, 1.4678632020950317), (0.0, 1.4679217338562012), (0.0, 1.4689284563064575), (0.0, 1.4695541858673096), (0.0, 1.4695578813552856), (0.0, 1.4697420597076416), (0.0, 1.4705833196640015), (0.0, 1.4710050821304321), (0.0, 1.472280740737915), (0.0, 1.5073955059051514), (0.0, 1.513837218284607), (0.0, 1.5147045850753784), (0.0, 1.5151993036270142), (0.0, 1.516021490097046), (0.0, 1.51678466796875), (0.0, 1.5171993970870972), (0.0, 1.517300009727478), (0.0, 1.5173335075378418), (0.0, 1.5174870491027832), (0.0, 1.5175254344940186), (0.0, 1.5176998376846313), (0.0, 1.5182347297668457), (0.0, 1.518370270729065), (0.0, 1.5184258222579956), (0.0, 1.5189329385757446), (0.0, 1.5191868543624878), (0.0, 1.5193032026290894), (0.0, 1.5194133520126343), (0.0, 1.5197316408157349), (0.0, 1.5197473764419556), (0.0, 1.519816279411316), (0.0, 1.5198848247528076), (0.0, 1.5199253559112549), (0.0, 1.5202564001083374), (0.0, 1.5202951431274414), (0.0, 1.520318865776062), (0.0, 1.520329236984253), (0.0, 1.5203853845596313), (0.0, 1.5204060077667236), (0.0, 1.5204174518585205), (0.0, 1.5205131769180298), (0.0, 1.5205856561660767), (0.0, 1.5206565856933594), (0.0, 1.52072274684906), (0.0, 1.521074891090393), (0.0, 1.5210760831832886), (0.0, 1.5210766792297363), (0.0, 1.5211398601531982), (0.0, 1.5214214324951172), (0.0, 1.5214413404464722), (0.0, 1.52150297164917), (0.0, 1.5215480327606201), (0.0, 1.5215821266174316), (0.0, 1.5217210054397583), (0.0, 1.521779179573059), (0.0, 1.5218944549560547), (0.0, 1.5220211744308472), (0.0, 1.522101879119873), (0.0, 1.5221540927886963), (0.0, 1.522158145904541), (0.0, 1.5222307443618774), (0.0, 1.5222883224487305), (0.0, 1.5225989818572998), (0.0, 1.5226020812988281), (0.0, 1.5226491689682007), (0.0, 1.522660255432129), (0.0, 1.5226757526397705), (0.0, 1.5228691101074219), (0.0, 1.52294921875), (0.0, 1.5230106115341187), (0.0, 1.5231167078018188), (0.0, 1.523126482963562), (0.0, 1.5232549905776978), (0.0, 1.5235209465026855), (0.0, 1.5235731601715088), (0.0, 1.523630142211914), (0.0, 1.5236653089523315), (0.0, 1.5237529277801514), (0.0, 1.5237534046173096), (0.0, 1.523825764656067), (0.0, 1.5239001512527466), (0.0, 1.5239121913909912), (0.0, 1.5239769220352173), (0.0, 1.5239784717559814), (0.0, 1.5240310430526733), (0.0, 1.524056077003479), (0.0, 1.5240658521652222), (0.0, 1.5241303443908691), (0.0, 1.5241841077804565), (0.0, 1.5242286920547485), (0.0, 1.52455472946167), (0.0, 1.5247257947921753), (0.0, 1.524793028831482), (0.0, 1.524870753288269), (0.0, 1.5249192714691162), (0.0, 1.5249212980270386), (0.0, 1.524937629699707), (0.0, 1.5250086784362793), (0.0, 1.5250986814498901), (0.0, 1.5251504182815552), (0.0, 1.52529776096344), (0.0, 1.5257692337036133), (0.0, 1.5258009433746338), (0.0, 1.525841236114502), (0.0, 1.5258474349975586), (0.0, 1.5258591175079346), (0.0, 1.526109218597412), (0.0, 1.526260256767273), (0.0, 1.5265098810195923), (0.0, 1.5266072750091553), (0.0, 1.5266189575195312), (0.0, 1.5266339778900146), (0.0, 1.5266913175582886), (0.0, 1.526902198791504), (0.0, 1.5269558429718018), (0.0, 1.5270392894744873), (0.0, 1.52724027633667), (0.0, 1.5272421836853027), (0.0, 1.5273170471191406), (0.0, 1.5276352167129517), (0.0, 1.5277726650238037), (0.0, 1.527795433998108), (0.0, 1.527820110321045), (0.0, 1.5280646085739136), (0.0, 1.5285056829452515), (0.0, 1.5285310745239258), (0.0, 1.5291967391967773), (0.0, 1.5292987823486328), (0.0, 1.529337763786316), (0.0, 1.529727816581726), (0.0, 1.5298722982406616), (0.0, 1.5317885875701904), (0.0, 1.5338304042816162), (0.0, 1.580002784729004)], [(8.523451805114746, 8.55520248413086), (8.006219863891602, 8.416970252990723), (7.086679458618164, 7.637279987335205), (6.84177827835083, 7.073756694793701), (6.828403949737549, 7.2055253982543945), (6.666980266571045, 7.2568511962890625), (6.563147068023682, 6.9859795570373535), (6.443130016326904, 7.082660675048828), (6.296314716339111, 7.8249921798706055), (6.075440883636475, 7.234186172485352), (6.072579383850098, 7.050199031829834), (5.939131259918213, 6.4410247802734375), (5.899518966674805, 6.055575847625732), (5.673395156860352, 5.73776388168335), (5.542388916015625, 8.692059516906738), (5.485748291015625, 5.559858798980713), (5.4255051612854, 8.28046703338623), (5.215892791748047, 8.496737480163574), (5.064194202423096, 6.712091445922852), (4.881453514099121, 7.066629886627197), (4.411713600158691, 4.832673072814941), (4.407356262207031, 7.889081001281738), (4.397771835327148, 4.462500095367432), (4.368495941162109, 7.880725860595703), (4.039572715759277, 4.056340217590332), (4.038238525390625, 4.266560077667236), (3.987125873565674, 4.794241428375244), (3.873535633087158, 4.2981414794921875), (3.6760339736938477, 8.663405418395996)], [(9.593707084655762, 9.594338417053223), (9.49022102355957, 10.139176368713379), (9.14061164855957, 10.5911283493042), (8.337547302246094, 8.350876808166504), (7.6931986808776855, 7.789623260498047)]]
[array([[0.        , 1.29664493],
       [0.        , 1.2986784 ],
       [0.        , 1.32420003],
       [0.        , 1.32481325],
       [0.        , 1.32541621],
       [0.        , 1.32568991],
       [0.        , 1.3257767 ],
       [0.        , 1.32655287],
       [0.        , 1.32686543],
       [0.        , 1.32690585],
       [0.        , 1.32750213],
       [0.        , 1.32764971],
       [0.        , 1.32865512],
       [0.        , 1.32870209],
       [0.        , 1.32891119],
       [0.        , 1.32895052],
       [0.        , 1.32915699],
       [0.        , 1.32916212],
       [0.        , 1.33026993],
       [0.        , 1.33047748],
       [0.        , 1.33101535],
       [0.        , 1.33118403],
       [0.        , 1.33132029],
       [0.        , 1.33135867],
       [0.        , 1.33147931],
       [0.        , 1.33159447],
       [0.        , 1.3316828 ],
       [0.        , 1.33169091],
       [0.        , 1.33185852],
       [0.        , 1.3319031 ],
       [0.        , 1.33205521],
       [0.        , 1.33222842],
       [0.        , 1.33226061],
       [0.        , 1.33233833],
       [0.        , 1.3324033 ],
       [0.        , 1.33250821],
       [0.        , 1.3326143 ],
       [0.        , 1.33265054],
       [0.        , 1.33271527],
       [0.        , 1.33303022],
       [0.        , 1.33319199],
       [0.        , 1.33331048],
       [0.        , 1.33336926],
       [0.        , 1.33346927],
       [0.        , 1.33350301],
       [0.        , 1.33351684],
       [0.        , 1.33357084],
       [0.        , 1.33361781],
       [0.        , 1.33371532],
       [0.        , 1.33373499],
       [0.        , 1.33380711],
       [0.        , 1.33386135],
       [0.        , 1.33392525],
       [0.        , 1.33395112],
       [0.        , 1.33410716],
       [0.        , 1.33418334],
       [0.        , 1.33431399],
       [0.        , 1.33434415],
       [0.        , 1.33442903],
       [0.        , 1.33446264],
       [0.        , 1.33448911],
       [0.        , 1.33463466],
       [0.        , 1.33465743],
       [0.        , 1.33466136],
       [0.        , 1.33469546],
       [0.        , 1.3348037 ],
       [0.        , 1.33487523],
       [0.        , 1.3349483 ],
       [0.        , 1.33497906],
       [0.        , 1.3349905 ],
       [0.        , 1.33500338],
       [0.        , 1.33501327],
       [0.        , 1.33503675],
       [0.        , 1.33513439],
       [0.        , 1.33535242],
       [0.        , 1.33537054],
       [0.        , 1.3354075 ],
       [0.        , 1.33546925],
       [0.        , 1.3354851 ],
       [0.        , 1.33548939],
       [0.        , 1.33564043],
       [0.        , 1.33582842],
       [0.        , 1.33622229],
       [0.        , 1.33638096],
       [0.        , 1.33642244],
       [0.        , 1.33644795],
       [0.        , 1.33653104],
       [0.        , 1.33661258],
       [0.        , 1.33666456],
       [0.        , 1.33676422],
       [0.        , 1.33679318],
       [0.        , 1.33685398],
       [0.        , 1.33690524],
       [0.        , 1.33692539],
       [0.        , 1.33692896],
       [0.        , 1.33694983],
       [0.        , 1.33704722],
       [0.        , 1.33713639],
       [0.        , 1.33721197],
       [0.        , 1.33731437],
       [0.        , 1.33737838],
       [0.        , 1.33739245],
       [0.        , 1.33767486],
       [0.        , 1.33767796],
       [0.        , 1.33770967],
       [0.        , 1.33775568],
       [0.        , 1.33783376],
       [0.        , 1.3378768 ],
       [0.        , 1.33798575],
       [0.        , 1.33801544],
       [0.        , 1.33803248],
       [0.        , 1.33825874],
       [0.        , 1.33830476],
       [0.        , 1.33916402],
       [0.        , 1.33939958],
       [0.        , 1.33955419],
       [0.        , 1.33972263],
       [0.        , 1.34043801],
       [0.        , 1.34059882],
       [0.        , 1.34095848],
       [0.        , 1.34098506],
       [0.        , 1.3414768 ],
       [0.        , 1.34204495],
       [0.        , 1.34772468],
       [0.        , 1.44300079],
       [0.        , 1.44781709],
       [0.        , 1.44869137],
       [0.        , 1.45172381],
       [0.        , 1.45219374],
       [0.        , 1.45292068],
       [0.        , 1.45312905],
       [0.        , 1.45313919],
       [0.        , 1.45370018],
       [0.        , 1.45384693],
       [0.        , 1.45400441],
       [0.        , 1.4544487 ],
       [0.        , 1.45452034],
       [0.        , 1.45499027],
       [0.        , 1.45508337],
       [0.        , 1.45554304],
       [0.        , 1.45567977],
       [0.        , 1.4557668 ],
       [0.        , 1.45585001],
       [0.        , 1.45596564],
       [0.        , 1.45600438],
       [0.        , 1.45604122],
       [0.        , 1.45607316],
       [0.        , 1.4561764 ],
       [0.        , 1.45647335],
       [0.        , 1.45659125],
       [0.        , 1.45660007],
       [0.        , 1.45663977],
       [0.        , 1.4566586 ],
       [0.        , 1.45669556],
       [0.        , 1.45675254],
       [0.        , 1.45676684],
       [0.        , 1.45697284],
       [0.        , 1.45698953],
       [0.        , 1.45708561],
       [0.        , 1.45708668],
       [0.        , 1.4570961 ],
       [0.        , 1.45710123],
       [0.        , 1.45716929],
       [0.        , 1.45729673],
       [0.        , 1.45735359],
       [0.        , 1.45745659],
       [0.        , 1.45745933],
       [0.        , 1.45755613],
       [0.        , 1.45760989],
       [0.        , 1.45762646],
       [0.        , 1.45777392],
       [0.        , 1.45782399],
       [0.        , 1.4578445 ],
       [0.        , 1.45804131],
       [0.        , 1.4580636 ],
       [0.        , 1.45807731],
       [0.        , 1.45813131],
       [0.        , 1.45826924],
       [0.        , 1.45836008],
       [0.        , 1.45854211],
       [0.        , 1.45859933],
       [0.        , 1.45866811],
       [0.        , 1.45877957],
       [0.        , 1.45893586],
       [0.        , 1.45895374],
       [0.        , 1.45901155],
       [0.        , 1.45914495],
       [0.        , 1.45914912],
       [0.        , 1.4592371 ],
       [0.        , 1.4593749 ],
       [0.        , 1.45937729],
       [0.        , 1.45938993],
       [0.        , 1.45939267],
       [0.        , 1.45940447],
       [0.        , 1.45942867],
       [0.        , 1.45953512],
       [0.        , 1.45999587],
       [0.        , 1.46039557],
       [0.        , 1.46048141],
       [0.        , 1.46061647],
       [0.        , 1.4606694 ],
       [0.        , 1.46067345],
       [0.        , 1.46071804],
       [0.        , 1.46089804],
       [0.        , 1.46096611],
       [0.        , 1.46118057],
       [0.        , 1.4613626 ],
       [0.        , 1.46139586],
       [0.        , 1.46140003],
       [0.        , 1.4614228 ],
       [0.        , 1.46160674],
       [0.        , 1.46165478],
       [0.        , 1.46170509],
       [0.        , 1.46182203],
       [0.        , 1.46187985],
       [0.        , 1.46201158],
       [0.        , 1.46217513],
       [0.        , 1.46218979],
       [0.        , 1.46226966],
       [0.        , 1.46233344],
       [0.        , 1.46235037],
       [0.        , 1.46237874],
       [0.        , 1.46243465],
       [0.        , 1.46253157],
       [0.        , 1.4625417 ],
       [0.        , 1.46255457],
       [0.        , 1.46289456],
       [0.        , 1.46324146],
       [0.        , 1.46353388],
       [0.        , 1.46398258],
       [0.        , 1.46436274],
       [0.        , 1.46471441],
       [0.        , 1.46501184],
       [0.        , 1.46503925],
       [0.        , 1.46573162],
       [0.        , 1.46622717],
       [0.        , 1.46674621],
       [0.        , 1.46691167],
       [0.        , 1.46738696],
       [0.        , 1.46758163],
       [0.        , 1.4678632 ],
       [0.        , 1.46792173],
       [0.        , 1.46892846],
       [0.        , 1.46955419],
       [0.        , 1.46955788],
       [0.        , 1.46974206],
       [0.        , 1.47058332],
       [0.        , 1.47100508],
       [0.        , 1.47228074],
       [0.        , 1.50739551],
       [0.        , 1.51383722],
       [0.        , 1.51470459],
       [0.        , 1.5151993 ],
       [0.        , 1.51602149],
       [0.        , 1.51678467],
       [0.        , 1.5171994 ],
       [0.        , 1.51730001],
       [0.        , 1.51733351],
       [0.        , 1.51748705],
       [0.        , 1.51752543],
       [0.        , 1.51769984],
       [0.        , 1.51823473],
       [0.        , 1.51837027],
       [0.        , 1.51842582],
       [0.        , 1.51893294],
       [0.        , 1.51918685],
       [0.        , 1.5193032 ],
       [0.        , 1.51941335],
       [0.        , 1.51973164],
       [0.        , 1.51974738],
       [0.        , 1.51981628],
       [0.        , 1.51988482],
       [0.        , 1.51992536],
       [0.        , 1.5202564 ],
       [0.        , 1.52029514],
       [0.        , 1.52031887],
       [0.        , 1.52032924],
       [0.        , 1.52038538],
       [0.        , 1.52040601],
       [0.        , 1.52041745],
       [0.        , 1.52051318],
       [0.        , 1.52058566],
       [0.        , 1.52065659],
       [0.        , 1.52072275],
       [0.        , 1.52107489],
       [0.        , 1.52107608],
       [0.        , 1.52107668],
       [0.        , 1.52113986],
       [0.        , 1.52142143],
       [0.        , 1.52144134],
       [0.        , 1.52150297],
       [0.        , 1.52154803],
       [0.        , 1.52158213],
       [0.        , 1.52172101],
       [0.        , 1.52177918],
       [0.        , 1.52189445],
       [0.        , 1.52202117],
       [0.        , 1.52210188],
       [0.        , 1.52215409],
       [0.        , 1.52215815],
       [0.        , 1.52223074],
       [0.        , 1.52228832],
       [0.        , 1.52259898],
       [0.        , 1.52260208],
       [0.        , 1.52264917],
       [0.        , 1.52266026],
       [0.        , 1.52267575],
       [0.        , 1.52286911],
       [0.        , 1.52294922],
       [0.        , 1.52301061],
       [0.        , 1.52311671],
       [0.        , 1.52312648],
       [0.        , 1.52325499],
       [0.        , 1.52352095],
       [0.        , 1.52357316],
       [0.        , 1.52363014],
       [0.        , 1.52366531],
       [0.        , 1.52375293],
       [0.        , 1.5237534 ],
       [0.        , 1.52382576],
       [0.        , 1.52390015],
       [0.        , 1.52391219],
       [0.        , 1.52397692],
       [0.        , 1.52397847],
       [0.        , 1.52403104],
       [0.        , 1.52405608],
       [0.        , 1.52406585],
       [0.        , 1.52413034],
       [0.        , 1.52418411],
       [0.        , 1.52422869],
       [0.        , 1.52455473],
       [0.        , 1.52472579],
       [0.        , 1.52479303],
       [0.        , 1.52487075],
       [0.        , 1.52491927],
       [0.        , 1.5249213 ],
       [0.        , 1.52493763],
       [0.        , 1.52500868],
       [0.        , 1.52509868],
       [0.        , 1.52515042],
       [0.        , 1.52529776],
       [0.        , 1.52576923],
       [0.        , 1.52580094],
       [0.        , 1.52584124],
       [0.        , 1.52584743],
       [0.        , 1.52585912],
       [0.        , 1.52610922],
       [0.        , 1.52626026],
       [0.        , 1.52650988],
       [0.        , 1.52660728],
       [0.        , 1.52661896],
       [0.        , 1.52663398],
       [0.        , 1.52669132],
       [0.        , 1.5269022 ],
       [0.        , 1.52695584],
       [0.        , 1.52703929],
       [0.        , 1.52724028],
       [0.        , 1.52724218],
       [0.        , 1.52731705],
       [0.        , 1.52763522],
       [0.        , 1.52777267],
       [0.        , 1.52779543],
       [0.        , 1.52782011],
       [0.        , 1.52806461],
       [0.        , 1.52850568],
       [0.        , 1.52853107],
       [0.        , 1.52919674],
       [0.        , 1.52929878],
       [0.        , 1.52933776],
       [0.        , 1.52972782],
       [0.        , 1.5298723 ],
       [0.        , 1.53178859],
       [0.        , 1.5338304 ],
       [0.        , 1.58000278]]), array([[8.52345181, 8.55520248],
       [8.00621986, 8.41697025],
       [7.08667946, 7.63727999],
       [6.84177828, 7.07375669],
       [6.82840395, 7.2055254 ],
       [6.66698027, 7.2568512 ],
       [6.56314707, 6.98597956],
       [6.44313002, 7.08266068],
       [6.29631472, 7.82499218],
       [6.07544088, 7.23418617],
       [6.07257938, 7.05019903],
       [5.93913126, 6.44102478],
       [5.89951897, 6.05557585],
       [5.67339516, 5.73776388],
       [5.54238892, 8.69205952],
       [5.48574829, 5.5598588 ],
       [5.42550516, 8.28046703],
       [5.21589279, 8.49673748],
       [5.0641942 , 6.71209145],
       [4.88145351, 7.06662989],
       [4.4117136 , 4.83267307],
       [4.40735626, 7.889081  ],
       [4.39777184, 4.4625001 ],
       [4.36849594, 7.88072586],
       [4.03957272, 4.05634022],
       [4.03823853, 4.26656008],
       [3.98712587, 4.79424143],
       [3.87353563, 4.29814148],
       [3.67603397, 8.66340542]]), array([[ 9.59370708,  9.59433842],
       [ 9.49022102, 10.13917637],
       [ 9.14061165, 10.59112835],
       [ 8.3375473 ,  8.35087681],
       [ 7.69319868,  7.78962326]])]2024-03-06 18:01:20.301767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LG5 ph vector generated, counter: 212
2024-03-06 18:01:24.656034: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:01:24.699356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:01:25.764483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.1446822), (0., 1.3088125), (0., 1.3157725), (0., 1.3221828),
       (0., 1.322809 ), (0., 1.3239923), (0., 1.3254108), (0., 1.3256108),
       (0., 1.3260715), (0., 1.3266443), (0., 1.3271626), (0., 1.3273138),
       (0., 1.3274728), (0., 1.3276514), (0., 1.3279887), (0., 1.3281485),
       (0., 1.3285336), (0., 1.3288622), (0., 1.3289921), (0., 1.3290329),
       (0., 1.3291527), (0., 1.3293997), (0., 1.3294375), (0., 1.3295768),
       (0., 1.3298432), (0., 1.3299112), (0., 1.3301766), (0., 1.3303218),
       (0., 1.3304814), (0., 1.3307962), (0., 1.3308454), (0., 1.3308579),
       (0., 1.3308955), (0., 1.3310099), (0., 1.3312335), (0., 1.3313878),
       (0., 1.3314426), (0., 1.3316106), (0., 1.3317394), (0., 1.3318585),
       (0., 1.3318657), (0., 1.3320451), (0., 1.3322465), (0., 1.3324804),
       (0., 1.3324871), (0., 1.3325998), (0., 1.3326594), (0., 1.3326672),
       (0., 1.3327903), (0., 1.3330724), (0., 1.3331265), (0., 1.3331423),
       (0., 1.3332192), (0., 1.3332825), (0., 1.3333719), (0., 1.3333755),
       (0., 1.3333899), (0., 1.3335078), (0., 1.3336589), (0., 1.3337736),
       (0., 1.3338686), (0., 1.3339068), (0., 1.3340453), (0., 1.3341237),
       (0., 1.3341494), (0., 1.3341658), (0., 1.334175 ), (0., 1.3343378),
       (0., 1.334475 ), (0., 1.3345685), (0., 1.3345696), (0., 1.334585 ),
       (0., 1.3345932), (0., 1.334724 ), (0., 1.33474  ), (0., 1.3347956),
       (0., 1.3349626), (0., 1.3350136), (0., 1.3350766), (0., 1.3350832),
       (0., 1.3352287), (0., 1.3352473), (0., 1.3352867), (0., 1.3353668),
       (0., 1.3353705), (0., 1.335427 ), (0., 1.3354968), (0., 1.3356236),
       (0., 1.3359354), (0., 1.3361305), (0., 1.3363955), (0., 1.336417 ),
       (0., 1.3365241), (0., 1.3365669), (0., 1.3366458), (0., 1.3366463),
       (0., 1.3368033), (0., 1.3369521), (0., 1.3369745), (0., 1.3371974),
       (0., 1.3373208), (0., 1.3373517), (0., 1.3375582), (0., 1.3376025),
       (0., 1.3376089), (0., 1.3376431), (0., 1.3380233), (0., 1.3381704),
       (0., 1.338304 ), (0., 1.3383187), (0., 1.3385775), (0., 1.3385829),
       (0., 1.3386201), (0., 1.3388358), (0., 1.338849 ), (0., 1.3390585),
       (0., 1.3392633), (0., 1.3395246), (0., 1.3399808), (0., 1.3401405),
       (0., 1.3414708), (0., 1.3421437), (0., 1.3425356), (0., 1.3433343),
       (0., 1.3633621), (0., 1.4230248), (0., 1.4427197), (0., 1.4479799),
       (0., 1.4489096), (0., 1.4494542), (0., 1.4497011), (0., 1.4502243),
       (0., 1.4503942), (0., 1.4504364), (0., 1.4513735), (0., 1.4515692),
       (0., 1.4517518), (0., 1.452406 ), (0., 1.4526734), (0., 1.452894 ),
       (0., 1.4531035), (0., 1.4531468), (0., 1.4534162), (0., 1.4540403),
       (0., 1.4541351), (0., 1.4543631), (0., 1.4543906), (0., 1.454418 ),
       (0., 1.4544975), (0., 1.4545751), (0., 1.4545789), (0., 1.4547821),
       (0., 1.4553307), (0., 1.4556578), (0., 1.455825 ), (0., 1.4559228),
       (0., 1.4559797), (0., 1.4560745), (0., 1.4561081), (0., 1.4562153),
       (0., 1.456416 ), (0., 1.456582 ), (0., 1.4566023), (0., 1.4566319),
       (0., 1.4568026), (0., 1.4568064), (0., 1.4569187), (0., 1.4570131),
       (0., 1.4570723), (0., 1.457173 ), (0., 1.457233 ), (0., 1.457317 ),
       (0., 1.4573652), (0., 1.4574548), (0., 1.4575231), (0., 1.4575465),
       (0., 1.4575772), (0., 1.4576019), (0., 1.4576708), (0., 1.4577726),
       (0., 1.4577922), (0., 1.4578009), (0., 1.4580084), (0., 1.4581342),
       (0., 1.4583149), (0., 1.4584521), (0., 1.4584823), (0., 1.4585713),
       (0., 1.4587129), (0., 1.4588059), (0., 1.4589986), (0., 1.4590997),
       (0., 1.4592539), (0., 1.4592843), (0., 1.4592851), (0., 1.4595078),
       (0., 1.4595094), (0., 1.4595709), (0., 1.4597038), (0., 1.45984  ),
       (0., 1.459895 ), (0., 1.4600084), (0., 1.4601626), (0., 1.4603424),
       (0., 1.4604142), (0., 1.460441 ), (0., 1.4605763), (0., 1.4606284),
       (0., 1.4607705), (0., 1.4607801), (0., 1.4609901), (0., 1.4610069),
       (0., 1.4610982), (0., 1.4611784), (0., 1.4612134), (0., 1.4612164),
       (0., 1.4613755), (0., 1.4614117), (0., 1.4614655), (0., 1.4615731),
       (0., 1.4619067), (0., 1.4619635), (0., 1.4620218), (0., 1.4620225),
       (0., 1.4620745), (0., 1.462089 ), (0., 1.4621307), (0., 1.4621688),
       (0., 1.4621753), (0., 1.4622132), (0., 1.4625819), (0., 1.4625841),
       (0., 1.462623 ), (0., 1.4627303), (0., 1.4627419), (0., 1.4633151),
       (0., 1.4636964), (0., 1.4641794), (0., 1.4649034), (0., 1.4660702),
       (0., 1.4661262), (0., 1.4664869), (0., 1.4665569), (0., 1.4678952),
       (0., 1.4683762), (0., 1.4687746), (0., 1.4720403), (0., 1.472502 ),
       (0., 1.4742602), (0., 1.5022206), (0., 1.5084585), (0., 1.5113349),
       (0., 1.5127107), (0., 1.513264 ), (0., 1.5144588), (0., 1.5145502),
       (0., 1.515208 ), (0., 1.5155845), (0., 1.5160066), (0., 1.5161599),
       (0., 1.5165913), (0., 1.5168474), (0., 1.5168864), (0., 1.517111 ),
       (0., 1.5171335), (0., 1.5171752), (0., 1.5172898), (0., 1.5174042),
       (0., 1.5180765), (0., 1.5182729), (0., 1.5184484), (0., 1.5186474),
       (0., 1.5188498), (0., 1.5190072), (0., 1.5190089), (0., 1.5192292),
       (0., 1.5196168), (0., 1.519661 ), (0., 1.5197258), (0., 1.5198683),
       (0., 1.519987 ), (0., 1.5199881), (0., 1.5202917), (0., 1.5204744),
       (0., 1.5205724), (0., 1.5208572), (0., 1.5208775), (0., 1.5209085),
       (0., 1.5209186), (0., 1.5209719), (0., 1.5209813), (0., 1.5210605),
       (0., 1.521097 ), (0., 1.5212005), (0., 1.5212134), (0., 1.521299 ),
       (0., 1.521451 ), (0., 1.5215526), (0., 1.5215935), (0., 1.5217347),
       (0., 1.521774 ), (0., 1.5217893), (0., 1.5219347), (0., 1.5219545),
       (0., 1.5219758), (0., 1.5221112), (0., 1.5221199), (0., 1.5221462),
       (0., 1.5221977), (0., 1.5224046), (0., 1.5225762), (0., 1.5226103),
       (0., 1.5226835), (0., 1.5227793), (0., 1.5227945), (0., 1.5229275),
       (0., 1.5229336), (0., 1.5230194), (0., 1.5230241), (0., 1.5231073),
       (0., 1.523142 ), (0., 1.5231496), (0., 1.5233089), (0., 1.5233271),
       (0., 1.5234985), (0., 1.5236838), (0., 1.523718 ), (0., 1.5237341),
       (0., 1.5238671), (0., 1.5240018), (0., 1.5240219), (0., 1.5240767),
       (0., 1.5244706), (0., 1.524595 ), (0., 1.5246321), (0., 1.5247893),
       (0., 1.5253268), (0., 1.5254765), (0., 1.5254817), (0., 1.5255666),
       (0., 1.5256126), (0., 1.5257854), (0., 1.5259557), (0., 1.5261542),
       (0., 1.5262355), (0., 1.5264192), (0., 1.5266783), (0., 1.5267311),
       (0., 1.5273255), (0., 1.5274479), (0., 1.5280168), (0., 1.5281601),
       (0., 1.5283313), (0., 1.5283529), (0., 1.5285901), (0., 1.5286337),
       (0., 1.5286608), (0., 1.5289327), (0., 1.5290179), (0., 1.5291411),
       (0., 1.5291988), (0., 1.5292684), (0., 1.5293076), (0., 1.5294846),
       (0., 1.5297024), (0., 1.5298973), (0., 1.5300368), (0., 1.530182 ),
       (0., 1.5310339), (0., 1.531491 ), (0., 1.5319464), (0., 1.5326507),
       (0., 1.5521775), (0., 1.5781107)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.009257 , 8.416391 ), (7.030032 , 7.6099997),
       (6.9428487, 7.3541512), (6.824067 , 7.0865107),
       (6.721408 , 7.349061 ), (6.5113626, 7.017472 ),
       (6.3998103, 6.433485 ), (6.384854 , 7.015646 ),
       (6.2432446, 7.3546042), (6.1701674, 7.1111116),
       (6.0644636, 7.9076614), (5.91178  , 6.4875565),
       (5.8076777, 6.0142756), (5.728029 , 5.929219 ),
       (5.6249213, 8.699658 ), (5.4067492, 5.4292536),
       (5.375844 , 5.4944773), (5.22683  , 8.334492 ),
       (5.1336064, 8.500248 ), (5.1284943, 6.6056147),
       (4.960318 , 7.0978956), (4.820175 , 4.834876 ),
       (4.4705863, 7.826769 ), (4.389845 , 4.40428  ),
       (4.102702 , 7.9699154), (4.0859303, 4.2873416),
       (4.024377 , 4.232951 ), (3.9783118, 4.6900163),
       (3.8813434, 4.0311446), (3.8438694, 8.630114 ),
       (3.7591379, 4.062479 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.77691 , 10.326502), (9.185976, 10.73069 ),
       (8.501564,  8.568811), (8.384763,  8.400544)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.1446821689605713), (0.0, 1.3088124990463257), (0.0, 1.315772533416748), (0.0, 1.3221827745437622), (0.0, 1.3228089809417725), (0.0, 1.3239922523498535), (0.0, 1.3254108428955078), (0.0, 1.3256107568740845), (0.0, 1.3260715007781982), (0.0, 1.3266443014144897), (0.0, 1.3271626234054565), (0.0, 1.327313780784607), (0.0, 1.3274728059768677), (0.0, 1.3276513814926147), (0.0, 1.3279887437820435), (0.0, 1.3281484842300415), (0.0, 1.32853364944458), (0.0, 1.328862190246582), (0.0, 1.3289921283721924), (0.0, 1.3290328979492188), (0.0, 1.3291527032852173), (0.0, 1.3293997049331665), (0.0, 1.329437494277954), (0.0, 1.329576849937439), (0.0, 1.3298431634902954), (0.0, 1.329911231994629), (0.0, 1.330176591873169), (0.0, 1.3303217887878418), (0.0, 1.3304814100265503), (0.0, 1.330796241760254), (0.0, 1.3308453559875488), (0.0, 1.3308578729629517), (0.0, 1.3308955430984497), (0.0, 1.331009864807129), (0.0, 1.3312335014343262), (0.0, 1.3313877582550049), (0.0, 1.3314425945281982), (0.0, 1.3316105604171753), (0.0, 1.3317394256591797), (0.0, 1.331858515739441), (0.0, 1.331865668296814), (0.0, 1.332045078277588), (0.0, 1.3322465419769287), (0.0, 1.3324804306030273), (0.0, 1.3324871063232422), (0.0, 1.3325997591018677), (0.0, 1.332659363746643), (0.0, 1.3326672315597534), (0.0, 1.3327902555465698), (0.0, 1.3330724239349365), (0.0, 1.3331265449523926), (0.0, 1.3331422805786133), (0.0, 1.3332191705703735), (0.0, 1.333282470703125), (0.0, 1.333371877670288), (0.0, 1.3333754539489746), (0.0, 1.3333898782730103), (0.0, 1.333507776260376), (0.0, 1.3336589336395264), (0.0, 1.3337736129760742), (0.0, 1.3338686227798462), (0.0, 1.3339067697525024), (0.0, 1.3340452909469604), (0.0, 1.3341237306594849), (0.0, 1.3341493606567383), (0.0, 1.3341658115386963), (0.0, 1.3341749906539917), (0.0, 1.334337830543518), (0.0, 1.334475040435791), (0.0, 1.3345685005187988), (0.0, 1.3345695734024048), (0.0, 1.3345849514007568), (0.0, 1.3345931768417358), (0.0, 1.334723949432373), (0.0, 1.3347400426864624), (0.0, 1.334795594215393), (0.0, 1.3349626064300537), (0.0, 1.3350136280059814), (0.0, 1.3350765705108643), (0.0, 1.335083246231079), (0.0, 1.335228681564331), (0.0, 1.335247278213501), (0.0, 1.3352867364883423), (0.0, 1.3353668451309204), (0.0, 1.3353705406188965), (0.0, 1.3354270458221436), (0.0, 1.3354967832565308), (0.0, 1.3356236219406128), (0.0, 1.335935354232788), (0.0, 1.3361304998397827), (0.0, 1.336395502090454), (0.0, 1.3364169597625732), (0.0, 1.3365241289138794), (0.0, 1.3365669250488281), (0.0, 1.3366458415985107), (0.0, 1.336646318435669), (0.0, 1.3368033170700073), (0.0, 1.3369520902633667), (0.0, 1.3369745016098022), (0.0, 1.3371974229812622), (0.0, 1.3373208045959473), (0.0, 1.337351679801941), (0.0, 1.3375581502914429), (0.0, 1.3376024961471558), (0.0, 1.3376089334487915), (0.0, 1.3376431465148926), (0.0, 1.33802330493927), (0.0, 1.3381704092025757), (0.0, 1.338304042816162), (0.0, 1.3383187055587769), (0.0, 1.3385775089263916), (0.0, 1.3385828733444214), (0.0, 1.3386200666427612), (0.0, 1.3388358354568481), (0.0, 1.3388489484786987), (0.0, 1.339058518409729), (0.0, 1.3392633199691772), (0.0, 1.3395246267318726), (0.0, 1.3399808406829834), (0.0, 1.340140461921692), (0.0, 1.3414708375930786), (0.0, 1.3421436548233032), (0.0, 1.3425356149673462), (0.0, 1.3433343172073364), (0.0, 1.3633620738983154), (0.0, 1.4230247735977173), (0.0, 1.4427196979522705), (0.0, 1.4479799270629883), (0.0, 1.4489096403121948), (0.0, 1.4494541883468628), (0.0, 1.4497010707855225), (0.0, 1.4502242803573608), (0.0, 1.4503941535949707), (0.0, 1.4504363536834717), (0.0, 1.4513734579086304), (0.0, 1.4515691995620728), (0.0, 1.4517518281936646), (0.0, 1.4524060487747192), (0.0, 1.4526734352111816), (0.0, 1.4528939723968506), (0.0, 1.4531035423278809), (0.0, 1.4531468152999878), (0.0, 1.4534162282943726), (0.0, 1.454040288925171), (0.0, 1.4541350603103638), (0.0, 1.4543631076812744), (0.0, 1.4543906450271606), (0.0, 1.4544179439544678), (0.0, 1.4544974565505981), (0.0, 1.4545750617980957), (0.0, 1.4545788764953613), (0.0, 1.4547821283340454), (0.0, 1.455330729484558), (0.0, 1.4556578397750854), (0.0, 1.4558249711990356), (0.0, 1.4559228420257568), (0.0, 1.4559797048568726), (0.0, 1.4560744762420654), (0.0, 1.4561080932617188), (0.0, 1.456215262413025), (0.0, 1.4564160108566284), (0.0, 1.456581950187683), (0.0, 1.4566023349761963), (0.0, 1.4566318988800049), (0.0, 1.4568026065826416), (0.0, 1.4568064212799072), (0.0, 1.456918716430664), (0.0, 1.4570131301879883), (0.0, 1.4570722579956055), (0.0, 1.4571729898452759), (0.0, 1.45723295211792), (0.0, 1.4573169946670532), (0.0, 1.4573651552200317), (0.0, 1.457454800605774), (0.0, 1.4575231075286865), (0.0, 1.4575464725494385), (0.0, 1.4575772285461426), (0.0, 1.4576019048690796), (0.0, 1.45767080783844), (0.0, 1.4577726125717163), (0.0, 1.4577921628952026), (0.0, 1.4578008651733398), (0.0, 1.4580084085464478), (0.0, 1.4581341743469238), (0.0, 1.4583148956298828), (0.0, 1.4584521055221558), (0.0, 1.458482265472412), (0.0, 1.4585713148117065), (0.0, 1.4587129354476929), (0.0, 1.4588059186935425), (0.0, 1.4589985609054565), (0.0, 1.4590996503829956), (0.0, 1.4592539072036743), (0.0, 1.4592843055725098), (0.0, 1.4592851400375366), (0.0, 1.4595078229904175), (0.0, 1.4595093727111816), (0.0, 1.4595708847045898), (0.0, 1.459703803062439), (0.0, 1.4598400592803955), (0.0, 1.4598950147628784), (0.0, 1.4600083827972412), (0.0, 1.46016263961792), (0.0, 1.4603424072265625), (0.0, 1.460414171218872), (0.0, 1.460440993309021), (0.0, 1.4605762958526611), (0.0, 1.4606283903121948), (0.0, 1.4607704877853394), (0.0, 1.460780143737793), (0.0, 1.460990071296692), (0.0, 1.4610068798065186), (0.0, 1.4610981941223145), (0.0, 1.4611784219741821), (0.0, 1.4612133502960205), (0.0, 1.4612164497375488), (0.0, 1.4613754749298096), (0.0, 1.461411714553833), (0.0, 1.4614654779434204), (0.0, 1.4615731239318848), (0.0, 1.4619066715240479), (0.0, 1.4619635343551636), (0.0, 1.462021827697754), (0.0, 1.4620225429534912), (0.0, 1.4620745182037354), (0.0, 1.462088942527771), (0.0, 1.4621306657791138), (0.0, 1.46216881275177), (0.0, 1.4621752500534058), (0.0, 1.462213158607483), (0.0, 1.4625818729400635), (0.0, 1.462584137916565), (0.0, 1.4626230001449585), (0.0, 1.4627302885055542), (0.0, 1.4627418518066406), (0.0, 1.4633151292800903), (0.0, 1.4636963605880737), (0.0, 1.4641793966293335), (0.0, 1.4649033546447754), (0.0, 1.4660701751708984), (0.0, 1.4661262035369873), (0.0, 1.466486930847168), (0.0, 1.4665569067001343), (0.0, 1.4678951501846313), (0.0, 1.4683761596679688), (0.0, 1.4687745571136475), (0.0, 1.4720402956008911), (0.0, 1.4725019931793213), (0.0, 1.4742602109909058), (0.0, 1.502220630645752), (0.0, 1.5084584951400757), (0.0, 1.5113348960876465), (0.0, 1.512710690498352), (0.0, 1.5132639408111572), (0.0, 1.5144587755203247), (0.0, 1.5145502090454102), (0.0, 1.5152080059051514), (0.0, 1.5155844688415527), (0.0, 1.516006588935852), (0.0, 1.5161598920822144), (0.0, 1.5165913105010986), (0.0, 1.5168473720550537), (0.0, 1.5168863534927368), (0.0, 1.5171109437942505), (0.0, 1.5171334743499756), (0.0, 1.5171751976013184), (0.0, 1.5172897577285767), (0.0, 1.5174041986465454), (0.0, 1.5180765390396118), (0.0, 1.518272876739502), (0.0, 1.5184483528137207), (0.0, 1.5186474323272705), (0.0, 1.5188498497009277), (0.0, 1.5190072059631348), (0.0, 1.5190088748931885), (0.0, 1.5192291736602783), (0.0, 1.5196168422698975), (0.0, 1.5196609497070312), (0.0, 1.5197257995605469), (0.0, 1.51986825466156), (0.0, 1.5199869871139526), (0.0, 1.5199880599975586), (0.0, 1.5202916860580444), (0.0, 1.5204744338989258), (0.0, 1.5205724239349365), (0.0, 1.5208572149276733), (0.0, 1.520877480506897), (0.0, 1.5209084749221802), (0.0, 1.520918607711792), (0.0, 1.5209718942642212), (0.0, 1.5209813117980957), (0.0, 1.5210604667663574), (0.0, 1.52109694480896), (0.0, 1.5212005376815796), (0.0, 1.521213412284851), (0.0, 1.5212990045547485), (0.0, 1.5214509963989258), (0.0, 1.521552562713623), (0.0, 1.521593451499939), (0.0, 1.5217347145080566), (0.0, 1.5217740535736084), (0.0, 1.521789312362671), (0.0, 1.5219347476959229), (0.0, 1.5219545364379883), (0.0, 1.5219757556915283), (0.0, 1.522111177444458), (0.0, 1.5221198797225952), (0.0, 1.522146224975586), (0.0, 1.5221977233886719), (0.0, 1.5224045515060425), (0.0, 1.5225762128829956), (0.0, 1.5226103067398071), (0.0, 1.5226835012435913), (0.0, 1.5227793455123901), (0.0, 1.522794485092163), (0.0, 1.5229275226593018), (0.0, 1.5229336023330688), (0.0, 1.5230194330215454), (0.0, 1.523024082183838), (0.0, 1.5231072902679443), (0.0, 1.5231419801712036), (0.0, 1.5231496095657349), (0.0, 1.5233088731765747), (0.0, 1.523327112197876), (0.0, 1.52349853515625), (0.0, 1.523683786392212), (0.0, 1.523717999458313), (0.0, 1.5237340927124023), (0.0, 1.523867130279541), (0.0, 1.5240018367767334), (0.0, 1.524021863937378), (0.0, 1.5240767002105713), (0.0, 1.524470567703247), (0.0, 1.524595022201538), (0.0, 1.5246320962905884), (0.0, 1.5247893333435059), (0.0, 1.5253268480300903), (0.0, 1.5254764556884766), (0.0, 1.5254817008972168), (0.0, 1.525566577911377), (0.0, 1.5256125926971436), (0.0, 1.5257854461669922), (0.0, 1.5259556770324707), (0.0, 1.5261541604995728), (0.0, 1.5262354612350464), (0.0, 1.5264191627502441), (0.0, 1.5266783237457275), (0.0, 1.5267311334609985), (0.0, 1.5273255109786987), (0.0, 1.5274479389190674), (0.0, 1.5280168056488037), (0.0, 1.5281600952148438), (0.0, 1.5283312797546387), (0.0, 1.5283528566360474), (0.0, 1.5285900831222534), (0.0, 1.528633713722229), (0.0, 1.528660774230957), (0.0, 1.5289326906204224), (0.0, 1.5290179252624512), (0.0, 1.5291410684585571), (0.0, 1.5291987657546997), (0.0, 1.5292683839797974), (0.0, 1.5293076038360596), (0.0, 1.5294846296310425), (0.0, 1.5297024250030518), (0.0, 1.5298973321914673), (0.0, 1.5300368070602417), (0.0, 1.5301820039749146), (0.0, 1.5310338735580444), (0.0, 1.5314910411834717), (0.0, 1.5319464206695557), (0.0, 1.5326507091522217), (0.0, 1.5521775484085083), (0.0, 1.578110694885254)], [(8.009257316589355, 8.416391372680664), (7.030032157897949, 7.609999656677246), (6.9428486824035645, 7.354151248931885), (6.824067115783691, 7.08651065826416), (6.721407890319824, 7.349061012268066), (6.511362552642822, 7.017471790313721), (6.399810314178467, 6.43348503112793), (6.384853839874268, 7.015645980834961), (6.243244647979736, 7.354604244232178), (6.170167446136475, 7.111111640930176), (6.0644636154174805, 7.907661437988281), (5.911779880523682, 6.487556457519531), (5.807677745819092, 6.014275550842285), (5.728028774261475, 5.929218769073486), (5.6249213218688965, 8.699658393859863), (5.406749248504639, 5.429253578186035), (5.3758440017700195, 5.494477272033691), (5.226830005645752, 8.334491729736328), (5.133606433868408, 8.500247955322266), (5.1284942626953125, 6.60561466217041), (4.960318088531494, 7.097895622253418), (4.8201751708984375, 4.83487606048584), (4.47058629989624, 7.82676887512207), (4.38984489440918, 4.404280185699463), (4.1027021408081055, 7.969915390014648), (4.085930347442627, 4.287341594696045), (4.02437686920166, 4.2329511642456055), (3.978311777114868, 4.690016269683838), (3.881343364715576, 4.031144618988037), (3.84386944770813, 8.63011360168457), (3.7591378688812256, 4.062479019165039)], [(9.776909828186035, 10.326501846313477), (9.185976028442383, 10.730690002441406), (8.501564025878906, 8.568811416625977), (8.38476276397705, 8.400544166564941)]]
[array([[0.        , 1.14468217],
       [0.        , 1.3088125 ],
       [0.        , 1.31577253],
       [0.        , 1.32218277],
       [0.        , 1.32280898],
       [0.        , 1.32399225],
       [0.        , 1.32541084],
       [0.        , 1.32561076],
       [0.        , 1.3260715 ],
       [0.        , 1.3266443 ],
       [0.        , 1.32716262],
       [0.        , 1.32731378],
       [0.        , 1.32747281],
       [0.        , 1.32765138],
       [0.        , 1.32798874],
       [0.        , 1.32814848],
       [0.        , 1.32853365],
       [0.        , 1.32886219],
       [0.        , 1.32899213],
       [0.        , 1.3290329 ],
       [0.        , 1.3291527 ],
       [0.        , 1.3293997 ],
       [0.        , 1.32943749],
       [0.        , 1.32957685],
       [0.        , 1.32984316],
       [0.        , 1.32991123],
       [0.        , 1.33017659],
       [0.        , 1.33032179],
       [0.        , 1.33048141],
       [0.        , 1.33079624],
       [0.        , 1.33084536],
       [0.        , 1.33085787],
       [0.        , 1.33089554],
       [0.        , 1.33100986],
       [0.        , 1.3312335 ],
       [0.        , 1.33138776],
       [0.        , 1.33144259],
       [0.        , 1.33161056],
       [0.        , 1.33173943],
       [0.        , 1.33185852],
       [0.        , 1.33186567],
       [0.        , 1.33204508],
       [0.        , 1.33224654],
       [0.        , 1.33248043],
       [0.        , 1.33248711],
       [0.        , 1.33259976],
       [0.        , 1.33265936],
       [0.        , 1.33266723],
       [0.        , 1.33279026],
       [0.        , 1.33307242],
       [0.        , 1.33312654],
       [0.        , 1.33314228],
       [0.        , 1.33321917],
       [0.        , 1.33328247],
       [0.        , 1.33337188],
       [0.        , 1.33337545],
       [0.        , 1.33338988],
       [0.        , 1.33350778],
       [0.        , 1.33365893],
       [0.        , 1.33377361],
       [0.        , 1.33386862],
       [0.        , 1.33390677],
       [0.        , 1.33404529],
       [0.        , 1.33412373],
       [0.        , 1.33414936],
       [0.        , 1.33416581],
       [0.        , 1.33417499],
       [0.        , 1.33433783],
       [0.        , 1.33447504],
       [0.        , 1.3345685 ],
       [0.        , 1.33456957],
       [0.        , 1.33458495],
       [0.        , 1.33459318],
       [0.        , 1.33472395],
       [0.        , 1.33474004],
       [0.        , 1.33479559],
       [0.        , 1.33496261],
       [0.        , 1.33501363],
       [0.        , 1.33507657],
       [0.        , 1.33508325],
       [0.        , 1.33522868],
       [0.        , 1.33524728],
       [0.        , 1.33528674],
       [0.        , 1.33536685],
       [0.        , 1.33537054],
       [0.        , 1.33542705],
       [0.        , 1.33549678],
       [0.        , 1.33562362],
       [0.        , 1.33593535],
       [0.        , 1.3361305 ],
       [0.        , 1.3363955 ],
       [0.        , 1.33641696],
       [0.        , 1.33652413],
       [0.        , 1.33656693],
       [0.        , 1.33664584],
       [0.        , 1.33664632],
       [0.        , 1.33680332],
       [0.        , 1.33695209],
       [0.        , 1.3369745 ],
       [0.        , 1.33719742],
       [0.        , 1.3373208 ],
       [0.        , 1.33735168],
       [0.        , 1.33755815],
       [0.        , 1.3376025 ],
       [0.        , 1.33760893],
       [0.        , 1.33764315],
       [0.        , 1.3380233 ],
       [0.        , 1.33817041],
       [0.        , 1.33830404],
       [0.        , 1.33831871],
       [0.        , 1.33857751],
       [0.        , 1.33858287],
       [0.        , 1.33862007],
       [0.        , 1.33883584],
       [0.        , 1.33884895],
       [0.        , 1.33905852],
       [0.        , 1.33926332],
       [0.        , 1.33952463],
       [0.        , 1.33998084],
       [0.        , 1.34014046],
       [0.        , 1.34147084],
       [0.        , 1.34214365],
       [0.        , 1.34253561],
       [0.        , 1.34333432],
       [0.        , 1.36336207],
       [0.        , 1.42302477],
       [0.        , 1.4427197 ],
       [0.        , 1.44797993],
       [0.        , 1.44890964],
       [0.        , 1.44945419],
       [0.        , 1.44970107],
       [0.        , 1.45022428],
       [0.        , 1.45039415],
       [0.        , 1.45043635],
       [0.        , 1.45137346],
       [0.        , 1.4515692 ],
       [0.        , 1.45175183],
       [0.        , 1.45240605],
       [0.        , 1.45267344],
       [0.        , 1.45289397],
       [0.        , 1.45310354],
       [0.        , 1.45314682],
       [0.        , 1.45341623],
       [0.        , 1.45404029],
       [0.        , 1.45413506],
       [0.        , 1.45436311],
       [0.        , 1.45439065],
       [0.        , 1.45441794],
       [0.        , 1.45449746],
       [0.        , 1.45457506],
       [0.        , 1.45457888],
       [0.        , 1.45478213],
       [0.        , 1.45533073],
       [0.        , 1.45565784],
       [0.        , 1.45582497],
       [0.        , 1.45592284],
       [0.        , 1.4559797 ],
       [0.        , 1.45607448],
       [0.        , 1.45610809],
       [0.        , 1.45621526],
       [0.        , 1.45641601],
       [0.        , 1.45658195],
       [0.        , 1.45660233],
       [0.        , 1.4566319 ],
       [0.        , 1.45680261],
       [0.        , 1.45680642],
       [0.        , 1.45691872],
       [0.        , 1.45701313],
       [0.        , 1.45707226],
       [0.        , 1.45717299],
       [0.        , 1.45723295],
       [0.        , 1.45731699],
       [0.        , 1.45736516],
       [0.        , 1.4574548 ],
       [0.        , 1.45752311],
       [0.        , 1.45754647],
       [0.        , 1.45757723],
       [0.        , 1.4576019 ],
       [0.        , 1.45767081],
       [0.        , 1.45777261],
       [0.        , 1.45779216],
       [0.        , 1.45780087],
       [0.        , 1.45800841],
       [0.        , 1.45813417],
       [0.        , 1.4583149 ],
       [0.        , 1.45845211],
       [0.        , 1.45848227],
       [0.        , 1.45857131],
       [0.        , 1.45871294],
       [0.        , 1.45880592],
       [0.        , 1.45899856],
       [0.        , 1.45909965],
       [0.        , 1.45925391],
       [0.        , 1.45928431],
       [0.        , 1.45928514],
       [0.        , 1.45950782],
       [0.        , 1.45950937],
       [0.        , 1.45957088],
       [0.        , 1.4597038 ],
       [0.        , 1.45984006],
       [0.        , 1.45989501],
       [0.        , 1.46000838],
       [0.        , 1.46016264],
       [0.        , 1.46034241],
       [0.        , 1.46041417],
       [0.        , 1.46044099],
       [0.        , 1.4605763 ],
       [0.        , 1.46062839],
       [0.        , 1.46077049],
       [0.        , 1.46078014],
       [0.        , 1.46099007],
       [0.        , 1.46100688],
       [0.        , 1.46109819],
       [0.        , 1.46117842],
       [0.        , 1.46121335],
       [0.        , 1.46121645],
       [0.        , 1.46137547],
       [0.        , 1.46141171],
       [0.        , 1.46146548],
       [0.        , 1.46157312],
       [0.        , 1.46190667],
       [0.        , 1.46196353],
       [0.        , 1.46202183],
       [0.        , 1.46202254],
       [0.        , 1.46207452],
       [0.        , 1.46208894],
       [0.        , 1.46213067],
       [0.        , 1.46216881],
       [0.        , 1.46217525],
       [0.        , 1.46221316],
       [0.        , 1.46258187],
       [0.        , 1.46258414],
       [0.        , 1.462623  ],
       [0.        , 1.46273029],
       [0.        , 1.46274185],
       [0.        , 1.46331513],
       [0.        , 1.46369636],
       [0.        , 1.4641794 ],
       [0.        , 1.46490335],
       [0.        , 1.46607018],
       [0.        , 1.4661262 ],
       [0.        , 1.46648693],
       [0.        , 1.46655691],
       [0.        , 1.46789515],
       [0.        , 1.46837616],
       [0.        , 1.46877456],
       [0.        , 1.4720403 ],
       [0.        , 1.47250199],
       [0.        , 1.47426021],
       [0.        , 1.50222063],
       [0.        , 1.5084585 ],
       [0.        , 1.5113349 ],
       [0.        , 1.51271069],
       [0.        , 1.51326394],
       [0.        , 1.51445878],
       [0.        , 1.51455021],
       [0.        , 1.51520801],
       [0.        , 1.51558447],
       [0.        , 1.51600659],
       [0.        , 1.51615989],
       [0.        , 1.51659131],
       [0.        , 1.51684737],
       [0.        , 1.51688635],
       [0.        , 1.51711094],
       [0.        , 1.51713347],
       [0.        , 1.5171752 ],
       [0.        , 1.51728976],
       [0.        , 1.5174042 ],
       [0.        , 1.51807654],
       [0.        , 1.51827288],
       [0.        , 1.51844835],
       [0.        , 1.51864743],
       [0.        , 1.51884985],
       [0.        , 1.51900721],
       [0.        , 1.51900887],
       [0.        , 1.51922917],
       [0.        , 1.51961684],
       [0.        , 1.51966095],
       [0.        , 1.5197258 ],
       [0.        , 1.51986825],
       [0.        , 1.51998699],
       [0.        , 1.51998806],
       [0.        , 1.52029169],
       [0.        , 1.52047443],
       [0.        , 1.52057242],
       [0.        , 1.52085721],
       [0.        , 1.52087748],
       [0.        , 1.52090847],
       [0.        , 1.52091861],
       [0.        , 1.52097189],
       [0.        , 1.52098131],
       [0.        , 1.52106047],
       [0.        , 1.52109694],
       [0.        , 1.52120054],
       [0.        , 1.52121341],
       [0.        , 1.521299  ],
       [0.        , 1.521451  ],
       [0.        , 1.52155256],
       [0.        , 1.52159345],
       [0.        , 1.52173471],
       [0.        , 1.52177405],
       [0.        , 1.52178931],
       [0.        , 1.52193475],
       [0.        , 1.52195454],
       [0.        , 1.52197576],
       [0.        , 1.52211118],
       [0.        , 1.52211988],
       [0.        , 1.52214622],
       [0.        , 1.52219772],
       [0.        , 1.52240455],
       [0.        , 1.52257621],
       [0.        , 1.52261031],
       [0.        , 1.5226835 ],
       [0.        , 1.52277935],
       [0.        , 1.52279449],
       [0.        , 1.52292752],
       [0.        , 1.5229336 ],
       [0.        , 1.52301943],
       [0.        , 1.52302408],
       [0.        , 1.52310729],
       [0.        , 1.52314198],
       [0.        , 1.52314961],
       [0.        , 1.52330887],
       [0.        , 1.52332711],
       [0.        , 1.52349854],
       [0.        , 1.52368379],
       [0.        , 1.523718  ],
       [0.        , 1.52373409],
       [0.        , 1.52386713],
       [0.        , 1.52400184],
       [0.        , 1.52402186],
       [0.        , 1.5240767 ],
       [0.        , 1.52447057],
       [0.        , 1.52459502],
       [0.        , 1.5246321 ],
       [0.        , 1.52478933],
       [0.        , 1.52532685],
       [0.        , 1.52547646],
       [0.        , 1.5254817 ],
       [0.        , 1.52556658],
       [0.        , 1.52561259],
       [0.        , 1.52578545],
       [0.        , 1.52595568],
       [0.        , 1.52615416],
       [0.        , 1.52623546],
       [0.        , 1.52641916],
       [0.        , 1.52667832],
       [0.        , 1.52673113],
       [0.        , 1.52732551],
       [0.        , 1.52744794],
       [0.        , 1.52801681],
       [0.        , 1.5281601 ],
       [0.        , 1.52833128],
       [0.        , 1.52835286],
       [0.        , 1.52859008],
       [0.        , 1.52863371],
       [0.        , 1.52866077],
       [0.        , 1.52893269],
       [0.        , 1.52901793],
       [0.        , 1.52914107],
       [0.        , 1.52919877],
       [0.        , 1.52926838],
       [0.        , 1.5293076 ],
       [0.        , 1.52948463],
       [0.        , 1.52970243],
       [0.        , 1.52989733],
       [0.        , 1.53003681],
       [0.        , 1.530182  ],
       [0.        , 1.53103387],
       [0.        , 1.53149104],
       [0.        , 1.53194642],
       [0.        , 1.53265071],
       [0.        , 1.55217755],
       [0.        , 1.57811069]]), array([[8.00925732, 8.41639137],
       [7.03003216, 7.60999966],
       [6.94284868, 7.35415125],
       [6.82406712, 7.08651066],
       [6.72140789, 7.34906101],
       [6.51136255, 7.01747179],
       [6.39981031, 6.43348503],
       [6.38485384, 7.01564598],
       [6.24324465, 7.35460424],
       [6.17016745, 7.11111164],
       [6.06446362, 7.90766144],
       [5.91177988, 6.48755646],
       [5.80767775, 6.01427555],
       [5.72802877, 5.92921877],
       [5.62492132, 8.69965839],
       [5.40674925, 5.42925358],
       [5.375844  , 5.49447727],
       [5.22683001, 8.33449173],
       [5.13360643, 8.50024796],
       [5.12849426, 6.60561466],
       [4.96031809, 7.09789562],
       [4.82017517, 4.83487606],
       [4.4705863 , 7.82676888],
       [4.38984489, 4.40428019],
       [4.10270214, 7.96991539],
       [4.08593035, 4.28734159],
       [4.02437687, 4.23295116],
       [3.97831178, 4.69001627],
       [3.88134336, 4.03114462],
       [3.84386945, 8.6301136 ],
       [3.75913787, 4.06247902]]), array([[ 9.77690983, 10.32650185],
       [ 9.18597603, 10.73069   ],
       [ 8.50156403,  8.56881142],
       [ 8.38476276,  8.40054417]])]2024-03-06 18:01:30.231328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LG6 ph vector generated, counter: 213
2024-03-06 18:01:34.332920: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:01:34.375733: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:01:35.436141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.2551173), (0., 1.3235009), (0., 1.3240288), (0., 1.3263646),
       (0., 1.3273278), (0., 1.3273354), (0., 1.3281583), (0., 1.3284686),
       (0., 1.3286269), (0., 1.3288919), (0., 1.329074 ), (0., 1.3290832),
       (0., 1.3292965), (0., 1.3297119), (0., 1.32973  ), (0., 1.3299005),
       (0., 1.3300799), (0., 1.3304831), (0., 1.3305063), (0., 1.3305774),
       (0., 1.3306078), (0., 1.3306863), (0., 1.3307809), (0., 1.3307953),
       (0., 1.3308203), (0., 1.3309338), (0., 1.3309982), (0., 1.3310847),
       (0., 1.331125 ), (0., 1.3311975), (0., 1.3312273), (0., 1.331274 ),
       (0., 1.331345 ), (0., 1.3313826), (0., 1.3314353), (0., 1.3315022),
       (0., 1.3315291), (0., 1.3315684), (0., 1.3316971), (0., 1.3317137),
       (0., 1.3317199), (0., 1.3317678), (0., 1.3317897), (0., 1.3319393),
       (0., 1.3320036), (0., 1.3321327), (0., 1.3323827), (0., 1.3327675),
       (0., 1.3327677), (0., 1.3328053), (0., 1.332875 ), (0., 1.3329759),
       (0., 1.3329886), (0., 1.3330071), (0., 1.3330073), (0., 1.3332603),
       (0., 1.3334482), (0., 1.333533 ), (0., 1.3335397), (0., 1.3336613),
       (0., 1.3336694), (0., 1.3336766), (0., 1.3337486), (0., 1.3337739),
       (0., 1.3340316), (0., 1.3340634), (0., 1.3341608), (0., 1.334332 ),
       (0., 1.3344128), (0., 1.3344197), (0., 1.334432 ), (0., 1.3344816),
       (0., 1.334529 ), (0., 1.3346063), (0., 1.33467  ), (0., 1.3347324),
       (0., 1.3348098), (0., 1.3349017), (0., 1.3349557), (0., 1.3349837),
       (0., 1.335075 ), (0., 1.3353139), (0., 1.3353952), (0., 1.3354559),
       (0., 1.3355999), (0., 1.3356671), (0., 1.3357135), (0., 1.3357306),
       (0., 1.3358757), (0., 1.3362873), (0., 1.3363653), (0., 1.3363732),
       (0., 1.336569 ), (0., 1.3365967), (0., 1.3366346), (0., 1.3367276),
       (0., 1.3367373), (0., 1.3370311), (0., 1.3370465), (0., 1.3370732),
       (0., 1.3370799), (0., 1.3372837), (0., 1.3374481), (0., 1.3379135),
       (0., 1.3380393), (0., 1.3382716), (0., 1.3386593), (0., 1.3391216),
       (0., 1.339182 ), (0., 1.3392614), (0., 1.33933  ), (0., 1.3395321),
       (0., 1.3405293), (0., 1.3406068), (0., 1.3410074), (0., 1.3410931),
       (0., 1.3410958), (0., 1.3414708), (0., 1.3416185), (0., 1.3416246),
       (0., 1.3418701), (0., 1.3418884), (0., 1.3427204), (0., 1.4025501),
       (0., 1.4459437), (0., 1.4489683), (0., 1.4501562), (0., 1.4510225),
       (0., 1.4520676), (0., 1.4528464), (0., 1.4529284), (0., 1.4530543),
       (0., 1.4530789), (0., 1.453312 ), (0., 1.4544265), (0., 1.4545265),
       (0., 1.455264 ), (0., 1.455279 ), (0., 1.4553393), (0., 1.4554795),
       (0., 1.45554  ), (0., 1.4555435), (0., 1.4557415), (0., 1.4557528),
       (0., 1.4558346), (0., 1.4558537), (0., 1.4559387), (0., 1.456214 ),
       (0., 1.4562274), (0., 1.4563619), (0., 1.456546 ), (0., 1.4566476),
       (0., 1.4568298), (0., 1.4568645), (0., 1.4571453), (0., 1.4571526),
       (0., 1.4572306), (0., 1.4573072), (0., 1.4573878), (0., 1.4574455),
       (0., 1.4574535), (0., 1.4574865), (0., 1.4575273), (0., 1.4575825),
       (0., 1.4576286), (0., 1.4577315), (0., 1.4578869), (0., 1.4581276),
       (0., 1.4581859), (0., 1.4582378), (0., 1.4582981), (0., 1.4583639),
       (0., 1.4583751), (0., 1.4583775), (0., 1.4584394), (0., 1.4586247),
       (0., 1.4586271), (0., 1.4586571), (0., 1.4588507), (0., 1.4588574),
       (0., 1.4589461), (0., 1.4589809), (0., 1.4590341), (0., 1.459112 ),
       (0., 1.4592668), (0., 1.4593316), (0., 1.459356 ), (0., 1.4594887),
       (0., 1.4596399), (0., 1.4596798), (0., 1.4600836), (0., 1.4601171),
       (0., 1.4601561), (0., 1.4602661), (0., 1.460347 ), (0., 1.4603494),
       (0., 1.4603664), (0., 1.4603703), (0., 1.4604621), (0., 1.4604677),
       (0., 1.4604955), (0., 1.4606712), (0., 1.4606764), (0., 1.4606783),
       (0., 1.4606966), (0., 1.4608505), (0., 1.4609239), (0., 1.4610164),
       (0., 1.4610744), (0., 1.4611393), (0., 1.4613606), (0., 1.4616182),
       (0., 1.4619141), (0., 1.4620154), (0., 1.4620453), (0., 1.4621148),
       (0., 1.462163 ), (0., 1.4621965), (0., 1.4623716), (0., 1.4626054),
       (0., 1.4626406), (0., 1.462877 ), (0., 1.4630196), (0., 1.4631957),
       (0., 1.4632531), (0., 1.4637959), (0., 1.464711 ), (0., 1.4647378),
       (0., 1.46479  ), (0., 1.4652035), (0., 1.4653077), (0., 1.4657964),
       (0., 1.4660128), (0., 1.4662675), (0., 1.4665804), (0., 1.4669206),
       (0., 1.4669303), (0., 1.467834 ), (0., 1.4681467), (0., 1.4682173),
       (0., 1.4684821), (0., 1.4685346), (0., 1.4691591), (0., 1.4695799),
       (0., 1.4698356), (0., 1.470263 ), (0., 1.4713633), (0., 1.474238 ),
       (0., 1.4755211), (0., 1.5095491), (0., 1.5144148), (0., 1.515069 ),
       (0., 1.5165424), (0., 1.5170925), (0., 1.5173856), (0., 1.5174303),
       (0., 1.5174575), (0., 1.5175328), (0., 1.5176697), (0., 1.517899 ),
       (0., 1.5180992), (0., 1.5185281), (0., 1.5187807), (0., 1.5187927),
       (0., 1.518834 ), (0., 1.519163 ), (0., 1.519192 ), (0., 1.5193428),
       (0., 1.5193964), (0., 1.5194505), (0., 1.5196445), (0., 1.5196646),
       (0., 1.5196891), (0., 1.5197024), (0., 1.5198009), (0., 1.5198721),
       (0., 1.5200052), (0., 1.5200608), (0., 1.5203409), (0., 1.520472 ),
       (0., 1.5204825), (0., 1.5205194), (0., 1.5206989), (0., 1.5208569),
       (0., 1.5212169), (0., 1.5212599), (0., 1.5212742), (0., 1.5213455),
       (0., 1.5213563), (0., 1.5213618), (0., 1.5213842), (0., 1.5215741),
       (0., 1.5216554), (0., 1.5216783), (0., 1.521729 ), (0., 1.5217295),
       (0., 1.5217824), (0., 1.5218999), (0., 1.5219239), (0., 1.5219519),
       (0., 1.5220397), (0., 1.5221305), (0., 1.5221901), (0., 1.52231  ),
       (0., 1.5223621), (0., 1.5225233), (0., 1.5226326), (0., 1.5226889),
       (0., 1.5227075), (0., 1.5227208), (0., 1.5227692), (0., 1.5228878),
       (0., 1.5229934), (0., 1.5230249), (0., 1.523122 ), (0., 1.5232135),
       (0., 1.5232475), (0., 1.5233145), (0., 1.5233426), (0., 1.5234659),
       (0., 1.5234789), (0., 1.5236517), (0., 1.5236688), (0., 1.5237217),
       (0., 1.5239439), (0., 1.5239564), (0., 1.523991 ), (0., 1.5241541),
       (0., 1.524163 ), (0., 1.524238 ), (0., 1.5242432), (0., 1.5242622),
       (0., 1.5242652), (0., 1.52441  ), (0., 1.5245211), (0., 1.5245574),
       (0., 1.524561 ), (0., 1.5245686), (0., 1.5246333), (0., 1.5247005),
       (0., 1.5248756), (0., 1.5249891), (0., 1.5250003), (0., 1.5252495),
       (0., 1.5253046), (0., 1.5254679), (0., 1.5258263), (0., 1.5260433),
       (0., 1.5260684), (0., 1.5265621), (0., 1.5265664), (0., 1.5266788),
       (0., 1.5267725), (0., 1.5269271), (0., 1.5272137), (0., 1.5272856),
       (0., 1.5274063), (0., 1.5274503), (0., 1.5279593), (0., 1.5284274),
       (0., 1.5284517), (0., 1.5286024), (0., 1.5289435), (0., 1.5290483),
       (0., 1.5292375), (0., 1.5293279), (0., 1.5301558), (0., 1.5303373),
       (0., 1.5307128), (0., 1.5307751), (0., 1.5311219), (0., 1.5315136),
       (0., 1.5318491), (0., 1.5325204)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.548391 , 8.597025 ), (7.9414163, 8.3544035),
       (7.058866 , 7.062139 ), (7.048495 , 7.6342025),
       (6.8415685, 7.1139026), (6.773953 , 7.4390907),
       (6.7174816, 7.1646295), (6.5340595, 7.0738273),
       (6.4135413, 7.0883117), (6.2211137, 7.901931 ),
       (6.15871  , 7.149448 ), (6.0855556, 6.1361804),
       (6.07234  , 7.2678394), (6.030202 , 6.5207686),
       (5.949979 , 6.0165896), (5.806181 , 5.953148 ),
       (5.7696824, 8.547955 ), (5.4397454, 5.5700827),
       (5.2866826, 8.30151  ), (5.190166 , 8.413542 ),
       (5.0863547, 6.6674714), (4.9423866, 7.090342 ),
       (4.598176 , 4.812283 ), (4.443327 , 7.8882318),
       (4.382181 , 4.482707 ), (4.3340034, 8.017355 ),
       (4.039142 , 4.2795906), (4.0318875, 4.2564044),
       (3.9976149, 4.781004 ), (3.8759882, 4.3020277),
       (3.7020032, 8.787334 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.624559 , 10.1697   ), (9.166948 , 10.547907 ),
       (8.380153 ,  8.4502325), (8.317655 ,  8.41108  ),
       (7.6731644,  7.8200746)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.2551172971725464), (0.0, 1.3235008716583252), (0.0, 1.3240288496017456), (0.0, 1.3263646364212036), (0.0, 1.327327847480774), (0.0, 1.3273353576660156), (0.0, 1.3281582593917847), (0.0, 1.3284685611724854), (0.0, 1.3286268711090088), (0.0, 1.3288918733596802), (0.0, 1.3290740251541138), (0.0, 1.3290832042694092), (0.0, 1.3292964696884155), (0.0, 1.3297119140625), (0.0, 1.3297300338745117), (0.0, 1.3299005031585693), (0.0, 1.3300799131393433), (0.0, 1.330483078956604), (0.0, 1.3305063247680664), (0.0, 1.3305773735046387), (0.0, 1.3306077718734741), (0.0, 1.330686330795288), (0.0, 1.3307808637619019), (0.0, 1.3307952880859375), (0.0, 1.3308203220367432), (0.0, 1.3309338092803955), (0.0, 1.330998182296753), (0.0, 1.3310847282409668), (0.0, 1.331125020980835), (0.0, 1.3311975002288818), (0.0, 1.3312273025512695), (0.0, 1.3312740325927734), (0.0, 1.3313449621200562), (0.0, 1.3313826322555542), (0.0, 1.3314353227615356), (0.0, 1.3315021991729736), (0.0, 1.331529140472412), (0.0, 1.3315683603286743), (0.0, 1.3316971063613892), (0.0, 1.3317136764526367), (0.0, 1.3317198753356934), (0.0, 1.3317677974700928), (0.0, 1.3317897319793701), (0.0, 1.3319393396377563), (0.0, 1.3320035934448242), (0.0, 1.3321326971054077), (0.0, 1.3323826789855957), (0.0, 1.3327674865722656), (0.0, 1.3327677249908447), (0.0, 1.3328052759170532), (0.0, 1.3328750133514404), (0.0, 1.3329758644104004), (0.0, 1.3329886198043823), (0.0, 1.3330070972442627), (0.0, 1.3330073356628418), (0.0, 1.3332602977752686), (0.0, 1.3334481716156006), (0.0, 1.3335330486297607), (0.0, 1.3335397243499756), (0.0, 1.3336613178253174), (0.0, 1.3336694240570068), (0.0, 1.3336765766143799), (0.0, 1.3337485790252686), (0.0, 1.3337738513946533), (0.0, 1.334031581878662), (0.0, 1.3340634107589722), (0.0, 1.3341608047485352), (0.0, 1.33433198928833), (0.0, 1.3344128131866455), (0.0, 1.3344197273254395), (0.0, 1.3344320058822632), (0.0, 1.3344815969467163), (0.0, 1.3345290422439575), (0.0, 1.3346062898635864), (0.0, 1.3346699476242065), (0.0, 1.3347324132919312), (0.0, 1.3348097801208496), (0.0, 1.3349016904830933), (0.0, 1.3349556922912598), (0.0, 1.3349837064743042), (0.0, 1.3350750207901), (0.0, 1.3353139162063599), (0.0, 1.3353952169418335), (0.0, 1.3354558944702148), (0.0, 1.3355998992919922), (0.0, 1.3356671333312988), (0.0, 1.335713505744934), (0.0, 1.3357305526733398), (0.0, 1.3358757495880127), (0.0, 1.336287260055542), (0.0, 1.3363653421401978), (0.0, 1.336373209953308), (0.0, 1.3365689516067505), (0.0, 1.3365967273712158), (0.0, 1.336634635925293), (0.0, 1.3367276191711426), (0.0, 1.3367372751235962), (0.0, 1.3370311260223389), (0.0, 1.337046504020691), (0.0, 1.3370732069015503), (0.0, 1.3370798826217651), (0.0, 1.337283730506897), (0.0, 1.3374481201171875), (0.0, 1.3379135131835938), (0.0, 1.3380392789840698), (0.0, 1.3382716178894043), (0.0, 1.3386592864990234), (0.0, 1.3391215801239014), (0.0, 1.3391820192337036), (0.0, 1.3392614126205444), (0.0, 1.3393299579620361), (0.0, 1.3395321369171143), (0.0, 1.3405293226242065), (0.0, 1.3406068086624146), (0.0, 1.3410073518753052), (0.0, 1.3410930633544922), (0.0, 1.3410958051681519), (0.0, 1.3414708375930786), (0.0, 1.341618537902832), (0.0, 1.3416246175765991), (0.0, 1.3418700695037842), (0.0, 1.341888427734375), (0.0, 1.34272038936615), (0.0, 1.4025501012802124), (0.0, 1.4459437131881714), (0.0, 1.4489682912826538), (0.0, 1.4501562118530273), (0.0, 1.4510225057601929), (0.0, 1.4520676136016846), (0.0, 1.4528464078903198), (0.0, 1.4529284238815308), (0.0, 1.4530543088912964), (0.0, 1.4530788660049438), (0.0, 1.4533120393753052), (0.0, 1.4544265270233154), (0.0, 1.4545265436172485), (0.0, 1.4552639722824097), (0.0, 1.455278992652893), (0.0, 1.4553393125534058), (0.0, 1.4554795026779175), (0.0, 1.4555399417877197), (0.0, 1.4555435180664062), (0.0, 1.45574152469635), (0.0, 1.4557528495788574), (0.0, 1.4558346271514893), (0.0, 1.4558537006378174), (0.0, 1.455938696861267), (0.0, 1.4562139511108398), (0.0, 1.456227421760559), (0.0, 1.4563618898391724), (0.0, 1.4565459489822388), (0.0, 1.4566476345062256), (0.0, 1.4568297863006592), (0.0, 1.4568644762039185), (0.0, 1.4571453332901), (0.0, 1.4571526050567627), (0.0, 1.457230567932129), (0.0, 1.45730721950531), (0.0, 1.4573878049850464), (0.0, 1.457445502281189), (0.0, 1.4574534893035889), (0.0, 1.4574865102767944), (0.0, 1.4575272798538208), (0.0, 1.4575824737548828), (0.0, 1.457628607749939), (0.0, 1.4577314853668213), (0.0, 1.4578869342803955), (0.0, 1.4581276178359985), (0.0, 1.4581859111785889), (0.0, 1.4582377672195435), (0.0, 1.4582980871200562), (0.0, 1.4583638906478882), (0.0, 1.458375096321106), (0.0, 1.458377480506897), (0.0, 1.4584393501281738), (0.0, 1.4586247205734253), (0.0, 1.4586271047592163), (0.0, 1.458657145500183), (0.0, 1.4588507413864136), (0.0, 1.4588574171066284), (0.0, 1.4589461088180542), (0.0, 1.458980917930603), (0.0, 1.4590340852737427), (0.0, 1.4591120481491089), (0.0, 1.4592667818069458), (0.0, 1.4593316316604614), (0.0, 1.4593559503555298), (0.0, 1.4594887495040894), (0.0, 1.4596399068832397), (0.0, 1.4596798419952393), (0.0, 1.4600836038589478), (0.0, 1.4601171016693115), (0.0, 1.4601560831069946), (0.0, 1.46026611328125), (0.0, 1.460347056388855), (0.0, 1.460349440574646), (0.0, 1.4603663682937622), (0.0, 1.4603703022003174), (0.0, 1.4604620933532715), (0.0, 1.4604676961898804), (0.0, 1.4604954719543457), (0.0, 1.4606711864471436), (0.0, 1.4606764316558838), (0.0, 1.4606783390045166), (0.0, 1.4606965780258179), (0.0, 1.460850477218628), (0.0, 1.4609239101409912), (0.0, 1.4610164165496826), (0.0, 1.4610743522644043), (0.0, 1.4611393213272095), (0.0, 1.4613605737686157), (0.0, 1.461618185043335), (0.0, 1.4619140625), (0.0, 1.4620153903961182), (0.0, 1.4620453119277954), (0.0, 1.4621148109436035), (0.0, 1.462162971496582), (0.0, 1.4621964693069458), (0.0, 1.462371587753296), (0.0, 1.462605357170105), (0.0, 1.462640643119812), (0.0, 1.4628770351409912), (0.0, 1.463019609451294), (0.0, 1.4631956815719604), (0.0, 1.463253140449524), (0.0, 1.4637959003448486), (0.0, 1.4647109508514404), (0.0, 1.4647377729415894), (0.0, 1.4647899866104126), (0.0, 1.4652035236358643), (0.0, 1.4653077125549316), (0.0, 1.4657963514328003), (0.0, 1.4660128355026245), (0.0, 1.466267466545105), (0.0, 1.4665803909301758), (0.0, 1.4669206142425537), (0.0, 1.4669302701950073), (0.0, 1.4678339958190918), (0.0, 1.4681466817855835), (0.0, 1.4682172536849976), (0.0, 1.4684821367263794), (0.0, 1.4685345888137817), (0.0, 1.4691591262817383), (0.0, 1.4695799350738525), (0.0, 1.469835638999939), (0.0, 1.4702630043029785), (0.0, 1.4713633060455322), (0.0, 1.4742380380630493), (0.0, 1.4755210876464844), (0.0, 1.5095491409301758), (0.0, 1.5144147872924805), (0.0, 1.5150690078735352), (0.0, 1.5165424346923828), (0.0, 1.5170924663543701), (0.0, 1.5173856019973755), (0.0, 1.517430305480957), (0.0, 1.5174574851989746), (0.0, 1.5175328254699707), (0.0, 1.517669677734375), (0.0, 1.5178990364074707), (0.0, 1.5180991888046265), (0.0, 1.5185281038284302), (0.0, 1.5187807083129883), (0.0, 1.518792748451233), (0.0, 1.5188339948654175), (0.0, 1.5191630125045776), (0.0, 1.5191919803619385), (0.0, 1.5193427801132202), (0.0, 1.519396424293518), (0.0, 1.5194505453109741), (0.0, 1.5196444988250732), (0.0, 1.5196646451950073), (0.0, 1.5196890830993652), (0.0, 1.519702434539795), (0.0, 1.5198009014129639), (0.0, 1.5198720693588257), (0.0, 1.520005226135254), (0.0, 1.5200607776641846), (0.0, 1.520340919494629), (0.0, 1.5204720497131348), (0.0, 1.5204825401306152), (0.0, 1.5205193758010864), (0.0, 1.52069890499115), (0.0, 1.5208568572998047), (0.0, 1.521216869354248), (0.0, 1.5212599039077759), (0.0, 1.521274209022522), (0.0, 1.5213454961776733), (0.0, 1.5213563442230225), (0.0, 1.5213618278503418), (0.0, 1.5213842391967773), (0.0, 1.5215741395950317), (0.0, 1.5216554403305054), (0.0, 1.5216783285140991), (0.0, 1.5217289924621582), (0.0, 1.5217294692993164), (0.0, 1.521782398223877), (0.0, 1.521899938583374), (0.0, 1.5219238996505737), (0.0, 1.5219519138336182), (0.0, 1.5220396518707275), (0.0, 1.5221304893493652), (0.0, 1.5221900939941406), (0.0, 1.5223100185394287), (0.0, 1.5223621129989624), (0.0, 1.522523283958435), (0.0, 1.5226325988769531), (0.0, 1.522688865661621), (0.0, 1.522707462310791), (0.0, 1.5227208137512207), (0.0, 1.5227692127227783), (0.0, 1.5228878259658813), (0.0, 1.5229934453964233), (0.0, 1.5230249166488647), (0.0, 1.523121953010559), (0.0, 1.523213505744934), (0.0, 1.523247480392456), (0.0, 1.5233144760131836), (0.0, 1.5233426094055176), (0.0, 1.523465871810913), (0.0, 1.5234788656234741), (0.0, 1.5236517190933228), (0.0, 1.5236687660217285), (0.0, 1.523721694946289), (0.0, 1.5239439010620117), (0.0, 1.5239564180374146), (0.0, 1.5239909887313843), (0.0, 1.5241540670394897), (0.0, 1.524163007736206), (0.0, 1.5242379903793335), (0.0, 1.5242432355880737), (0.0, 1.5242621898651123), (0.0, 1.524265170097351), (0.0, 1.5244100093841553), (0.0, 1.5245211124420166), (0.0, 1.52455735206604), (0.0, 1.5245610475540161), (0.0, 1.5245685577392578), (0.0, 1.5246332883834839), (0.0, 1.5247005224227905), (0.0, 1.5248756408691406), (0.0, 1.524989128112793), (0.0, 1.5250003337860107), (0.0, 1.5252494812011719), (0.0, 1.5253045558929443), (0.0, 1.525467872619629), (0.0, 1.525826334953308), (0.0, 1.5260432958602905), (0.0, 1.5260684490203857), (0.0, 1.5265620946884155), (0.0, 1.5265663862228394), (0.0, 1.5266788005828857), (0.0, 1.5267724990844727), (0.0, 1.52692711353302), (0.0, 1.5272136926651), (0.0, 1.5272855758666992), (0.0, 1.5274063348770142), (0.0, 1.5274503231048584), (0.0, 1.5279593467712402), (0.0, 1.5284273624420166), (0.0, 1.528451681137085), (0.0, 1.5286023616790771), (0.0, 1.5289435386657715), (0.0, 1.5290483236312866), (0.0, 1.5292375087738037), (0.0, 1.5293278694152832), (0.0, 1.5301557779312134), (0.0, 1.5303373336791992), (0.0, 1.5307128429412842), (0.0, 1.5307750701904297), (0.0, 1.531121850013733), (0.0, 1.5315135717391968), (0.0, 1.5318491458892822), (0.0, 1.5325204133987427)], [(8.548391342163086, 8.597024917602539), (7.941416263580322, 8.354403495788574), (7.058866024017334, 7.06213903427124), (7.048494815826416, 7.634202480316162), (6.841568470001221, 7.113902568817139), (6.773952960968018, 7.439090728759766), (6.71748161315918, 7.1646294593811035), (6.534059524536133, 7.073827266693115), (6.413541316986084, 7.088311672210693), (6.221113681793213, 7.901930809020996), (6.15871000289917, 7.149447917938232), (6.085555553436279, 6.136180400848389), (6.07234001159668, 7.267839431762695), (6.0302019119262695, 6.520768642425537), (5.949978828430176, 6.016589641571045), (5.806180953979492, 5.953147888183594), (5.76968240737915, 8.547954559326172), (5.4397454261779785, 5.570082664489746), (5.286682605743408, 8.301509857177734), (5.190165996551514, 8.413541793823242), (5.086354732513428, 6.667471408843994), (4.942386627197266, 7.090342044830322), (4.598176002502441, 4.812283039093018), (4.443326950073242, 7.8882317543029785), (4.382181167602539, 4.4827070236206055), (4.334003448486328, 8.017354965209961), (4.03914213180542, 4.279590606689453), (4.031887531280518, 4.256404399871826), (3.997614860534668, 4.781003952026367), (3.875988245010376, 4.302027702331543), (3.702003240585327, 8.787334442138672)], [(9.62455940246582, 10.169699668884277), (9.166948318481445, 10.547906875610352), (8.380152702331543, 8.45023250579834), (8.317654609680176, 8.411080360412598), (7.673164367675781, 7.820074558258057)]]
[array([[0.        , 1.2551173 ],
       [0.        , 1.32350087],
       [0.        , 1.32402885],
       [0.        , 1.32636464],
       [0.        , 1.32732785],
       [0.        , 1.32733536],
       [0.        , 1.32815826],
       [0.        , 1.32846856],
       [0.        , 1.32862687],
       [0.        , 1.32889187],
       [0.        , 1.32907403],
       [0.        , 1.3290832 ],
       [0.        , 1.32929647],
       [0.        , 1.32971191],
       [0.        , 1.32973003],
       [0.        , 1.3299005 ],
       [0.        , 1.33007991],
       [0.        , 1.33048308],
       [0.        , 1.33050632],
       [0.        , 1.33057737],
       [0.        , 1.33060777],
       [0.        , 1.33068633],
       [0.        , 1.33078086],
       [0.        , 1.33079529],
       [0.        , 1.33082032],
       [0.        , 1.33093381],
       [0.        , 1.33099818],
       [0.        , 1.33108473],
       [0.        , 1.33112502],
       [0.        , 1.3311975 ],
       [0.        , 1.3312273 ],
       [0.        , 1.33127403],
       [0.        , 1.33134496],
       [0.        , 1.33138263],
       [0.        , 1.33143532],
       [0.        , 1.3315022 ],
       [0.        , 1.33152914],
       [0.        , 1.33156836],
       [0.        , 1.33169711],
       [0.        , 1.33171368],
       [0.        , 1.33171988],
       [0.        , 1.3317678 ],
       [0.        , 1.33178973],
       [0.        , 1.33193934],
       [0.        , 1.33200359],
       [0.        , 1.3321327 ],
       [0.        , 1.33238268],
       [0.        , 1.33276749],
       [0.        , 1.33276772],
       [0.        , 1.33280528],
       [0.        , 1.33287501],
       [0.        , 1.33297586],
       [0.        , 1.33298862],
       [0.        , 1.3330071 ],
       [0.        , 1.33300734],
       [0.        , 1.3332603 ],
       [0.        , 1.33344817],
       [0.        , 1.33353305],
       [0.        , 1.33353972],
       [0.        , 1.33366132],
       [0.        , 1.33366942],
       [0.        , 1.33367658],
       [0.        , 1.33374858],
       [0.        , 1.33377385],
       [0.        , 1.33403158],
       [0.        , 1.33406341],
       [0.        , 1.3341608 ],
       [0.        , 1.33433199],
       [0.        , 1.33441281],
       [0.        , 1.33441973],
       [0.        , 1.33443201],
       [0.        , 1.3344816 ],
       [0.        , 1.33452904],
       [0.        , 1.33460629],
       [0.        , 1.33466995],
       [0.        , 1.33473241],
       [0.        , 1.33480978],
       [0.        , 1.33490169],
       [0.        , 1.33495569],
       [0.        , 1.33498371],
       [0.        , 1.33507502],
       [0.        , 1.33531392],
       [0.        , 1.33539522],
       [0.        , 1.33545589],
       [0.        , 1.3355999 ],
       [0.        , 1.33566713],
       [0.        , 1.33571351],
       [0.        , 1.33573055],
       [0.        , 1.33587575],
       [0.        , 1.33628726],
       [0.        , 1.33636534],
       [0.        , 1.33637321],
       [0.        , 1.33656895],
       [0.        , 1.33659673],
       [0.        , 1.33663464],
       [0.        , 1.33672762],
       [0.        , 1.33673728],
       [0.        , 1.33703113],
       [0.        , 1.3370465 ],
       [0.        , 1.33707321],
       [0.        , 1.33707988],
       [0.        , 1.33728373],
       [0.        , 1.33744812],
       [0.        , 1.33791351],
       [0.        , 1.33803928],
       [0.        , 1.33827162],
       [0.        , 1.33865929],
       [0.        , 1.33912158],
       [0.        , 1.33918202],
       [0.        , 1.33926141],
       [0.        , 1.33932996],
       [0.        , 1.33953214],
       [0.        , 1.34052932],
       [0.        , 1.34060681],
       [0.        , 1.34100735],
       [0.        , 1.34109306],
       [0.        , 1.34109581],
       [0.        , 1.34147084],
       [0.        , 1.34161854],
       [0.        , 1.34162462],
       [0.        , 1.34187007],
       [0.        , 1.34188843],
       [0.        , 1.34272039],
       [0.        , 1.4025501 ],
       [0.        , 1.44594371],
       [0.        , 1.44896829],
       [0.        , 1.45015621],
       [0.        , 1.45102251],
       [0.        , 1.45206761],
       [0.        , 1.45284641],
       [0.        , 1.45292842],
       [0.        , 1.45305431],
       [0.        , 1.45307887],
       [0.        , 1.45331204],
       [0.        , 1.45442653],
       [0.        , 1.45452654],
       [0.        , 1.45526397],
       [0.        , 1.45527899],
       [0.        , 1.45533931],
       [0.        , 1.4554795 ],
       [0.        , 1.45553994],
       [0.        , 1.45554352],
       [0.        , 1.45574152],
       [0.        , 1.45575285],
       [0.        , 1.45583463],
       [0.        , 1.4558537 ],
       [0.        , 1.4559387 ],
       [0.        , 1.45621395],
       [0.        , 1.45622742],
       [0.        , 1.45636189],
       [0.        , 1.45654595],
       [0.        , 1.45664763],
       [0.        , 1.45682979],
       [0.        , 1.45686448],
       [0.        , 1.45714533],
       [0.        , 1.45715261],
       [0.        , 1.45723057],
       [0.        , 1.45730722],
       [0.        , 1.4573878 ],
       [0.        , 1.4574455 ],
       [0.        , 1.45745349],
       [0.        , 1.45748651],
       [0.        , 1.45752728],
       [0.        , 1.45758247],
       [0.        , 1.45762861],
       [0.        , 1.45773149],
       [0.        , 1.45788693],
       [0.        , 1.45812762],
       [0.        , 1.45818591],
       [0.        , 1.45823777],
       [0.        , 1.45829809],
       [0.        , 1.45836389],
       [0.        , 1.4583751 ],
       [0.        , 1.45837748],
       [0.        , 1.45843935],
       [0.        , 1.45862472],
       [0.        , 1.4586271 ],
       [0.        , 1.45865715],
       [0.        , 1.45885074],
       [0.        , 1.45885742],
       [0.        , 1.45894611],
       [0.        , 1.45898092],
       [0.        , 1.45903409],
       [0.        , 1.45911205],
       [0.        , 1.45926678],
       [0.        , 1.45933163],
       [0.        , 1.45935595],
       [0.        , 1.45948875],
       [0.        , 1.45963991],
       [0.        , 1.45967984],
       [0.        , 1.4600836 ],
       [0.        , 1.4601171 ],
       [0.        , 1.46015608],
       [0.        , 1.46026611],
       [0.        , 1.46034706],
       [0.        , 1.46034944],
       [0.        , 1.46036637],
       [0.        , 1.4603703 ],
       [0.        , 1.46046209],
       [0.        , 1.4604677 ],
       [0.        , 1.46049547],
       [0.        , 1.46067119],
       [0.        , 1.46067643],
       [0.        , 1.46067834],
       [0.        , 1.46069658],
       [0.        , 1.46085048],
       [0.        , 1.46092391],
       [0.        , 1.46101642],
       [0.        , 1.46107435],
       [0.        , 1.46113932],
       [0.        , 1.46136057],
       [0.        , 1.46161819],
       [0.        , 1.46191406],
       [0.        , 1.46201539],
       [0.        , 1.46204531],
       [0.        , 1.46211481],
       [0.        , 1.46216297],
       [0.        , 1.46219647],
       [0.        , 1.46237159],
       [0.        , 1.46260536],
       [0.        , 1.46264064],
       [0.        , 1.46287704],
       [0.        , 1.46301961],
       [0.        , 1.46319568],
       [0.        , 1.46325314],
       [0.        , 1.4637959 ],
       [0.        , 1.46471095],
       [0.        , 1.46473777],
       [0.        , 1.46478999],
       [0.        , 1.46520352],
       [0.        , 1.46530771],
       [0.        , 1.46579635],
       [0.        , 1.46601284],
       [0.        , 1.46626747],
       [0.        , 1.46658039],
       [0.        , 1.46692061],
       [0.        , 1.46693027],
       [0.        , 1.467834  ],
       [0.        , 1.46814668],
       [0.        , 1.46821725],
       [0.        , 1.46848214],
       [0.        , 1.46853459],
       [0.        , 1.46915913],
       [0.        , 1.46957994],
       [0.        , 1.46983564],
       [0.        , 1.470263  ],
       [0.        , 1.47136331],
       [0.        , 1.47423804],
       [0.        , 1.47552109],
       [0.        , 1.50954914],
       [0.        , 1.51441479],
       [0.        , 1.51506901],
       [0.        , 1.51654243],
       [0.        , 1.51709247],
       [0.        , 1.5173856 ],
       [0.        , 1.51743031],
       [0.        , 1.51745749],
       [0.        , 1.51753283],
       [0.        , 1.51766968],
       [0.        , 1.51789904],
       [0.        , 1.51809919],
       [0.        , 1.5185281 ],
       [0.        , 1.51878071],
       [0.        , 1.51879275],
       [0.        , 1.51883399],
       [0.        , 1.51916301],
       [0.        , 1.51919198],
       [0.        , 1.51934278],
       [0.        , 1.51939642],
       [0.        , 1.51945055],
       [0.        , 1.5196445 ],
       [0.        , 1.51966465],
       [0.        , 1.51968908],
       [0.        , 1.51970243],
       [0.        , 1.5198009 ],
       [0.        , 1.51987207],
       [0.        , 1.52000523],
       [0.        , 1.52006078],
       [0.        , 1.52034092],
       [0.        , 1.52047205],
       [0.        , 1.52048254],
       [0.        , 1.52051938],
       [0.        , 1.5206989 ],
       [0.        , 1.52085686],
       [0.        , 1.52121687],
       [0.        , 1.5212599 ],
       [0.        , 1.52127421],
       [0.        , 1.5213455 ],
       [0.        , 1.52135634],
       [0.        , 1.52136183],
       [0.        , 1.52138424],
       [0.        , 1.52157414],
       [0.        , 1.52165544],
       [0.        , 1.52167833],
       [0.        , 1.52172899],
       [0.        , 1.52172947],
       [0.        , 1.5217824 ],
       [0.        , 1.52189994],
       [0.        , 1.5219239 ],
       [0.        , 1.52195191],
       [0.        , 1.52203965],
       [0.        , 1.52213049],
       [0.        , 1.52219009],
       [0.        , 1.52231002],
       [0.        , 1.52236211],
       [0.        , 1.52252328],
       [0.        , 1.5226326 ],
       [0.        , 1.52268887],
       [0.        , 1.52270746],
       [0.        , 1.52272081],
       [0.        , 1.52276921],
       [0.        , 1.52288783],
       [0.        , 1.52299345],
       [0.        , 1.52302492],
       [0.        , 1.52312195],
       [0.        , 1.52321351],
       [0.        , 1.52324748],
       [0.        , 1.52331448],
       [0.        , 1.52334261],
       [0.        , 1.52346587],
       [0.        , 1.52347887],
       [0.        , 1.52365172],
       [0.        , 1.52366877],
       [0.        , 1.52372169],
       [0.        , 1.5239439 ],
       [0.        , 1.52395642],
       [0.        , 1.52399099],
       [0.        , 1.52415407],
       [0.        , 1.52416301],
       [0.        , 1.52423799],
       [0.        , 1.52424324],
       [0.        , 1.52426219],
       [0.        , 1.52426517],
       [0.        , 1.52441001],
       [0.        , 1.52452111],
       [0.        , 1.52455735],
       [0.        , 1.52456105],
       [0.        , 1.52456856],
       [0.        , 1.52463329],
       [0.        , 1.52470052],
       [0.        , 1.52487564],
       [0.        , 1.52498913],
       [0.        , 1.52500033],
       [0.        , 1.52524948],
       [0.        , 1.52530456],
       [0.        , 1.52546787],
       [0.        , 1.52582633],
       [0.        , 1.5260433 ],
       [0.        , 1.52606845],
       [0.        , 1.52656209],
       [0.        , 1.52656639],
       [0.        , 1.5266788 ],
       [0.        , 1.5267725 ],
       [0.        , 1.52692711],
       [0.        , 1.52721369],
       [0.        , 1.52728558],
       [0.        , 1.52740633],
       [0.        , 1.52745032],
       [0.        , 1.52795935],
       [0.        , 1.52842736],
       [0.        , 1.52845168],
       [0.        , 1.52860236],
       [0.        , 1.52894354],
       [0.        , 1.52904832],
       [0.        , 1.52923751],
       [0.        , 1.52932787],
       [0.        , 1.53015578],
       [0.        , 1.53033733],
       [0.        , 1.53071284],
       [0.        , 1.53077507],
       [0.        , 1.53112185],
       [0.        , 1.53151357],
       [0.        , 1.53184915],
       [0.        , 1.53252041]]), array([[8.54839134, 8.59702492],
       [7.94141626, 8.3544035 ],
       [7.05886602, 7.06213903],
       [7.04849482, 7.63420248],
       [6.84156847, 7.11390257],
       [6.77395296, 7.43909073],
       [6.71748161, 7.16462946],
       [6.53405952, 7.07382727],
       [6.41354132, 7.08831167],
       [6.22111368, 7.90193081],
       [6.15871   , 7.14944792],
       [6.08555555, 6.1361804 ],
       [6.07234001, 7.26783943],
       [6.03020191, 6.52076864],
       [5.94997883, 6.01658964],
       [5.80618095, 5.95314789],
       [5.76968241, 8.54795456],
       [5.43974543, 5.57008266],
       [5.28668261, 8.30150986],
       [5.190166  , 8.41354179],
       [5.08635473, 6.66747141],
       [4.94238663, 7.09034204],
       [4.598176  , 4.81228304],
       [4.44332695, 7.88823175],
       [4.38218117, 4.48270702],
       [4.33400345, 8.01735497],
       [4.03914213, 4.27959061],
       [4.03188753, 4.2564044 ],
       [3.99761486, 4.78100395],
       [3.87598825, 4.3020277 ],
       [3.70200324, 8.78733444]]), array([[ 9.6245594 , 10.16969967],
       [ 9.16694832, 10.54790688],
       [ 8.3801527 ,  8.45023251],
       [ 8.31765461,  8.41108036],
       [ 7.67316437,  7.82007456]])]2024-03-06 18:01:40.347975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LG7 ph vector generated, counter: 214
2024-03-06 18:01:44.625925: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:01:44.668599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:01:45.621269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3136591), (0., 1.3241524), (0., 1.3251822), (0., 1.3264337),
       (0., 1.3267556), (0., 1.3271536), (0., 1.3275491), (0., 1.3275907),
       (0., 1.3279767), (0., 1.3280234), (0., 1.3281581), (0., 1.3282447),
       (0., 1.3289169), (0., 1.3290148), (0., 1.3292699), (0., 1.329358 ),
       (0., 1.3297377), (0., 1.3301682), (0., 1.3301929), (0., 1.3303065),
       (0., 1.3304157), (0., 1.3304505), (0., 1.3306445), (0., 1.3306752),
       (0., 1.3306955), (0., 1.3310832), (0., 1.3311518), (0., 1.3311819),
       (0., 1.3314191), (0., 1.3314521), (0., 1.3315033), (0., 1.331909 ),
       (0., 1.3319361), (0., 1.3319814), (0., 1.332035 ), (0., 1.3320918),
       (0., 1.3322816), (0., 1.3323127), (0., 1.3323653), (0., 1.3324119),
       (0., 1.332419 ), (0., 1.3324772), (0., 1.3325266), (0., 1.3325541),
       (0., 1.3326012), (0., 1.3326613), (0., 1.3327138), (0., 1.3327184),
       (0., 1.3327887), (0., 1.3328171), (0., 1.3328245), (0., 1.3330425),
       (0., 1.3331429), (0., 1.3332283), (0., 1.3336006), (0., 1.3336093),
       (0., 1.3336945), (0., 1.333711 ), (0., 1.3337803), (0., 1.3338398),
       (0., 1.3338746), (0., 1.3340561), (0., 1.3341851), (0., 1.3342365),
       (0., 1.334286 ), (0., 1.3342981), (0., 1.3344412), (0., 1.3344704),
       (0., 1.3346052), (0., 1.3347652), (0., 1.3347942), (0., 1.3349029),
       (0., 1.3349054), (0., 1.3349994), (0., 1.3350241), (0., 1.3351581),
       (0., 1.3351663), (0., 1.3351736), (0., 1.3353622), (0., 1.3353834),
       (0., 1.335388 ), (0., 1.3354135), (0., 1.335514 ), (0., 1.335535 ),
       (0., 1.3355901), (0., 1.3356649), (0., 1.3356838), (0., 1.3358355),
       (0., 1.3359629), (0., 1.3360432), (0., 1.3360577), (0., 1.3360778),
       (0., 1.3361124), (0., 1.3361747), (0., 1.3361827), (0., 1.3362885),
       (0., 1.3365631), (0., 1.3367409), (0., 1.33685  ), (0., 1.3369657),
       (0., 1.3371536), (0., 1.3372605), (0., 1.3373878), (0., 1.3375949),
       (0., 1.3380301), (0., 1.3383406), (0., 1.3384101), (0., 1.3384951),
       (0., 1.3386048), (0., 1.3387469), (0., 1.3389608), (0., 1.3390609),
       (0., 1.3395468), (0., 1.3396646), (0., 1.3397167), (0., 1.34007  ),
       (0., 1.3405006), (0., 1.3415374), (0., 1.3421887), (0., 1.3423059),
       (0., 1.3426403), (0., 1.346469 ), (0., 1.3511524), (0., 1.4083889),
       (0., 1.4414663), (0., 1.450231 ), (0., 1.450876 ), (0., 1.4516101),
       (0., 1.4519678), (0., 1.4532888), (0., 1.453609 ), (0., 1.453831 ),
       (0., 1.4539534), (0., 1.4541143), (0., 1.454158 ), (0., 1.4541597),
       (0., 1.4542465), (0., 1.4542933), (0., 1.4543607), (0., 1.4543941),
       (0., 1.4546378), (0., 1.4547212), (0., 1.4548584), (0., 1.4550362),
       (0., 1.4551702), (0., 1.4551791), (0., 1.4557402), (0., 1.4559045),
       (0., 1.4560817), (0., 1.456165 ), (0., 1.4562378), (0., 1.4563047),
       (0., 1.4564633), (0., 1.4565246), (0., 1.4565635), (0., 1.4565744),
       (0., 1.4566613), (0., 1.4566622), (0., 1.4566742), (0., 1.4570606),
       (0., 1.4571974), (0., 1.4574926), (0., 1.45751  ), (0., 1.4577522),
       (0., 1.4579622), (0., 1.4579656), (0., 1.4579729), (0., 1.4579759),
       (0., 1.4580829), (0., 1.4581689), (0., 1.4581912), (0., 1.458213 ),
       (0., 1.4582169), (0., 1.4582533), (0., 1.458274 ), (0., 1.4583725),
       (0., 1.4583791), (0., 1.4584382), (0., 1.4585186), (0., 1.4588757),
       (0., 1.4588968), (0., 1.4589796), (0., 1.4591213), (0., 1.4592532),
       (0., 1.459254 ), (0., 1.4593171), (0., 1.4593259), (0., 1.459546 ),
       (0., 1.4595853), (0., 1.4597566), (0., 1.459797 ), (0., 1.4597983),
       (0., 1.4598267), (0., 1.4598359), (0., 1.459859 ), (0., 1.4598633),
       (0., 1.4598935), (0., 1.4599421), (0., 1.4599465), (0., 1.4600219),
       (0., 1.4600642), (0., 1.4600772), (0., 1.4601084), (0., 1.4602606),
       (0., 1.4602757), (0., 1.4604441), (0., 1.4605337), (0., 1.4605343),
       (0., 1.4605516), (0., 1.4605719), (0., 1.4605777), (0., 1.4606386),
       (0., 1.4607356), (0., 1.460772 ), (0., 1.460988 ), (0., 1.4611053),
       (0., 1.4614568), (0., 1.4618529), (0., 1.4619036), (0., 1.4619343),
       (0., 1.4619917), (0., 1.4621307), (0., 1.4623231), (0., 1.4623289),
       (0., 1.4625125), (0., 1.4627174), (0., 1.4627544), (0., 1.4628086),
       (0., 1.4632156), (0., 1.4635342), (0., 1.4640726), (0., 1.464138 ),
       (0., 1.4642403), (0., 1.4645289), (0., 1.4647167), (0., 1.4652474),
       (0., 1.465366 ), (0., 1.4655362), (0., 1.4656241), (0., 1.4665582),
       (0., 1.4677726), (0., 1.468043 ), (0., 1.4684261), (0., 1.4691238),
       (0., 1.4691733), (0., 1.4703186), (0., 1.4721022), (0., 1.4737245),
       (0., 1.4769799), (0., 1.5022647), (0., 1.5109494), (0., 1.5137022),
       (0., 1.5140928), (0., 1.5145069), (0., 1.5153458), (0., 1.5154248),
       (0., 1.5159879), (0., 1.5160434), (0., 1.5161375), (0., 1.516155 ),
       (0., 1.5165203), (0., 1.5169088), (0., 1.5171818), (0., 1.5174361),
       (0., 1.5177608), (0., 1.5179765), (0., 1.5180736), (0., 1.5183617),
       (0., 1.5186689), (0., 1.5190766), (0., 1.5190775), (0., 1.5191426),
       (0., 1.5193907), (0., 1.5195373), (0., 1.5195462), (0., 1.5196089),
       (0., 1.5196872), (0., 1.5197816), (0., 1.5199718), (0., 1.5202602),
       (0., 1.5202698), (0., 1.5204743), (0., 1.5205522), (0., 1.5206596),
       (0., 1.5206858), (0., 1.5208238), (0., 1.5209377), (0., 1.5209559),
       (0., 1.5209659), (0., 1.5211458), (0., 1.5212365), (0., 1.5212641),
       (0., 1.5212739), (0., 1.5212828), (0., 1.5213023), (0., 1.5213283),
       (0., 1.5213299), (0., 1.5214207), (0., 1.5214261), (0., 1.5214742),
       (0., 1.5214814), (0., 1.5215374), (0., 1.521538 ), (0., 1.5216877),
       (0., 1.5216999), (0., 1.5217052), (0., 1.5217676), (0., 1.5219392),
       (0., 1.5220954), (0., 1.5222169), (0., 1.5222803), (0., 1.5225836),
       (0., 1.5226903), (0., 1.5226916), (0., 1.5227053), (0., 1.5227833),
       (0., 1.5228444), (0., 1.522903 ), (0., 1.5229785), (0., 1.5230733),
       (0., 1.5230854), (0., 1.5230973), (0., 1.5231148), (0., 1.5232179),
       (0., 1.5233763), (0., 1.5234199), (0., 1.5234469), (0., 1.5234611),
       (0., 1.523523 ), (0., 1.5236963), (0., 1.5237381), (0., 1.5238087),
       (0., 1.5238441), (0., 1.5238754), (0., 1.5239336), (0., 1.5239545),
       (0., 1.5240269), (0., 1.5241042), (0., 1.5241807), (0., 1.5243227),
       (0., 1.5243415), (0., 1.5244486), (0., 1.5245072), (0., 1.5245278),
       (0., 1.524594 ), (0., 1.524691 ), (0., 1.5248529), (0., 1.5250863),
       (0., 1.5255904), (0., 1.5257369), (0., 1.5258164), (0., 1.5259033),
       (0., 1.525918 ), (0., 1.5259424), (0., 1.5261185), (0., 1.5262142),
       (0., 1.5262249), (0., 1.5262761), (0., 1.5263855), (0., 1.5264211),
       (0., 1.5264287), (0., 1.5267241), (0., 1.5269554), (0., 1.5269728),
       (0., 1.5270482), (0., 1.5271415), (0., 1.5271418), (0., 1.5273215),
       (0., 1.5286238), (0., 1.5294762), (0., 1.5303129), (0., 1.5310068),
       (0., 1.5310796), (0., 1.5313652)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.482063 , 8.552541 ), (7.997282 , 8.400183 ),
       (7.0868196, 7.6074247), (6.7147083, 6.9931083),
       (6.7121673, 7.1365266), (6.643417 , 7.2361746),
       (6.4775314, 6.8844123), (6.42291  , 7.0853486),
       (6.1772494, 7.853896 ), (6.162478 , 7.095627 ),
       (5.9517207, 7.2458487), (5.941772 , 6.497645 ),
       (5.838324 , 6.0363493), (5.6326694, 5.8424983),
       (5.607192 , 8.69119  ), (5.5440235, 5.6394315),
       (5.368506 , 8.314703 ), (5.3641872, 8.344167 ),
       (5.06433  , 6.800629 ), (4.9810176, 5.003058 ),
       (4.9617815, 7.050435 ), (4.453658 , 4.7926183),
       (4.379138 , 7.854076 ), (4.3661504, 4.495945 ),
       (4.3183956, 7.899284 ), (4.032456 , 4.2589645),
       (3.9656107, 4.791879 ), (3.9618516, 4.094858 ),
       (3.9158494, 4.280482 ), (3.6215372, 8.691237 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.522661 ,  9.586365), (9.501103 , 10.143137),
       (9.16799  , 10.493284), (8.325213 ,  8.440941),
       (8.2800865,  8.321653), (6.701456 ,  6.775496)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3136590719223022), (0.0, 1.3241523504257202), (0.0, 1.3251821994781494), (0.0, 1.3264336585998535), (0.0, 1.3267556428909302), (0.0, 1.3271535634994507), (0.0, 1.3275490999221802), (0.0, 1.3275907039642334), (0.0, 1.3279767036437988), (0.0, 1.3280234336853027), (0.0, 1.3281581401824951), (0.0, 1.328244686126709), (0.0, 1.3289169073104858), (0.0, 1.329014778137207), (0.0, 1.3292698860168457), (0.0, 1.3293579816818237), (0.0, 1.329737663269043), (0.0, 1.3301682472229004), (0.0, 1.3301929235458374), (0.0, 1.3303065299987793), (0.0, 1.3304157257080078), (0.0, 1.3304505348205566), (0.0, 1.3306444883346558), (0.0, 1.3306752443313599), (0.0, 1.3306955099105835), (0.0, 1.3310831785202026), (0.0, 1.3311518430709839), (0.0, 1.3311818838119507), (0.0, 1.3314191102981567), (0.0, 1.3314521312713623), (0.0, 1.3315032720565796), (0.0, 1.331908941268921), (0.0, 1.3319361209869385), (0.0, 1.3319814205169678), (0.0, 1.332034945487976), (0.0, 1.3320918083190918), (0.0, 1.3322815895080566), (0.0, 1.3323127031326294), (0.0, 1.3323652744293213), (0.0, 1.3324118852615356), (0.0, 1.3324190378189087), (0.0, 1.3324772119522095), (0.0, 1.3325265645980835), (0.0, 1.3325541019439697), (0.0, 1.3326011896133423), (0.0, 1.3326612710952759), (0.0, 1.3327138423919678), (0.0, 1.3327183723449707), (0.0, 1.3327887058258057), (0.0, 1.3328170776367188), (0.0, 1.332824468612671), (0.0, 1.3330425024032593), (0.0, 1.333142876625061), (0.0, 1.333228349685669), (0.0, 1.333600640296936), (0.0, 1.3336093425750732), (0.0, 1.3336944580078125), (0.0, 1.33371102809906), (0.0, 1.333780288696289), (0.0, 1.333839774131775), (0.0, 1.3338745832443237), (0.0, 1.3340561389923096), (0.0, 1.3341851234436035), (0.0, 1.3342365026474), (0.0, 1.3342859745025635), (0.0, 1.3342981338500977), (0.0, 1.3344411849975586), (0.0, 1.3344703912734985), (0.0, 1.3346052169799805), (0.0, 1.3347651958465576), (0.0, 1.3347941637039185), (0.0, 1.3349028825759888), (0.0, 1.3349053859710693), (0.0, 1.334999442100525), (0.0, 1.335024118423462), (0.0, 1.335158109664917), (0.0, 1.335166335105896), (0.0, 1.3351736068725586), (0.0, 1.335362195968628), (0.0, 1.335383415222168), (0.0, 1.335387945175171), (0.0, 1.3354134559631348), (0.0, 1.335513949394226), (0.0, 1.3355350494384766), (0.0, 1.335590124130249), (0.0, 1.3356648683547974), (0.0, 1.335683822631836), (0.0, 1.3358354568481445), (0.0, 1.3359628915786743), (0.0, 1.3360432386398315), (0.0, 1.3360576629638672), (0.0, 1.3360778093338013), (0.0, 1.336112380027771), (0.0, 1.336174726486206), (0.0, 1.336182713508606), (0.0, 1.3362884521484375), (0.0, 1.3365631103515625), (0.0, 1.3367408514022827), (0.0, 1.3368500471115112), (0.0, 1.3369656801223755), (0.0, 1.3371535539627075), (0.0, 1.3372604846954346), (0.0, 1.3373878002166748), (0.0, 1.3375948667526245), (0.0, 1.3380300998687744), (0.0, 1.3383406400680542), (0.0, 1.3384101390838623), (0.0, 1.338495135307312), (0.0, 1.3386048078536987), (0.0, 1.3387469053268433), (0.0, 1.3389607667922974), (0.0, 1.33906090259552), (0.0, 1.339546799659729), (0.0, 1.3396645784378052), (0.0, 1.3397166728973389), (0.0, 1.3400700092315674), (0.0, 1.3405005931854248), (0.0, 1.341537356376648), (0.0, 1.3421887159347534), (0.0, 1.3423058986663818), (0.0, 1.3426402807235718), (0.0, 1.3464690446853638), (0.0, 1.3511524200439453), (0.0, 1.4083888530731201), (0.0, 1.4414663314819336), (0.0, 1.4502309560775757), (0.0, 1.450875997543335), (0.0, 1.4516100883483887), (0.0, 1.4519678354263306), (0.0, 1.4532887935638428), (0.0, 1.4536089897155762), (0.0, 1.4538309574127197), (0.0, 1.4539533853530884), (0.0, 1.454114317893982), (0.0, 1.4541579484939575), (0.0, 1.4541597366333008), (0.0, 1.4542465209960938), (0.0, 1.4542932510375977), (0.0, 1.4543607234954834), (0.0, 1.4543941020965576), (0.0, 1.4546377658843994), (0.0, 1.454721212387085), (0.0, 1.454858422279358), (0.0, 1.4550361633300781), (0.0, 1.4551701545715332), (0.0, 1.4551790952682495), (0.0, 1.455740213394165), (0.0, 1.455904483795166), (0.0, 1.456081748008728), (0.0, 1.4561649560928345), (0.0, 1.45623779296875), (0.0, 1.456304669380188), (0.0, 1.45646333694458), (0.0, 1.4565246105194092), (0.0, 1.4565634727478027), (0.0, 1.4565744400024414), (0.0, 1.456661343574524), (0.0, 1.4566621780395508), (0.0, 1.4566742181777954), (0.0, 1.4570605754852295), (0.0, 1.4571974277496338), (0.0, 1.4574925899505615), (0.0, 1.457509994506836), (0.0, 1.4577522277832031), (0.0, 1.457962155342102), (0.0, 1.457965612411499), (0.0, 1.4579728841781616), (0.0, 1.4579758644104004), (0.0, 1.458082914352417), (0.0, 1.458168864250183), (0.0, 1.458191156387329), (0.0, 1.458212971687317), (0.0, 1.458216905593872), (0.0, 1.458253264427185), (0.0, 1.458274006843567), (0.0, 1.4583724737167358), (0.0, 1.4583791494369507), (0.0, 1.4584381580352783), (0.0, 1.458518624305725), (0.0, 1.4588756561279297), (0.0, 1.4588967561721802), (0.0, 1.458979606628418), (0.0, 1.4591213464736938), (0.0, 1.459253191947937), (0.0, 1.4592540264129639), (0.0, 1.4593170881271362), (0.0, 1.459325909614563), (0.0, 1.4595459699630737), (0.0, 1.4595853090286255), (0.0, 1.45975661277771), (0.0, 1.4597970247268677), (0.0, 1.4597983360290527), (0.0, 1.4598267078399658), (0.0, 1.4598358869552612), (0.0, 1.459859013557434), (0.0, 1.459863305091858), (0.0, 1.4598934650421143), (0.0, 1.459942102432251), (0.0, 1.4599465131759644), (0.0, 1.4600218534469604), (0.0, 1.460064172744751), (0.0, 1.460077166557312), (0.0, 1.4601083993911743), (0.0, 1.4602606296539307), (0.0, 1.460275650024414), (0.0, 1.4604440927505493), (0.0, 1.4605337381362915), (0.0, 1.4605343341827393), (0.0, 1.4605516195297241), (0.0, 1.4605718851089478), (0.0, 1.4605777263641357), (0.0, 1.4606386423110962), (0.0, 1.460735559463501), (0.0, 1.4607720375061035), (0.0, 1.4609880447387695), (0.0, 1.4611053466796875), (0.0, 1.4614567756652832), (0.0, 1.4618529081344604), (0.0, 1.4619035720825195), (0.0, 1.4619343280792236), (0.0, 1.4619916677474976), (0.0, 1.4621306657791138), (0.0, 1.4623230695724487), (0.0, 1.4623289108276367), (0.0, 1.462512493133545), (0.0, 1.4627174139022827), (0.0, 1.4627543687820435), (0.0, 1.462808609008789), (0.0, 1.4632155895233154), (0.0, 1.4635342359542847), (0.0, 1.464072585105896), (0.0, 1.4641380310058594), (0.0, 1.464240312576294), (0.0, 1.4645289182662964), (0.0, 1.4647166728973389), (0.0, 1.465247392654419), (0.0, 1.465366005897522), (0.0, 1.4655362367630005), (0.0, 1.4656240940093994), (0.0, 1.4665582180023193), (0.0, 1.4677726030349731), (0.0, 1.4680429697036743), (0.0, 1.4684261083602905), (0.0, 1.4691238403320312), (0.0, 1.4691733121871948), (0.0, 1.4703185558319092), (0.0, 1.472102165222168), (0.0, 1.4737244844436646), (0.0, 1.4769798517227173), (0.0, 1.5022647380828857), (0.0, 1.5109493732452393), (0.0, 1.513702154159546), (0.0, 1.5140928030014038), (0.0, 1.5145069360733032), (0.0, 1.515345811843872), (0.0, 1.5154248476028442), (0.0, 1.5159878730773926), (0.0, 1.5160434246063232), (0.0, 1.5161374807357788), (0.0, 1.5161550045013428), (0.0, 1.5165202617645264), (0.0, 1.5169087648391724), (0.0, 1.5171817541122437), (0.0, 1.517436146736145), (0.0, 1.5177607536315918), (0.0, 1.5179765224456787), (0.0, 1.518073558807373), (0.0, 1.5183616876602173), (0.0, 1.5186688899993896), (0.0, 1.5190765857696533), (0.0, 1.5190775394439697), (0.0, 1.5191426277160645), (0.0, 1.5193907022476196), (0.0, 1.519537329673767), (0.0, 1.5195461511611938), (0.0, 1.5196088552474976), (0.0, 1.5196871757507324), (0.0, 1.5197815895080566), (0.0, 1.5199718475341797), (0.0, 1.520260214805603), (0.0, 1.520269751548767), (0.0, 1.5204743146896362), (0.0, 1.520552158355713), (0.0, 1.5206595659255981), (0.0, 1.5206857919692993), (0.0, 1.5208238363265991), (0.0, 1.5209376811981201), (0.0, 1.5209559202194214), (0.0, 1.5209659337997437), (0.0, 1.5211458206176758), (0.0, 1.521236538887024), (0.0, 1.5212640762329102), (0.0, 1.5212738513946533), (0.0, 1.5212827920913696), (0.0, 1.521302342414856), (0.0, 1.521328330039978), (0.0, 1.5213298797607422), (0.0, 1.5214207172393799), (0.0, 1.5214260816574097), (0.0, 1.5214742422103882), (0.0, 1.5214813947677612), (0.0, 1.52153742313385), (0.0, 1.5215380191802979), (0.0, 1.5216877460479736), (0.0, 1.5216999053955078), (0.0, 1.521705150604248), (0.0, 1.5217676162719727), (0.0, 1.5219391584396362), (0.0, 1.5220954418182373), (0.0, 1.5222169160842896), (0.0, 1.5222803354263306), (0.0, 1.5225836038589478), (0.0, 1.5226902961730957), (0.0, 1.5226916074752808), (0.0, 1.522705316543579), (0.0, 1.5227832794189453), (0.0, 1.5228444337844849), (0.0, 1.5229029655456543), (0.0, 1.5229785442352295), (0.0, 1.5230733156204224), (0.0, 1.523085355758667), (0.0, 1.523097276687622), (0.0, 1.523114800453186), (0.0, 1.5232179164886475), (0.0, 1.5233763456344604), (0.0, 1.5234198570251465), (0.0, 1.5234469175338745), (0.0, 1.523461103439331), (0.0, 1.523522973060608), (0.0, 1.5236963033676147), (0.0, 1.523738145828247), (0.0, 1.5238087177276611), (0.0, 1.5238441228866577), (0.0, 1.52387535572052), (0.0, 1.5239336490631104), (0.0, 1.5239545106887817), (0.0, 1.524026870727539), (0.0, 1.5241042375564575), (0.0, 1.5241806507110596), (0.0, 1.524322748184204), (0.0, 1.5243414640426636), (0.0, 1.5244486331939697), (0.0, 1.5245071649551392), (0.0, 1.5245277881622314), (0.0, 1.5245939493179321), (0.0, 1.5246909856796265), (0.0, 1.5248528718948364), (0.0, 1.5250862836837769), (0.0, 1.525590419769287), (0.0, 1.525736927986145), (0.0, 1.5258164405822754), (0.0, 1.525903344154358), (0.0, 1.5259180068969727), (0.0, 1.5259424448013306), (0.0, 1.526118516921997), (0.0, 1.5262142419815063), (0.0, 1.5262248516082764), (0.0, 1.5262761116027832), (0.0, 1.5263855457305908), (0.0, 1.526421070098877), (0.0, 1.5264286994934082), (0.0, 1.526724100112915), (0.0, 1.5269553661346436), (0.0, 1.526972770690918), (0.0, 1.5270482301712036), (0.0, 1.5271414518356323), (0.0, 1.527141809463501), (0.0, 1.527321457862854), (0.0, 1.5286238193511963), (0.0, 1.5294761657714844), (0.0, 1.5303128957748413), (0.0, 1.5310068130493164), (0.0, 1.531079649925232), (0.0, 1.531365156173706)], [(8.482063293457031, 8.55254077911377), (7.997282028198242, 8.400182723999023), (7.086819648742676, 7.607424736022949), (6.71470832824707, 6.99310827255249), (6.712167263031006, 7.136526584625244), (6.643416881561279, 7.236174583435059), (6.477531433105469, 6.8844122886657715), (6.422910213470459, 7.085348606109619), (6.177249431610107, 7.853896141052246), (6.162477970123291, 7.0956268310546875), (5.951720714569092, 7.245848655700684), (5.941771984100342, 6.497644901275635), (5.838324069976807, 6.036349296569824), (5.632669448852539, 5.842498302459717), (5.607192039489746, 8.691189765930176), (5.544023513793945, 5.639431476593018), (5.368505954742432, 8.314702987670898), (5.364187240600586, 8.34416675567627), (5.064330101013184, 6.800629138946533), (4.981017589569092, 5.003057956695557), (4.9617815017700195, 7.0504350662231445), (4.453658103942871, 4.792618274688721), (4.379137992858887, 7.854075908660889), (4.366150379180908, 4.495944976806641), (4.318395614624023, 7.8992838859558105), (4.032455921173096, 4.258964538574219), (3.9656107425689697, 4.791879177093506), (3.9618515968322754, 4.094858169555664), (3.915849447250366, 4.280481815338135), (3.621537208557129, 8.691237449645996)], [(9.522661209106445, 9.58636474609375), (9.501103401184082, 10.143136978149414), (9.167989730834961, 10.493284225463867), (8.325213432312012, 8.440940856933594), (8.280086517333984, 8.321653366088867), (6.701456069946289, 6.775496006011963)]]
[array([[0.        , 1.31365907],
       [0.        , 1.32415235],
       [0.        , 1.3251822 ],
       [0.        , 1.32643366],
       [0.        , 1.32675564],
       [0.        , 1.32715356],
       [0.        , 1.3275491 ],
       [0.        , 1.3275907 ],
       [0.        , 1.3279767 ],
       [0.        , 1.32802343],
       [0.        , 1.32815814],
       [0.        , 1.32824469],
       [0.        , 1.32891691],
       [0.        , 1.32901478],
       [0.        , 1.32926989],
       [0.        , 1.32935798],
       [0.        , 1.32973766],
       [0.        , 1.33016825],
       [0.        , 1.33019292],
       [0.        , 1.33030653],
       [0.        , 1.33041573],
       [0.        , 1.33045053],
       [0.        , 1.33064449],
       [0.        , 1.33067524],
       [0.        , 1.33069551],
       [0.        , 1.33108318],
       [0.        , 1.33115184],
       [0.        , 1.33118188],
       [0.        , 1.33141911],
       [0.        , 1.33145213],
       [0.        , 1.33150327],
       [0.        , 1.33190894],
       [0.        , 1.33193612],
       [0.        , 1.33198142],
       [0.        , 1.33203495],
       [0.        , 1.33209181],
       [0.        , 1.33228159],
       [0.        , 1.3323127 ],
       [0.        , 1.33236527],
       [0.        , 1.33241189],
       [0.        , 1.33241904],
       [0.        , 1.33247721],
       [0.        , 1.33252656],
       [0.        , 1.3325541 ],
       [0.        , 1.33260119],
       [0.        , 1.33266127],
       [0.        , 1.33271384],
       [0.        , 1.33271837],
       [0.        , 1.33278871],
       [0.        , 1.33281708],
       [0.        , 1.33282447],
       [0.        , 1.3330425 ],
       [0.        , 1.33314288],
       [0.        , 1.33322835],
       [0.        , 1.33360064],
       [0.        , 1.33360934],
       [0.        , 1.33369446],
       [0.        , 1.33371103],
       [0.        , 1.33378029],
       [0.        , 1.33383977],
       [0.        , 1.33387458],
       [0.        , 1.33405614],
       [0.        , 1.33418512],
       [0.        , 1.3342365 ],
       [0.        , 1.33428597],
       [0.        , 1.33429813],
       [0.        , 1.33444118],
       [0.        , 1.33447039],
       [0.        , 1.33460522],
       [0.        , 1.3347652 ],
       [0.        , 1.33479416],
       [0.        , 1.33490288],
       [0.        , 1.33490539],
       [0.        , 1.33499944],
       [0.        , 1.33502412],
       [0.        , 1.33515811],
       [0.        , 1.33516634],
       [0.        , 1.33517361],
       [0.        , 1.3353622 ],
       [0.        , 1.33538342],
       [0.        , 1.33538795],
       [0.        , 1.33541346],
       [0.        , 1.33551395],
       [0.        , 1.33553505],
       [0.        , 1.33559012],
       [0.        , 1.33566487],
       [0.        , 1.33568382],
       [0.        , 1.33583546],
       [0.        , 1.33596289],
       [0.        , 1.33604324],
       [0.        , 1.33605766],
       [0.        , 1.33607781],
       [0.        , 1.33611238],
       [0.        , 1.33617473],
       [0.        , 1.33618271],
       [0.        , 1.33628845],
       [0.        , 1.33656311],
       [0.        , 1.33674085],
       [0.        , 1.33685005],
       [0.        , 1.33696568],
       [0.        , 1.33715355],
       [0.        , 1.33726048],
       [0.        , 1.3373878 ],
       [0.        , 1.33759487],
       [0.        , 1.3380301 ],
       [0.        , 1.33834064],
       [0.        , 1.33841014],
       [0.        , 1.33849514],
       [0.        , 1.33860481],
       [0.        , 1.33874691],
       [0.        , 1.33896077],
       [0.        , 1.3390609 ],
       [0.        , 1.3395468 ],
       [0.        , 1.33966458],
       [0.        , 1.33971667],
       [0.        , 1.34007001],
       [0.        , 1.34050059],
       [0.        , 1.34153736],
       [0.        , 1.34218872],
       [0.        , 1.3423059 ],
       [0.        , 1.34264028],
       [0.        , 1.34646904],
       [0.        , 1.35115242],
       [0.        , 1.40838885],
       [0.        , 1.44146633],
       [0.        , 1.45023096],
       [0.        , 1.450876  ],
       [0.        , 1.45161009],
       [0.        , 1.45196784],
       [0.        , 1.45328879],
       [0.        , 1.45360899],
       [0.        , 1.45383096],
       [0.        , 1.45395339],
       [0.        , 1.45411432],
       [0.        , 1.45415795],
       [0.        , 1.45415974],
       [0.        , 1.45424652],
       [0.        , 1.45429325],
       [0.        , 1.45436072],
       [0.        , 1.4543941 ],
       [0.        , 1.45463777],
       [0.        , 1.45472121],
       [0.        , 1.45485842],
       [0.        , 1.45503616],
       [0.        , 1.45517015],
       [0.        , 1.4551791 ],
       [0.        , 1.45574021],
       [0.        , 1.45590448],
       [0.        , 1.45608175],
       [0.        , 1.45616496],
       [0.        , 1.45623779],
       [0.        , 1.45630467],
       [0.        , 1.45646334],
       [0.        , 1.45652461],
       [0.        , 1.45656347],
       [0.        , 1.45657444],
       [0.        , 1.45666134],
       [0.        , 1.45666218],
       [0.        , 1.45667422],
       [0.        , 1.45706058],
       [0.        , 1.45719743],
       [0.        , 1.45749259],
       [0.        , 1.45750999],
       [0.        , 1.45775223],
       [0.        , 1.45796216],
       [0.        , 1.45796561],
       [0.        , 1.45797288],
       [0.        , 1.45797586],
       [0.        , 1.45808291],
       [0.        , 1.45816886],
       [0.        , 1.45819116],
       [0.        , 1.45821297],
       [0.        , 1.45821691],
       [0.        , 1.45825326],
       [0.        , 1.45827401],
       [0.        , 1.45837247],
       [0.        , 1.45837915],
       [0.        , 1.45843816],
       [0.        , 1.45851862],
       [0.        , 1.45887566],
       [0.        , 1.45889676],
       [0.        , 1.45897961],
       [0.        , 1.45912135],
       [0.        , 1.45925319],
       [0.        , 1.45925403],
       [0.        , 1.45931709],
       [0.        , 1.45932591],
       [0.        , 1.45954597],
       [0.        , 1.45958531],
       [0.        , 1.45975661],
       [0.        , 1.45979702],
       [0.        , 1.45979834],
       [0.        , 1.45982671],
       [0.        , 1.45983589],
       [0.        , 1.45985901],
       [0.        , 1.45986331],
       [0.        , 1.45989347],
       [0.        , 1.4599421 ],
       [0.        , 1.45994651],
       [0.        , 1.46002185],
       [0.        , 1.46006417],
       [0.        , 1.46007717],
       [0.        , 1.4601084 ],
       [0.        , 1.46026063],
       [0.        , 1.46027565],
       [0.        , 1.46044409],
       [0.        , 1.46053374],
       [0.        , 1.46053433],
       [0.        , 1.46055162],
       [0.        , 1.46057189],
       [0.        , 1.46057773],
       [0.        , 1.46063864],
       [0.        , 1.46073556],
       [0.        , 1.46077204],
       [0.        , 1.46098804],
       [0.        , 1.46110535],
       [0.        , 1.46145678],
       [0.        , 1.46185291],
       [0.        , 1.46190357],
       [0.        , 1.46193433],
       [0.        , 1.46199167],
       [0.        , 1.46213067],
       [0.        , 1.46232307],
       [0.        , 1.46232891],
       [0.        , 1.46251249],
       [0.        , 1.46271741],
       [0.        , 1.46275437],
       [0.        , 1.46280861],
       [0.        , 1.46321559],
       [0.        , 1.46353424],
       [0.        , 1.46407259],
       [0.        , 1.46413803],
       [0.        , 1.46424031],
       [0.        , 1.46452892],
       [0.        , 1.46471667],
       [0.        , 1.46524739],
       [0.        , 1.46536601],
       [0.        , 1.46553624],
       [0.        , 1.46562409],
       [0.        , 1.46655822],
       [0.        , 1.4677726 ],
       [0.        , 1.46804297],
       [0.        , 1.46842611],
       [0.        , 1.46912384],
       [0.        , 1.46917331],
       [0.        , 1.47031856],
       [0.        , 1.47210217],
       [0.        , 1.47372448],
       [0.        , 1.47697985],
       [0.        , 1.50226474],
       [0.        , 1.51094937],
       [0.        , 1.51370215],
       [0.        , 1.5140928 ],
       [0.        , 1.51450694],
       [0.        , 1.51534581],
       [0.        , 1.51542485],
       [0.        , 1.51598787],
       [0.        , 1.51604342],
       [0.        , 1.51613748],
       [0.        , 1.516155  ],
       [0.        , 1.51652026],
       [0.        , 1.51690876],
       [0.        , 1.51718175],
       [0.        , 1.51743615],
       [0.        , 1.51776075],
       [0.        , 1.51797652],
       [0.        , 1.51807356],
       [0.        , 1.51836169],
       [0.        , 1.51866889],
       [0.        , 1.51907659],
       [0.        , 1.51907754],
       [0.        , 1.51914263],
       [0.        , 1.5193907 ],
       [0.        , 1.51953733],
       [0.        , 1.51954615],
       [0.        , 1.51960886],
       [0.        , 1.51968718],
       [0.        , 1.51978159],
       [0.        , 1.51997185],
       [0.        , 1.52026021],
       [0.        , 1.52026975],
       [0.        , 1.52047431],
       [0.        , 1.52055216],
       [0.        , 1.52065957],
       [0.        , 1.52068579],
       [0.        , 1.52082384],
       [0.        , 1.52093768],
       [0.        , 1.52095592],
       [0.        , 1.52096593],
       [0.        , 1.52114582],
       [0.        , 1.52123654],
       [0.        , 1.52126408],
       [0.        , 1.52127385],
       [0.        , 1.52128279],
       [0.        , 1.52130234],
       [0.        , 1.52132833],
       [0.        , 1.52132988],
       [0.        , 1.52142072],
       [0.        , 1.52142608],
       [0.        , 1.52147424],
       [0.        , 1.52148139],
       [0.        , 1.52153742],
       [0.        , 1.52153802],
       [0.        , 1.52168775],
       [0.        , 1.52169991],
       [0.        , 1.52170515],
       [0.        , 1.52176762],
       [0.        , 1.52193916],
       [0.        , 1.52209544],
       [0.        , 1.52221692],
       [0.        , 1.52228034],
       [0.        , 1.5225836 ],
       [0.        , 1.5226903 ],
       [0.        , 1.52269161],
       [0.        , 1.52270532],
       [0.        , 1.52278328],
       [0.        , 1.52284443],
       [0.        , 1.52290297],
       [0.        , 1.52297854],
       [0.        , 1.52307332],
       [0.        , 1.52308536],
       [0.        , 1.52309728],
       [0.        , 1.5231148 ],
       [0.        , 1.52321792],
       [0.        , 1.52337635],
       [0.        , 1.52341986],
       [0.        , 1.52344692],
       [0.        , 1.5234611 ],
       [0.        , 1.52352297],
       [0.        , 1.5236963 ],
       [0.        , 1.52373815],
       [0.        , 1.52380872],
       [0.        , 1.52384412],
       [0.        , 1.52387536],
       [0.        , 1.52393365],
       [0.        , 1.52395451],
       [0.        , 1.52402687],
       [0.        , 1.52410424],
       [0.        , 1.52418065],
       [0.        , 1.52432275],
       [0.        , 1.52434146],
       [0.        , 1.52444863],
       [0.        , 1.52450716],
       [0.        , 1.52452779],
       [0.        , 1.52459395],
       [0.        , 1.52469099],
       [0.        , 1.52485287],
       [0.        , 1.52508628],
       [0.        , 1.52559042],
       [0.        , 1.52573693],
       [0.        , 1.52581644],
       [0.        , 1.52590334],
       [0.        , 1.52591801],
       [0.        , 1.52594244],
       [0.        , 1.52611852],
       [0.        , 1.52621424],
       [0.        , 1.52622485],
       [0.        , 1.52627611],
       [0.        , 1.52638555],
       [0.        , 1.52642107],
       [0.        , 1.5264287 ],
       [0.        , 1.5267241 ],
       [0.        , 1.52695537],
       [0.        , 1.52697277],
       [0.        , 1.52704823],
       [0.        , 1.52714145],
       [0.        , 1.52714181],
       [0.        , 1.52732146],
       [0.        , 1.52862382],
       [0.        , 1.52947617],
       [0.        , 1.5303129 ],
       [0.        , 1.53100681],
       [0.        , 1.53107965],
       [0.        , 1.53136516]]), array([[8.48206329, 8.55254078],
       [7.99728203, 8.40018272],
       [7.08681965, 7.60742474],
       [6.71470833, 6.99310827],
       [6.71216726, 7.13652658],
       [6.64341688, 7.23617458],
       [6.47753143, 6.88441229],
       [6.42291021, 7.08534861],
       [6.17724943, 7.85389614],
       [6.16247797, 7.09562683],
       [5.95172071, 7.24584866],
       [5.94177198, 6.4976449 ],
       [5.83832407, 6.0363493 ],
       [5.63266945, 5.8424983 ],
       [5.60719204, 8.69118977],
       [5.54402351, 5.63943148],
       [5.36850595, 8.31470299],
       [5.36418724, 8.34416676],
       [5.0643301 , 6.80062914],
       [4.98101759, 5.00305796],
       [4.9617815 , 7.05043507],
       [4.4536581 , 4.79261827],
       [4.37913799, 7.85407591],
       [4.36615038, 4.49594498],
       [4.31839561, 7.89928389],
       [4.03245592, 4.25896454],
       [3.96561074, 4.79187918],
       [3.9618516 , 4.09485817],
       [3.91584945, 4.28048182],
       [3.62153721, 8.69123745]]), array([[ 9.52266121,  9.58636475],
       [ 9.5011034 , 10.14313698],
       [ 9.16798973, 10.49328423],
       [ 8.32521343,  8.44094086],
       [ 8.28008652,  8.32165337],
       [ 6.70145607,  6.77549601]])]2024-03-06 18:01:49.804724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LG8 ph vector generated, counter: 215
2024-03-06 18:01:53.903862: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:01:53.946795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:01:55.044548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3004258), (0., 1.3065522), (0., 1.3120594), (0., 1.3246566),
       (0., 1.3248411), (0., 1.3254787), (0., 1.3256234), (0., 1.3265265),
       (0., 1.3269551), (0., 1.3269846), (0., 1.3273379), (0., 1.3289195),
       (0., 1.3289443), (0., 1.3290088), (0., 1.3293884), (0., 1.3293916),
       (0., 1.3295856), (0., 1.3295888), (0., 1.3297918), (0., 1.329901 ),
       (0., 1.3301157), (0., 1.330182 ), (0., 1.3303735), (0., 1.3303944),
       (0., 1.3304715), (0., 1.3304838), (0., 1.3305646), (0., 1.331032 ),
       (0., 1.3311063), (0., 1.3311787), (0., 1.3311902), (0., 1.3313789),
       (0., 1.3313997), (0., 1.3314179), (0., 1.331505 ), (0., 1.3315784),
       (0., 1.3315959), (0., 1.3316925), (0., 1.3317156), (0., 1.3317181),
       (0., 1.331733 ), (0., 1.3317411), (0., 1.3317912), (0., 1.3319176),
       (0., 1.3320365), (0., 1.3322119), (0., 1.3323972), (0., 1.3324566),
       (0., 1.3326025), (0., 1.332798 ), (0., 1.3329594), (0., 1.3329844),
       (0., 1.3330001), (0., 1.3333445), (0., 1.333361 ), (0., 1.3334068),
       (0., 1.3334804), (0., 1.3335056), (0., 1.3337593), (0., 1.3339334),
       (0., 1.3339394), (0., 1.3339462), (0., 1.3339475), (0., 1.3340142),
       (0., 1.3341144), (0., 1.3341444), (0., 1.3342209), (0., 1.3342255),
       (0., 1.3343196), (0., 1.3345714), (0., 1.3346521), (0., 1.3347064),
       (0., 1.334912 ), (0., 1.3349186), (0., 1.3349719), (0., 1.3350887),
       (0., 1.3351002), (0., 1.3351446), (0., 1.3353071), (0., 1.3353534),
       (0., 1.3354181), (0., 1.3357207), (0., 1.3357671), (0., 1.3357763),
       (0., 1.335853 ), (0., 1.3359376), (0., 1.3360463), (0., 1.3361847),
       (0., 1.3362976), (0., 1.3363284), (0., 1.3363979), (0., 1.3364023),
       (0., 1.3365549), (0., 1.3366541), (0., 1.3366752), (0., 1.3367409),
       (0., 1.3367953), (0., 1.3368857), (0., 1.3369437), (0., 1.3369461),
       (0., 1.3370028), (0., 1.3371334), (0., 1.3372064), (0., 1.337325 ),
       (0., 1.3373733), (0., 1.337484 ), (0., 1.3375577), (0., 1.3377004),
       (0., 1.3377146), (0., 1.3377988), (0., 1.3378589), (0., 1.3381342),
       (0., 1.3384397), (0., 1.3385885), (0., 1.3387136), (0., 1.3388844),
       (0., 1.3397087), (0., 1.3399659), (0., 1.3401351), (0., 1.3405267),
       (0., 1.3423004), (0., 1.3423882), (0., 1.3432959), (0., 1.3436354),
       (0., 1.4450189), (0., 1.4488907), (0., 1.4500107), (0., 1.4504373),
       (0., 1.4505429), (0., 1.4517221), (0., 1.452348 ), (0., 1.4523749),
       (0., 1.452408 ), (0., 1.453057 ), (0., 1.4534187), (0., 1.4539549),
       (0., 1.4540291), (0., 1.4543712), (0., 1.454799 ), (0., 1.454809 ),
       (0., 1.4550363), (0., 1.4551768), (0., 1.4552845), (0., 1.4553292),
       (0., 1.4553756), (0., 1.4554759), (0., 1.455586 ), (0., 1.4555982),
       (0., 1.4556683), (0., 1.4556963), (0., 1.4557201), (0., 1.4557847),
       (0., 1.455805 ), (0., 1.4560431), (0., 1.4563478), (0., 1.4563881),
       (0., 1.4564859), (0., 1.4565731), (0., 1.4567114), (0., 1.4567722),
       (0., 1.4568148), (0., 1.4571264), (0., 1.4571543), (0., 1.4571921),
       (0., 1.4573023), (0., 1.4573071), (0., 1.4573094), (0., 1.4574184),
       (0., 1.4574746), (0., 1.4575405), (0., 1.4577011), (0., 1.4577837),
       (0., 1.4579146), (0., 1.4579502), (0., 1.4581773), (0., 1.4581976),
       (0., 1.4582349), (0., 1.4585533), (0., 1.45859  ), (0., 1.4587044),
       (0., 1.4587146), (0., 1.4587244), (0., 1.4588021), (0., 1.4589239),
       (0., 1.4591739), (0., 1.4593856), (0., 1.4594414), (0., 1.4595749),
       (0., 1.4596035), (0., 1.4596237), (0., 1.459743 ), (0., 1.4598738),
       (0., 1.4599402), (0., 1.4600577), (0., 1.4602028), (0., 1.4603606),
       (0., 1.460411 ), (0., 1.4604743), (0., 1.4606107), (0., 1.4606117),
       (0., 1.4608014), (0., 1.4608716), (0., 1.4609004), (0., 1.4609662),
       (0., 1.4610367), (0., 1.4610623), (0., 1.4611568), (0., 1.4611748),
       (0., 1.4613717), (0., 1.4613756), (0., 1.4614657), (0., 1.4615465),
       (0., 1.461732 ), (0., 1.46178  ), (0., 1.4617937), (0., 1.4618568),
       (0., 1.461865 ), (0., 1.4618734), (0., 1.4619993), (0., 1.4624007),
       (0., 1.4624177), (0., 1.4628187), (0., 1.4630522), (0., 1.4631717),
       (0., 1.4632256), (0., 1.4634938), (0., 1.4637991), (0., 1.4640067),
       (0., 1.4640706), (0., 1.4642571), (0., 1.4645025), (0., 1.4645818),
       (0., 1.4646676), (0., 1.4648459), (0., 1.4649473), (0., 1.4649575),
       (0., 1.4650832), (0., 1.4652048), (0., 1.4655504), (0., 1.4656156),
       (0., 1.4661742), (0., 1.4678004), (0., 1.4682689), (0., 1.4683584),
       (0., 1.4698192), (0., 1.4711138), (0., 1.4749336), (0., 1.4756887),
       (0., 1.5083058), (0., 1.5131755), (0., 1.5133327), (0., 1.5136751),
       (0., 1.5145023), (0., 1.5146223), (0., 1.5155863), (0., 1.5165975),
       (0., 1.5166042), (0., 1.5176209), (0., 1.5178442), (0., 1.5179901),
       (0., 1.5180827), (0., 1.5182236), (0., 1.5182971), (0., 1.5184157),
       (0., 1.5185918), (0., 1.5185982), (0., 1.5186561), (0., 1.518719 ),
       (0., 1.5187835), (0., 1.5188087), (0., 1.518973 ), (0., 1.519122 ),
       (0., 1.5193006), (0., 1.5196042), (0., 1.5198494), (0., 1.5198536),
       (0., 1.5199217), (0., 1.5199422), (0., 1.520162 ), (0., 1.5202281),
       (0., 1.5202899), (0., 1.5203079), (0., 1.5203254), (0., 1.5204073),
       (0., 1.5204312), (0., 1.520497 ), (0., 1.5205745), (0., 1.5207132),
       (0., 1.5208024), (0., 1.5208056), (0., 1.5209186), (0., 1.520947 ),
       (0., 1.5210822), (0., 1.5211902), (0., 1.5212215), (0., 1.5212219),
       (0., 1.5212982), (0., 1.5214765), (0., 1.5215106), (0., 1.5216409),
       (0., 1.521817 ), (0., 1.521986 ), (0., 1.5220408), (0., 1.5221524),
       (0., 1.522205 ), (0., 1.5222093), (0., 1.522212 ), (0., 1.5222278),
       (0., 1.522325 ), (0., 1.5223525), (0., 1.5225002), (0., 1.522693 ),
       (0., 1.5228169), (0., 1.5229839), (0., 1.5231919), (0., 1.5232062),
       (0., 1.5233153), (0., 1.5233383), (0., 1.5233743), (0., 1.5234681),
       (0., 1.5235577), (0., 1.5235645), (0., 1.5238173), (0., 1.5238508),
       (0., 1.5239439), (0., 1.5239557), (0., 1.5240189), (0., 1.5240393),
       (0., 1.5241704), (0., 1.5241936), (0., 1.5243446), (0., 1.5244061),
       (0., 1.5244958), (0., 1.5245423), (0., 1.5245751), (0., 1.5246292),
       (0., 1.5247552), (0., 1.5247866), (0., 1.5250471), (0., 1.5250894),
       (0., 1.5251215), (0., 1.5251338), (0., 1.5251511), (0., 1.5253347),
       (0., 1.5253726), (0., 1.5253776), (0., 1.5254178), (0., 1.525438 ),
       (0., 1.5254549), (0., 1.5254729), (0., 1.5256586), (0., 1.5257889),
       (0., 1.5258224), (0., 1.526213 ), (0., 1.5266345), (0., 1.5269022),
       (0., 1.5272357), (0., 1.5272505), (0., 1.5278779), (0., 1.5282054),
       (0., 1.5285856), (0., 1.5288014), (0., 1.5289696), (0., 1.5290489),
       (0., 1.5295274), (0., 1.5295756), (0., 1.5296048), (0., 1.5296183),
       (0., 1.5299478), (0., 1.5301191), (0., 1.5303785), (0., 1.5326754),
       (0., 1.5337443), (0., 1.553496 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.512526 , 8.529316 ), (7.8800244, 8.392454 ),
       (7.031955 , 7.56029  ), (7.010104 , 7.0177765),
       (6.864896 , 7.2483907), (6.746569 , 7.4323583),
       (6.6696362, 6.9741287), (6.333556 , 7.008752 ),
       (6.316862 , 6.9565053), (6.169785 , 7.035931 ),
       (6.169078 , 7.913817 ), (6.0738583, 6.4423556),
       (6.019864 , 7.334825 ), (5.989191 , 6.0112667),
       (5.7781425, 5.820473 ), (5.605395 , 8.561343 ),
       (5.3683352, 8.332938 ), (5.3147345, 5.4488177),
       (5.214077 , 6.8163104), (5.14314  , 8.484235 ),
       (4.9619274, 7.1506305), (4.440183 , 7.8643694),
       (4.3195024, 4.57122  ), (4.1137824, 7.9191303),
       (3.9832654, 4.137554 ), (3.9562924, 4.2498426),
       (3.9331827, 4.6604743), (3.8692698, 4.3335714),
       (3.600759 , 8.761107 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.471412 , 10.034945), (9.373    ,  9.409411),
       (9.117696 , 10.443892), (8.9581375,  9.064179),
       (8.475093 ,  8.580185), (7.658632 ,  7.742599)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3004257678985596), (0.0, 1.3065521717071533), (0.0, 1.3120594024658203), (0.0, 1.32465660572052), (0.0, 1.3248411417007446), (0.0, 1.3254786729812622), (0.0, 1.3256233930587769), (0.0, 1.3265265226364136), (0.0, 1.3269550800323486), (0.0, 1.3269846439361572), (0.0, 1.3273378610610962), (0.0, 1.328919529914856), (0.0, 1.3289443254470825), (0.0, 1.3290088176727295), (0.0, 1.3293883800506592), (0.0, 1.329391598701477), (0.0, 1.3295855522155762), (0.0, 1.329588770866394), (0.0, 1.329791784286499), (0.0, 1.3299009799957275), (0.0, 1.3301156759262085), (0.0, 1.3301819562911987), (0.0, 1.3303735256195068), (0.0, 1.3303943872451782), (0.0, 1.3304715156555176), (0.0, 1.3304837942123413), (0.0, 1.3305646181106567), (0.0, 1.3310320377349854), (0.0, 1.3311063051223755), (0.0, 1.3311786651611328), (0.0, 1.3311902284622192), (0.0, 1.3313789367675781), (0.0, 1.33139967918396), (0.0, 1.3314179182052612), (0.0, 1.3315049409866333), (0.0, 1.3315783739089966), (0.0, 1.3315958976745605), (0.0, 1.3316924571990967), (0.0, 1.3317155838012695), (0.0, 1.33171808719635), (0.0, 1.331732988357544), (0.0, 1.3317410945892334), (0.0, 1.3317911624908447), (0.0, 1.331917643547058), (0.0, 1.3320364952087402), (0.0, 1.3322118520736694), (0.0, 1.332397222518921), (0.0, 1.3324565887451172), (0.0, 1.3326025009155273), (0.0, 1.3327980041503906), (0.0, 1.3329594135284424), (0.0, 1.332984447479248), (0.0, 1.3330000638961792), (0.0, 1.3333444595336914), (0.0, 1.333361029624939), (0.0, 1.3334068059921265), (0.0, 1.3334803581237793), (0.0, 1.333505630493164), (0.0, 1.3337593078613281), (0.0, 1.3339333534240723), (0.0, 1.3339394330978394), (0.0, 1.3339462280273438), (0.0, 1.3339475393295288), (0.0, 1.3340141773223877), (0.0, 1.3341144323349), (0.0, 1.3341443538665771), (0.0, 1.3342208862304688), (0.0, 1.3342255353927612), (0.0, 1.3343195915222168), (0.0, 1.334571361541748), (0.0, 1.334652066230774), (0.0, 1.334706425666809), (0.0, 1.3349119424819946), (0.0, 1.3349186182022095), (0.0, 1.3349719047546387), (0.0, 1.3350887298583984), (0.0, 1.3351001739501953), (0.0, 1.3351446390151978), (0.0, 1.3353071212768555), (0.0, 1.3353533744812012), (0.0, 1.3354181051254272), (0.0, 1.3357206583023071), (0.0, 1.335767149925232), (0.0, 1.3357763290405273), (0.0, 1.3358529806137085), (0.0, 1.3359376192092896), (0.0, 1.3360463380813599), (0.0, 1.3361847400665283), (0.0, 1.336297631263733), (0.0, 1.336328387260437), (0.0, 1.3363978862762451), (0.0, 1.3364022970199585), (0.0, 1.3365548849105835), (0.0, 1.3366540670394897), (0.0, 1.3366751670837402), (0.0, 1.3367408514022827), (0.0, 1.3367953300476074), (0.0, 1.336885690689087), (0.0, 1.3369437456130981), (0.0, 1.3369461297988892), (0.0, 1.3370027542114258), (0.0, 1.3371334075927734), (0.0, 1.3372063636779785), (0.0, 1.3373249769210815), (0.0, 1.3373732566833496), (0.0, 1.3374840021133423), (0.0, 1.3375576734542847), (0.0, 1.337700366973877), (0.0, 1.3377145528793335), (0.0, 1.337798833847046), (0.0, 1.3378589153289795), (0.0, 1.3381341695785522), (0.0, 1.338439702987671), (0.0, 1.3385884761810303), (0.0, 1.3387136459350586), (0.0, 1.3388843536376953), (0.0, 1.339708685874939), (0.0, 1.3399659395217896), (0.0, 1.340135097503662), (0.0, 1.3405267000198364), (0.0, 1.3423004150390625), (0.0, 1.3423881530761719), (0.0, 1.343295931816101), (0.0, 1.3436354398727417), (0.0, 1.4450188875198364), (0.0, 1.4488906860351562), (0.0, 1.4500106573104858), (0.0, 1.450437307357788), (0.0, 1.45054292678833), (0.0, 1.4517221450805664), (0.0, 1.452347993850708), (0.0, 1.4523749351501465), (0.0, 1.452407956123352), (0.0, 1.453057050704956), (0.0, 1.4534187316894531), (0.0, 1.4539549350738525), (0.0, 1.4540290832519531), (0.0, 1.4543712139129639), (0.0, 1.4547990560531616), (0.0, 1.4548089504241943), (0.0, 1.4550362825393677), (0.0, 1.455176830291748), (0.0, 1.4552844762802124), (0.0, 1.455329179763794), (0.0, 1.4553755521774292), (0.0, 1.455475926399231), (0.0, 1.4555859565734863), (0.0, 1.45559823513031), (0.0, 1.455668330192566), (0.0, 1.4556963443756104), (0.0, 1.455720067024231), (0.0, 1.4557846784591675), (0.0, 1.4558049440383911), (0.0, 1.4560431241989136), (0.0, 1.4563478231430054), (0.0, 1.4563881158828735), (0.0, 1.4564858675003052), (0.0, 1.4565731287002563), (0.0, 1.4567114114761353), (0.0, 1.4567722082138062), (0.0, 1.4568147659301758), (0.0, 1.4571263790130615), (0.0, 1.4571542739868164), (0.0, 1.457192063331604), (0.0, 1.4573023319244385), (0.0, 1.4573071002960205), (0.0, 1.457309365272522), (0.0, 1.457418441772461), (0.0, 1.4574745893478394), (0.0, 1.457540512084961), (0.0, 1.4577010869979858), (0.0, 1.4577836990356445), (0.0, 1.4579145908355713), (0.0, 1.457950234413147), (0.0, 1.4581773281097412), (0.0, 1.4581975936889648), (0.0, 1.4582349061965942), (0.0, 1.4585533142089844), (0.0, 1.458590030670166), (0.0, 1.4587043523788452), (0.0, 1.4587146043777466), (0.0, 1.4587243795394897), (0.0, 1.4588021039962769), (0.0, 1.4589239358901978), (0.0, 1.4591739177703857), (0.0, 1.459385633468628), (0.0, 1.4594414234161377), (0.0, 1.4595749378204346), (0.0, 1.4596035480499268), (0.0, 1.4596236944198608), (0.0, 1.4597430229187012), (0.0, 1.4598737955093384), (0.0, 1.4599401950836182), (0.0, 1.4600577354431152), (0.0, 1.4602028131484985), (0.0, 1.4603606462478638), (0.0, 1.4604109525680542), (0.0, 1.4604742527008057), (0.0, 1.4606107473373413), (0.0, 1.4606117010116577), (0.0, 1.460801362991333), (0.0, 1.4608715772628784), (0.0, 1.4609004259109497), (0.0, 1.4609662294387817), (0.0, 1.4610366821289062), (0.0, 1.4610623121261597), (0.0, 1.4611568450927734), (0.0, 1.4611748456954956), (0.0, 1.461371660232544), (0.0, 1.4613755941390991), (0.0, 1.4614657163619995), (0.0, 1.461546540260315), (0.0, 1.461732029914856), (0.0, 1.4617799520492554), (0.0, 1.4617936611175537), (0.0, 1.4618568420410156), (0.0, 1.461864948272705), (0.0, 1.4618734121322632), (0.0, 1.4619992971420288), (0.0, 1.4624006748199463), (0.0, 1.462417721748352), (0.0, 1.4628187417984009), (0.0, 1.4630521535873413), (0.0, 1.4631717205047607), (0.0, 1.4632256031036377), (0.0, 1.463493824005127), (0.0, 1.4637991189956665), (0.0, 1.4640066623687744), (0.0, 1.4640705585479736), (0.0, 1.4642571210861206), (0.0, 1.4645024538040161), (0.0, 1.464581847190857), (0.0, 1.464667558670044), (0.0, 1.464845895767212), (0.0, 1.4649473428726196), (0.0, 1.4649574756622314), (0.0, 1.4650832414627075), (0.0, 1.4652048349380493), (0.0, 1.465550422668457), (0.0, 1.4656156301498413), (0.0, 1.4661742448806763), (0.0, 1.4678003787994385), (0.0, 1.468268871307373), (0.0, 1.4683583974838257), (0.0, 1.469819188117981), (0.0, 1.4711138010025024), (0.0, 1.4749336242675781), (0.0, 1.4756886959075928), (0.0, 1.5083057880401611), (0.0, 1.5131754875183105), (0.0, 1.513332724571228), (0.0, 1.5136750936508179), (0.0, 1.5145022869110107), (0.0, 1.5146223306655884), (0.0, 1.515586256980896), (0.0, 1.5165975093841553), (0.0, 1.5166041851043701), (0.0, 1.5176209211349487), (0.0, 1.5178442001342773), (0.0, 1.5179901123046875), (0.0, 1.5180827379226685), (0.0, 1.5182236433029175), (0.0, 1.5182970762252808), (0.0, 1.5184156894683838), (0.0, 1.5185917615890503), (0.0, 1.518598198890686), (0.0, 1.5186561346054077), (0.0, 1.518718957901001), (0.0, 1.518783450126648), (0.0, 1.5188087224960327), (0.0, 1.5189729928970337), (0.0, 1.5191220045089722), (0.0, 1.5193005800247192), (0.0, 1.519604206085205), (0.0, 1.519849419593811), (0.0, 1.5198535919189453), (0.0, 1.5199216604232788), (0.0, 1.5199421644210815), (0.0, 1.5201619863510132), (0.0, 1.5202281475067139), (0.0, 1.5202898979187012), (0.0, 1.5203078985214233), (0.0, 1.5203254222869873), (0.0, 1.5204073190689087), (0.0, 1.5204311609268188), (0.0, 1.5204969644546509), (0.0, 1.5205744504928589), (0.0, 1.520713210105896), (0.0, 1.52080237865448), (0.0, 1.5208055973052979), (0.0, 1.520918607711792), (0.0, 1.520946979522705), (0.0, 1.5210821628570557), (0.0, 1.5211901664733887), (0.0, 1.5212215185165405), (0.0, 1.5212218761444092), (0.0, 1.5212981700897217), (0.0, 1.5214765071868896), (0.0, 1.5215106010437012), (0.0, 1.5216408967971802), (0.0, 1.5218169689178467), (0.0, 1.5219860076904297), (0.0, 1.522040843963623), (0.0, 1.5221524238586426), (0.0, 1.5222049951553345), (0.0, 1.5222092866897583), (0.0, 1.522212028503418), (0.0, 1.5222277641296387), (0.0, 1.522325038909912), (0.0, 1.5223524570465088), (0.0, 1.5225001573562622), (0.0, 1.5226930379867554), (0.0, 1.5228168964385986), (0.0, 1.5229839086532593), (0.0, 1.5231919288635254), (0.0, 1.5232062339782715), (0.0, 1.5233153104782104), (0.0, 1.5233383178710938), (0.0, 1.523374319076538), (0.0, 1.5234681367874146), (0.0, 1.5235576629638672), (0.0, 1.5235644578933716), (0.0, 1.5238173007965088), (0.0, 1.5238507986068726), (0.0, 1.5239439010620117), (0.0, 1.5239557027816772), (0.0, 1.5240188837051392), (0.0, 1.5240392684936523), (0.0, 1.5241703987121582), (0.0, 1.5241936445236206), (0.0, 1.524344563484192), (0.0, 1.5244060754776), (0.0, 1.5244958400726318), (0.0, 1.5245423316955566), (0.0, 1.524575114250183), (0.0, 1.5246292352676392), (0.0, 1.5247552394866943), (0.0, 1.5247865915298462), (0.0, 1.5250470638275146), (0.0, 1.5250893831253052), (0.0, 1.5251214504241943), (0.0, 1.5251338481903076), (0.0, 1.5251511335372925), (0.0, 1.5253347158432007), (0.0, 1.5253726243972778), (0.0, 1.525377631187439), (0.0, 1.5254178047180176), (0.0, 1.5254379510879517), (0.0, 1.5254548788070679), (0.0, 1.52547287940979), (0.0, 1.5256586074829102), (0.0, 1.5257889032363892), (0.0, 1.525822401046753), (0.0, 1.5262130498886108), (0.0, 1.5266344547271729), (0.0, 1.526902198791504), (0.0, 1.527235746383667), (0.0, 1.5272505283355713), (0.0, 1.527877926826477), (0.0, 1.528205394744873), (0.0, 1.5285855531692505), (0.0, 1.528801441192627), (0.0, 1.528969645500183), (0.0, 1.5290489196777344), (0.0, 1.5295274257659912), (0.0, 1.5295755863189697), (0.0, 1.5296047925949097), (0.0, 1.529618263244629), (0.0, 1.5299477577209473), (0.0, 1.5301190614700317), (0.0, 1.5303784608840942), (0.0, 1.5326753854751587), (0.0, 1.5337443351745605), (0.0, 1.55349600315094)], [(8.51252555847168, 8.529315948486328), (7.880024433135986, 8.392454147338867), (7.031954765319824, 7.5602898597717285), (7.010104179382324, 7.0177764892578125), (6.864895820617676, 7.2483906745910645), (6.7465691566467285, 7.432358264923096), (6.669636249542236, 6.974128723144531), (6.333556175231934, 7.00875186920166), (6.316862106323242, 6.956505298614502), (6.169785022735596, 7.03593111038208), (6.1690778732299805, 7.913816928863525), (6.073858261108398, 6.442355632781982), (6.019864082336426, 7.334825038909912), (5.989191055297852, 6.011266708374023), (5.77814245223999, 5.8204731941223145), (5.6053948402404785, 8.5613431930542), (5.368335247039795, 8.332938194274902), (5.31473445892334, 5.448817729949951), (5.214076995849609, 6.816310405731201), (5.143139839172363, 8.484234809875488), (4.96192741394043, 7.150630474090576), (4.440183162689209, 7.8643693923950195), (4.319502353668213, 4.5712199211120605), (4.1137824058532715, 7.919130325317383), (3.9832653999328613, 4.137554168701172), (3.9562923908233643, 4.249842643737793), (3.933182716369629, 4.6604743003845215), (3.869269847869873, 4.333571434020996), (3.6007590293884277, 8.761107444763184)], [(9.47141170501709, 10.034944534301758), (9.373000144958496, 9.409411430358887), (9.117695808410645, 10.443891525268555), (8.958137512207031, 9.064179420471191), (8.475092887878418, 8.580184936523438), (7.658631801605225, 7.742599010467529)]]
[array([[0.        , 1.30042577],
       [0.        , 1.30655217],
       [0.        , 1.3120594 ],
       [0.        , 1.32465661],
       [0.        , 1.32484114],
       [0.        , 1.32547867],
       [0.        , 1.32562339],
       [0.        , 1.32652652],
       [0.        , 1.32695508],
       [0.        , 1.32698464],
       [0.        , 1.32733786],
       [0.        , 1.32891953],
       [0.        , 1.32894433],
       [0.        , 1.32900882],
       [0.        , 1.32938838],
       [0.        , 1.3293916 ],
       [0.        , 1.32958555],
       [0.        , 1.32958877],
       [0.        , 1.32979178],
       [0.        , 1.32990098],
       [0.        , 1.33011568],
       [0.        , 1.33018196],
       [0.        , 1.33037353],
       [0.        , 1.33039439],
       [0.        , 1.33047152],
       [0.        , 1.33048379],
       [0.        , 1.33056462],
       [0.        , 1.33103204],
       [0.        , 1.33110631],
       [0.        , 1.33117867],
       [0.        , 1.33119023],
       [0.        , 1.33137894],
       [0.        , 1.33139968],
       [0.        , 1.33141792],
       [0.        , 1.33150494],
       [0.        , 1.33157837],
       [0.        , 1.3315959 ],
       [0.        , 1.33169246],
       [0.        , 1.33171558],
       [0.        , 1.33171809],
       [0.        , 1.33173299],
       [0.        , 1.33174109],
       [0.        , 1.33179116],
       [0.        , 1.33191764],
       [0.        , 1.3320365 ],
       [0.        , 1.33221185],
       [0.        , 1.33239722],
       [0.        , 1.33245659],
       [0.        , 1.3326025 ],
       [0.        , 1.332798  ],
       [0.        , 1.33295941],
       [0.        , 1.33298445],
       [0.        , 1.33300006],
       [0.        , 1.33334446],
       [0.        , 1.33336103],
       [0.        , 1.33340681],
       [0.        , 1.33348036],
       [0.        , 1.33350563],
       [0.        , 1.33375931],
       [0.        , 1.33393335],
       [0.        , 1.33393943],
       [0.        , 1.33394623],
       [0.        , 1.33394754],
       [0.        , 1.33401418],
       [0.        , 1.33411443],
       [0.        , 1.33414435],
       [0.        , 1.33422089],
       [0.        , 1.33422554],
       [0.        , 1.33431959],
       [0.        , 1.33457136],
       [0.        , 1.33465207],
       [0.        , 1.33470643],
       [0.        , 1.33491194],
       [0.        , 1.33491862],
       [0.        , 1.3349719 ],
       [0.        , 1.33508873],
       [0.        , 1.33510017],
       [0.        , 1.33514464],
       [0.        , 1.33530712],
       [0.        , 1.33535337],
       [0.        , 1.33541811],
       [0.        , 1.33572066],
       [0.        , 1.33576715],
       [0.        , 1.33577633],
       [0.        , 1.33585298],
       [0.        , 1.33593762],
       [0.        , 1.33604634],
       [0.        , 1.33618474],
       [0.        , 1.33629763],
       [0.        , 1.33632839],
       [0.        , 1.33639789],
       [0.        , 1.3364023 ],
       [0.        , 1.33655488],
       [0.        , 1.33665407],
       [0.        , 1.33667517],
       [0.        , 1.33674085],
       [0.        , 1.33679533],
       [0.        , 1.33688569],
       [0.        , 1.33694375],
       [0.        , 1.33694613],
       [0.        , 1.33700275],
       [0.        , 1.33713341],
       [0.        , 1.33720636],
       [0.        , 1.33732498],
       [0.        , 1.33737326],
       [0.        , 1.337484  ],
       [0.        , 1.33755767],
       [0.        , 1.33770037],
       [0.        , 1.33771455],
       [0.        , 1.33779883],
       [0.        , 1.33785892],
       [0.        , 1.33813417],
       [0.        , 1.3384397 ],
       [0.        , 1.33858848],
       [0.        , 1.33871365],
       [0.        , 1.33888435],
       [0.        , 1.33970869],
       [0.        , 1.33996594],
       [0.        , 1.3401351 ],
       [0.        , 1.3405267 ],
       [0.        , 1.34230042],
       [0.        , 1.34238815],
       [0.        , 1.34329593],
       [0.        , 1.34363544],
       [0.        , 1.44501889],
       [0.        , 1.44889069],
       [0.        , 1.45001066],
       [0.        , 1.45043731],
       [0.        , 1.45054293],
       [0.        , 1.45172215],
       [0.        , 1.45234799],
       [0.        , 1.45237494],
       [0.        , 1.45240796],
       [0.        , 1.45305705],
       [0.        , 1.45341873],
       [0.        , 1.45395494],
       [0.        , 1.45402908],
       [0.        , 1.45437121],
       [0.        , 1.45479906],
       [0.        , 1.45480895],
       [0.        , 1.45503628],
       [0.        , 1.45517683],
       [0.        , 1.45528448],
       [0.        , 1.45532918],
       [0.        , 1.45537555],
       [0.        , 1.45547593],
       [0.        , 1.45558596],
       [0.        , 1.45559824],
       [0.        , 1.45566833],
       [0.        , 1.45569634],
       [0.        , 1.45572007],
       [0.        , 1.45578468],
       [0.        , 1.45580494],
       [0.        , 1.45604312],
       [0.        , 1.45634782],
       [0.        , 1.45638812],
       [0.        , 1.45648587],
       [0.        , 1.45657313],
       [0.        , 1.45671141],
       [0.        , 1.45677221],
       [0.        , 1.45681477],
       [0.        , 1.45712638],
       [0.        , 1.45715427],
       [0.        , 1.45719206],
       [0.        , 1.45730233],
       [0.        , 1.4573071 ],
       [0.        , 1.45730937],
       [0.        , 1.45741844],
       [0.        , 1.45747459],
       [0.        , 1.45754051],
       [0.        , 1.45770109],
       [0.        , 1.4577837 ],
       [0.        , 1.45791459],
       [0.        , 1.45795023],
       [0.        , 1.45817733],
       [0.        , 1.45819759],
       [0.        , 1.45823491],
       [0.        , 1.45855331],
       [0.        , 1.45859003],
       [0.        , 1.45870435],
       [0.        , 1.4587146 ],
       [0.        , 1.45872438],
       [0.        , 1.4588021 ],
       [0.        , 1.45892394],
       [0.        , 1.45917392],
       [0.        , 1.45938563],
       [0.        , 1.45944142],
       [0.        , 1.45957494],
       [0.        , 1.45960355],
       [0.        , 1.45962369],
       [0.        , 1.45974302],
       [0.        , 1.4598738 ],
       [0.        , 1.4599402 ],
       [0.        , 1.46005774],
       [0.        , 1.46020281],
       [0.        , 1.46036065],
       [0.        , 1.46041095],
       [0.        , 1.46047425],
       [0.        , 1.46061075],
       [0.        , 1.4606117 ],
       [0.        , 1.46080136],
       [0.        , 1.46087158],
       [0.        , 1.46090043],
       [0.        , 1.46096623],
       [0.        , 1.46103668],
       [0.        , 1.46106231],
       [0.        , 1.46115685],
       [0.        , 1.46117485],
       [0.        , 1.46137166],
       [0.        , 1.46137559],
       [0.        , 1.46146572],
       [0.        , 1.46154654],
       [0.        , 1.46173203],
       [0.        , 1.46177995],
       [0.        , 1.46179366],
       [0.        , 1.46185684],
       [0.        , 1.46186495],
       [0.        , 1.46187341],
       [0.        , 1.4619993 ],
       [0.        , 1.46240067],
       [0.        , 1.46241772],
       [0.        , 1.46281874],
       [0.        , 1.46305215],
       [0.        , 1.46317172],
       [0.        , 1.4632256 ],
       [0.        , 1.46349382],
       [0.        , 1.46379912],
       [0.        , 1.46400666],
       [0.        , 1.46407056],
       [0.        , 1.46425712],
       [0.        , 1.46450245],
       [0.        , 1.46458185],
       [0.        , 1.46466756],
       [0.        , 1.4648459 ],
       [0.        , 1.46494734],
       [0.        , 1.46495748],
       [0.        , 1.46508324],
       [0.        , 1.46520483],
       [0.        , 1.46555042],
       [0.        , 1.46561563],
       [0.        , 1.46617424],
       [0.        , 1.46780038],
       [0.        , 1.46826887],
       [0.        , 1.4683584 ],
       [0.        , 1.46981919],
       [0.        , 1.4711138 ],
       [0.        , 1.47493362],
       [0.        , 1.4756887 ],
       [0.        , 1.50830579],
       [0.        , 1.51317549],
       [0.        , 1.51333272],
       [0.        , 1.51367509],
       [0.        , 1.51450229],
       [0.        , 1.51462233],
       [0.        , 1.51558626],
       [0.        , 1.51659751],
       [0.        , 1.51660419],
       [0.        , 1.51762092],
       [0.        , 1.5178442 ],
       [0.        , 1.51799011],
       [0.        , 1.51808274],
       [0.        , 1.51822364],
       [0.        , 1.51829708],
       [0.        , 1.51841569],
       [0.        , 1.51859176],
       [0.        , 1.5185982 ],
       [0.        , 1.51865613],
       [0.        , 1.51871896],
       [0.        , 1.51878345],
       [0.        , 1.51880872],
       [0.        , 1.51897299],
       [0.        , 1.519122  ],
       [0.        , 1.51930058],
       [0.        , 1.51960421],
       [0.        , 1.51984942],
       [0.        , 1.51985359],
       [0.        , 1.51992166],
       [0.        , 1.51994216],
       [0.        , 1.52016199],
       [0.        , 1.52022815],
       [0.        , 1.5202899 ],
       [0.        , 1.5203079 ],
       [0.        , 1.52032542],
       [0.        , 1.52040732],
       [0.        , 1.52043116],
       [0.        , 1.52049696],
       [0.        , 1.52057445],
       [0.        , 1.52071321],
       [0.        , 1.52080238],
       [0.        , 1.5208056 ],
       [0.        , 1.52091861],
       [0.        , 1.52094698],
       [0.        , 1.52108216],
       [0.        , 1.52119017],
       [0.        , 1.52122152],
       [0.        , 1.52122188],
       [0.        , 1.52129817],
       [0.        , 1.52147651],
       [0.        , 1.5215106 ],
       [0.        , 1.5216409 ],
       [0.        , 1.52181697],
       [0.        , 1.52198601],
       [0.        , 1.52204084],
       [0.        , 1.52215242],
       [0.        , 1.522205  ],
       [0.        , 1.52220929],
       [0.        , 1.52221203],
       [0.        , 1.52222776],
       [0.        , 1.52232504],
       [0.        , 1.52235246],
       [0.        , 1.52250016],
       [0.        , 1.52269304],
       [0.        , 1.5228169 ],
       [0.        , 1.52298391],
       [0.        , 1.52319193],
       [0.        , 1.52320623],
       [0.        , 1.52331531],
       [0.        , 1.52333832],
       [0.        , 1.52337432],
       [0.        , 1.52346814],
       [0.        , 1.52355766],
       [0.        , 1.52356446],
       [0.        , 1.5238173 ],
       [0.        , 1.5238508 ],
       [0.        , 1.5239439 ],
       [0.        , 1.5239557 ],
       [0.        , 1.52401888],
       [0.        , 1.52403927],
       [0.        , 1.5241704 ],
       [0.        , 1.52419364],
       [0.        , 1.52434456],
       [0.        , 1.52440608],
       [0.        , 1.52449584],
       [0.        , 1.52454233],
       [0.        , 1.52457511],
       [0.        , 1.52462924],
       [0.        , 1.52475524],
       [0.        , 1.52478659],
       [0.        , 1.52504706],
       [0.        , 1.52508938],
       [0.        , 1.52512145],
       [0.        , 1.52513385],
       [0.        , 1.52515113],
       [0.        , 1.52533472],
       [0.        , 1.52537262],
       [0.        , 1.52537763],
       [0.        , 1.5254178 ],
       [0.        , 1.52543795],
       [0.        , 1.52545488],
       [0.        , 1.52547288],
       [0.        , 1.52565861],
       [0.        , 1.5257889 ],
       [0.        , 1.5258224 ],
       [0.        , 1.52621305],
       [0.        , 1.52663445],
       [0.        , 1.5269022 ],
       [0.        , 1.52723575],
       [0.        , 1.52725053],
       [0.        , 1.52787793],
       [0.        , 1.52820539],
       [0.        , 1.52858555],
       [0.        , 1.52880144],
       [0.        , 1.52896965],
       [0.        , 1.52904892],
       [0.        , 1.52952743],
       [0.        , 1.52957559],
       [0.        , 1.52960479],
       [0.        , 1.52961826],
       [0.        , 1.52994776],
       [0.        , 1.53011906],
       [0.        , 1.53037846],
       [0.        , 1.53267539],
       [0.        , 1.53374434],
       [0.        , 1.553496  ]]), array([[8.51252556, 8.52931595],
       [7.88002443, 8.39245415],
       [7.03195477, 7.56028986],
       [7.01010418, 7.01777649],
       [6.86489582, 7.24839067],
       [6.74656916, 7.43235826],
       [6.66963625, 6.97412872],
       [6.33355618, 7.00875187],
       [6.31686211, 6.9565053 ],
       [6.16978502, 7.03593111],
       [6.16907787, 7.91381693],
       [6.07385826, 6.44235563],
       [6.01986408, 7.33482504],
       [5.98919106, 6.01126671],
       [5.77814245, 5.82047319],
       [5.60539484, 8.56134319],
       [5.36833525, 8.33293819],
       [5.31473446, 5.44881773],
       [5.214077  , 6.81631041],
       [5.14313984, 8.48423481],
       [4.96192741, 7.15063047],
       [4.44018316, 7.86436939],
       [4.31950235, 4.57121992],
       [4.11378241, 7.91913033],
       [3.9832654 , 4.13755417],
       [3.95629239, 4.24984264],
       [3.93318272, 4.6604743 ],
       [3.86926985, 4.33357143],
       [3.60075903, 8.76110744]]), array([[ 9.47141171, 10.03494453],
       [ 9.37300014,  9.40941143],
       [ 9.11769581, 10.44389153],
       [ 8.95813751,  9.06417942],
       [ 8.47509289,  8.58018494],
       [ 7.6586318 ,  7.74259901]])]2024-03-06 18:01:59.309774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LG9 ph vector generated, counter: 216
2024-03-06 18:02:04.341456: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:04.387961: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:05.514458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LH4 ph vector generated, counter: 217
2024-03-06 18:02:08.835547: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:08.892787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:10.106346: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LHD ph vector generated, counter: 218
2024-03-06 18:02:13.598192: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:13.641805: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:14.627792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3081064), (0., 1.3225353), (0., 1.3232571), (0., 1.3236802),
       (0., 1.3239616), (0., 1.3250886), (0., 1.3251468), (0., 1.3255906),
       (0., 1.325903 ), (0., 1.3265176), (0., 1.3265862), (0., 1.3270365),
       (0., 1.3273079), (0., 1.3273432), (0., 1.3273764), (0., 1.3273997),
       (0., 1.3274215), (0., 1.3282158), (0., 1.328279 ), (0., 1.3285931),
       (0., 1.3287531), (0., 1.3287536), (0., 1.3288232), (0., 1.3290787),
       (0., 1.3292748), (0., 1.3294129), (0., 1.3297039), (0., 1.3297164),
       (0., 1.3298568), (0., 1.3299818), (0., 1.330153 ), (0., 1.3301629),
       (0., 1.3304472), (0., 1.3307298), (0., 1.3307371), (0., 1.3309224),
       (0., 1.331259 ), (0., 1.3312601), (0., 1.3315676), (0., 1.3315771),
       (0., 1.3316352), (0., 1.3318337), (0., 1.3318839), (0., 1.3320477),
       (0., 1.3320562), (0., 1.3320624), (0., 1.3322507), (0., 1.332341 ),
       (0., 1.3324208), (0., 1.3324306), (0., 1.3325939), (0., 1.3327284),
       (0., 1.3327953), (0., 1.3328408), (0., 1.3329983), (0., 1.333023 ),
       (0., 1.333036 ), (0., 1.3331418), (0., 1.3331846), (0., 1.3333185),
       (0., 1.3333274), (0., 1.3333769), (0., 1.3335896), (0., 1.3336401),
       (0., 1.3336921), (0., 1.333739 ), (0., 1.3337396), (0., 1.3338463),
       (0., 1.3338944), (0., 1.3340158), (0., 1.3340261), (0., 1.3340989),
       (0., 1.3341923), (0., 1.3342183), (0., 1.3343668), (0., 1.3343798),
       (0., 1.3343799), (0., 1.3346986), (0., 1.3347167), (0., 1.334837 ),
       (0., 1.3348513), (0., 1.334926 ), (0., 1.3349715), (0., 1.3349916),
       (0., 1.3352113), (0., 1.3352352), (0., 1.3352376), (0., 1.3353134),
       (0., 1.3353144), (0., 1.335438 ), (0., 1.3354578), (0., 1.3356155),
       (0., 1.3356353), (0., 1.3356551), (0., 1.3358709), (0., 1.3361248),
       (0., 1.3361319), (0., 1.3362236), (0., 1.3362727), (0., 1.3363855),
       (0., 1.3364835), (0., 1.3365198), (0., 1.3369747), (0., 1.3371899),
       (0., 1.3371984), (0., 1.3373398), (0., 1.3378326), (0., 1.3383111),
       (0., 1.3383601), (0., 1.3387777), (0., 1.3388804), (0., 1.3388937),
       (0., 1.3389081), (0., 1.3390179), (0., 1.3391087), (0., 1.3391685),
       (0., 1.3392606), (0., 1.3404453), (0., 1.340778 ), (0., 1.3408781),
       (0., 1.3411463), (0., 1.3413156), (0., 1.3415762), (0., 1.44012  ),
       (0., 1.4489897), (0., 1.4496487), (0., 1.4499173), (0., 1.4499757),
       (0., 1.45047  ), (0., 1.4510965), (0., 1.4517856), (0., 1.4518658),
       (0., 1.4520175), (0., 1.4520279), (0., 1.4526299), (0., 1.4527574),
       (0., 1.4529555), (0., 1.4529876), (0., 1.4533464), (0., 1.4536965),
       (0., 1.4538314), (0., 1.454812 ), (0., 1.4550409), (0., 1.4551079),
       (0., 1.4551188), (0., 1.4552535), (0., 1.4553214), (0., 1.4554205),
       (0., 1.4555336), (0., 1.4556139), (0., 1.4556172), (0., 1.4557517),
       (0., 1.4559288), (0., 1.4559766), (0., 1.4560859), (0., 1.456168 ),
       (0., 1.4564843), (0., 1.4565455), (0., 1.4566118), (0., 1.45683  ),
       (0., 1.4569416), (0., 1.4572852), (0., 1.4574105), (0., 1.4575113),
       (0., 1.4575713), (0., 1.4576082), (0., 1.4577055), (0., 1.457729 ),
       (0., 1.4582697), (0., 1.4583418), (0., 1.4587679), (0., 1.4587708),
       (0., 1.4587853), (0., 1.4588076), (0., 1.4588106), (0., 1.4588308),
       (0., 1.4588741), (0., 1.4590594), (0., 1.4591992), (0., 1.4593076),
       (0., 1.459357 ), (0., 1.4594935), (0., 1.4595374), (0., 1.4595928),
       (0., 1.4596347), (0., 1.4597605), (0., 1.4597985), (0., 1.4598074),
       (0., 1.459831 ), (0., 1.4598333), (0., 1.4598441), (0., 1.4598521),
       (0., 1.4598956), (0., 1.4599375), (0., 1.4600004), (0., 1.4601235),
       (0., 1.4603004), (0., 1.4605455), (0., 1.4605601), (0., 1.4605684),
       (0., 1.4605714), (0., 1.4606453), (0., 1.4608172), (0., 1.4609977),
       (0., 1.4612783), (0., 1.4618293), (0., 1.462058 ), (0., 1.4620982),
       (0., 1.4621321), (0., 1.4621415), (0., 1.4621686), (0., 1.4621801),
       (0., 1.4623792), (0., 1.462677 ), (0., 1.4627571), (0., 1.4627589),
       (0., 1.4628166), (0., 1.4628384), (0., 1.4628568), (0., 1.4628637),
       (0., 1.4629093), (0., 1.4631252), (0., 1.4633099), (0., 1.4635401),
       (0., 1.4636676), (0., 1.463771 ), (0., 1.4637828), (0., 1.4639845),
       (0., 1.4641838), (0., 1.4643053), (0., 1.4643133), (0., 1.4644104),
       (0., 1.4644223), (0., 1.4646405), (0., 1.4650502), (0., 1.4654548),
       (0., 1.4655169), (0., 1.4657421), (0., 1.4662182), (0., 1.4663949),
       (0., 1.4673584), (0., 1.4680719), (0., 1.4693096), (0., 1.4694992),
       (0., 1.4695094), (0., 1.4717965), (0., 1.4731915), (0., 1.514026 ),
       (0., 1.5145806), (0., 1.5151426), (0., 1.5152553), (0., 1.5152918),
       (0., 1.5156482), (0., 1.5157049), (0., 1.5163873), (0., 1.5166703),
       (0., 1.5168024), (0., 1.517458 ), (0., 1.5176753), (0., 1.5176809),
       (0., 1.5177517), (0., 1.5178161), (0., 1.5178481), (0., 1.5178784),
       (0., 1.5183488), (0., 1.5185523), (0., 1.5189103), (0., 1.5190855),
       (0., 1.519296 ), (0., 1.5194017), (0., 1.5194079), (0., 1.5194389),
       (0., 1.5195434), (0., 1.5198191), (0., 1.5199876), (0., 1.520197 ),
       (0., 1.5203545), (0., 1.5205758), (0., 1.5206071), (0., 1.5207083),
       (0., 1.5207229), (0., 1.520886 ), (0., 1.5209285), (0., 1.5209706),
       (0., 1.5210346), (0., 1.5210392), (0., 1.5214001), (0., 1.52144  ),
       (0., 1.5214406), (0., 1.5214585), (0., 1.5215135), (0., 1.5215961),
       (0., 1.5216684), (0., 1.5217562), (0., 1.5220641), (0., 1.5220793),
       (0., 1.522241 ), (0., 1.522331 ), (0., 1.5223798), (0., 1.5225607),
       (0., 1.5226915), (0., 1.5227098), (0., 1.522725 ), (0., 1.5227393),
       (0., 1.5228771), (0., 1.5228916), (0., 1.5228956), (0., 1.5229006),
       (0., 1.5229306), (0., 1.5230033), (0., 1.5230728), (0., 1.5231158),
       (0., 1.5232342), (0., 1.5233881), (0., 1.5234364), (0., 1.5236853),
       (0., 1.523772 ), (0., 1.523809 ), (0., 1.5238093), (0., 1.5239149),
       (0., 1.5240363), (0., 1.5244082), (0., 1.5245209), (0., 1.5245246),
       (0., 1.5245492), (0., 1.5246254), (0., 1.5246431), (0., 1.5247325),
       (0., 1.524778 ), (0., 1.5248116), (0., 1.5248696), (0., 1.5248743),
       (0., 1.5249876), (0., 1.5250292), (0., 1.525185 ), (0., 1.5252813),
       (0., 1.5253077), (0., 1.5253236), (0., 1.5254489), (0., 1.5255418),
       (0., 1.5257907), (0., 1.5259193), (0., 1.5260395), (0., 1.526127 ),
       (0., 1.5261356), (0., 1.5262752), (0., 1.5264122), (0., 1.526524 ),
       (0., 1.5266609), (0., 1.5267568), (0., 1.5267973), (0., 1.5268295),
       (0., 1.5271583), (0., 1.5276413), (0., 1.5280387), (0., 1.5282487),
       (0., 1.5284536), (0., 1.5286539), (0., 1.5287513), (0., 1.5289817),
       (0., 1.5289981), (0., 1.5290699), (0., 1.5292046), (0., 1.529513 ),
       (0., 1.5296627), (0., 1.5297812), (0., 1.529955 ), (0., 1.5301409),
       (0., 1.5307925), (0., 1.530979 ), (0., 1.5320871)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.60976  , 8.638047 ), (8.091394 , 8.505477 ),
       (7.030808 , 7.038979 ), (6.964181 , 7.182456 ),
       (6.9431076, 7.031292 ), (6.8912897, 7.5647116),
       (6.648816 , 7.0186286), (6.515954 , 7.03573  ),
       (6.3842015, 7.0148544), (6.260879 , 7.12609  ),
       (6.203921 , 7.80799  ), (6.1997585, 6.3031855),
       (6.147155 , 7.048603 ), (6.068904 , 6.589968 ),
       (5.966976 , 6.003873 ), (5.611068 , 5.680768 ),
       (5.442665 , 8.234555 ), (5.418004 , 5.4320974),
       (5.376845 , 8.557716 ), (5.1210103, 8.355719 ),
       (5.0710664, 6.7582765), (4.8695583, 6.8472133),
       (4.7110715, 4.8383646), (4.502396 , 7.824932 ),
       (4.3945556, 7.846145 ), (3.987745 , 4.202328 ),
       (3.9700544, 4.7899075), (3.9528952, 3.9661374),
       (3.9402506, 4.2641077), (3.7848806, 8.5954   )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.56674 , 10.184606 ), (9.525379,  9.592375 ),
       (9.232132, 10.527337 ), (9.09434 ,  9.139538 ),
       (8.35804 ,  8.509706 ), (6.699176,  6.7213597)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3081064224243164), (0.0, 1.3225352764129639), (0.0, 1.3232570886611938), (0.0, 1.3236801624298096), (0.0, 1.323961615562439), (0.0, 1.325088620185852), (0.0, 1.3251467943191528), (0.0, 1.3255906105041504), (0.0, 1.325903058052063), (0.0, 1.3265175819396973), (0.0, 1.3265862464904785), (0.0, 1.3270364999771118), (0.0, 1.327307939529419), (0.0, 1.327343225479126), (0.0, 1.327376365661621), (0.0, 1.327399730682373), (0.0, 1.3274215459823608), (0.0, 1.3282158374786377), (0.0, 1.3282790184020996), (0.0, 1.328593134880066), (0.0, 1.328753113746643), (0.0, 1.3287535905838013), (0.0, 1.328823208808899), (0.0, 1.3290786743164062), (0.0, 1.3292747735977173), (0.0, 1.3294129371643066), (0.0, 1.3297039270401), (0.0, 1.329716444015503), (0.0, 1.3298567533493042), (0.0, 1.329981803894043), (0.0, 1.330152988433838), (0.0, 1.3301628828048706), (0.0, 1.3304471969604492), (0.0, 1.3307298421859741), (0.0, 1.3307371139526367), (0.0, 1.3309223651885986), (0.0, 1.33125901222229), (0.0, 1.331260085105896), (0.0, 1.331567645072937), (0.0, 1.3315770626068115), (0.0, 1.3316352367401123), (0.0, 1.3318337202072144), (0.0, 1.3318839073181152), (0.0, 1.332047700881958), (0.0, 1.3320561647415161), (0.0, 1.3320623636245728), (0.0, 1.332250714302063), (0.0, 1.332340955734253), (0.0, 1.332420825958252), (0.0, 1.3324306011199951), (0.0, 1.3325939178466797), (0.0, 1.332728385925293), (0.0, 1.332795262336731), (0.0, 1.3328408002853394), (0.0, 1.332998275756836), (0.0, 1.333022952079773), (0.0, 1.333035945892334), (0.0, 1.333141803741455), (0.0, 1.3331845998764038), (0.0, 1.3333184719085693), (0.0, 1.3333274126052856), (0.0, 1.3333768844604492), (0.0, 1.3335895538330078), (0.0, 1.3336400985717773), (0.0, 1.3336920738220215), (0.0, 1.3337390422821045), (0.0, 1.3337396383285522), (0.0, 1.3338463306427002), (0.0, 1.3338943719863892), (0.0, 1.3340158462524414), (0.0, 1.3340260982513428), (0.0, 1.3340989351272583), (0.0, 1.3341922760009766), (0.0, 1.3342182636260986), (0.0, 1.334366798400879), (0.0, 1.33437979221344), (0.0, 1.3343799114227295), (0.0, 1.3346985578536987), (0.0, 1.3347166776657104), (0.0, 1.3348369598388672), (0.0, 1.3348512649536133), (0.0, 1.3349260091781616), (0.0, 1.33497154712677), (0.0, 1.3349915742874146), (0.0, 1.3352112770080566), (0.0, 1.3352352380752563), (0.0, 1.3352376222610474), (0.0, 1.3353134393692017), (0.0, 1.335314393043518), (0.0, 1.3354380130767822), (0.0, 1.3354578018188477), (0.0, 1.3356155157089233), (0.0, 1.3356353044509888), (0.0, 1.3356550931930542), (0.0, 1.3358708620071411), (0.0, 1.3361247777938843), (0.0, 1.3361319303512573), (0.0, 1.3362236022949219), (0.0, 1.3362727165222168), (0.0, 1.3363854885101318), (0.0, 1.3364834785461426), (0.0, 1.3365198373794556), (0.0, 1.3369747400283813), (0.0, 1.3371899127960205), (0.0, 1.3371983766555786), (0.0, 1.3373397588729858), (0.0, 1.3378325700759888), (0.0, 1.3383110761642456), (0.0, 1.338360071182251), (0.0, 1.3387776613235474), (0.0, 1.3388804197311401), (0.0, 1.3388936519622803), (0.0, 1.338908076286316), (0.0, 1.3390178680419922), (0.0, 1.3391087055206299), (0.0, 1.3391685485839844), (0.0, 1.3392605781555176), (0.0, 1.3404452800750732), (0.0, 1.3407779932022095), (0.0, 1.3408781290054321), (0.0, 1.3411463499069214), (0.0, 1.3413156270980835), (0.0, 1.3415762186050415), (0.0, 1.440119981765747), (0.0, 1.448989748954773), (0.0, 1.4496487379074097), (0.0, 1.4499173164367676), (0.0, 1.4499757289886475), (0.0, 1.450469970703125), (0.0, 1.451096534729004), (0.0, 1.4517855644226074), (0.0, 1.451865792274475), (0.0, 1.4520175457000732), (0.0, 1.4520279169082642), (0.0, 1.4526299238204956), (0.0, 1.4527573585510254), (0.0, 1.4529554843902588), (0.0, 1.452987551689148), (0.0, 1.4533463716506958), (0.0, 1.4536964893341064), (0.0, 1.453831434249878), (0.0, 1.4548120498657227), (0.0, 1.4550409317016602), (0.0, 1.4551079273223877), (0.0, 1.4551187753677368), (0.0, 1.4552534818649292), (0.0, 1.4553214311599731), (0.0, 1.4554204940795898), (0.0, 1.4555336236953735), (0.0, 1.4556138515472412), (0.0, 1.4556171894073486), (0.0, 1.455751657485962), (0.0, 1.4559288024902344), (0.0, 1.4559766054153442), (0.0, 1.4560859203338623), (0.0, 1.4561680555343628), (0.0, 1.456484317779541), (0.0, 1.4565454721450806), (0.0, 1.4566117525100708), (0.0, 1.4568300247192383), (0.0, 1.4569416046142578), (0.0, 1.4572851657867432), (0.0, 1.457410454750061), (0.0, 1.457511305809021), (0.0, 1.457571268081665), (0.0, 1.4576082229614258), (0.0, 1.4577054977416992), (0.0, 1.4577289819717407), (0.0, 1.458269715309143), (0.0, 1.4583418369293213), (0.0, 1.4587678909301758), (0.0, 1.458770751953125), (0.0, 1.4587852954864502), (0.0, 1.4588075876235962), (0.0, 1.458810567855835), (0.0, 1.4588308334350586), (0.0, 1.4588741064071655), (0.0, 1.4590593576431274), (0.0, 1.4591991901397705), (0.0, 1.4593075513839722), (0.0, 1.4593570232391357), (0.0, 1.4594935178756714), (0.0, 1.459537386894226), (0.0, 1.4595928192138672), (0.0, 1.4596346616744995), (0.0, 1.4597605466842651), (0.0, 1.4597984552383423), (0.0, 1.4598073959350586), (0.0, 1.4598309993743896), (0.0, 1.4598332643508911), (0.0, 1.4598441123962402), (0.0, 1.4598520994186401), (0.0, 1.4598956108093262), (0.0, 1.4599374532699585), (0.0, 1.4600003957748413), (0.0, 1.4601235389709473), (0.0, 1.4603004455566406), (0.0, 1.460545539855957), (0.0, 1.4605600833892822), (0.0, 1.4605684280395508), (0.0, 1.4605714082717896), (0.0, 1.460645318031311), (0.0, 1.4608172178268433), (0.0, 1.4609977006912231), (0.0, 1.4612783193588257), (0.0, 1.4618293046951294), (0.0, 1.4620579481124878), (0.0, 1.462098240852356), (0.0, 1.4621320962905884), (0.0, 1.462141513824463), (0.0, 1.462168574333191), (0.0, 1.4621801376342773), (0.0, 1.4623792171478271), (0.0, 1.462677001953125), (0.0, 1.4627571105957031), (0.0, 1.4627588987350464), (0.0, 1.462816596031189), (0.0, 1.4628384113311768), (0.0, 1.4628567695617676), (0.0, 1.4628636837005615), (0.0, 1.4629093408584595), (0.0, 1.463125228881836), (0.0, 1.46330988407135), (0.0, 1.4635400772094727), (0.0, 1.463667631149292), (0.0, 1.4637709856033325), (0.0, 1.463782787322998), (0.0, 1.463984489440918), (0.0, 1.4641838073730469), (0.0, 1.4643052816390991), (0.0, 1.464313268661499), (0.0, 1.464410424232483), (0.0, 1.464422345161438), (0.0, 1.464640498161316), (0.0, 1.465050220489502), (0.0, 1.4654548168182373), (0.0, 1.4655169248580933), (0.0, 1.4657421112060547), (0.0, 1.4662182331085205), (0.0, 1.4663949012756348), (0.0, 1.4673583507537842), (0.0, 1.4680719375610352), (0.0, 1.4693095684051514), (0.0, 1.4694992303848267), (0.0, 1.4695093631744385), (0.0, 1.4717965126037598), (0.0, 1.473191499710083), (0.0, 1.5140260457992554), (0.0, 1.5145806074142456), (0.0, 1.515142560005188), (0.0, 1.515255331993103), (0.0, 1.5152918100357056), (0.0, 1.5156482458114624), (0.0, 1.515704870223999), (0.0, 1.5163873434066772), (0.0, 1.5166703462600708), (0.0, 1.516802430152893), (0.0, 1.5174579620361328), (0.0, 1.5176752805709839), (0.0, 1.5176808834075928), (0.0, 1.517751693725586), (0.0, 1.5178160667419434), (0.0, 1.5178481340408325), (0.0, 1.5178784132003784), (0.0, 1.5183488130569458), (0.0, 1.518552303314209), (0.0, 1.51891028881073), (0.0, 1.5190855264663696), (0.0, 1.5192960500717163), (0.0, 1.5194016695022583), (0.0, 1.519407868385315), (0.0, 1.5194388628005981), (0.0, 1.5195434093475342), (0.0, 1.5198191404342651), (0.0, 1.5199875831604004), (0.0, 1.5201970338821411), (0.0, 1.5203545093536377), (0.0, 1.520575761795044), (0.0, 1.5206071138381958), (0.0, 1.5207083225250244), (0.0, 1.5207228660583496), (0.0, 1.520885944366455), (0.0, 1.5209285020828247), (0.0, 1.5209705829620361), (0.0, 1.521034598350525), (0.0, 1.5210392475128174), (0.0, 1.5214000940322876), (0.0, 1.521440029144287), (0.0, 1.5214406251907349), (0.0, 1.5214585065841675), (0.0, 1.5215134620666504), (0.0, 1.521596074104309), (0.0, 1.5216684341430664), (0.0, 1.5217561721801758), (0.0, 1.5220640897750854), (0.0, 1.522079348564148), (0.0, 1.5222409963607788), (0.0, 1.5223309993743896), (0.0, 1.522379755973816), (0.0, 1.522560715675354), (0.0, 1.5226914882659912), (0.0, 1.522709846496582), (0.0, 1.522724986076355), (0.0, 1.522739291191101), (0.0, 1.5228770971298218), (0.0, 1.522891640663147), (0.0, 1.5228955745697021), (0.0, 1.5229005813598633), (0.0, 1.52293062210083), (0.0, 1.523003339767456), (0.0, 1.5230728387832642), (0.0, 1.5231157541275024), (0.0, 1.523234248161316), (0.0, 1.523388147354126), (0.0, 1.523436427116394), (0.0, 1.523685336112976), (0.0, 1.5237720012664795), (0.0, 1.5238089561462402), (0.0, 1.5238093137741089), (0.0, 1.5239149332046509), (0.0, 1.5240362882614136), (0.0, 1.524408221244812), (0.0, 1.5245208740234375), (0.0, 1.5245245695114136), (0.0, 1.5245492458343506), (0.0, 1.5246254205703735), (0.0, 1.524643063545227), (0.0, 1.5247324705123901), (0.0, 1.5247780084609985), (0.0, 1.5248116254806519), (0.0, 1.5248695611953735), (0.0, 1.5248743295669556), (0.0, 1.5249875783920288), (0.0, 1.525029182434082), (0.0, 1.525184988975525), (0.0, 1.525281310081482), (0.0, 1.5253076553344727), (0.0, 1.5253236293792725), (0.0, 1.5254489183425903), (0.0, 1.5255417823791504), (0.0, 1.5257906913757324), (0.0, 1.5259193181991577), (0.0, 1.526039481163025), (0.0, 1.5261269807815552), (0.0, 1.5261355638504028), (0.0, 1.5262751579284668), (0.0, 1.5264122486114502), (0.0, 1.5265239477157593), (0.0, 1.5266609191894531), (0.0, 1.526756763458252), (0.0, 1.5267972946166992), (0.0, 1.526829481124878), (0.0, 1.527158260345459), (0.0, 1.5276412963867188), (0.0, 1.528038740158081), (0.0, 1.52824866771698), (0.0, 1.5284535884857178), (0.0, 1.528653860092163), (0.0, 1.528751254081726), (0.0, 1.5289816856384277), (0.0, 1.5289981365203857), (0.0, 1.5290699005126953), (0.0, 1.5292046070098877), (0.0, 1.5295130014419556), (0.0, 1.5296627283096313), (0.0, 1.5297812223434448), (0.0, 1.5299550294876099), (0.0, 1.5301408767700195), (0.0, 1.530792474746704), (0.0, 1.530979037284851), (0.0, 1.5320870876312256)], [(8.609760284423828, 8.638047218322754), (8.091394424438477, 8.505476951599121), (7.030807971954346, 7.0389790534973145), (6.964180946350098, 7.182456016540527), (6.943107604980469, 7.031291961669922), (6.891289710998535, 7.564711570739746), (6.648816108703613, 7.0186285972595215), (6.51595401763916, 7.035729885101318), (6.384201526641846, 7.014854431152344), (6.260879039764404, 7.126090049743652), (6.203920841217041, 7.807990074157715), (6.199758529663086, 6.30318546295166), (6.147154808044434, 7.048603057861328), (6.068903923034668, 6.589968204498291), (5.966976165771484, 6.003872871398926), (5.611067771911621, 5.680768013000488), (5.442665100097656, 8.2345552444458), (5.418004035949707, 5.432097434997559), (5.376844882965088, 8.557716369628906), (5.1210103034973145, 8.355718612670898), (5.071066379547119, 6.758276462554932), (4.869558334350586, 6.847213268280029), (4.711071491241455, 4.838364601135254), (4.502396106719971, 7.824932098388672), (4.394555568695068, 7.846145153045654), (3.9877450466156006, 4.202328205108643), (3.9700543880462646, 4.789907455444336), (3.952895164489746, 3.966137409210205), (3.9402506351470947, 4.264107704162598), (3.7848806381225586, 8.595399856567383)], [(9.566740036010742, 10.184605598449707), (9.525379180908203, 9.592374801635742), (9.232131958007812, 10.527337074279785), (9.094340324401855, 9.139537811279297), (8.358039855957031, 8.509705543518066), (6.699175834655762, 6.721359729766846)]]
[array([[0.        , 1.30810642],
       [0.        , 1.32253528],
       [0.        , 1.32325709],
       [0.        , 1.32368016],
       [0.        , 1.32396162],
       [0.        , 1.32508862],
       [0.        , 1.32514679],
       [0.        , 1.32559061],
       [0.        , 1.32590306],
       [0.        , 1.32651758],
       [0.        , 1.32658625],
       [0.        , 1.3270365 ],
       [0.        , 1.32730794],
       [0.        , 1.32734323],
       [0.        , 1.32737637],
       [0.        , 1.32739973],
       [0.        , 1.32742155],
       [0.        , 1.32821584],
       [0.        , 1.32827902],
       [0.        , 1.32859313],
       [0.        , 1.32875311],
       [0.        , 1.32875359],
       [0.        , 1.32882321],
       [0.        , 1.32907867],
       [0.        , 1.32927477],
       [0.        , 1.32941294],
       [0.        , 1.32970393],
       [0.        , 1.32971644],
       [0.        , 1.32985675],
       [0.        , 1.3299818 ],
       [0.        , 1.33015299],
       [0.        , 1.33016288],
       [0.        , 1.3304472 ],
       [0.        , 1.33072984],
       [0.        , 1.33073711],
       [0.        , 1.33092237],
       [0.        , 1.33125901],
       [0.        , 1.33126009],
       [0.        , 1.33156765],
       [0.        , 1.33157706],
       [0.        , 1.33163524],
       [0.        , 1.33183372],
       [0.        , 1.33188391],
       [0.        , 1.3320477 ],
       [0.        , 1.33205616],
       [0.        , 1.33206236],
       [0.        , 1.33225071],
       [0.        , 1.33234096],
       [0.        , 1.33242083],
       [0.        , 1.3324306 ],
       [0.        , 1.33259392],
       [0.        , 1.33272839],
       [0.        , 1.33279526],
       [0.        , 1.3328408 ],
       [0.        , 1.33299828],
       [0.        , 1.33302295],
       [0.        , 1.33303595],
       [0.        , 1.3331418 ],
       [0.        , 1.3331846 ],
       [0.        , 1.33331847],
       [0.        , 1.33332741],
       [0.        , 1.33337688],
       [0.        , 1.33358955],
       [0.        , 1.3336401 ],
       [0.        , 1.33369207],
       [0.        , 1.33373904],
       [0.        , 1.33373964],
       [0.        , 1.33384633],
       [0.        , 1.33389437],
       [0.        , 1.33401585],
       [0.        , 1.3340261 ],
       [0.        , 1.33409894],
       [0.        , 1.33419228],
       [0.        , 1.33421826],
       [0.        , 1.3343668 ],
       [0.        , 1.33437979],
       [0.        , 1.33437991],
       [0.        , 1.33469856],
       [0.        , 1.33471668],
       [0.        , 1.33483696],
       [0.        , 1.33485126],
       [0.        , 1.33492601],
       [0.        , 1.33497155],
       [0.        , 1.33499157],
       [0.        , 1.33521128],
       [0.        , 1.33523524],
       [0.        , 1.33523762],
       [0.        , 1.33531344],
       [0.        , 1.33531439],
       [0.        , 1.33543801],
       [0.        , 1.3354578 ],
       [0.        , 1.33561552],
       [0.        , 1.3356353 ],
       [0.        , 1.33565509],
       [0.        , 1.33587086],
       [0.        , 1.33612478],
       [0.        , 1.33613193],
       [0.        , 1.3362236 ],
       [0.        , 1.33627272],
       [0.        , 1.33638549],
       [0.        , 1.33648348],
       [0.        , 1.33651984],
       [0.        , 1.33697474],
       [0.        , 1.33718991],
       [0.        , 1.33719838],
       [0.        , 1.33733976],
       [0.        , 1.33783257],
       [0.        , 1.33831108],
       [0.        , 1.33836007],
       [0.        , 1.33877766],
       [0.        , 1.33888042],
       [0.        , 1.33889365],
       [0.        , 1.33890808],
       [0.        , 1.33901787],
       [0.        , 1.33910871],
       [0.        , 1.33916855],
       [0.        , 1.33926058],
       [0.        , 1.34044528],
       [0.        , 1.34077799],
       [0.        , 1.34087813],
       [0.        , 1.34114635],
       [0.        , 1.34131563],
       [0.        , 1.34157622],
       [0.        , 1.44011998],
       [0.        , 1.44898975],
       [0.        , 1.44964874],
       [0.        , 1.44991732],
       [0.        , 1.44997573],
       [0.        , 1.45046997],
       [0.        , 1.45109653],
       [0.        , 1.45178556],
       [0.        , 1.45186579],
       [0.        , 1.45201755],
       [0.        , 1.45202792],
       [0.        , 1.45262992],
       [0.        , 1.45275736],
       [0.        , 1.45295548],
       [0.        , 1.45298755],
       [0.        , 1.45334637],
       [0.        , 1.45369649],
       [0.        , 1.45383143],
       [0.        , 1.45481205],
       [0.        , 1.45504093],
       [0.        , 1.45510793],
       [0.        , 1.45511878],
       [0.        , 1.45525348],
       [0.        , 1.45532143],
       [0.        , 1.45542049],
       [0.        , 1.45553362],
       [0.        , 1.45561385],
       [0.        , 1.45561719],
       [0.        , 1.45575166],
       [0.        , 1.4559288 ],
       [0.        , 1.45597661],
       [0.        , 1.45608592],
       [0.        , 1.45616806],
       [0.        , 1.45648432],
       [0.        , 1.45654547],
       [0.        , 1.45661175],
       [0.        , 1.45683002],
       [0.        , 1.4569416 ],
       [0.        , 1.45728517],
       [0.        , 1.45741045],
       [0.        , 1.45751131],
       [0.        , 1.45757127],
       [0.        , 1.45760822],
       [0.        , 1.4577055 ],
       [0.        , 1.45772898],
       [0.        , 1.45826972],
       [0.        , 1.45834184],
       [0.        , 1.45876789],
       [0.        , 1.45877075],
       [0.        , 1.4587853 ],
       [0.        , 1.45880759],
       [0.        , 1.45881057],
       [0.        , 1.45883083],
       [0.        , 1.45887411],
       [0.        , 1.45905936],
       [0.        , 1.45919919],
       [0.        , 1.45930755],
       [0.        , 1.45935702],
       [0.        , 1.45949352],
       [0.        , 1.45953739],
       [0.        , 1.45959282],
       [0.        , 1.45963466],
       [0.        , 1.45976055],
       [0.        , 1.45979846],
       [0.        , 1.4598074 ],
       [0.        , 1.459831  ],
       [0.        , 1.45983326],
       [0.        , 1.45984411],
       [0.        , 1.4598521 ],
       [0.        , 1.45989561],
       [0.        , 1.45993745],
       [0.        , 1.4600004 ],
       [0.        , 1.46012354],
       [0.        , 1.46030045],
       [0.        , 1.46054554],
       [0.        , 1.46056008],
       [0.        , 1.46056843],
       [0.        , 1.46057141],
       [0.        , 1.46064532],
       [0.        , 1.46081722],
       [0.        , 1.4609977 ],
       [0.        , 1.46127832],
       [0.        , 1.4618293 ],
       [0.        , 1.46205795],
       [0.        , 1.46209824],
       [0.        , 1.4621321 ],
       [0.        , 1.46214151],
       [0.        , 1.46216857],
       [0.        , 1.46218014],
       [0.        , 1.46237922],
       [0.        , 1.462677  ],
       [0.        , 1.46275711],
       [0.        , 1.4627589 ],
       [0.        , 1.4628166 ],
       [0.        , 1.46283841],
       [0.        , 1.46285677],
       [0.        , 1.46286368],
       [0.        , 1.46290934],
       [0.        , 1.46312523],
       [0.        , 1.46330988],
       [0.        , 1.46354008],
       [0.        , 1.46366763],
       [0.        , 1.46377099],
       [0.        , 1.46378279],
       [0.        , 1.46398449],
       [0.        , 1.46418381],
       [0.        , 1.46430528],
       [0.        , 1.46431327],
       [0.        , 1.46441042],
       [0.        , 1.46442235],
       [0.        , 1.4646405 ],
       [0.        , 1.46505022],
       [0.        , 1.46545482],
       [0.        , 1.46551692],
       [0.        , 1.46574211],
       [0.        , 1.46621823],
       [0.        , 1.4663949 ],
       [0.        , 1.46735835],
       [0.        , 1.46807194],
       [0.        , 1.46930957],
       [0.        , 1.46949923],
       [0.        , 1.46950936],
       [0.        , 1.47179651],
       [0.        , 1.4731915 ],
       [0.        , 1.51402605],
       [0.        , 1.51458061],
       [0.        , 1.51514256],
       [0.        , 1.51525533],
       [0.        , 1.51529181],
       [0.        , 1.51564825],
       [0.        , 1.51570487],
       [0.        , 1.51638734],
       [0.        , 1.51667035],
       [0.        , 1.51680243],
       [0.        , 1.51745796],
       [0.        , 1.51767528],
       [0.        , 1.51768088],
       [0.        , 1.51775169],
       [0.        , 1.51781607],
       [0.        , 1.51784813],
       [0.        , 1.51787841],
       [0.        , 1.51834881],
       [0.        , 1.5185523 ],
       [0.        , 1.51891029],
       [0.        , 1.51908553],
       [0.        , 1.51929605],
       [0.        , 1.51940167],
       [0.        , 1.51940787],
       [0.        , 1.51943886],
       [0.        , 1.51954341],
       [0.        , 1.51981914],
       [0.        , 1.51998758],
       [0.        , 1.52019703],
       [0.        , 1.52035451],
       [0.        , 1.52057576],
       [0.        , 1.52060711],
       [0.        , 1.52070832],
       [0.        , 1.52072287],
       [0.        , 1.52088594],
       [0.        , 1.5209285 ],
       [0.        , 1.52097058],
       [0.        , 1.5210346 ],
       [0.        , 1.52103925],
       [0.        , 1.52140009],
       [0.        , 1.52144003],
       [0.        , 1.52144063],
       [0.        , 1.52145851],
       [0.        , 1.52151346],
       [0.        , 1.52159607],
       [0.        , 1.52166843],
       [0.        , 1.52175617],
       [0.        , 1.52206409],
       [0.        , 1.52207935],
       [0.        , 1.522241  ],
       [0.        , 1.522331  ],
       [0.        , 1.52237976],
       [0.        , 1.52256072],
       [0.        , 1.52269149],
       [0.        , 1.52270985],
       [0.        , 1.52272499],
       [0.        , 1.52273929],
       [0.        , 1.5228771 ],
       [0.        , 1.52289164],
       [0.        , 1.52289557],
       [0.        , 1.52290058],
       [0.        , 1.52293062],
       [0.        , 1.52300334],
       [0.        , 1.52307284],
       [0.        , 1.52311575],
       [0.        , 1.52323425],
       [0.        , 1.52338815],
       [0.        , 1.52343643],
       [0.        , 1.52368534],
       [0.        , 1.523772  ],
       [0.        , 1.52380896],
       [0.        , 1.52380931],
       [0.        , 1.52391493],
       [0.        , 1.52403629],
       [0.        , 1.52440822],
       [0.        , 1.52452087],
       [0.        , 1.52452457],
       [0.        , 1.52454925],
       [0.        , 1.52462542],
       [0.        , 1.52464306],
       [0.        , 1.52473247],
       [0.        , 1.52477801],
       [0.        , 1.52481163],
       [0.        , 1.52486956],
       [0.        , 1.52487433],
       [0.        , 1.52498758],
       [0.        , 1.52502918],
       [0.        , 1.52518499],
       [0.        , 1.52528131],
       [0.        , 1.52530766],
       [0.        , 1.52532363],
       [0.        , 1.52544892],
       [0.        , 1.52554178],
       [0.        , 1.52579069],
       [0.        , 1.52591932],
       [0.        , 1.52603948],
       [0.        , 1.52612698],
       [0.        , 1.52613556],
       [0.        , 1.52627516],
       [0.        , 1.52641225],
       [0.        , 1.52652395],
       [0.        , 1.52666092],
       [0.        , 1.52675676],
       [0.        , 1.52679729],
       [0.        , 1.52682948],
       [0.        , 1.52715826],
       [0.        , 1.5276413 ],
       [0.        , 1.52803874],
       [0.        , 1.52824867],
       [0.        , 1.52845359],
       [0.        , 1.52865386],
       [0.        , 1.52875125],
       [0.        , 1.52898169],
       [0.        , 1.52899814],
       [0.        , 1.5290699 ],
       [0.        , 1.52920461],
       [0.        , 1.529513  ],
       [0.        , 1.52966273],
       [0.        , 1.52978122],
       [0.        , 1.52995503],
       [0.        , 1.53014088],
       [0.        , 1.53079247],
       [0.        , 1.53097904],
       [0.        , 1.53208709]]), array([[8.60976028, 8.63804722],
       [8.09139442, 8.50547695],
       [7.03080797, 7.03897905],
       [6.96418095, 7.18245602],
       [6.9431076 , 7.03129196],
       [6.89128971, 7.56471157],
       [6.64881611, 7.0186286 ],
       [6.51595402, 7.03572989],
       [6.38420153, 7.01485443],
       [6.26087904, 7.12609005],
       [6.20392084, 7.80799007],
       [6.19975853, 6.30318546],
       [6.14715481, 7.04860306],
       [6.06890392, 6.5899682 ],
       [5.96697617, 6.00387287],
       [5.61106777, 5.68076801],
       [5.4426651 , 8.23455524],
       [5.41800404, 5.43209743],
       [5.37684488, 8.55771637],
       [5.1210103 , 8.35571861],
       [5.07106638, 6.75827646],
       [4.86955833, 6.84721327],
       [4.71107149, 4.8383646 ],
       [4.50239611, 7.8249321 ],
       [4.39455557, 7.84614515],
       [3.98774505, 4.20232821],
       [3.97005439, 4.78990746],
       [3.95289516, 3.96613741],
       [3.94025064, 4.2641077 ],
       [3.78488064, 8.59539986]]), array([[ 9.56674004, 10.1846056 ],
       [ 9.52537918,  9.5923748 ],
       [ 9.23213196, 10.52733707],
       [ 9.09434032,  9.13953781],
       [ 8.35803986,  8.50970554],
       [ 6.69917583,  6.72135973]])]2024-03-06 18:02:18.903163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LIH ph vector generated, counter: 219
2024-03-06 18:02:23.064512: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:23.111546: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:24.301853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LII ph vector generated, counter: 220
2024-03-06 18:02:27.844627: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:27.887702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:28.961219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LIL ph vector generated, counter: 221
2024-03-06 18:02:32.393929: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:32.436929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:33.471008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3085618), (0., 1.3141885), (0., 1.316327 ), (0., 1.3177476),
       (0., 1.3201739), (0., 1.3209444), (0., 1.3224678), (0., 1.323133 ),
       (0., 1.323897 ), (0., 1.3241594), (0., 1.3244077), (0., 1.3247651),
       (0., 1.3247857), (0., 1.32521  ), (0., 1.3252728), (0., 1.3255613),
       (0., 1.3256848), (0., 1.3260512), (0., 1.3260618), (0., 1.3265549),
       (0., 1.3269142), (0., 1.3270029), (0., 1.327214 ), (0., 1.3272234),
       (0., 1.3273345), (0., 1.327465 ), (0., 1.3280313), (0., 1.3284063),
       (0., 1.3285309), (0., 1.3286345), (0., 1.328696 ), (0., 1.3293056),
       (0., 1.329318 ), (0., 1.3293219), (0., 1.3296645), (0., 1.3297927),
       (0., 1.3298022), (0., 1.3299899), (0., 1.330019 ), (0., 1.3301613),
       (0., 1.3302672), (0., 1.3302995), (0., 1.3303666), (0., 1.3309616),
       (0., 1.3311161), (0., 1.3311785), (0., 1.3312058), (0., 1.3312331),
       (0., 1.3314182), (0., 1.3315523), (0., 1.331553 ), (0., 1.3316605),
       (0., 1.3317109), (0., 1.3317226), (0., 1.3317978), (0., 1.3318142),
       (0., 1.3318417), (0., 1.3318424), (0., 1.3319588), (0., 1.3320749),
       (0., 1.3321993), (0., 1.3323231), (0., 1.3323673), (0., 1.3326807),
       (0., 1.3327678), (0., 1.3327713), (0., 1.3328495), (0., 1.332971 ),
       (0., 1.333086 ), (0., 1.3332206), (0., 1.3334469), (0., 1.3335087),
       (0., 1.3337226), (0., 1.3337266), (0., 1.3341407), (0., 1.3341671),
       (0., 1.3342056), (0., 1.3343018), (0., 1.3346506), (0., 1.3347571),
       (0., 1.3348265), (0., 1.3351651), (0., 1.3352277), (0., 1.3352876),
       (0., 1.3353605), (0., 1.3354275), (0., 1.3354354), (0., 1.3355302),
       (0., 1.335817 ), (0., 1.3362786), (0., 1.3363289), (0., 1.3364494),
       (0., 1.3366439), (0., 1.3367325), (0., 1.3371958), (0., 1.3372213),
       (0., 1.3372724), (0., 1.3377448), (0., 1.3377792), (0., 1.3378001),
       (0., 1.3378037), (0., 1.3381193), (0., 1.3385203), (0., 1.3385276),
       (0., 1.3387771), (0., 1.3389715), (0., 1.3394796), (0., 1.3400866),
       (0., 1.3403425), (0., 1.3404425), (0., 1.3406156), (0., 1.3406324),
       (0., 1.340989 ), (0., 1.3411227), (0., 1.3413205), (0., 1.3424457),
       (0., 1.3425593), (0., 1.3438648), (0., 1.3464458), (0., 1.3473461),
       (0., 1.3477727), (0., 1.3479512), (0., 1.3498996), (0., 1.4361805),
       (0., 1.4447249), (0., 1.4466538), (0., 1.4474529), (0., 1.447453 ),
       (0., 1.447508 ), (0., 1.4483347), (0., 1.4490983), (0., 1.4491451),
       (0., 1.4496214), (0., 1.4506019), (0., 1.4506774), (0., 1.4509207),
       (0., 1.4512056), (0., 1.4512964), (0., 1.4514129), (0., 1.4522319),
       (0., 1.4525088), (0., 1.4525319), (0., 1.452536 ), (0., 1.452729 ),
       (0., 1.4527941), (0., 1.4530104), (0., 1.4530228), (0., 1.4534286),
       (0., 1.4534817), (0., 1.4536045), (0., 1.4536278), (0., 1.4541366),
       (0., 1.4542723), (0., 1.4544938), (0., 1.4545319), (0., 1.4546368),
       (0., 1.4546498), (0., 1.4547552), (0., 1.454883 ), (0., 1.4550143),
       (0., 1.4550784), (0., 1.4553543), (0., 1.455718 ), (0., 1.4558808),
       (0., 1.456012 ), (0., 1.4561003), (0., 1.4562148), (0., 1.4565194),
       (0., 1.4565582), (0., 1.4567455), (0., 1.456886 ), (0., 1.4569602),
       (0., 1.4569731), (0., 1.4570475), (0., 1.4570708), (0., 1.4571116),
       (0., 1.4572088), (0., 1.4573815), (0., 1.4574306), (0., 1.4575272),
       (0., 1.4575481), (0., 1.457715 ), (0., 1.4578816), (0., 1.4578974),
       (0., 1.4581643), (0., 1.4581875), (0., 1.4583709), (0., 1.4585156),
       (0., 1.4586239), (0., 1.4586644), (0., 1.4586862), (0., 1.4587213),
       (0., 1.458764 ), (0., 1.4587749), (0., 1.4587904), (0., 1.4590292),
       (0., 1.4590592), (0., 1.4591196), (0., 1.459227 ), (0., 1.4592901),
       (0., 1.4595094), (0., 1.4596405), (0., 1.4596454), (0., 1.4596816),
       (0., 1.4599416), (0., 1.459984 ), (0., 1.4600945), (0., 1.4601885),
       (0., 1.46044  ), (0., 1.4605079), (0., 1.4608929), (0., 1.4609294),
       (0., 1.4610633), (0., 1.4611895), (0., 1.4612582), (0., 1.4613442),
       (0., 1.4614006), (0., 1.4614012), (0., 1.4618018), (0., 1.4620697),
       (0., 1.4620868), (0., 1.4622813), (0., 1.4623368), (0., 1.4632884),
       (0., 1.4635826), (0., 1.4638778), (0., 1.4638817), (0., 1.4639832),
       (0., 1.4641184), (0., 1.4643339), (0., 1.4651827), (0., 1.4653676),
       (0., 1.466774 ), (0., 1.4669785), (0., 1.4672356), (0., 1.4674256),
       (0., 1.4677525), (0., 1.4679577), (0., 1.4680473), (0., 1.468594 ),
       (0., 1.4701543), (0., 1.4703027), (0., 1.4704808), (0., 1.4717739),
       (0., 1.4733417), (0., 1.4733962), (0., 1.4792348), (0., 1.5057361),
       (0., 1.5085715), (0., 1.5106202), (0., 1.5110644), (0., 1.5111704),
       (0., 1.5117279), (0., 1.5120803), (0., 1.512509 ), (0., 1.5130405),
       (0., 1.5131285), (0., 1.5140358), (0., 1.5141542), (0., 1.5147598),
       (0., 1.5150614), (0., 1.5151653), (0., 1.5152909), (0., 1.5160376),
       (0., 1.5161624), (0., 1.5163091), (0., 1.5171198), (0., 1.5171725),
       (0., 1.5173869), (0., 1.5174134), (0., 1.5174192), (0., 1.5176215),
       (0., 1.5176605), (0., 1.5177013), (0., 1.5177466), (0., 1.5181774),
       (0., 1.5185443), (0., 1.5186795), (0., 1.5187818), (0., 1.5188179),
       (0., 1.5188315), (0., 1.5190715), (0., 1.519075 ), (0., 1.5192796),
       (0., 1.5194639), (0., 1.5196803), (0., 1.5199486), (0., 1.519998 ),
       (0., 1.5200762), (0., 1.520113 ), (0., 1.5201212), (0., 1.5204179),
       (0., 1.5205796), (0., 1.5207298), (0., 1.5207801), (0., 1.5209557),
       (0., 1.521007 ), (0., 1.5210975), (0., 1.5212519), (0., 1.5215291),
       (0., 1.52156  ), (0., 1.5216292), (0., 1.5217617), (0., 1.5218633),
       (0., 1.5218732), (0., 1.5218949), (0., 1.522032 ), (0., 1.5220449),
       (0., 1.5223258), (0., 1.5224165), (0., 1.522746 ), (0., 1.5230237),
       (0., 1.5232615), (0., 1.5233809), (0., 1.5240903), (0., 1.5241113),
       (0., 1.5243425), (0., 1.5243434), (0., 1.5243609), (0., 1.5244074),
       (0., 1.5244521), (0., 1.5245208), (0., 1.5246593), (0., 1.5247369),
       (0., 1.5248362), (0., 1.5249559), (0., 1.5249828), (0., 1.525009 ),
       (0., 1.5251166), (0., 1.5252019), (0., 1.5252441), (0., 1.5252777),
       (0., 1.5254333), (0., 1.5254877), (0., 1.5256433), (0., 1.5257992),
       (0., 1.5260749), (0., 1.5261004), (0., 1.5261676), (0., 1.526171 ),
       (0., 1.5262116), (0., 1.5267673), (0., 1.5268469), (0., 1.5268601),
       (0., 1.5274162), (0., 1.5274849), (0., 1.5275167), (0., 1.5275674),
       (0., 1.5277978), (0., 1.5281647), (0., 1.5281905), (0., 1.5282056),
       (0., 1.5282844), (0., 1.5287586), (0., 1.528816 ), (0., 1.5290552),
       (0., 1.5292841), (0., 1.5294359), (0., 1.5299528), (0., 1.5299575),
       (0., 1.5302111), (0., 1.5309017), (0., 1.5309788), (0., 1.5317004),
       (0., 1.5322595), (0., 1.5331342), (0., 1.5332546), (0., 1.5336001),
       (0., 1.5352433), (0., 1.5359114), (0., 1.5361683)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.574952 , 8.585416 ), (7.9961247, 8.392253 ),
       (7.024654 , 7.052817 ), (6.880063 , 7.0317564),
       (6.8710656, 7.438543 ), (6.8319063, 7.1509895),
       (6.4717207, 7.1264243), (6.4532657, 6.991649 ),
       (6.3501716, 7.102067 ), (6.13435  , 7.0214634),
       (6.1118526, 7.7562656), (5.9914713, 6.6224422),
       (5.9141607, 7.207226 ), (5.643011 , 5.791106 ),
       (5.5255055, 5.8682175), (5.464007 , 5.6852336),
       (5.4397006, 8.070014 ), (5.351883 , 8.560192 ),
       (5.288718 , 8.407239 ), (5.0701256, 6.8572855),
       (4.979776 , 7.298709 ), (4.6283097, 7.9034405),
       (4.146855 , 8.020342 ), (3.9876487, 4.1351905),
       (3.9310024, 4.330512 ), (3.9048684, 4.7248077),
       (3.7737968, 8.581965 )], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.571637, 10.246965), (9.3339  , 10.694962),
       (9.304689,  9.418888), (8.146032,  8.17053 )],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3085618019104004), (0.0, 1.3141884803771973), (0.0, 1.3163269758224487), (0.0, 1.3177475929260254), (0.0, 1.3201738595962524), (0.0, 1.3209444284439087), (0.0, 1.3224678039550781), (0.0, 1.3231329917907715), (0.0, 1.3238970041275024), (0.0, 1.3241593837738037), (0.0, 1.324407696723938), (0.0, 1.3247650861740112), (0.0, 1.3247857093811035), (0.0, 1.3252099752426147), (0.0, 1.325272798538208), (0.0, 1.325561285018921), (0.0, 1.3256847858428955), (0.0, 1.3260512351989746), (0.0, 1.3260618448257446), (0.0, 1.3265548944473267), (0.0, 1.3269141912460327), (0.0, 1.3270028829574585), (0.0, 1.327214002609253), (0.0, 1.3272234201431274), (0.0, 1.3273345232009888), (0.0, 1.3274650573730469), (0.0, 1.328031301498413), (0.0, 1.3284063339233398), (0.0, 1.3285309076309204), (0.0, 1.32863450050354), (0.0, 1.3286960124969482), (0.0, 1.329305648803711), (0.0, 1.3293180465698242), (0.0, 1.3293218612670898), (0.0, 1.3296644687652588), (0.0, 1.3297927379608154), (0.0, 1.32980215549469), (0.0, 1.3299899101257324), (0.0, 1.3300189971923828), (0.0, 1.3301613330841064), (0.0, 1.3302671909332275), (0.0, 1.3302994966506958), (0.0, 1.330366611480713), (0.0, 1.3309615850448608), (0.0, 1.3311160802841187), (0.0, 1.3311785459518433), (0.0, 1.3312058448791504), (0.0, 1.3312331438064575), (0.0, 1.3314181566238403), (0.0, 1.331552267074585), (0.0, 1.3315529823303223), (0.0, 1.331660509109497), (0.0, 1.331710934638977), (0.0, 1.331722617149353), (0.0, 1.3317978382110596), (0.0, 1.331814169883728), (0.0, 1.3318417072296143), (0.0, 1.3318424224853516), (0.0, 1.3319587707519531), (0.0, 1.3320748805999756), (0.0, 1.3321993350982666), (0.0, 1.3323230743408203), (0.0, 1.3323673009872437), (0.0, 1.3326807022094727), (0.0, 1.3327678442001343), (0.0, 1.3327713012695312), (0.0, 1.3328495025634766), (0.0, 1.3329709768295288), (0.0, 1.3330860137939453), (0.0, 1.3332206010818481), (0.0, 1.3334468603134155), (0.0, 1.3335087299346924), (0.0, 1.3337225914001465), (0.0, 1.3337266445159912), (0.0, 1.334140658378601), (0.0, 1.3341671228408813), (0.0, 1.3342056274414062), (0.0, 1.3343018293380737), (0.0, 1.3346506357192993), (0.0, 1.3347570896148682), (0.0, 1.3348264694213867), (0.0, 1.3351651430130005), (0.0, 1.3352277278900146), (0.0, 1.3352875709533691), (0.0, 1.3353605270385742), (0.0, 1.3354275226593018), (0.0, 1.335435390472412), (0.0, 1.335530161857605), (0.0, 1.3358169794082642), (0.0, 1.3362785577774048), (0.0, 1.3363288640975952), (0.0, 1.336449384689331), (0.0, 1.336643934249878), (0.0, 1.3367325067520142), (0.0, 1.3371957540512085), (0.0, 1.3372212648391724), (0.0, 1.3372724056243896), (0.0, 1.3377448320388794), (0.0, 1.33777916431427), (0.0, 1.337800145149231), (0.0, 1.3378037214279175), (0.0, 1.3381192684173584), (0.0, 1.3385202884674072), (0.0, 1.3385275602340698), (0.0, 1.3387770652770996), (0.0, 1.338971495628357), (0.0, 1.3394795656204224), (0.0, 1.340086579322815), (0.0, 1.3403425216674805), (0.0, 1.3404425382614136), (0.0, 1.3406156301498413), (0.0, 1.340632438659668), (0.0, 1.3409889936447144), (0.0, 1.3411227464675903), (0.0, 1.341320514678955), (0.0, 1.342445731163025), (0.0, 1.3425593376159668), (0.0, 1.3438647985458374), (0.0, 1.3464457988739014), (0.0, 1.3473460674285889), (0.0, 1.3477727174758911), (0.0, 1.3479511737823486), (0.0, 1.3498996496200562), (0.0, 1.4361804723739624), (0.0, 1.4447249174118042), (0.0, 1.4466538429260254), (0.0, 1.4474529027938843), (0.0, 1.4474530220031738), (0.0, 1.4475079774856567), (0.0, 1.4483346939086914), (0.0, 1.4490983486175537), (0.0, 1.4491450786590576), (0.0, 1.4496214389801025), (0.0, 1.4506019353866577), (0.0, 1.4506773948669434), (0.0, 1.4509207010269165), (0.0, 1.4512056112289429), (0.0, 1.4512964487075806), (0.0, 1.4514129161834717), (0.0, 1.4522318840026855), (0.0, 1.452508807182312), (0.0, 1.4525319337844849), (0.0, 1.4525359869003296), (0.0, 1.4527289867401123), (0.0, 1.452794075012207), (0.0, 1.4530104398727417), (0.0, 1.453022837638855), (0.0, 1.4534286260604858), (0.0, 1.453481674194336), (0.0, 1.4536044597625732), (0.0, 1.4536278247833252), (0.0, 1.454136610031128), (0.0, 1.4542722702026367), (0.0, 1.454493761062622), (0.0, 1.4545319080352783), (0.0, 1.454636812210083), (0.0, 1.454649806022644), (0.0, 1.454755187034607), (0.0, 1.4548829793930054), (0.0, 1.4550143480300903), (0.0, 1.455078363418579), (0.0, 1.4553543329238892), (0.0, 1.4557180404663086), (0.0, 1.4558807611465454), (0.0, 1.4560120105743408), (0.0, 1.456100344657898), (0.0, 1.4562147855758667), (0.0, 1.456519365310669), (0.0, 1.4565582275390625), (0.0, 1.4567455053329468), (0.0, 1.4568860530853271), (0.0, 1.4569602012634277), (0.0, 1.4569730758666992), (0.0, 1.457047462463379), (0.0, 1.4570708274841309), (0.0, 1.4571115970611572), (0.0, 1.4572087526321411), (0.0, 1.4573814868927002), (0.0, 1.4574306011199951), (0.0, 1.4575271606445312), (0.0, 1.4575481414794922), (0.0, 1.4577150344848633), (0.0, 1.4578815698623657), (0.0, 1.457897424697876), (0.0, 1.4581643342971802), (0.0, 1.458187460899353), (0.0, 1.4583709239959717), (0.0, 1.4585156440734863), (0.0, 1.4586238861083984), (0.0, 1.4586644172668457), (0.0, 1.4586862325668335), (0.0, 1.4587212800979614), (0.0, 1.4587639570236206), (0.0, 1.4587749242782593), (0.0, 1.4587904214859009), (0.0, 1.459029197692871), (0.0, 1.459059238433838), (0.0, 1.4591195583343506), (0.0, 1.4592269659042358), (0.0, 1.4592901468276978), (0.0, 1.4595093727111816), (0.0, 1.4596405029296875), (0.0, 1.459645390510559), (0.0, 1.4596816301345825), (0.0, 1.4599416255950928), (0.0, 1.4599839448928833), (0.0, 1.4600944519042969), (0.0, 1.4601885080337524), (0.0, 1.4604400396347046), (0.0, 1.460507869720459), (0.0, 1.460892915725708), (0.0, 1.4609293937683105), (0.0, 1.461063265800476), (0.0, 1.4611895084381104), (0.0, 1.4612581729888916), (0.0, 1.4613442420959473), (0.0, 1.4614006280899048), (0.0, 1.4614012241363525), (0.0, 1.4618017673492432), (0.0, 1.4620697498321533), (0.0, 1.462086796760559), (0.0, 1.462281346321106), (0.0, 1.462336778640747), (0.0, 1.463288426399231), (0.0, 1.4635826349258423), (0.0, 1.46387779712677), (0.0, 1.4638817310333252), (0.0, 1.463983178138733), (0.0, 1.4641183614730835), (0.0, 1.4643338918685913), (0.0, 1.4651826620101929), (0.0, 1.4653675556182861), (0.0, 1.4667739868164062), (0.0, 1.4669785499572754), (0.0, 1.4672355651855469), (0.0, 1.4674255847930908), (0.0, 1.467752456665039), (0.0, 1.4679577350616455), (0.0, 1.4680472612380981), (0.0, 1.468593955039978), (0.0, 1.4701542854309082), (0.0, 1.470302700996399), (0.0, 1.4704807996749878), (0.0, 1.4717738628387451), (0.0, 1.473341703414917), (0.0, 1.4733961820602417), (0.0, 1.4792348146438599), (0.0, 1.5057361125946045), (0.0, 1.5085715055465698), (0.0, 1.5106202363967896), (0.0, 1.5110644102096558), (0.0, 1.5111703872680664), (0.0, 1.5117279291152954), (0.0, 1.5120803117752075), (0.0, 1.5125089883804321), (0.0, 1.513040542602539), (0.0, 1.5131285190582275), (0.0, 1.5140358209609985), (0.0, 1.5141541957855225), (0.0, 1.5147597789764404), (0.0, 1.515061378479004), (0.0, 1.5151653289794922), (0.0, 1.5152908563613892), (0.0, 1.5160375833511353), (0.0, 1.516162395477295), (0.0, 1.516309142112732), (0.0, 1.5171197652816772), (0.0, 1.5171724557876587), (0.0, 1.5173869132995605), (0.0, 1.5174133777618408), (0.0, 1.5174192190170288), (0.0, 1.5176215171813965), (0.0, 1.5176604986190796), (0.0, 1.517701268196106), (0.0, 1.5177465677261353), (0.0, 1.5181773900985718), (0.0, 1.518544316291809), (0.0, 1.5186794996261597), (0.0, 1.5187817811965942), (0.0, 1.5188179016113281), (0.0, 1.518831491470337), (0.0, 1.5190714597702026), (0.0, 1.5190750360488892), (0.0, 1.5192795991897583), (0.0, 1.5194638967514038), (0.0, 1.5196802616119385), (0.0, 1.5199486017227173), (0.0, 1.5199979543685913), (0.0, 1.5200761556625366), (0.0, 1.5201129913330078), (0.0, 1.5201212167739868), (0.0, 1.5204179286956787), (0.0, 1.5205795764923096), (0.0, 1.5207297801971436), (0.0, 1.520780086517334), (0.0, 1.5209556818008423), (0.0, 1.5210069417953491), (0.0, 1.5210975408554077), (0.0, 1.521251916885376), (0.0, 1.5215290784835815), (0.0, 1.5215599536895752), (0.0, 1.5216292142868042), (0.0, 1.5217616558074951), (0.0, 1.521863341331482), (0.0, 1.5218732357025146), (0.0, 1.521894931793213), (0.0, 1.5220320224761963), (0.0, 1.5220448970794678), (0.0, 1.5223257541656494), (0.0, 1.5224164724349976), (0.0, 1.522745966911316), (0.0, 1.5230237245559692), (0.0, 1.523261547088623), (0.0, 1.5233808755874634), (0.0, 1.52409029006958), (0.0, 1.524111270904541), (0.0, 1.5243425369262695), (0.0, 1.5243433713912964), (0.0, 1.5243608951568604), (0.0, 1.5244073867797852), (0.0, 1.5244520902633667), (0.0, 1.524520754814148), (0.0, 1.524659276008606), (0.0, 1.5247368812561035), (0.0, 1.5248361825942993), (0.0, 1.5249558687210083), (0.0, 1.5249828100204468), (0.0, 1.525009036064148), (0.0, 1.5251165628433228), (0.0, 1.5252019166946411), (0.0, 1.525244116783142), (0.0, 1.5252777338027954), (0.0, 1.5254333019256592), (0.0, 1.5254876613616943), (0.0, 1.5256433486938477), (0.0, 1.5257991552352905), (0.0, 1.5260748863220215), (0.0, 1.5261003971099854), (0.0, 1.526167631149292), (0.0, 1.5261709690093994), (0.0, 1.5262116193771362), (0.0, 1.5267672538757324), (0.0, 1.5268468856811523), (0.0, 1.5268601179122925), (0.0, 1.5274162292480469), (0.0, 1.5274848937988281), (0.0, 1.5275167226791382), (0.0, 1.5275673866271973), (0.0, 1.527797818183899), (0.0, 1.5281647443771362), (0.0, 1.5281904935836792), (0.0, 1.5282056331634521), (0.0, 1.5282844305038452), (0.0, 1.5287586450576782), (0.0, 1.5288159847259521), (0.0, 1.5290552377700806), (0.0, 1.529284119606018), (0.0, 1.5294358730316162), (0.0, 1.5299527645111084), (0.0, 1.5299575328826904), (0.0, 1.530211091041565), (0.0, 1.5309016704559326), (0.0, 1.530978798866272), (0.0, 1.5317003726959229), (0.0, 1.532259464263916), (0.0, 1.5331342220306396), (0.0, 1.533254623413086), (0.0, 1.533600091934204), (0.0, 1.535243272781372), (0.0, 1.5359114408493042), (0.0, 1.5361683368682861)], [(8.574952125549316, 8.585415840148926), (7.996124744415283, 8.392252922058105), (7.024653911590576, 7.052816867828369), (6.880063056945801, 7.031756401062012), (6.871065616607666, 7.43854284286499), (6.831906318664551, 7.150989532470703), (6.4717206954956055, 7.126424312591553), (6.45326566696167, 6.991649150848389), (6.3501715660095215, 7.102066993713379), (6.134349822998047, 7.021463394165039), (6.111852645874023, 7.756265640258789), (5.991471290588379, 6.622442245483398), (5.91416072845459, 7.207225799560547), (5.643011093139648, 5.791106224060059), (5.525505542755127, 5.868217468261719), (5.4640069007873535, 5.6852335929870605), (5.439700603485107, 8.070013999938965), (5.3518829345703125, 8.560192108154297), (5.288718223571777, 8.407238960266113), (5.070125579833984, 6.857285499572754), (4.979775905609131, 7.298708915710449), (4.628309726715088, 7.903440475463867), (4.146854877471924, 8.020341873168945), (3.9876487255096436, 4.135190486907959), (3.931002378463745, 4.330512046813965), (3.9048683643341064, 4.7248077392578125), (3.773796796798706, 8.581965446472168)], [(9.571637153625488, 10.246965408325195), (9.333900451660156, 10.694961547851562), (9.304689407348633, 9.418888092041016), (8.146032333374023, 8.170530319213867)]]
[array([[0.        , 1.3085618 ],
       [0.        , 1.31418848],
       [0.        , 1.31632698],
       [0.        , 1.31774759],
       [0.        , 1.32017386],
       [0.        , 1.32094443],
       [0.        , 1.3224678 ],
       [0.        , 1.32313299],
       [0.        , 1.323897  ],
       [0.        , 1.32415938],
       [0.        , 1.3244077 ],
       [0.        , 1.32476509],
       [0.        , 1.32478571],
       [0.        , 1.32520998],
       [0.        , 1.3252728 ],
       [0.        , 1.32556129],
       [0.        , 1.32568479],
       [0.        , 1.32605124],
       [0.        , 1.32606184],
       [0.        , 1.32655489],
       [0.        , 1.32691419],
       [0.        , 1.32700288],
       [0.        , 1.327214  ],
       [0.        , 1.32722342],
       [0.        , 1.32733452],
       [0.        , 1.32746506],
       [0.        , 1.3280313 ],
       [0.        , 1.32840633],
       [0.        , 1.32853091],
       [0.        , 1.3286345 ],
       [0.        , 1.32869601],
       [0.        , 1.32930565],
       [0.        , 1.32931805],
       [0.        , 1.32932186],
       [0.        , 1.32966447],
       [0.        , 1.32979274],
       [0.        , 1.32980216],
       [0.        , 1.32998991],
       [0.        , 1.330019  ],
       [0.        , 1.33016133],
       [0.        , 1.33026719],
       [0.        , 1.3302995 ],
       [0.        , 1.33036661],
       [0.        , 1.33096159],
       [0.        , 1.33111608],
       [0.        , 1.33117855],
       [0.        , 1.33120584],
       [0.        , 1.33123314],
       [0.        , 1.33141816],
       [0.        , 1.33155227],
       [0.        , 1.33155298],
       [0.        , 1.33166051],
       [0.        , 1.33171093],
       [0.        , 1.33172262],
       [0.        , 1.33179784],
       [0.        , 1.33181417],
       [0.        , 1.33184171],
       [0.        , 1.33184242],
       [0.        , 1.33195877],
       [0.        , 1.33207488],
       [0.        , 1.33219934],
       [0.        , 1.33232307],
       [0.        , 1.3323673 ],
       [0.        , 1.3326807 ],
       [0.        , 1.33276784],
       [0.        , 1.3327713 ],
       [0.        , 1.3328495 ],
       [0.        , 1.33297098],
       [0.        , 1.33308601],
       [0.        , 1.3332206 ],
       [0.        , 1.33344686],
       [0.        , 1.33350873],
       [0.        , 1.33372259],
       [0.        , 1.33372664],
       [0.        , 1.33414066],
       [0.        , 1.33416712],
       [0.        , 1.33420563],
       [0.        , 1.33430183],
       [0.        , 1.33465064],
       [0.        , 1.33475709],
       [0.        , 1.33482647],
       [0.        , 1.33516514],
       [0.        , 1.33522773],
       [0.        , 1.33528757],
       [0.        , 1.33536053],
       [0.        , 1.33542752],
       [0.        , 1.33543539],
       [0.        , 1.33553016],
       [0.        , 1.33581698],
       [0.        , 1.33627856],
       [0.        , 1.33632886],
       [0.        , 1.33644938],
       [0.        , 1.33664393],
       [0.        , 1.33673251],
       [0.        , 1.33719575],
       [0.        , 1.33722126],
       [0.        , 1.33727241],
       [0.        , 1.33774483],
       [0.        , 1.33777916],
       [0.        , 1.33780015],
       [0.        , 1.33780372],
       [0.        , 1.33811927],
       [0.        , 1.33852029],
       [0.        , 1.33852756],
       [0.        , 1.33877707],
       [0.        , 1.3389715 ],
       [0.        , 1.33947957],
       [0.        , 1.34008658],
       [0.        , 1.34034252],
       [0.        , 1.34044254],
       [0.        , 1.34061563],
       [0.        , 1.34063244],
       [0.        , 1.34098899],
       [0.        , 1.34112275],
       [0.        , 1.34132051],
       [0.        , 1.34244573],
       [0.        , 1.34255934],
       [0.        , 1.3438648 ],
       [0.        , 1.3464458 ],
       [0.        , 1.34734607],
       [0.        , 1.34777272],
       [0.        , 1.34795117],
       [0.        , 1.34989965],
       [0.        , 1.43618047],
       [0.        , 1.44472492],
       [0.        , 1.44665384],
       [0.        , 1.4474529 ],
       [0.        , 1.44745302],
       [0.        , 1.44750798],
       [0.        , 1.44833469],
       [0.        , 1.44909835],
       [0.        , 1.44914508],
       [0.        , 1.44962144],
       [0.        , 1.45060194],
       [0.        , 1.45067739],
       [0.        , 1.4509207 ],
       [0.        , 1.45120561],
       [0.        , 1.45129645],
       [0.        , 1.45141292],
       [0.        , 1.45223188],
       [0.        , 1.45250881],
       [0.        , 1.45253193],
       [0.        , 1.45253599],
       [0.        , 1.45272899],
       [0.        , 1.45279408],
       [0.        , 1.45301044],
       [0.        , 1.45302284],
       [0.        , 1.45342863],
       [0.        , 1.45348167],
       [0.        , 1.45360446],
       [0.        , 1.45362782],
       [0.        , 1.45413661],
       [0.        , 1.45427227],
       [0.        , 1.45449376],
       [0.        , 1.45453191],
       [0.        , 1.45463681],
       [0.        , 1.45464981],
       [0.        , 1.45475519],
       [0.        , 1.45488298],
       [0.        , 1.45501435],
       [0.        , 1.45507836],
       [0.        , 1.45535433],
       [0.        , 1.45571804],
       [0.        , 1.45588076],
       [0.        , 1.45601201],
       [0.        , 1.45610034],
       [0.        , 1.45621479],
       [0.        , 1.45651937],
       [0.        , 1.45655823],
       [0.        , 1.45674551],
       [0.        , 1.45688605],
       [0.        , 1.4569602 ],
       [0.        , 1.45697308],
       [0.        , 1.45704746],
       [0.        , 1.45707083],
       [0.        , 1.4571116 ],
       [0.        , 1.45720875],
       [0.        , 1.45738149],
       [0.        , 1.4574306 ],
       [0.        , 1.45752716],
       [0.        , 1.45754814],
       [0.        , 1.45771503],
       [0.        , 1.45788157],
       [0.        , 1.45789742],
       [0.        , 1.45816433],
       [0.        , 1.45818746],
       [0.        , 1.45837092],
       [0.        , 1.45851564],
       [0.        , 1.45862389],
       [0.        , 1.45866442],
       [0.        , 1.45868623],
       [0.        , 1.45872128],
       [0.        , 1.45876396],
       [0.        , 1.45877492],
       [0.        , 1.45879042],
       [0.        , 1.4590292 ],
       [0.        , 1.45905924],
       [0.        , 1.45911956],
       [0.        , 1.45922697],
       [0.        , 1.45929015],
       [0.        , 1.45950937],
       [0.        , 1.4596405 ],
       [0.        , 1.45964539],
       [0.        , 1.45968163],
       [0.        , 1.45994163],
       [0.        , 1.45998394],
       [0.        , 1.46009445],
       [0.        , 1.46018851],
       [0.        , 1.46044004],
       [0.        , 1.46050787],
       [0.        , 1.46089292],
       [0.        , 1.46092939],
       [0.        , 1.46106327],
       [0.        , 1.46118951],
       [0.        , 1.46125817],
       [0.        , 1.46134424],
       [0.        , 1.46140063],
       [0.        , 1.46140122],
       [0.        , 1.46180177],
       [0.        , 1.46206975],
       [0.        , 1.4620868 ],
       [0.        , 1.46228135],
       [0.        , 1.46233678],
       [0.        , 1.46328843],
       [0.        , 1.46358263],
       [0.        , 1.4638778 ],
       [0.        , 1.46388173],
       [0.        , 1.46398318],
       [0.        , 1.46411836],
       [0.        , 1.46433389],
       [0.        , 1.46518266],
       [0.        , 1.46536756],
       [0.        , 1.46677399],
       [0.        , 1.46697855],
       [0.        , 1.46723557],
       [0.        , 1.46742558],
       [0.        , 1.46775246],
       [0.        , 1.46795774],
       [0.        , 1.46804726],
       [0.        , 1.46859396],
       [0.        , 1.47015429],
       [0.        , 1.4703027 ],
       [0.        , 1.4704808 ],
       [0.        , 1.47177386],
       [0.        , 1.4733417 ],
       [0.        , 1.47339618],
       [0.        , 1.47923481],
       [0.        , 1.50573611],
       [0.        , 1.50857151],
       [0.        , 1.51062024],
       [0.        , 1.51106441],
       [0.        , 1.51117039],
       [0.        , 1.51172793],
       [0.        , 1.51208031],
       [0.        , 1.51250899],
       [0.        , 1.51304054],
       [0.        , 1.51312852],
       [0.        , 1.51403582],
       [0.        , 1.5141542 ],
       [0.        , 1.51475978],
       [0.        , 1.51506138],
       [0.        , 1.51516533],
       [0.        , 1.51529086],
       [0.        , 1.51603758],
       [0.        , 1.5161624 ],
       [0.        , 1.51630914],
       [0.        , 1.51711977],
       [0.        , 1.51717246],
       [0.        , 1.51738691],
       [0.        , 1.51741338],
       [0.        , 1.51741922],
       [0.        , 1.51762152],
       [0.        , 1.5176605 ],
       [0.        , 1.51770127],
       [0.        , 1.51774657],
       [0.        , 1.51817739],
       [0.        , 1.51854432],
       [0.        , 1.5186795 ],
       [0.        , 1.51878178],
       [0.        , 1.5188179 ],
       [0.        , 1.51883149],
       [0.        , 1.51907146],
       [0.        , 1.51907504],
       [0.        , 1.5192796 ],
       [0.        , 1.5194639 ],
       [0.        , 1.51968026],
       [0.        , 1.5199486 ],
       [0.        , 1.51999795],
       [0.        , 1.52007616],
       [0.        , 1.52011299],
       [0.        , 1.52012122],
       [0.        , 1.52041793],
       [0.        , 1.52057958],
       [0.        , 1.52072978],
       [0.        , 1.52078009],
       [0.        , 1.52095568],
       [0.        , 1.52100694],
       [0.        , 1.52109754],
       [0.        , 1.52125192],
       [0.        , 1.52152908],
       [0.        , 1.52155995],
       [0.        , 1.52162921],
       [0.        , 1.52176166],
       [0.        , 1.52186334],
       [0.        , 1.52187324],
       [0.        , 1.52189493],
       [0.        , 1.52203202],
       [0.        , 1.5220449 ],
       [0.        , 1.52232575],
       [0.        , 1.52241647],
       [0.        , 1.52274597],
       [0.        , 1.52302372],
       [0.        , 1.52326155],
       [0.        , 1.52338088],
       [0.        , 1.52409029],
       [0.        , 1.52411127],
       [0.        , 1.52434254],
       [0.        , 1.52434337],
       [0.        , 1.5243609 ],
       [0.        , 1.52440739],
       [0.        , 1.52445209],
       [0.        , 1.52452075],
       [0.        , 1.52465928],
       [0.        , 1.52473688],
       [0.        , 1.52483618],
       [0.        , 1.52495587],
       [0.        , 1.52498281],
       [0.        , 1.52500904],
       [0.        , 1.52511656],
       [0.        , 1.52520192],
       [0.        , 1.52524412],
       [0.        , 1.52527773],
       [0.        , 1.5254333 ],
       [0.        , 1.52548766],
       [0.        , 1.52564335],
       [0.        , 1.52579916],
       [0.        , 1.52607489],
       [0.        , 1.5261004 ],
       [0.        , 1.52616763],
       [0.        , 1.52617097],
       [0.        , 1.52621162],
       [0.        , 1.52676725],
       [0.        , 1.52684689],
       [0.        , 1.52686012],
       [0.        , 1.52741623],
       [0.        , 1.52748489],
       [0.        , 1.52751672],
       [0.        , 1.52756739],
       [0.        , 1.52779782],
       [0.        , 1.52816474],
       [0.        , 1.52819049],
       [0.        , 1.52820563],
       [0.        , 1.52828443],
       [0.        , 1.52875865],
       [0.        , 1.52881598],
       [0.        , 1.52905524],
       [0.        , 1.52928412],
       [0.        , 1.52943587],
       [0.        , 1.52995276],
       [0.        , 1.52995753],
       [0.        , 1.53021109],
       [0.        , 1.53090167],
       [0.        , 1.5309788 ],
       [0.        , 1.53170037],
       [0.        , 1.53225946],
       [0.        , 1.53313422],
       [0.        , 1.53325462],
       [0.        , 1.53360009],
       [0.        , 1.53524327],
       [0.        , 1.53591144],
       [0.        , 1.53616834]]), array([[8.57495213, 8.58541584],
       [7.99612474, 8.39225292],
       [7.02465391, 7.05281687],
       [6.88006306, 7.0317564 ],
       [6.87106562, 7.43854284],
       [6.83190632, 7.15098953],
       [6.4717207 , 7.12642431],
       [6.45326567, 6.99164915],
       [6.35017157, 7.10206699],
       [6.13434982, 7.02146339],
       [6.11185265, 7.75626564],
       [5.99147129, 6.62244225],
       [5.91416073, 7.2072258 ],
       [5.64301109, 5.79110622],
       [5.52550554, 5.86821747],
       [5.4640069 , 5.68523359],
       [5.4397006 , 8.070014  ],
       [5.35188293, 8.56019211],
       [5.28871822, 8.40723896],
       [5.07012558, 6.8572855 ],
       [4.97977591, 7.29870892],
       [4.62830973, 7.90344048],
       [4.14685488, 8.02034187],
       [3.98764873, 4.13519049],
       [3.93100238, 4.33051205],
       [3.90486836, 4.72480774],
       [3.7737968 , 8.58196545]]), array([[ 9.57163715, 10.24696541],
       [ 9.33390045, 10.69496155],
       [ 9.30468941,  9.41888809],
       [ 8.14603233,  8.17053032]])]2024-03-06 18:02:37.914427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LIM ph vector generated, counter: 222
2024-03-06 18:02:41.923806: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:41.966972: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:43.042702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LIO ph vector generated, counter: 223
2024-03-06 18:02:47.005143: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:47.047676: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:48.140610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LIT ph vector generated, counter: 224
2024-03-06 18:02:51.624231: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:51.669461: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:53.176301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LJE ph vector generated, counter: 225
2024-03-06 18:02:56.704895: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:02:56.747713: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:02:57.757442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LJF ph vector generated, counter: 226
2024-03-06 18:03:01.182791: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:01.224215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:02.187937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LJK ph vector generated, counter: 227
2024-03-06 18:03:05.496039: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:05.540084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:06.547522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LJM ph vector generated, counter: 228
2024-03-06 18:03:10.114374: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:10.156502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:11.113425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LJN ph vector generated, counter: 229
2024-03-06 18:03:14.576838: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:14.619669: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:15.773648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LLX ph vector generated, counter: 230
2024-03-06 18:03:19.220437: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:19.263583: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:20.175387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LMY ph vector generated, counter: 231
2024-03-06 18:03:23.457322: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:23.500644: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:24.684053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LO2 ph vector generated, counter: 232
2024-03-06 18:03:28.355986: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:28.399294: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:29.555210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LO9 ph vector generated, counter: 233
2024-03-06 18:03:33.130612: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:33.176284: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:34.205273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LOB ph vector generated, counter: 234
2024-03-06 18:03:37.919218: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:37.967595: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:39.142342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LOC ph vector generated, counter: 235
2024-03-06 18:03:42.411231: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:42.462306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:43.630101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LOT ph vector generated, counter: 236
2024-03-06 18:03:47.890329: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:47.933448: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:49.002282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LOU ph vector generated, counter: 237
2024-03-06 18:03:52.464386: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:52.507228: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:53.750246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LP6 ph vector generated, counter: 238
2024-03-06 18:03:57.297673: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:03:57.340530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:03:58.459177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LP7 ph vector generated, counter: 239
2024-03-06 18:04:01.931616: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:01.975293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:03.098671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LP8 ph vector generated, counter: 240
2024-03-06 18:04:06.973754: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:07.016617: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:08.262206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LPF ph vector generated, counter: 241
2024-03-06 18:04:12.040248: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:12.082988: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:13.223516: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LPG ph vector generated, counter: 242
2024-03-06 18:04:16.844180: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:16.887420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:17.935576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LPJ ph vector generated, counter: 243
2024-03-06 18:04:21.765367: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:21.808273: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:22.900143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LPK ph vector generated, counter: 244
2024-03-06 18:04:26.371677: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:26.414646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:27.460911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LPL ph vector generated, counter: 245
2024-03-06 18:04:31.175555: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:31.220506: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:32.319904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LPN ph vector generated, counter: 246
2024-03-06 18:04:35.926192: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:35.969019: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:37.051863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LQX ph vector generated, counter: 247
2024-03-06 18:04:40.641426: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:40.683965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:41.663030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LR9 ph vector generated, counter: 248
2024-03-06 18:04:45.042248: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:45.084756: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:46.210905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LRC ph vector generated, counter: 249
2024-03-06 18:04:50.144064: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:50.187176: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:51.358791: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LRK ph vector generated, counter: 250
2024-03-06 18:04:54.785592: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:54.828729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:04:55.788880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LRM ph vector generated, counter: 251
2024-03-06 18:04:59.218827: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:04:59.261554: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:00.142034: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LS5 ph vector generated, counter: 252
2024-03-06 18:05:03.793737: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:03.836766: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:04.823584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LS6 ph vector generated, counter: 253
2024-03-06 18:05:08.178558: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:08.221633: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:09.419954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LSB ph vector generated, counter: 254
2024-03-06 18:05:12.806862: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:12.849692: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:13.980522: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LSD ph vector generated, counter: 255
2024-03-06 18:05:17.502968: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:17.545684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:18.572881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LSZ ph vector generated, counter: 256
2024-03-06 18:05:22.203480: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:22.246707: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:23.313171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LT8 ph vector generated, counter: 257
2024-03-06 18:05:26.770777: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:26.813565: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:28.030621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LTI ph vector generated, counter: 258
2024-03-06 18:05:31.803183: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:31.846551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:32.991510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LTK ph vector generated, counter: 259
2024-03-06 18:05:36.652943: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:36.695901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:37.701967: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.2655019), (0., 1.3109576), (0., 1.3214461), (0., 1.3246609),
       (0., 1.3250378), (0., 1.3255063), (0., 1.326446 ), (0., 1.3275712),
       (0., 1.3276758), (0., 1.3279787), (0., 1.3281437), (0., 1.3281755),
       (0., 1.3282803), (0., 1.3285587), (0., 1.328733 ), (0., 1.3287855),
       (0., 1.3289737), (0., 1.3290786), (0., 1.3290974), (0., 1.3292035),
       (0., 1.3293407), (0., 1.3293813), (0., 1.3294388), (0., 1.3294759),
       (0., 1.3295857), (0., 1.329658 ), (0., 1.3296717), (0., 1.3296992),
       (0., 1.3297696), (0., 1.3300686), (0., 1.3301482), (0., 1.3301504),
       (0., 1.3301927), (0., 1.3303646), (0., 1.3304349), (0., 1.3306402),
       (0., 1.3306543), (0., 1.3308674), (0., 1.3312649), (0., 1.3313605),
       (0., 1.3317534), (0., 1.3317666), (0., 1.3318943), (0., 1.3319074),
       (0., 1.3320845), (0., 1.3322583), (0., 1.3322932), (0., 1.33245  ),
       (0., 1.3325244), (0., 1.3325871), (0., 1.3327391), (0., 1.3328718),
       (0., 1.3331664), (0., 1.3331981), (0., 1.3332549), (0., 1.3334107),
       (0., 1.3335135), (0., 1.3335903), (0., 1.3337493), (0., 1.3337517),
       (0., 1.3337712), (0., 1.33422  ), (0., 1.334283 ), (0., 1.3343351),
       (0., 1.3345393), (0., 1.3347484), (0., 1.3349704), (0., 1.3349938),
       (0., 1.3350064), (0., 1.3350604), (0., 1.3350972), (0., 1.3351299),
       (0., 1.3351322), (0., 1.3352107), (0., 1.3353262), (0., 1.3357826),
       (0., 1.3358492), (0., 1.3359338), (0., 1.3360138), (0., 1.3363682),
       (0., 1.3364134), (0., 1.3366888), (0., 1.3367081), (0., 1.3367236),
       (0., 1.3368971), (0., 1.3370209), (0., 1.3371071), (0., 1.3371717),
       (0., 1.3372327), (0., 1.3372834), (0., 1.3373617), (0., 1.337373 ),
       (0., 1.3374463), (0., 1.338019 ), (0., 1.3381013), (0., 1.3385948),
       (0., 1.3391131), (0., 1.3391407), (0., 1.3396679), (0., 1.3400731),
       (0., 1.3401153), (0., 1.3402188), (0., 1.3402458), (0., 1.3408694),
       (0., 1.3412628), (0., 1.3416842), (0., 1.3469332), (0., 1.449103 ),
       (0., 1.4497856), (0., 1.4504728), (0., 1.4525211), (0., 1.4526507),
       (0., 1.4530727), (0., 1.4534444), (0., 1.4536897), (0., 1.4537635),
       (0., 1.4538373), (0., 1.4545664), (0., 1.4545878), (0., 1.4545963),
       (0., 1.4546487), (0., 1.4549911), (0., 1.4551415), (0., 1.4552559),
       (0., 1.455608 ), (0., 1.4557683), (0., 1.4558271), (0., 1.4558692),
       (0., 1.4559003), (0., 1.4559728), (0., 1.4560175), (0., 1.4560747),
       (0., 1.4561651), (0., 1.4561942), (0., 1.4565659), (0., 1.4566927),
       (0., 1.4567047), (0., 1.4568702), (0., 1.4568807), (0., 1.4570197),
       (0., 1.4571738), (0., 1.4572105), (0., 1.4573015), (0., 1.4574203),
       (0., 1.4575261), (0., 1.4576287), (0., 1.457722 ), (0., 1.4577507),
       (0., 1.4577929), (0., 1.4579138), (0., 1.4579967), (0., 1.4583497),
       (0., 1.4583576), (0., 1.4584678), (0., 1.458594 ), (0., 1.4588525),
       (0., 1.4589032), (0., 1.4591032), (0., 1.4591593), (0., 1.4596038),
       (0., 1.4597838), (0., 1.459903 ), (0., 1.4599289), (0., 1.4599954),
       (0., 1.4600582), (0., 1.4602293), (0., 1.4602907), (0., 1.4604064),
       (0., 1.4606502), (0., 1.4607213), (0., 1.4607612), (0., 1.4607847),
       (0., 1.4608032), (0., 1.4608307), (0., 1.4608691), (0., 1.4610391),
       (0., 1.4611812), (0., 1.4611998), (0., 1.461219 ), (0., 1.4612404),
       (0., 1.4612626), (0., 1.461313 ), (0., 1.4614451), (0., 1.4616672),
       (0., 1.4617282), (0., 1.461791 ), (0., 1.4617959), (0., 1.4618025),
       (0., 1.4618202), (0., 1.4621195), (0., 1.4622566), (0., 1.462367 ),
       (0., 1.4629103), (0., 1.4631553), (0., 1.4634781), (0., 1.4635524),
       (0., 1.4640785), (0., 1.4641356), (0., 1.4644004), (0., 1.4650191),
       (0., 1.4650457), (0., 1.4650549), (0., 1.4659913), (0., 1.46611  ),
       (0., 1.4663893), (0., 1.4676815), (0., 1.469262 ), (0., 1.469749 ),
       (0., 1.4697691), (0., 1.4702146), (0., 1.470861 ), (0., 1.4709722),
       (0., 1.4714065), (0., 1.4724816), (0., 1.4773003), (0., 1.5096732),
       (0., 1.5098598), (0., 1.5125759), (0., 1.5136671), (0., 1.5136691),
       (0., 1.5140733), (0., 1.5144973), (0., 1.5155879), (0., 1.5156403),
       (0., 1.5164449), (0., 1.5165087), (0., 1.5168426), (0., 1.5169984),
       (0., 1.5171103), (0., 1.5175157), (0., 1.5175533), (0., 1.5179029),
       (0., 1.5181487), (0., 1.5184501), (0., 1.5185488), (0., 1.518695 ),
       (0., 1.518702 ), (0., 1.5187472), (0., 1.5192548), (0., 1.5192819),
       (0., 1.5195518), (0., 1.5195597), (0., 1.5197083), (0., 1.5198561),
       (0., 1.5198747), (0., 1.5198832), (0., 1.5199486), (0., 1.5202134),
       (0., 1.520361 ), (0., 1.5203681), (0., 1.5205535), (0., 1.5206019),
       (0., 1.5206603), (0., 1.520933 ), (0., 1.5212548), (0., 1.5212775),
       (0., 1.5213559), (0., 1.5217047), (0., 1.5217222), (0., 1.5220091),
       (0., 1.5221833), (0., 1.5222385), (0., 1.522249 ), (0., 1.5223492),
       (0., 1.5224056), (0., 1.5226014), (0., 1.5227189), (0., 1.5227327),
       (0., 1.5228984), (0., 1.5229337), (0., 1.5229486), (0., 1.5229686),
       (0., 1.5230519), (0., 1.5231189), (0., 1.523274 ), (0., 1.5234827),
       (0., 1.5236367), (0., 1.5238584), (0., 1.5239196), (0., 1.5239387),
       (0., 1.52401  ), (0., 1.5240122), (0., 1.5242244), (0., 1.5242425),
       (0., 1.5242733), (0., 1.5244235), (0., 1.5245154), (0., 1.524528 ),
       (0., 1.5245936), (0., 1.5246649), (0., 1.5248816), (0., 1.5249138),
       (0., 1.5250008), (0., 1.5250957), (0., 1.5253171), (0., 1.525345 ),
       (0., 1.5254023), (0., 1.5254023), (0., 1.5255469), (0., 1.5255568),
       (0., 1.5258265), (0., 1.5262823), (0., 1.5262891), (0., 1.526306 ),
       (0., 1.5265317), (0., 1.5266017), (0., 1.5266157), (0., 1.5266343),
       (0., 1.5267148), (0., 1.5272849), (0., 1.5276608), (0., 1.5277181),
       (0., 1.527905 ), (0., 1.5280663), (0., 1.5284743), (0., 1.5285723),
       (0., 1.5289595), (0., 1.529816 ), (0., 1.5309702), (0., 1.5319488),
       (0., 1.5342455), (0., 1.5359005), (0., 1.5361073)],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.582215 , 8.645893 ), (8.467967 , 9.083308 ),
       (7.245451 , 7.4977155), (7.115696 , 7.365825 ),
       (7.106744 , 7.4138374), (7.020074 , 7.709149 ),
       (7.015802 , 7.2872005), (6.5883074, 6.976289 ),
       (6.5520806, 7.6674056), (6.3545747, 7.693522 ),
       (5.6913357, 5.8085155), (5.326921 , 5.8085155),
       (5.1595883, 6.4462175), (5.0923505, 8.466182 ),
       (4.898866 , 8.550011 ), (4.0877743, 4.383828 ),
       (4.075371 , 8.34184  ), (4.0100336, 8.857749 ),
       (3.9332125, 3.9527562), (3.8911483, 4.175315 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(11.8741665, 12.154477), ( 9.535665 , 10.39823 ),
       ( 9.515406 , 10.259883), ( 9.001707 , 10.074003),
       ( 8.655245 ,  8.661142)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.265501856803894), (0.0, 1.3109575510025024), (0.0, 1.3214460611343384), (0.0, 1.3246608972549438), (0.0, 1.3250378370285034), (0.0, 1.325506329536438), (0.0, 1.3264460563659668), (0.0, 1.327571153640747), (0.0, 1.3276758193969727), (0.0, 1.3279787302017212), (0.0, 1.3281437158584595), (0.0, 1.3281755447387695), (0.0, 1.3282803297042847), (0.0, 1.3285586833953857), (0.0, 1.328732967376709), (0.0, 1.3287855386734009), (0.0, 1.328973650932312), (0.0, 1.3290785551071167), (0.0, 1.3290973901748657), (0.0, 1.329203486442566), (0.0, 1.3293406963348389), (0.0, 1.3293813467025757), (0.0, 1.3294388055801392), (0.0, 1.3294758796691895), (0.0, 1.3295856714248657), (0.0, 1.329658031463623), (0.0, 1.3296717405319214), (0.0, 1.329699158668518), (0.0, 1.3297696113586426), (0.0, 1.330068588256836), (0.0, 1.3301482200622559), (0.0, 1.3301503658294678), (0.0, 1.3301926851272583), (0.0, 1.3303645849227905), (0.0, 1.3304349184036255), (0.0, 1.330640196800232), (0.0, 1.330654263496399), (0.0, 1.3308674097061157), (0.0, 1.331264853477478), (0.0, 1.3313604593276978), (0.0, 1.3317533731460571), (0.0, 1.3317666053771973), (0.0, 1.3318942785263062), (0.0, 1.3319073915481567), (0.0, 1.3320845365524292), (0.0, 1.3322583436965942), (0.0, 1.332293152809143), (0.0, 1.332450032234192), (0.0, 1.3325244188308716), (0.0, 1.3325871229171753), (0.0, 1.3327391147613525), (0.0, 1.3328717947006226), (0.0, 1.3331663608551025), (0.0, 1.333198070526123), (0.0, 1.3332549333572388), (0.0, 1.3334107398986816), (0.0, 1.3335134983062744), (0.0, 1.3335902690887451), (0.0, 1.3337492942810059), (0.0, 1.3337516784667969), (0.0, 1.3337712287902832), (0.0, 1.334220051765442), (0.0, 1.3342829942703247), (0.0, 1.3343350887298584), (0.0, 1.3345392942428589), (0.0, 1.334748387336731), (0.0, 1.3349703550338745), (0.0, 1.334993839263916), (0.0, 1.3350063562393188), (0.0, 1.3350603580474854), (0.0, 1.3350971937179565), (0.0, 1.3351298570632935), (0.0, 1.3351322412490845), (0.0, 1.3352106809616089), (0.0, 1.3353261947631836), (0.0, 1.3357826471328735), (0.0, 1.3358491659164429), (0.0, 1.335933804512024), (0.0, 1.3360137939453125), (0.0, 1.336368203163147), (0.0, 1.3364133834838867), (0.0, 1.336688756942749), (0.0, 1.3367080688476562), (0.0, 1.3367235660552979), (0.0, 1.3368971347808838), (0.0, 1.3370208740234375), (0.0, 1.3371070623397827), (0.0, 1.3371716737747192), (0.0, 1.3372327089309692), (0.0, 1.3372833728790283), (0.0, 1.3373616933822632), (0.0, 1.3373730182647705), (0.0, 1.3374463319778442), (0.0, 1.3380190134048462), (0.0, 1.3381012678146362), (0.0, 1.3385947942733765), (0.0, 1.3391131162643433), (0.0, 1.3391406536102295), (0.0, 1.3396679162979126), (0.0, 1.3400731086730957), (0.0, 1.3401153087615967), (0.0, 1.3402187824249268), (0.0, 1.3402458429336548), (0.0, 1.340869426727295), (0.0, 1.3412628173828125), (0.0, 1.3416842222213745), (0.0, 1.3469332456588745), (0.0, 1.4491029977798462), (0.0, 1.449785590171814), (0.0, 1.4504728317260742), (0.0, 1.4525210857391357), (0.0, 1.4526506662368774), (0.0, 1.4530726671218872), (0.0, 1.4534443616867065), (0.0, 1.453689694404602), (0.0, 1.453763484954834), (0.0, 1.453837275505066), (0.0, 1.4545663595199585), (0.0, 1.4545878171920776), (0.0, 1.4545962810516357), (0.0, 1.454648733139038), (0.0, 1.454991102218628), (0.0, 1.455141544342041), (0.0, 1.4552558660507202), (0.0, 1.4556080102920532), (0.0, 1.455768346786499), (0.0, 1.4558271169662476), (0.0, 1.455869197845459), (0.0, 1.4559003114700317), (0.0, 1.4559727907180786), (0.0, 1.4560174942016602), (0.0, 1.4560747146606445), (0.0, 1.456165075302124), (0.0, 1.4561941623687744), (0.0, 1.4565658569335938), (0.0, 1.4566926956176758), (0.0, 1.4567047357559204), (0.0, 1.456870198249817), (0.0, 1.4568806886672974), (0.0, 1.4570196866989136), (0.0, 1.4571738243103027), (0.0, 1.4572105407714844), (0.0, 1.4573014974594116), (0.0, 1.4574203491210938), (0.0, 1.4575260877609253), (0.0, 1.4576287269592285), (0.0, 1.4577219486236572), (0.0, 1.457750678062439), (0.0, 1.45779287815094), (0.0, 1.4579137563705444), (0.0, 1.4579967260360718), (0.0, 1.4583497047424316), (0.0, 1.458357572555542), (0.0, 1.4584678411483765), (0.0, 1.4585939645767212), (0.0, 1.4588525295257568), (0.0, 1.458903193473816), (0.0, 1.4591032266616821), (0.0, 1.459159255027771), (0.0, 1.4596037864685059), (0.0, 1.4597837924957275), (0.0, 1.4599030017852783), (0.0, 1.4599288702011108), (0.0, 1.4599953889846802), (0.0, 1.4600582122802734), (0.0, 1.4602292776107788), (0.0, 1.4602906703948975), (0.0, 1.4604064226150513), (0.0, 1.4606502056121826), (0.0, 1.4607212543487549), (0.0, 1.4607611894607544), (0.0, 1.460784673690796), (0.0, 1.4608031511306763), (0.0, 1.4608306884765625), (0.0, 1.4608690738677979), (0.0, 1.4610390663146973), (0.0, 1.4611811637878418), (0.0, 1.4611997604370117), (0.0, 1.4612189531326294), (0.0, 1.4612404108047485), (0.0, 1.461262583732605), (0.0, 1.461313009262085), (0.0, 1.4614450931549072), (0.0, 1.4616671800613403), (0.0, 1.4617282152175903), (0.0, 1.4617910385131836), (0.0, 1.4617959260940552), (0.0, 1.4618024826049805), (0.0, 1.4618202447891235), (0.0, 1.462119460105896), (0.0, 1.4622565507888794), (0.0, 1.462367057800293), (0.0, 1.4629102945327759), (0.0, 1.4631552696228027), (0.0, 1.4634780883789062), (0.0, 1.4635523557662964), (0.0, 1.4640785455703735), (0.0, 1.4641356468200684), (0.0, 1.4644004106521606), (0.0, 1.4650191068649292), (0.0, 1.465045690536499), (0.0, 1.4650548696517944), (0.0, 1.4659912586212158), (0.0, 1.4661099910736084), (0.0, 1.4663892984390259), (0.0, 1.4676815271377563), (0.0, 1.4692620038986206), (0.0, 1.4697489738464355), (0.0, 1.4697691202163696), (0.0, 1.470214605331421), (0.0, 1.4708609580993652), (0.0, 1.4709721803665161), (0.0, 1.4714064598083496), (0.0, 1.472481608390808), (0.0, 1.4773002862930298), (0.0, 1.5096732378005981), (0.0, 1.5098598003387451), (0.0, 1.5125758647918701), (0.0, 1.513667106628418), (0.0, 1.5136691331863403), (0.0, 1.5140732526779175), (0.0, 1.5144972801208496), (0.0, 1.5155879259109497), (0.0, 1.5156402587890625), (0.0, 1.5164449214935303), (0.0, 1.51650869846344), (0.0, 1.5168426036834717), (0.0, 1.5169984102249146), (0.0, 1.5171103477478027), (0.0, 1.5175156593322754), (0.0, 1.5175533294677734), (0.0, 1.5179028511047363), (0.0, 1.51814866065979), (0.0, 1.518450140953064), (0.0, 1.518548846244812), (0.0, 1.5186949968338013), (0.0, 1.5187020301818848), (0.0, 1.5187472105026245), (0.0, 1.5192548036575317), (0.0, 1.5192818641662598), (0.0, 1.5195517539978027), (0.0, 1.5195597410202026), (0.0, 1.519708275794983), (0.0, 1.5198560953140259), (0.0, 1.5198746919631958), (0.0, 1.519883155822754), (0.0, 1.5199486017227173), (0.0, 1.5202133655548096), (0.0, 1.5203609466552734), (0.0, 1.5203680992126465), (0.0, 1.520553469657898), (0.0, 1.5206018686294556), (0.0, 1.5206602811813354), (0.0, 1.5209330320358276), (0.0, 1.5212547779083252), (0.0, 1.5212775468826294), (0.0, 1.5213558673858643), (0.0, 1.5217046737670898), (0.0, 1.5217221975326538), (0.0, 1.5220091342926025), (0.0, 1.5221832990646362), (0.0, 1.5222384929656982), (0.0, 1.5222489833831787), (0.0, 1.522349238395691), (0.0, 1.5224056243896484), (0.0, 1.5226013660430908), (0.0, 1.522718906402588), (0.0, 1.5227327346801758), (0.0, 1.5228984355926514), (0.0, 1.5229337215423584), (0.0, 1.5229486227035522), (0.0, 1.5229686498641968), (0.0, 1.5230518579483032), (0.0, 1.5231188535690308), (0.0, 1.5232739448547363), (0.0, 1.5234826803207397), (0.0, 1.5236366987228394), (0.0, 1.5238584280014038), (0.0, 1.5239195823669434), (0.0, 1.5239386558532715), (0.0, 1.5240099430084229), (0.0, 1.5240122079849243), (0.0, 1.5242244005203247), (0.0, 1.5242425203323364), (0.0, 1.5242732763290405), (0.0, 1.5244234800338745), (0.0, 1.5245153903961182), (0.0, 1.5245280265808105), (0.0, 1.5245935916900635), (0.0, 1.5246648788452148), (0.0, 1.5248816013336182), (0.0, 1.5249137878417969), (0.0, 1.525000810623169), (0.0, 1.5250957012176514), (0.0, 1.5253170728683472), (0.0, 1.525344967842102), (0.0, 1.525402307510376), (0.0, 1.525402307510376), (0.0, 1.525546908378601), (0.0, 1.5255568027496338), (0.0, 1.5258264541625977), (0.0, 1.5262823104858398), (0.0, 1.5262891054153442), (0.0, 1.5263060331344604), (0.0, 1.52653169631958), (0.0, 1.5266016721725464), (0.0, 1.5266157388687134), (0.0, 1.5266343355178833), (0.0, 1.52671480178833), (0.0, 1.527284860610962), (0.0, 1.527660846710205), (0.0, 1.5277180671691895), (0.0, 1.527904987335205), (0.0, 1.5280662775039673), (0.0, 1.5284743309020996), (0.0, 1.5285723209381104), (0.0, 1.5289595127105713), (0.0, 1.5298160314559937), (0.0, 1.5309702157974243), (0.0, 1.5319488048553467), (0.0, 1.534245491027832), (0.0, 1.5359004735946655), (0.0, 1.5361073017120361)], [(8.582215309143066, 8.645893096923828), (8.46796703338623, 9.083308219909668), (7.245450973510742, 7.497715473175049), (7.115695953369141, 7.365825176239014), (7.106743812561035, 7.413837432861328), (7.020073890686035, 7.70914888381958), (7.015801906585693, 7.287200450897217), (6.5883073806762695, 6.976288795471191), (6.5520806312561035, 7.667405605316162), (6.354574680328369, 7.693521976470947), (5.691335678100586, 5.808515548706055), (5.326920986175537, 5.808515548706055), (5.15958833694458, 6.4462175369262695), (5.092350482940674, 8.466181755065918), (4.898866176605225, 8.550010681152344), (4.087774276733398, 4.383828163146973), (4.075370788574219, 8.341839790344238), (4.01003360748291, 8.857748985290527), (3.9332125186920166, 3.95275616645813), (3.891148328781128, 4.175314903259277)], [(11.874166488647461, 12.1544771194458), (9.535664558410645, 10.398229598999023), (9.515405654907227, 10.259882926940918), (9.001707077026367, 10.074003219604492), (8.655244827270508, 8.661142349243164)]]
[array([[0.        , 1.26550186],
       [0.        , 1.31095755],
       [0.        , 1.32144606],
       [0.        , 1.3246609 ],
       [0.        , 1.32503784],
       [0.        , 1.32550633],
       [0.        , 1.32644606],
       [0.        , 1.32757115],
       [0.        , 1.32767582],
       [0.        , 1.32797873],
       [0.        , 1.32814372],
       [0.        , 1.32817554],
       [0.        , 1.32828033],
       [0.        , 1.32855868],
       [0.        , 1.32873297],
       [0.        , 1.32878554],
       [0.        , 1.32897365],
       [0.        , 1.32907856],
       [0.        , 1.32909739],
       [0.        , 1.32920349],
       [0.        , 1.3293407 ],
       [0.        , 1.32938135],
       [0.        , 1.32943881],
       [0.        , 1.32947588],
       [0.        , 1.32958567],
       [0.        , 1.32965803],
       [0.        , 1.32967174],
       [0.        , 1.32969916],
       [0.        , 1.32976961],
       [0.        , 1.33006859],
       [0.        , 1.33014822],
       [0.        , 1.33015037],
       [0.        , 1.33019269],
       [0.        , 1.33036458],
       [0.        , 1.33043492],
       [0.        , 1.3306402 ],
       [0.        , 1.33065426],
       [0.        , 1.33086741],
       [0.        , 1.33126485],
       [0.        , 1.33136046],
       [0.        , 1.33175337],
       [0.        , 1.33176661],
       [0.        , 1.33189428],
       [0.        , 1.33190739],
       [0.        , 1.33208454],
       [0.        , 1.33225834],
       [0.        , 1.33229315],
       [0.        , 1.33245003],
       [0.        , 1.33252442],
       [0.        , 1.33258712],
       [0.        , 1.33273911],
       [0.        , 1.33287179],
       [0.        , 1.33316636],
       [0.        , 1.33319807],
       [0.        , 1.33325493],
       [0.        , 1.33341074],
       [0.        , 1.3335135 ],
       [0.        , 1.33359027],
       [0.        , 1.33374929],
       [0.        , 1.33375168],
       [0.        , 1.33377123],
       [0.        , 1.33422005],
       [0.        , 1.33428299],
       [0.        , 1.33433509],
       [0.        , 1.33453929],
       [0.        , 1.33474839],
       [0.        , 1.33497036],
       [0.        , 1.33499384],
       [0.        , 1.33500636],
       [0.        , 1.33506036],
       [0.        , 1.33509719],
       [0.        , 1.33512986],
       [0.        , 1.33513224],
       [0.        , 1.33521068],
       [0.        , 1.33532619],
       [0.        , 1.33578265],
       [0.        , 1.33584917],
       [0.        , 1.3359338 ],
       [0.        , 1.33601379],
       [0.        , 1.3363682 ],
       [0.        , 1.33641338],
       [0.        , 1.33668876],
       [0.        , 1.33670807],
       [0.        , 1.33672357],
       [0.        , 1.33689713],
       [0.        , 1.33702087],
       [0.        , 1.33710706],
       [0.        , 1.33717167],
       [0.        , 1.33723271],
       [0.        , 1.33728337],
       [0.        , 1.33736169],
       [0.        , 1.33737302],
       [0.        , 1.33744633],
       [0.        , 1.33801901],
       [0.        , 1.33810127],
       [0.        , 1.33859479],
       [0.        , 1.33911312],
       [0.        , 1.33914065],
       [0.        , 1.33966792],
       [0.        , 1.34007311],
       [0.        , 1.34011531],
       [0.        , 1.34021878],
       [0.        , 1.34024584],
       [0.        , 1.34086943],
       [0.        , 1.34126282],
       [0.        , 1.34168422],
       [0.        , 1.34693325],
       [0.        , 1.449103  ],
       [0.        , 1.44978559],
       [0.        , 1.45047283],
       [0.        , 1.45252109],
       [0.        , 1.45265067],
       [0.        , 1.45307267],
       [0.        , 1.45344436],
       [0.        , 1.45368969],
       [0.        , 1.45376348],
       [0.        , 1.45383728],
       [0.        , 1.45456636],
       [0.        , 1.45458782],
       [0.        , 1.45459628],
       [0.        , 1.45464873],
       [0.        , 1.4549911 ],
       [0.        , 1.45514154],
       [0.        , 1.45525587],
       [0.        , 1.45560801],
       [0.        , 1.45576835],
       [0.        , 1.45582712],
       [0.        , 1.4558692 ],
       [0.        , 1.45590031],
       [0.        , 1.45597279],
       [0.        , 1.45601749],
       [0.        , 1.45607471],
       [0.        , 1.45616508],
       [0.        , 1.45619416],
       [0.        , 1.45656586],
       [0.        , 1.4566927 ],
       [0.        , 1.45670474],
       [0.        , 1.4568702 ],
       [0.        , 1.45688069],
       [0.        , 1.45701969],
       [0.        , 1.45717382],
       [0.        , 1.45721054],
       [0.        , 1.4573015 ],
       [0.        , 1.45742035],
       [0.        , 1.45752609],
       [0.        , 1.45762873],
       [0.        , 1.45772195],
       [0.        , 1.45775068],
       [0.        , 1.45779288],
       [0.        , 1.45791376],
       [0.        , 1.45799673],
       [0.        , 1.4583497 ],
       [0.        , 1.45835757],
       [0.        , 1.45846784],
       [0.        , 1.45859396],
       [0.        , 1.45885253],
       [0.        , 1.45890319],
       [0.        , 1.45910323],
       [0.        , 1.45915926],
       [0.        , 1.45960379],
       [0.        , 1.45978379],
       [0.        , 1.459903  ],
       [0.        , 1.45992887],
       [0.        , 1.45999539],
       [0.        , 1.46005821],
       [0.        , 1.46022928],
       [0.        , 1.46029067],
       [0.        , 1.46040642],
       [0.        , 1.46065021],
       [0.        , 1.46072125],
       [0.        , 1.46076119],
       [0.        , 1.46078467],
       [0.        , 1.46080315],
       [0.        , 1.46083069],
       [0.        , 1.46086907],
       [0.        , 1.46103907],
       [0.        , 1.46118116],
       [0.        , 1.46119976],
       [0.        , 1.46121895],
       [0.        , 1.46124041],
       [0.        , 1.46126258],
       [0.        , 1.46131301],
       [0.        , 1.46144509],
       [0.        , 1.46166718],
       [0.        , 1.46172822],
       [0.        , 1.46179104],
       [0.        , 1.46179593],
       [0.        , 1.46180248],
       [0.        , 1.46182024],
       [0.        , 1.46211946],
       [0.        , 1.46225655],
       [0.        , 1.46236706],
       [0.        , 1.46291029],
       [0.        , 1.46315527],
       [0.        , 1.46347809],
       [0.        , 1.46355236],
       [0.        , 1.46407855],
       [0.        , 1.46413565],
       [0.        , 1.46440041],
       [0.        , 1.46501911],
       [0.        , 1.46504569],
       [0.        , 1.46505487],
       [0.        , 1.46599126],
       [0.        , 1.46610999],
       [0.        , 1.4663893 ],
       [0.        , 1.46768153],
       [0.        , 1.469262  ],
       [0.        , 1.46974897],
       [0.        , 1.46976912],
       [0.        , 1.47021461],
       [0.        , 1.47086096],
       [0.        , 1.47097218],
       [0.        , 1.47140646],
       [0.        , 1.47248161],
       [0.        , 1.47730029],
       [0.        , 1.50967324],
       [0.        , 1.5098598 ],
       [0.        , 1.51257586],
       [0.        , 1.51366711],
       [0.        , 1.51366913],
       [0.        , 1.51407325],
       [0.        , 1.51449728],
       [0.        , 1.51558793],
       [0.        , 1.51564026],
       [0.        , 1.51644492],
       [0.        , 1.5165087 ],
       [0.        , 1.5168426 ],
       [0.        , 1.51699841],
       [0.        , 1.51711035],
       [0.        , 1.51751566],
       [0.        , 1.51755333],
       [0.        , 1.51790285],
       [0.        , 1.51814866],
       [0.        , 1.51845014],
       [0.        , 1.51854885],
       [0.        , 1.518695  ],
       [0.        , 1.51870203],
       [0.        , 1.51874721],
       [0.        , 1.5192548 ],
       [0.        , 1.51928186],
       [0.        , 1.51955175],
       [0.        , 1.51955974],
       [0.        , 1.51970828],
       [0.        , 1.5198561 ],
       [0.        , 1.51987469],
       [0.        , 1.51988316],
       [0.        , 1.5199486 ],
       [0.        , 1.52021337],
       [0.        , 1.52036095],
       [0.        , 1.5203681 ],
       [0.        , 1.52055347],
       [0.        , 1.52060187],
       [0.        , 1.52066028],
       [0.        , 1.52093303],
       [0.        , 1.52125478],
       [0.        , 1.52127755],
       [0.        , 1.52135587],
       [0.        , 1.52170467],
       [0.        , 1.5217222 ],
       [0.        , 1.52200913],
       [0.        , 1.5221833 ],
       [0.        , 1.52223849],
       [0.        , 1.52224898],
       [0.        , 1.52234924],
       [0.        , 1.52240562],
       [0.        , 1.52260137],
       [0.        , 1.52271891],
       [0.        , 1.52273273],
       [0.        , 1.52289844],
       [0.        , 1.52293372],
       [0.        , 1.52294862],
       [0.        , 1.52296865],
       [0.        , 1.52305186],
       [0.        , 1.52311885],
       [0.        , 1.52327394],
       [0.        , 1.52348268],
       [0.        , 1.5236367 ],
       [0.        , 1.52385843],
       [0.        , 1.52391958],
       [0.        , 1.52393866],
       [0.        , 1.52400994],
       [0.        , 1.52401221],
       [0.        , 1.5242244 ],
       [0.        , 1.52424252],
       [0.        , 1.52427328],
       [0.        , 1.52442348],
       [0.        , 1.52451539],
       [0.        , 1.52452803],
       [0.        , 1.52459359],
       [0.        , 1.52466488],
       [0.        , 1.5248816 ],
       [0.        , 1.52491379],
       [0.        , 1.52500081],
       [0.        , 1.5250957 ],
       [0.        , 1.52531707],
       [0.        , 1.52534497],
       [0.        , 1.52540231],
       [0.        , 1.52540231],
       [0.        , 1.52554691],
       [0.        , 1.5255568 ],
       [0.        , 1.52582645],
       [0.        , 1.52628231],
       [0.        , 1.52628911],
       [0.        , 1.52630603],
       [0.        , 1.5265317 ],
       [0.        , 1.52660167],
       [0.        , 1.52661574],
       [0.        , 1.52663434],
       [0.        , 1.5267148 ],
       [0.        , 1.52728486],
       [0.        , 1.52766085],
       [0.        , 1.52771807],
       [0.        , 1.52790499],
       [0.        , 1.52806628],
       [0.        , 1.52847433],
       [0.        , 1.52857232],
       [0.        , 1.52895951],
       [0.        , 1.52981603],
       [0.        , 1.53097022],
       [0.        , 1.5319488 ],
       [0.        , 1.53424549],
       [0.        , 1.53590047],
       [0.        , 1.5361073 ]]), array([[8.58221531, 8.6458931 ],
       [8.46796703, 9.08330822],
       [7.24545097, 7.49771547],
       [7.11569595, 7.36582518],
       [7.10674381, 7.41383743],
       [7.02007389, 7.70914888],
       [7.01580191, 7.28720045],
       [6.58830738, 6.9762888 ],
       [6.55208063, 7.66740561],
       [6.35457468, 7.69352198],
       [5.69133568, 5.80851555],
       [5.32692099, 5.80851555],
       [5.15958834, 6.44621754],
       [5.09235048, 8.46618176],
       [4.89886618, 8.55001068],
       [4.08777428, 4.38382816],
       [4.07537079, 8.34183979],
       [4.01003361, 8.85774899],
       [3.93321252, 3.95275617],
       [3.89114833, 4.1753149 ]]), array([[11.87416649, 12.15447712],
       [ 9.53566456, 10.3982296 ],
       [ 9.51540565, 10.25988293],
       [ 9.00170708, 10.07400322],
       [ 8.65524483,  8.66114235]])]2024-03-06 18:05:41.502656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LU5 ph vector generated, counter: 260
2024-03-06 18:05:45.674041: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:45.716468: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:47.141219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.3121612), (0., 1.3151463), (0., 1.3226347), (0., 1.3227035),
       (0., 1.3231769), (0., 1.3248833), (0., 1.3253245), (0., 1.3256047),
       (0., 1.3258127), (0., 1.326089 ), (0., 1.3262259), (0., 1.3272175),
       (0., 1.3273137), (0., 1.3273675), (0., 1.3274925), (0., 1.3277179),
       (0., 1.3277762), (0., 1.3280846), (0., 1.328193 ), (0., 1.3282484),
       (0., 1.3282614), (0., 1.3284671), (0., 1.3287923), (0., 1.3291423),
       (0., 1.3293954), (0., 1.3294435), (0., 1.3294625), (0., 1.3294908),
       (0., 1.3294946), (0., 1.329575 ), (0., 1.3296669), (0., 1.3300544),
       (0., 1.3302051), (0., 1.3302683), (0., 1.3304634), (0., 1.3305323),
       (0., 1.3305554), (0., 1.3306689), (0., 1.3306832), (0., 1.3308809),
       (0., 1.3310859), (0., 1.3310974), (0., 1.3311449), (0., 1.3312107),
       (0., 1.3313327), (0., 1.3313968), (0., 1.3314493), (0., 1.3314675),
       (0., 1.3316585), (0., 1.3317654), (0., 1.3320246), (0., 1.3320369),
       (0., 1.3322884), (0., 1.3323632), (0., 1.3326229), (0., 1.3327031),
       (0., 1.3327717), (0., 1.3333412), (0., 1.3333726), (0., 1.3336006),
       (0., 1.333861 ), (0., 1.333935 ), (0., 1.3339543), (0., 1.3340094),
       (0., 1.3340304), (0., 1.3341341), (0., 1.3348325), (0., 1.3350577),
       (0., 1.335395 ), (0., 1.3355293), (0., 1.33553  ), (0., 1.3358012),
       (0., 1.3361652), (0., 1.3366032), (0., 1.3368301), (0., 1.336961 ),
       (0., 1.3372116), (0., 1.3374013), (0., 1.3374528), (0., 1.3375001),
       (0., 1.3377043), (0., 1.3380616), (0., 1.338191 ), (0., 1.3384604),
       (0., 1.3384807), (0., 1.3387902), (0., 1.3395   ), (0., 1.339674 ),
       (0., 1.3396873), (0., 1.3397651), (0., 1.3398585), (0., 1.3410541),
       (0., 1.3416497), (0., 1.3422226), (0., 1.3422375), (0., 1.3422618),
       (0., 1.3424349), (0., 1.3425016), (0., 1.3428013), (0., 1.3433199),
       (0., 1.3437636), (0., 1.3438077), (0., 1.3440003), (0., 1.344198 ),
       (0., 1.3443253), (0., 1.3447   ), (0., 1.3469306), (0., 1.3492122),
       (0., 1.3551105), (0., 1.4456656), (0., 1.4492754), (0., 1.4498616),
       (0., 1.4515237), (0., 1.4521526), (0., 1.452522 ), (0., 1.4527112),
       (0., 1.4527647), (0., 1.4530456), (0., 1.4532824), (0., 1.4532866),
       (0., 1.4532925), (0., 1.4533676), (0., 1.4534583), (0., 1.4536287),
       (0., 1.4543514), (0., 1.4547662), (0., 1.4548426), (0., 1.4549457),
       (0., 1.4553542), (0., 1.4554436), (0., 1.4555393), (0., 1.4557256),
       (0., 1.4557321), (0., 1.4559507), (0., 1.4560003), (0., 1.4563092),
       (0., 1.4563938), (0., 1.4565588), (0., 1.4566   ), (0., 1.45661  ),
       (0., 1.456619 ), (0., 1.4566282), (0., 1.456854 ), (0., 1.4570383),
       (0., 1.4570552), (0., 1.457068 ), (0., 1.4571825), (0., 1.457198 ),
       (0., 1.4575393), (0., 1.4578203), (0., 1.4578662), (0., 1.457908 ),
       (0., 1.4579108), (0., 1.4579223), (0., 1.4580183), (0., 1.4580454),
       (0., 1.458298 ), (0., 1.4585797), (0., 1.4590347), (0., 1.459105 ),
       (0., 1.4591327), (0., 1.4591471), (0., 1.4591972), (0., 1.4592177),
       (0., 1.4592557), (0., 1.4594277), (0., 1.4594886), (0., 1.459906 ),
       (0., 1.4600906), (0., 1.46013  ), (0., 1.4603362), (0., 1.4603424),
       (0., 1.4604594), (0., 1.4604913), (0., 1.460796 ), (0., 1.4608792),
       (0., 1.4610314), (0., 1.4610401), (0., 1.4611006), (0., 1.4611739),
       (0., 1.4611789), (0., 1.4612535), (0., 1.4612738), (0., 1.4613805),
       (0., 1.4616672), (0., 1.4618133), (0., 1.4619968), (0., 1.4623307),
       (0., 1.4628727), (0., 1.4629035), (0., 1.4629292), (0., 1.463505 ),
       (0., 1.4636862), (0., 1.463816 ), (0., 1.4640946), (0., 1.4643049),
       (0., 1.4647225), (0., 1.4647827), (0., 1.4648503), (0., 1.4648852),
       (0., 1.4650437), (0., 1.4652992), (0., 1.4659694), (0., 1.466053 ),
       (0., 1.4662473), (0., 1.4663777), (0., 1.4666427), (0., 1.4674321),
       (0., 1.4678981), (0., 1.468323 ), (0., 1.4699979), (0., 1.4703737),
       (0., 1.470418 ), (0., 1.4708204), (0., 1.4713467), (0., 1.4718404),
       (0., 1.4724338), (0., 1.4725996), (0., 1.4750744), (0., 1.5094405),
       (0., 1.5125422), (0., 1.5129733), (0., 1.5135161), (0., 1.5148188),
       (0., 1.5157689), (0., 1.5158324), (0., 1.5159914), (0., 1.5169854),
       (0., 1.5172797), (0., 1.5175573), (0., 1.5177447), (0., 1.5177488),
       (0., 1.5180621), (0., 1.518093 ), (0., 1.5183948), (0., 1.5184952),
       (0., 1.5185874), (0., 1.5187689), (0., 1.5188056), (0., 1.5188959),
       (0., 1.5190368), (0., 1.5193172), (0., 1.5195774), (0., 1.5195805),
       (0., 1.5198301), (0., 1.5199229), (0., 1.5199385), (0., 1.5200169),
       (0., 1.520057 ), (0., 1.5202762), (0., 1.5202945), (0., 1.5203046),
       (0., 1.5205259), (0., 1.5206177), (0., 1.5207648), (0., 1.5207727),
       (0., 1.5208292), (0., 1.5208786), (0., 1.5209758), (0., 1.5211809),
       (0., 1.5211875), (0., 1.5213286), (0., 1.521347 ), (0., 1.5214734),
       (0., 1.5215315), (0., 1.5215421), (0., 1.52176  ), (0., 1.5217681),
       (0., 1.5217909), (0., 1.5217998), (0., 1.5218296), (0., 1.5220251),
       (0., 1.5220436), (0., 1.5223464), (0., 1.5224284), (0., 1.5224513),
       (0., 1.5224999), (0., 1.5225537), (0., 1.5225948), (0., 1.522747 ),
       (0., 1.5228297), (0., 1.5228584), (0., 1.5230045), (0., 1.5230883),
       (0., 1.5231625), (0., 1.5235537), (0., 1.5236   ), (0., 1.5236742),
       (0., 1.5237399), (0., 1.5239165), (0., 1.524147 ), (0., 1.5241759),
       (0., 1.5243876), (0., 1.5246278), (0., 1.5246353), (0., 1.524708 ),
       (0., 1.5247576), (0., 1.5248538), (0., 1.5249559), (0., 1.5250342),
       (0., 1.5250732), (0., 1.5252991), (0., 1.5254679), (0., 1.525583 ),
       (0., 1.5256106), (0., 1.5259479), (0., 1.5259753), (0., 1.526035 ),
       (0., 1.5261317), (0., 1.526405 ), (0., 1.5264295), (0., 1.5265532),
       (0., 1.5266249), (0., 1.5266763), (0., 1.5266765), (0., 1.5267904),
       (0., 1.5268344), (0., 1.5280348), (0., 1.5283445), (0., 1.5286597),
       (0., 1.5288222), (0., 1.5290908), (0., 1.5293   ), (0., 1.5294675),
       (0., 1.5298667), (0., 1.5305303), (0., 1.5311272), (0., 1.5334398),
       (0., 1.5435425)], dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(8.726413 , 9.296411 ), (8.630574 , 9.078854 ),
       (8.474457 , 8.563481 ), (7.372732 , 7.8049493),
       (7.2451425, 7.3580756), (7.2096186, 7.5750246),
       (7.0931416, 7.408452 ), (7.0790725, 7.7808437),
       (6.9025555, 7.6069293), (6.5814915, 6.924294 ),
       (6.308239 , 7.671525 ), (6.205718 , 7.6296954),
       (5.4217796, 8.658673 ), (5.378969 , 5.8298306),
       (5.362987 , 6.423188 ), (5.0970974, 8.446252 ),
       (4.581785 , 5.802195 ), (4.1518617, 8.711928 ),
       (4.129867 , 4.199936 ), (4.108202 , 8.207804 ),
       (3.9442716, 3.9657001), (3.8851972, 4.0894113),
       (3.7023418, 3.7217302)], dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(9.560961 , 10.339269), (9.5257435, 10.445732),
       (8.968754 , 10.04768 )], dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.3121612071990967), (0.0, 1.3151463270187378), (0.0, 1.3226346969604492), (0.0, 1.32270348072052), (0.0, 1.3231768608093262), (0.0, 1.3248833417892456), (0.0, 1.325324535369873), (0.0, 1.3256046772003174), (0.0, 1.3258126974105835), (0.0, 1.3260890245437622), (0.0, 1.3262258768081665), (0.0, 1.32721745967865), (0.0, 1.3273136615753174), (0.0, 1.3273675441741943), (0.0, 1.3274924755096436), (0.0, 1.327717900276184), (0.0, 1.3277761936187744), (0.0, 1.3280845880508423), (0.0, 1.328192949295044), (0.0, 1.328248381614685), (0.0, 1.328261375427246), (0.0, 1.3284671306610107), (0.0, 1.3287923336029053), (0.0, 1.3291423320770264), (0.0, 1.3293954133987427), (0.0, 1.3294434547424316), (0.0, 1.3294625282287598), (0.0, 1.3294907808303833), (0.0, 1.329494595527649), (0.0, 1.3295749425888062), (0.0, 1.3296668529510498), (0.0, 1.3300544023513794), (0.0, 1.3302050828933716), (0.0, 1.3302682638168335), (0.0, 1.3304634094238281), (0.0, 1.3305323123931885), (0.0, 1.3305554389953613), (0.0, 1.3306689262390137), (0.0, 1.3306832313537598), (0.0, 1.330880880355835), (0.0, 1.3310859203338623), (0.0, 1.3310973644256592), (0.0, 1.33114492893219), (0.0, 1.331210732460022), (0.0, 1.3313326835632324), (0.0, 1.3313968181610107), (0.0, 1.331449270248413), (0.0, 1.3314675092697144), (0.0, 1.3316584825515747), (0.0, 1.3317654132843018), (0.0, 1.3320245742797852), (0.0, 1.3320368528366089), (0.0, 1.332288384437561), (0.0, 1.332363247871399), (0.0, 1.3326228857040405), (0.0, 1.3327031135559082), (0.0, 1.3327716588974), (0.0, 1.3333412408828735), (0.0, 1.3333725929260254), (0.0, 1.333600640296936), (0.0, 1.333860993385315), (0.0, 1.333935022354126), (0.0, 1.3339543342590332), (0.0, 1.3340094089508057), (0.0, 1.3340303897857666), (0.0, 1.3341341018676758), (0.0, 1.3348325490951538), (0.0, 1.3350577354431152), (0.0, 1.3353949785232544), (0.0, 1.3355293273925781), (0.0, 1.3355300426483154), (0.0, 1.3358012437820435), (0.0, 1.336165189743042), (0.0, 1.3366031646728516), (0.0, 1.3368301391601562), (0.0, 1.336961030960083), (0.0, 1.3372116088867188), (0.0, 1.337401270866394), (0.0, 1.33745276927948), (0.0, 1.3375000953674316), (0.0, 1.3377043008804321), (0.0, 1.3380615711212158), (0.0, 1.338191032409668), (0.0, 1.3384604454040527), (0.0, 1.3384807109832764), (0.0, 1.3387901782989502), (0.0, 1.3394999504089355), (0.0, 1.3396739959716797), (0.0, 1.3396873474121094), (0.0, 1.3397650718688965), (0.0, 1.3398585319519043), (0.0, 1.341054081916809), (0.0, 1.3416496515274048), (0.0, 1.3422225713729858), (0.0, 1.3422374725341797), (0.0, 1.342261791229248), (0.0, 1.3424348831176758), (0.0, 1.3425016403198242), (0.0, 1.3428013324737549), (0.0, 1.3433198928833008), (0.0, 1.3437635898590088), (0.0, 1.3438076972961426), (0.0, 1.3440003395080566), (0.0, 1.3441979885101318), (0.0, 1.344325304031372), (0.0, 1.3446999788284302), (0.0, 1.3469306230545044), (0.0, 1.3492121696472168), (0.0, 1.3551105260849), (0.0, 1.4456655979156494), (0.0, 1.4492753744125366), (0.0, 1.4498616456985474), (0.0, 1.4515236616134644), (0.0, 1.4521526098251343), (0.0, 1.4525220394134521), (0.0, 1.4527112245559692), (0.0, 1.4527647495269775), (0.0, 1.4530456066131592), (0.0, 1.453282356262207), (0.0, 1.4532866477966309), (0.0, 1.4532924890518188), (0.0, 1.4533675909042358), (0.0, 1.453458309173584), (0.0, 1.453628659248352), (0.0, 1.4543514251708984), (0.0, 1.4547661542892456), (0.0, 1.4548425674438477), (0.0, 1.454945683479309), (0.0, 1.4553542137145996), (0.0, 1.4554436206817627), (0.0, 1.455539345741272), (0.0, 1.4557255506515503), (0.0, 1.4557321071624756), (0.0, 1.4559507369995117), (0.0, 1.4560003280639648), (0.0, 1.456309199333191), (0.0, 1.456393837928772), (0.0, 1.4565588235855103), (0.0, 1.4565999507904053), (0.0, 1.4566099643707275), (0.0, 1.4566190242767334), (0.0, 1.4566282033920288), (0.0, 1.456853985786438), (0.0, 1.4570382833480835), (0.0, 1.4570552110671997), (0.0, 1.4570679664611816), (0.0, 1.45718252658844), (0.0, 1.4571980237960815), (0.0, 1.4575393199920654), (0.0, 1.4578202962875366), (0.0, 1.4578661918640137), (0.0, 1.457908034324646), (0.0, 1.4579107761383057), (0.0, 1.457922339439392), (0.0, 1.4580183029174805), (0.0, 1.4580453634262085), (0.0, 1.4582979679107666), (0.0, 1.458579659461975), (0.0, 1.4590346813201904), (0.0, 1.4591050148010254), (0.0, 1.4591326713562012), (0.0, 1.4591470956802368), (0.0, 1.4591971635818481), (0.0, 1.4592176675796509), (0.0, 1.4592556953430176), (0.0, 1.4594277143478394), (0.0, 1.4594886302947998), (0.0, 1.459905982017517), (0.0, 1.4600906372070312), (0.0, 1.460129976272583), (0.0, 1.4603362083435059), (0.0, 1.4603424072265625), (0.0, 1.4604593515396118), (0.0, 1.4604912996292114), (0.0, 1.4607959985733032), (0.0, 1.4608792066574097), (0.0, 1.461031436920166), (0.0, 1.4610401391983032), (0.0, 1.4611005783081055), (0.0, 1.4611738920211792), (0.0, 1.4611788988113403), (0.0, 1.4612535238265991), (0.0, 1.4612737894058228), (0.0, 1.4613804817199707), (0.0, 1.4616671800613403), (0.0, 1.4618133306503296), (0.0, 1.4619967937469482), (0.0, 1.46233069896698), (0.0, 1.4628727436065674), (0.0, 1.4629034996032715), (0.0, 1.4629292488098145), (0.0, 1.4635050296783447), (0.0, 1.463686227798462), (0.0, 1.4638160467147827), (0.0, 1.464094638824463), (0.0, 1.4643049240112305), (0.0, 1.4647225141525269), (0.0, 1.46478271484375), (0.0, 1.4648503065109253), (0.0, 1.4648852348327637), (0.0, 1.4650436639785767), (0.0, 1.4652992486953735), (0.0, 1.465969443321228), (0.0, 1.4660530090332031), (0.0, 1.466247320175171), (0.0, 1.4663777351379395), (0.0, 1.4666427373886108), (0.0, 1.4674321413040161), (0.0, 1.4678981304168701), (0.0, 1.468322992324829), (0.0, 1.4699978828430176), (0.0, 1.4703737497329712), (0.0, 1.4704179763793945), (0.0, 1.470820426940918), (0.0, 1.4713467359542847), (0.0, 1.4718403816223145), (0.0, 1.4724338054656982), (0.0, 1.4725996255874634), (0.0, 1.4750744104385376), (0.0, 1.509440541267395), (0.0, 1.5125422477722168), (0.0, 1.5129733085632324), (0.0, 1.5135160684585571), (0.0, 1.514818787574768), (0.0, 1.5157688856124878), (0.0, 1.5158324241638184), (0.0, 1.515991449356079), (0.0, 1.5169854164123535), (0.0, 1.5172797441482544), (0.0, 1.5175572633743286), (0.0, 1.5177446603775024), (0.0, 1.5177488327026367), (0.0, 1.5180621147155762), (0.0, 1.5180929899215698), (0.0, 1.5183948278427124), (0.0, 1.5184952020645142), (0.0, 1.518587350845337), (0.0, 1.5187689065933228), (0.0, 1.5188056230545044), (0.0, 1.5188958644866943), (0.0, 1.5190367698669434), (0.0, 1.5193171501159668), (0.0, 1.5195773839950562), (0.0, 1.5195804834365845), (0.0, 1.5198301076889038), (0.0, 1.5199228525161743), (0.0, 1.5199384689331055), (0.0, 1.5200169086456299), (0.0, 1.520056962966919), (0.0, 1.5202761888504028), (0.0, 1.5202945470809937), (0.0, 1.520304560661316), (0.0, 1.5205259323120117), (0.0, 1.5206177234649658), (0.0, 1.5207648277282715), (0.0, 1.5207726955413818), (0.0, 1.520829200744629), (0.0, 1.520878553390503), (0.0, 1.5209758281707764), (0.0, 1.5211808681488037), (0.0, 1.5211875438690186), (0.0, 1.5213285684585571), (0.0, 1.5213470458984375), (0.0, 1.5214734077453613), (0.0, 1.5215314626693726), (0.0, 1.5215420722961426), (0.0, 1.5217599868774414), (0.0, 1.5217680931091309), (0.0, 1.521790862083435), (0.0, 1.5217998027801514), (0.0, 1.521829605102539), (0.0, 1.5220251083374023), (0.0, 1.5220435857772827), (0.0, 1.5223463773727417), (0.0, 1.5224283933639526), (0.0, 1.5224512815475464), (0.0, 1.522499918937683), (0.0, 1.5225536823272705), (0.0, 1.5225948095321655), (0.0, 1.5227470397949219), (0.0, 1.5228296518325806), (0.0, 1.5228583812713623), (0.0, 1.5230045318603516), (0.0, 1.5230883359909058), (0.0, 1.5231624841690063), (0.0, 1.523553729057312), (0.0, 1.5235999822616577), (0.0, 1.5236742496490479), (0.0, 1.5237399339675903), (0.0, 1.523916482925415), (0.0, 1.5241470336914062), (0.0, 1.5241758823394775), (0.0, 1.5243875980377197), (0.0, 1.5246278047561646), (0.0, 1.5246353149414062), (0.0, 1.5247080326080322), (0.0, 1.5247576236724854), (0.0, 1.5248538255691528), (0.0, 1.5249558687210083), (0.0, 1.5250341892242432), (0.0, 1.5250731706619263), (0.0, 1.525299072265625), (0.0, 1.525467872619629), (0.0, 1.525583028793335), (0.0, 1.5256105661392212), (0.0, 1.52594792842865), (0.0, 1.5259753465652466), (0.0, 1.526034951210022), (0.0, 1.5261317491531372), (0.0, 1.5264049768447876), (0.0, 1.526429533958435), (0.0, 1.5265531539916992), (0.0, 1.5266249179840088), (0.0, 1.5266762971878052), (0.0, 1.5266765356063843), (0.0, 1.5267903804779053), (0.0, 1.5268343687057495), (0.0, 1.5280348062515259), (0.0, 1.5283445119857788), (0.0, 1.528659701347351), (0.0, 1.5288221836090088), (0.0, 1.5290907621383667), (0.0, 1.5292999744415283), (0.0, 1.5294674634933472), (0.0, 1.5298666954040527), (0.0, 1.530530333518982), (0.0, 1.5311272144317627), (0.0, 1.5334397554397583), (0.0, 1.543542504310608)], [(8.726412773132324, 9.29641056060791), (8.630574226379395, 9.078853607177734), (8.474456787109375, 8.563481330871582), (7.372732162475586, 7.8049492835998535), (7.245142459869385, 7.3580756187438965), (7.20961856842041, 7.575024604797363), (7.093141555786133, 7.408452033996582), (7.07907247543335, 7.780843734741211), (6.902555465698242, 7.606929302215576), (6.581491470336914, 6.9242939949035645), (6.308238983154297, 7.671525001525879), (6.205718040466309, 7.629695415496826), (5.421779632568359, 8.658673286437988), (5.378969192504883, 5.829830646514893), (5.362987041473389, 6.423188209533691), (5.097097396850586, 8.44625186920166), (4.581785202026367, 5.802195072174072), (4.151861667633057, 8.711928367614746), (4.129867076873779, 4.1999359130859375), (4.10820198059082, 8.207803726196289), (3.9442715644836426, 3.965700149536133), (3.885197162628174, 4.08941125869751), (3.7023417949676514, 3.7217302322387695)], [(9.56096076965332, 10.339268684387207), (9.52574348449707, 10.445732116699219), (8.968753814697266, 10.047679901123047)]]
[array([[0.        , 1.31216121],
       [0.        , 1.31514633],
       [0.        , 1.3226347 ],
       [0.        , 1.32270348],
       [0.        , 1.32317686],
       [0.        , 1.32488334],
       [0.        , 1.32532454],
       [0.        , 1.32560468],
       [0.        , 1.3258127 ],
       [0.        , 1.32608902],
       [0.        , 1.32622588],
       [0.        , 1.32721746],
       [0.        , 1.32731366],
       [0.        , 1.32736754],
       [0.        , 1.32749248],
       [0.        , 1.3277179 ],
       [0.        , 1.32777619],
       [0.        , 1.32808459],
       [0.        , 1.32819295],
       [0.        , 1.32824838],
       [0.        , 1.32826138],
       [0.        , 1.32846713],
       [0.        , 1.32879233],
       [0.        , 1.32914233],
       [0.        , 1.32939541],
       [0.        , 1.32944345],
       [0.        , 1.32946253],
       [0.        , 1.32949078],
       [0.        , 1.3294946 ],
       [0.        , 1.32957494],
       [0.        , 1.32966685],
       [0.        , 1.3300544 ],
       [0.        , 1.33020508],
       [0.        , 1.33026826],
       [0.        , 1.33046341],
       [0.        , 1.33053231],
       [0.        , 1.33055544],
       [0.        , 1.33066893],
       [0.        , 1.33068323],
       [0.        , 1.33088088],
       [0.        , 1.33108592],
       [0.        , 1.33109736],
       [0.        , 1.33114493],
       [0.        , 1.33121073],
       [0.        , 1.33133268],
       [0.        , 1.33139682],
       [0.        , 1.33144927],
       [0.        , 1.33146751],
       [0.        , 1.33165848],
       [0.        , 1.33176541],
       [0.        , 1.33202457],
       [0.        , 1.33203685],
       [0.        , 1.33228838],
       [0.        , 1.33236325],
       [0.        , 1.33262289],
       [0.        , 1.33270311],
       [0.        , 1.33277166],
       [0.        , 1.33334124],
       [0.        , 1.33337259],
       [0.        , 1.33360064],
       [0.        , 1.33386099],
       [0.        , 1.33393502],
       [0.        , 1.33395433],
       [0.        , 1.33400941],
       [0.        , 1.33403039],
       [0.        , 1.3341341 ],
       [0.        , 1.33483255],
       [0.        , 1.33505774],
       [0.        , 1.33539498],
       [0.        , 1.33552933],
       [0.        , 1.33553004],
       [0.        , 1.33580124],
       [0.        , 1.33616519],
       [0.        , 1.33660316],
       [0.        , 1.33683014],
       [0.        , 1.33696103],
       [0.        , 1.33721161],
       [0.        , 1.33740127],
       [0.        , 1.33745277],
       [0.        , 1.3375001 ],
       [0.        , 1.3377043 ],
       [0.        , 1.33806157],
       [0.        , 1.33819103],
       [0.        , 1.33846045],
       [0.        , 1.33848071],
       [0.        , 1.33879018],
       [0.        , 1.33949995],
       [0.        , 1.339674  ],
       [0.        , 1.33968735],
       [0.        , 1.33976507],
       [0.        , 1.33985853],
       [0.        , 1.34105408],
       [0.        , 1.34164965],
       [0.        , 1.34222257],
       [0.        , 1.34223747],
       [0.        , 1.34226179],
       [0.        , 1.34243488],
       [0.        , 1.34250164],
       [0.        , 1.34280133],
       [0.        , 1.34331989],
       [0.        , 1.34376359],
       [0.        , 1.3438077 ],
       [0.        , 1.34400034],
       [0.        , 1.34419799],
       [0.        , 1.3443253 ],
       [0.        , 1.34469998],
       [0.        , 1.34693062],
       [0.        , 1.34921217],
       [0.        , 1.35511053],
       [0.        , 1.4456656 ],
       [0.        , 1.44927537],
       [0.        , 1.44986165],
       [0.        , 1.45152366],
       [0.        , 1.45215261],
       [0.        , 1.45252204],
       [0.        , 1.45271122],
       [0.        , 1.45276475],
       [0.        , 1.45304561],
       [0.        , 1.45328236],
       [0.        , 1.45328665],
       [0.        , 1.45329249],
       [0.        , 1.45336759],
       [0.        , 1.45345831],
       [0.        , 1.45362866],
       [0.        , 1.45435143],
       [0.        , 1.45476615],
       [0.        , 1.45484257],
       [0.        , 1.45494568],
       [0.        , 1.45535421],
       [0.        , 1.45544362],
       [0.        , 1.45553935],
       [0.        , 1.45572555],
       [0.        , 1.45573211],
       [0.        , 1.45595074],
       [0.        , 1.45600033],
       [0.        , 1.4563092 ],
       [0.        , 1.45639384],
       [0.        , 1.45655882],
       [0.        , 1.45659995],
       [0.        , 1.45660996],
       [0.        , 1.45661902],
       [0.        , 1.4566282 ],
       [0.        , 1.45685399],
       [0.        , 1.45703828],
       [0.        , 1.45705521],
       [0.        , 1.45706797],
       [0.        , 1.45718253],
       [0.        , 1.45719802],
       [0.        , 1.45753932],
       [0.        , 1.4578203 ],
       [0.        , 1.45786619],
       [0.        , 1.45790803],
       [0.        , 1.45791078],
       [0.        , 1.45792234],
       [0.        , 1.4580183 ],
       [0.        , 1.45804536],
       [0.        , 1.45829797],
       [0.        , 1.45857966],
       [0.        , 1.45903468],
       [0.        , 1.45910501],
       [0.        , 1.45913267],
       [0.        , 1.4591471 ],
       [0.        , 1.45919716],
       [0.        , 1.45921767],
       [0.        , 1.4592557 ],
       [0.        , 1.45942771],
       [0.        , 1.45948863],
       [0.        , 1.45990598],
       [0.        , 1.46009064],
       [0.        , 1.46012998],
       [0.        , 1.46033621],
       [0.        , 1.46034241],
       [0.        , 1.46045935],
       [0.        , 1.4604913 ],
       [0.        , 1.460796  ],
       [0.        , 1.46087921],
       [0.        , 1.46103144],
       [0.        , 1.46104014],
       [0.        , 1.46110058],
       [0.        , 1.46117389],
       [0.        , 1.4611789 ],
       [0.        , 1.46125352],
       [0.        , 1.46127379],
       [0.        , 1.46138048],
       [0.        , 1.46166718],
       [0.        , 1.46181333],
       [0.        , 1.46199679],
       [0.        , 1.4623307 ],
       [0.        , 1.46287274],
       [0.        , 1.4629035 ],
       [0.        , 1.46292925],
       [0.        , 1.46350503],
       [0.        , 1.46368623],
       [0.        , 1.46381605],
       [0.        , 1.46409464],
       [0.        , 1.46430492],
       [0.        , 1.46472251],
       [0.        , 1.46478271],
       [0.        , 1.46485031],
       [0.        , 1.46488523],
       [0.        , 1.46504366],
       [0.        , 1.46529925],
       [0.        , 1.46596944],
       [0.        , 1.46605301],
       [0.        , 1.46624732],
       [0.        , 1.46637774],
       [0.        , 1.46664274],
       [0.        , 1.46743214],
       [0.        , 1.46789813],
       [0.        , 1.46832299],
       [0.        , 1.46999788],
       [0.        , 1.47037375],
       [0.        , 1.47041798],
       [0.        , 1.47082043],
       [0.        , 1.47134674],
       [0.        , 1.47184038],
       [0.        , 1.47243381],
       [0.        , 1.47259963],
       [0.        , 1.47507441],
       [0.        , 1.50944054],
       [0.        , 1.51254225],
       [0.        , 1.51297331],
       [0.        , 1.51351607],
       [0.        , 1.51481879],
       [0.        , 1.51576889],
       [0.        , 1.51583242],
       [0.        , 1.51599145],
       [0.        , 1.51698542],
       [0.        , 1.51727974],
       [0.        , 1.51755726],
       [0.        , 1.51774466],
       [0.        , 1.51774883],
       [0.        , 1.51806211],
       [0.        , 1.51809299],
       [0.        , 1.51839483],
       [0.        , 1.5184952 ],
       [0.        , 1.51858735],
       [0.        , 1.51876891],
       [0.        , 1.51880562],
       [0.        , 1.51889586],
       [0.        , 1.51903677],
       [0.        , 1.51931715],
       [0.        , 1.51957738],
       [0.        , 1.51958048],
       [0.        , 1.51983011],
       [0.        , 1.51992285],
       [0.        , 1.51993847],
       [0.        , 1.52001691],
       [0.        , 1.52005696],
       [0.        , 1.52027619],
       [0.        , 1.52029455],
       [0.        , 1.52030456],
       [0.        , 1.52052593],
       [0.        , 1.52061772],
       [0.        , 1.52076483],
       [0.        , 1.5207727 ],
       [0.        , 1.5208292 ],
       [0.        , 1.52087855],
       [0.        , 1.52097583],
       [0.        , 1.52118087],
       [0.        , 1.52118754],
       [0.        , 1.52132857],
       [0.        , 1.52134705],
       [0.        , 1.52147341],
       [0.        , 1.52153146],
       [0.        , 1.52154207],
       [0.        , 1.52175999],
       [0.        , 1.52176809],
       [0.        , 1.52179086],
       [0.        , 1.5217998 ],
       [0.        , 1.52182961],
       [0.        , 1.52202511],
       [0.        , 1.52204359],
       [0.        , 1.52234638],
       [0.        , 1.52242839],
       [0.        , 1.52245128],
       [0.        , 1.52249992],
       [0.        , 1.52255368],
       [0.        , 1.52259481],
       [0.        , 1.52274704],
       [0.        , 1.52282965],
       [0.        , 1.52285838],
       [0.        , 1.52300453],
       [0.        , 1.52308834],
       [0.        , 1.52316248],
       [0.        , 1.52355373],
       [0.        , 1.52359998],
       [0.        , 1.52367425],
       [0.        , 1.52373993],
       [0.        , 1.52391648],
       [0.        , 1.52414703],
       [0.        , 1.52417588],
       [0.        , 1.5243876 ],
       [0.        , 1.5246278 ],
       [0.        , 1.52463531],
       [0.        , 1.52470803],
       [0.        , 1.52475762],
       [0.        , 1.52485383],
       [0.        , 1.52495587],
       [0.        , 1.52503419],
       [0.        , 1.52507317],
       [0.        , 1.52529907],
       [0.        , 1.52546787],
       [0.        , 1.52558303],
       [0.        , 1.52561057],
       [0.        , 1.52594793],
       [0.        , 1.52597535],
       [0.        , 1.52603495],
       [0.        , 1.52613175],
       [0.        , 1.52640498],
       [0.        , 1.52642953],
       [0.        , 1.52655315],
       [0.        , 1.52662492],
       [0.        , 1.5266763 ],
       [0.        , 1.52667654],
       [0.        , 1.52679038],
       [0.        , 1.52683437],
       [0.        , 1.52803481],
       [0.        , 1.52834451],
       [0.        , 1.5286597 ],
       [0.        , 1.52882218],
       [0.        , 1.52909076],
       [0.        , 1.52929997],
       [0.        , 1.52946746],
       [0.        , 1.5298667 ],
       [0.        , 1.53053033],
       [0.        , 1.53112721],
       [0.        , 1.53343976],
       [0.        , 1.5435425 ]]), array([[8.72641277, 9.29641056],
       [8.63057423, 9.07885361],
       [8.47445679, 8.56348133],
       [7.37273216, 7.80494928],
       [7.24514246, 7.35807562],
       [7.20961857, 7.5750246 ],
       [7.09314156, 7.40845203],
       [7.07907248, 7.78084373],
       [6.90255547, 7.6069293 ],
       [6.58149147, 6.92429399],
       [6.30823898, 7.671525  ],
       [6.20571804, 7.62969542],
       [5.42177963, 8.65867329],
       [5.37896919, 5.82983065],
       [5.36298704, 6.42318821],
       [5.0970974 , 8.44625187],
       [4.5817852 , 5.80219507],
       [4.15186167, 8.71192837],
       [4.12986708, 4.19993591],
       [4.10820198, 8.20780373],
       [3.94427156, 3.96570015],
       [3.88519716, 4.08941126],
       [3.70234179, 3.72173023]]), array([[ 9.56096077, 10.33926868],
       [ 9.52574348, 10.44573212],
       [ 8.96875381, 10.0476799 ]])]2024-03-06 18:05:52.195090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0

Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6LU6 ph vector generated, counter: 261
2024-03-06 18:05:56.053726: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:05:56.187966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:05:57.237314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUB ph vector generated, counter: 262
2024-03-06 18:06:00.618843: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:00.661645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:01.586882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUD ph vector generated, counter: 263
2024-03-06 18:06:04.961530: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:05.004246: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:05.901483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUR ph vector generated, counter: 264
2024-03-06 18:06:09.581366: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:09.624976: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:10.829783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUU ph vector generated, counter: 265
2024-03-06 18:06:14.718501: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:14.762645: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:15.801096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUV ph vector generated, counter: 266
2024-03-06 18:06:19.470092: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:19.512864: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:20.519882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUW ph vector generated, counter: 267
2024-03-06 18:06:23.830480: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:23.874306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:24.884246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUX ph vector generated, counter: 268
2024-03-06 18:06:28.604403: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:28.647058: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:29.948077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUY ph vector generated, counter: 269
2024-03-06 18:06:33.435092: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:33.478435: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:34.562321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LUZ ph vector generated, counter: 270
2024-03-06 18:06:37.917818: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:37.961307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:39.056744: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV1 ph vector generated, counter: 271
2024-03-06 18:06:42.492911: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:42.535777: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:43.700906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV2 ph vector generated, counter: 272
2024-03-06 18:06:47.406796: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:47.479436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:48.760180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV3 ph vector generated, counter: 273
2024-03-06 18:06:53.560103: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:53.603222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:54.794400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV4 ph vector generated, counter: 274
2024-03-06 18:06:57.932421: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:06:57.975556: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:06:59.173984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV5 ph vector generated, counter: 275
2024-03-06 18:07:02.499380: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:02.541993: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:03.619145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV6 ph vector generated, counter: 276
2024-03-06 18:07:07.030236: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:07.073421: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:08.073859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV7 ph vector generated, counter: 277
2024-03-06 18:07:11.416855: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:11.459897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:12.491299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV8 ph vector generated, counter: 278
2024-03-06 18:07:15.976848: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:16.019438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:17.185111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LV9 ph vector generated, counter: 279
2024-03-06 18:07:20.654614: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:20.702805: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:21.607757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LVA ph vector generated, counter: 280
2024-03-06 18:07:25.159174: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:25.201894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:26.149653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LVK ph vector generated, counter: 281
2024-03-06 18:07:29.425366: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:29.468167: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:30.433443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LVQ ph vector generated, counter: 282
2024-03-06 18:07:33.882639: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:33.927066: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:35.186660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LW2 ph vector generated, counter: 283
2024-03-06 18:07:38.697716: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:38.740974: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:39.763479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LWD ph vector generated, counter: 284
2024-03-06 18:07:43.338387: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:43.382990: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:44.393189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LX4 ph vector generated, counter: 285
2024-03-06 18:07:47.797647: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:47.840747: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:48.989578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LX5 ph vector generated, counter: 286
2024-03-06 18:07:52.771040: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:52.813732: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:54.128490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LX6 ph vector generated, counter: 287
2024-03-06 18:07:58.098068: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:07:58.149177: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:07:59.291928: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LX7 ph vector generated, counter: 288
2024-03-06 18:08:03.328878: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:03.371459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:04.577532: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LX8 ph vector generated, counter: 289
2024-03-06 18:08:08.863953: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:08.906800: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:09.875872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LX9 ph vector generated, counter: 290
2024-03-06 18:08:13.170025: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:13.235208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:14.305102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LXA ph vector generated, counter: 291
2024-03-06 18:08:17.681241: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:17.723999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:18.814072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LXB ph vector generated, counter: 292
2024-03-06 18:08:22.309584: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:22.352628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:23.565349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LXC ph vector generated, counter: 293
2024-03-06 18:08:27.087171: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:27.132033: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:28.269053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LZB ph vector generated, counter: 294
2024-03-06 18:08:31.805124: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:31.848315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:32.786215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LZC ph vector generated, counter: 295
2024-03-06 18:08:36.390309: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:36.433381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:37.607759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6LZZ ph vector generated, counter: 296
2024-03-06 18:08:40.799499: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:40.842279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:41.945088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M06 ph vector generated, counter: 297
2024-03-06 18:08:45.480298: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:45.524451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:46.642697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M08 ph vector generated, counter: 298
2024-03-06 18:08:50.418420: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:50.461204: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:51.678008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M0J ph vector generated, counter: 299
2024-03-06 18:08:55.577168: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:08:55.642607: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:08:56.737738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M14 ph vector generated, counter: 300
2024-03-06 18:09:00.493797: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:00.536942: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:01.541148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M1K ph vector generated, counter: 301
2024-03-06 18:09:04.909029: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:04.951873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:05.981420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M24 ph vector generated, counter: 302
2024-03-06 18:09:09.392205: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:09.434598: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:10.458372: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M2B ph vector generated, counter: 303
2024-03-06 18:09:13.668593: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:13.711880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:14.780506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M2D ph vector generated, counter: 304
2024-03-06 18:09:18.139449: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:18.200936: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:19.133588: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M2J ph vector generated, counter: 305
2024-03-06 18:09:22.555250: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:22.598185: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:23.573703: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M3B ph vector generated, counter: 306
2024-03-06 18:09:27.082976: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:27.126528: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:28.095215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M3I ph vector generated, counter: 307
2024-03-06 18:09:31.669767: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:31.712599: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:32.783830: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M4I ph vector generated, counter: 308
2024-03-06 18:09:36.025329: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:36.067935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:37.132577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M4U ph vector generated, counter: 309
2024-03-06 18:09:40.771963: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:40.815572: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:42.055458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M5O ph vector generated, counter: 310
2024-03-06 18:09:45.726615: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:45.769523: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:46.849619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M60 ph vector generated, counter: 311
2024-03-06 18:09:50.274607: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:50.318086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:51.450646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M61 ph vector generated, counter: 312
2024-03-06 18:09:54.995630: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:55.038877: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:09:56.072491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6M64 ph vector generated, counter: 313
2024-03-06 18:09:59.908615: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:09:59.951914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:00.931809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6MNX ph vector generated, counter: 314
2024-03-06 18:10:04.635934: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:04.678579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:05.696979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6MQT ph vector generated, counter: 315
2024-03-06 18:10:09.074817: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:09.118556: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:10.303382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6MS9 ph vector generated, counter: 316
2024-03-06 18:10:13.709073: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:13.752091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:14.778652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6MTA ph vector generated, counter: 317
2024-03-06 18:10:18.092126: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:18.135031: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:19.191870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NBE ph vector generated, counter: 318
2024-03-06 18:10:22.606981: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:22.652077: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:23.713000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NID ph vector generated, counter: 319
2024-03-06 18:10:27.018482: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:27.061605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:28.279058: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NKR ph vector generated, counter: 320
2024-03-06 18:10:31.466507: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:31.509381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:32.674658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NLQ ph vector generated, counter: 321
2024-03-06 18:10:35.924355: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:35.967689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:36.883582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NLV ph vector generated, counter: 322
2024-03-06 18:10:40.215851: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:40.258848: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:41.433673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NLY ph vector generated, counter: 323
2024-03-06 18:10:44.893077: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:44.935566: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:45.960520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NM0 ph vector generated, counter: 324
2024-03-06 18:10:49.443899: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:49.489969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:50.697724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NQ5 ph vector generated, counter: 325
2024-03-06 18:10:54.557552: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:54.600431: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:10:55.656285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NR0 ph vector generated, counter: 326
2024-03-06 18:10:59.310965: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:10:59.359045: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:00.352257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NRY ph vector generated, counter: 327
2024-03-06 18:11:04.043032: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:04.086124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:04.970246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NTH ph vector generated, counter: 328
2024-03-06 18:11:09.384099: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:09.433113: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:10.437803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NTK ph vector generated, counter: 329
2024-03-06 18:11:14.069155: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:14.112598: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:15.156060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NTL ph vector generated, counter: 330
2024-03-06 18:11:18.796678: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:18.839488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:19.846009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NTO ph vector generated, counter: 331
2024-03-06 18:11:24.022714: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:24.065349: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:25.053971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NTP ph vector generated, counter: 332
2024-03-06 18:11:28.670328: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:28.713667: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:29.642990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NTU ph vector generated, counter: 333
2024-03-06 18:11:33.021386: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:33.064003: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:34.020709: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NV2 ph vector generated, counter: 334
2024-03-06 18:11:37.484226: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:37.533058: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:38.560946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6NWV ph vector generated, counter: 335
2024-03-06 18:11:42.047931: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:42.090740: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:43.300358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Persistence pairs: {0: array([(0., 1.2984483), (0., 1.3058386), (0., 1.3069553), (0., 1.3081881),
       (0., 1.3096472), (0., 1.3111578), (0., 1.311488 ), (0., 1.3129338),
       (0., 1.3163599), (0., 1.3181725), (0., 1.3190428), (0., 1.3199229),
       (0., 1.321463 ), (0., 1.321671 ), (0., 1.322071 ), (0., 1.3220803),
       (0., 1.3224368), (0., 1.3230286), (0., 1.323489 ), (0., 1.3238724),
       (0., 1.3246516), (0., 1.3249412), (0., 1.3264534), (0., 1.3268044),
       (0., 1.3278308), (0., 1.3278412), (0., 1.3281473), (0., 1.3294067),
       (0., 1.3299364), (0., 1.3305949), (0., 1.3309176), (0., 1.3311402),
       (0., 1.3312323), (0., 1.3318928), (0., 1.3324513), (0., 1.3330239),
       (0., 1.3331264), (0., 1.3358421), (0., 1.3360622), (0., 1.3369601),
       (0., 1.3378896), (0., 1.3389477), (0., 1.3433686), (0., 1.344911 ),
       (0., 1.346932 ), (0., 1.3500696), (0., 1.3603041), (0., 1.3638215),
       (0., 1.4549125), (0., 1.4568441), (0., 1.4569978), (0., 1.4572648),
       (0., 1.4588081), (0., 1.459102 ), (0., 1.4596248), (0., 1.4619915),
       (0., 1.4644849), (0., 1.4651511), (0., 1.4666977), (0., 1.4683934),
       (0., 1.4696147), (0., 1.4717845), (0., 1.4725201), (0., 1.4738963),
       (0., 1.4760804), (0., 1.4786673), (0., 1.4788507), (0., 1.4791896),
       (0., 1.4796162), (0., 1.4801315), (0., 1.4802388), (0., 1.4804716),
       (0., 1.4818512), (0., 1.4819965), (0., 1.4821677), (0., 1.4828738),
       (0., 1.4833544), (0., 1.4835346), (0., 1.4837608), (0., 1.4838518),
       (0., 1.4846308), (0., 1.4856921), (0., 1.4862386), (0., 1.4863108),
       (0., 1.4866654), (0., 1.4874736), (0., 1.4876258), (0., 1.4876577),
       (0., 1.4896067), (0., 1.4896756), (0., 1.4896927), (0., 1.4899414),
       (0., 1.4905199), (0., 1.4920444), (0., 1.4933437), (0., 1.493904 ),
       (0., 1.4984286), (0., 1.4999179), (0., 1.5007142), (0., 1.50123  ),
       (0., 1.5063378), (0., 1.5063806), (0., 1.510807 ), (0., 1.5120157),
       (0., 1.5131161), (0., 1.5156544), (0., 1.5158556), (0., 1.5159584),
       (0., 1.5170726), (0., 1.5175749), (0., 1.518205 ), (0., 1.518575 ),
       (0., 1.5190704), (0., 1.5200009), (0., 1.5211799), (0., 1.5223064),
       (0., 1.5230763), (0., 1.5230974), (0., 1.5239648), (0., 1.5262638),
       (0., 1.5273209), (0., 1.5279207), (0., 1.5288893), (0., 1.5288994),
       (0., 1.5302896), (0., 1.5319989), (0., 1.5321113), (0., 1.5324703),
       (0., 1.5324781), (0., 1.5328579), (0., 1.5333644), (0., 1.5338562),
       (0., 1.5347241), (0., 1.5353163), (0., 1.5353918), (0., 1.5358571),
       (0., 1.5361165), (0., 1.5362494), (0., 1.5366819), (0., 1.5388405),
       (0., 1.5398632), (0., 1.5401074), (0., 1.5411664), (0., 1.5412542),
       (0., 1.5429436), (0., 1.543092 ), (0., 1.5440909), (0., 1.5523002),
       (0., 3.8211393)], dtype=[('birth', '<f4'), ('death', '<f4')]), 1: array([(7.5184975, 7.830173 ), (7.1113057, 8.186932 ),
       (6.497494 , 7.7747936), (6.258245 , 7.0680766),
       (5.5745573, 5.870914 ), (5.2873583, 8.273104 ),
       (5.0640883, 5.0875683), (4.5224104, 4.7492075),
       (4.3569875, 4.525617 ), (4.177534 , 4.23058  ),
       (4.1423473, 4.3061767), (3.915348 , 3.9814954),
       (3.8833895, 4.047399 ), (3.8472457, 4.8321414),
       (3.838292 , 8.127538 ), (3.778885 , 4.432548 )],
      dtype=[('birth', '<f4'), ('death', '<f4')]), 2: array([(8.824028, 9.370985 ), (8.704136, 8.9519825),
       (7.116996, 7.1302314), (4.554322, 4.7116747)],
      dtype=[('birth', '<f4'), ('death', '<f4')])}
Result dictionary stored.
[[(0.0, 1.2984483242034912), (0.0, 1.3058385848999023), (0.0, 1.306955337524414), (0.0, 1.3081880807876587), (0.0, 1.3096472024917603), (0.0, 1.3111578226089478), (0.0, 1.3114880323410034), (0.0, 1.3129338026046753), (0.0, 1.3163598775863647), (0.0, 1.3181724548339844), (0.0, 1.3190428018569946), (0.0, 1.319922924041748), (0.0, 1.3214629888534546), (0.0, 1.3216710090637207), (0.0, 1.3220709562301636), (0.0, 1.3220802545547485), (0.0, 1.322436809539795), (0.0, 1.323028564453125), (0.0, 1.3234889507293701), (0.0, 1.323872447013855), (0.0, 1.3246515989303589), (0.0, 1.3249411582946777), (0.0, 1.326453447341919), (0.0, 1.3268043994903564), (0.0, 1.3278307914733887), (0.0, 1.3278411626815796), (0.0, 1.328147292137146), (0.0, 1.32940673828125), (0.0, 1.3299363851547241), (0.0, 1.3305948972702026), (0.0, 1.3309175968170166), (0.0, 1.331140160560608), (0.0, 1.3312323093414307), (0.0, 1.3318928480148315), (0.0, 1.332451343536377), (0.0, 1.3330239057540894), (0.0, 1.333126425743103), (0.0, 1.3358421325683594), (0.0, 1.3360621929168701), (0.0, 1.3369600772857666), (0.0, 1.337889552116394), (0.0, 1.3389476537704468), (0.0, 1.343368649482727), (0.0, 1.344910979270935), (0.0, 1.346932053565979), (0.0, 1.3500696420669556), (0.0, 1.3603041172027588), (0.0, 1.3638215065002441), (0.0, 1.454912543296814), (0.0, 1.4568440914154053), (0.0, 1.4569977521896362), (0.0, 1.45726478099823), (0.0, 1.4588080644607544), (0.0, 1.4591020345687866), (0.0, 1.4596247673034668), (0.0, 1.461991548538208), (0.0, 1.4644849300384521), (0.0, 1.465151071548462), (0.0, 1.4666976928710938), (0.0, 1.4683934450149536), (0.0, 1.4696147441864014), (0.0, 1.4717844724655151), (0.0, 1.472520112991333), (0.0, 1.4738962650299072), (0.0, 1.4760804176330566), (0.0, 1.4786672592163086), (0.0, 1.4788507223129272), (0.0, 1.4791896343231201), (0.0, 1.4796161651611328), (0.0, 1.4801315069198608), (0.0, 1.4802387952804565), (0.0, 1.4804716110229492), (0.0, 1.4818512201309204), (0.0, 1.4819965362548828), (0.0, 1.4821677207946777), (0.0, 1.482873797416687), (0.0, 1.4833544492721558), (0.0, 1.483534574508667), (0.0, 1.4837608337402344), (0.0, 1.4838517904281616), (0.0, 1.484630823135376), (0.0, 1.4856921434402466), (0.0, 1.4862385988235474), (0.0, 1.4863108396530151), (0.0, 1.4866653680801392), (0.0, 1.4874736070632935), (0.0, 1.4876258373260498), (0.0, 1.4876576662063599), (0.0, 1.4896067380905151), (0.0, 1.4896756410598755), (0.0, 1.4896926879882812), (0.0, 1.4899413585662842), (0.0, 1.4905198812484741), (0.0, 1.492044448852539), (0.0, 1.493343710899353), (0.0, 1.4939039945602417), (0.0, 1.4984285831451416), (0.0, 1.4999178647994995), (0.0, 1.5007141828536987), (0.0, 1.501230001449585), (0.0, 1.5063377618789673), (0.0, 1.506380558013916), (0.0, 1.5108070373535156), (0.0, 1.512015700340271), (0.0, 1.5131161212921143), (0.0, 1.515654444694519), (0.0, 1.5158555507659912), (0.0, 1.5159584283828735), (0.0, 1.5170725584030151), (0.0, 1.5175749063491821), (0.0, 1.5182050466537476), (0.0, 1.5185749530792236), (0.0, 1.5190703868865967), (0.0, 1.52000093460083), (0.0, 1.5211799144744873), (0.0, 1.5223064422607422), (0.0, 1.5230762958526611), (0.0, 1.5230973958969116), (0.0, 1.523964762687683), (0.0, 1.5262638330459595), (0.0, 1.5273208618164062), (0.0, 1.5279207229614258), (0.0, 1.5288892984390259), (0.0, 1.5288994312286377), (0.0, 1.530289649963379), (0.0, 1.531998872756958), (0.0, 1.5321112871170044), (0.0, 1.5324703454971313), (0.0, 1.5324780941009521), (0.0, 1.532857894897461), (0.0, 1.5333644151687622), (0.0, 1.5338561534881592), (0.0, 1.5347241163253784), (0.0, 1.5353163480758667), (0.0, 1.5353918075561523), (0.0, 1.535857081413269), (0.0, 1.5361164808273315), (0.0, 1.5362493991851807), (0.0, 1.536681890487671), (0.0, 1.5388405323028564), (0.0, 1.5398632287979126), (0.0, 1.5401073694229126), (0.0, 1.5411664247512817), (0.0, 1.5412541627883911), (0.0, 1.5429435968399048), (0.0, 1.5430920124053955), (0.0, 1.5440908670425415), (0.0, 1.552300214767456), (0.0, 3.821139335632324)], [(7.518497467041016, 7.830173015594482), (7.1113057136535645, 8.186931610107422), (6.497494220733643, 7.77479362487793), (6.25824499130249, 7.0680766105651855), (5.574557304382324, 5.870913982391357), (5.287358283996582, 8.273103713989258), (5.064088344573975, 5.087568283081055), (4.5224103927612305, 4.749207496643066), (4.356987476348877, 4.5256171226501465), (4.177534103393555, 4.230579853057861), (4.14234733581543, 4.306176662445068), (3.9153480529785156, 3.9814953804016113), (3.883389472961426, 4.047399044036865), (3.847245693206787, 4.832141399383545), (3.838291883468628, 8.127537727355957), (3.7788848876953125, 4.4325480461120605)], [(8.824028015136719, 9.37098503112793), (8.70413589477539, 8.951982498168945), (7.116995811462402, 7.1302313804626465), (4.554321765899658, 4.711674690246582)]]
2024-03-06 18:11:46.652204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 728 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0
[array([[0.        , 1.29844832],
       [0.        , 1.30583858],
       [0.        , 1.30695534],
       [0.        , 1.30818808],
       [0.        , 1.3096472 ],
       [0.        , 1.31115782],
       [0.        , 1.31148803],
       [0.        , 1.3129338 ],
       [0.        , 1.31635988],
       [0.        , 1.31817245],
       [0.        , 1.3190428 ],
       [0.        , 1.31992292],
       [0.        , 1.32146299],
       [0.        , 1.32167101],
       [0.        , 1.32207096],
       [0.        , 1.32208025],
       [0.        , 1.32243681],
       [0.        , 1.32302856],
       [0.        , 1.32348895],
       [0.        , 1.32387245],
       [0.        , 1.3246516 ],
       [0.        , 1.32494116],
       [0.        , 1.32645345],
       [0.        , 1.3268044 ],
       [0.        , 1.32783079],
       [0.        , 1.32784116],
       [0.        , 1.32814729],
       [0.        , 1.32940674],
       [0.        , 1.32993639],
       [0.        , 1.3305949 ],
       [0.        , 1.3309176 ],
       [0.        , 1.33114016],
       [0.        , 1.33123231],
       [0.        , 1.33189285],
       [0.        , 1.33245134],
       [0.        , 1.33302391],
       [0.        , 1.33312643],
       [0.        , 1.33584213],
       [0.        , 1.33606219],
       [0.        , 1.33696008],
       [0.        , 1.33788955],
       [0.        , 1.33894765],
       [0.        , 1.34336865],
       [0.        , 1.34491098],
       [0.        , 1.34693205],
       [0.        , 1.35006964],
       [0.        , 1.36030412],
       [0.        , 1.36382151],
       [0.        , 1.45491254],
       [0.        , 1.45684409],
       [0.        , 1.45699775],
       [0.        , 1.45726478],
       [0.        , 1.45880806],
       [0.        , 1.45910203],
       [0.        , 1.45962477],
       [0.        , 1.46199155],
       [0.        , 1.46448493],
       [0.        , 1.46515107],
       [0.        , 1.46669769],
       [0.        , 1.46839345],
       [0.        , 1.46961474],
       [0.        , 1.47178447],
       [0.        , 1.47252011],
       [0.        , 1.47389627],
       [0.        , 1.47608042],
       [0.        , 1.47866726],
       [0.        , 1.47885072],
       [0.        , 1.47918963],
       [0.        , 1.47961617],
       [0.        , 1.48013151],
       [0.        , 1.4802388 ],
       [0.        , 1.48047161],
       [0.        , 1.48185122],
       [0.        , 1.48199654],
       [0.        , 1.48216772],
       [0.        , 1.4828738 ],
       [0.        , 1.48335445],
       [0.        , 1.48353457],
       [0.        , 1.48376083],
       [0.        , 1.48385179],
       [0.        , 1.48463082],
       [0.        , 1.48569214],
       [0.        , 1.4862386 ],
       [0.        , 1.48631084],
       [0.        , 1.48666537],
       [0.        , 1.48747361],
       [0.        , 1.48762584],
       [0.        , 1.48765767],
       [0.        , 1.48960674],
       [0.        , 1.48967564],
       [0.        , 1.48969269],
       [0.        , 1.48994136],
       [0.        , 1.49051988],
       [0.        , 1.49204445],
       [0.        , 1.49334371],
       [0.        , 1.49390399],
       [0.        , 1.49842858],
       [0.        , 1.49991786],
       [0.        , 1.50071418],
       [0.        , 1.50123   ],
       [0.        , 1.50633776],
       [0.        , 1.50638056],
       [0.        , 1.51080704],
       [0.        , 1.5120157 ],
       [0.        , 1.51311612],
       [0.        , 1.51565444],
       [0.        , 1.51585555],
       [0.        , 1.51595843],
       [0.        , 1.51707256],
       [0.        , 1.51757491],
       [0.        , 1.51820505],
       [0.        , 1.51857495],
       [0.        , 1.51907039],
       [0.        , 1.52000093],
       [0.        , 1.52117991],
       [0.        , 1.52230644],
       [0.        , 1.5230763 ],
       [0.        , 1.5230974 ],
       [0.        , 1.52396476],
       [0.        , 1.52626383],
       [0.        , 1.52732086],
       [0.        , 1.52792072],
       [0.        , 1.5288893 ],
       [0.        , 1.52889943],
       [0.        , 1.53028965],
       [0.        , 1.53199887],
       [0.        , 1.53211129],
       [0.        , 1.53247035],
       [0.        , 1.53247809],
       [0.        , 1.53285789],
       [0.        , 1.53336442],
       [0.        , 1.53385615],
       [0.        , 1.53472412],
       [0.        , 1.53531635],
       [0.        , 1.53539181],
       [0.        , 1.53585708],
       [0.        , 1.53611648],
       [0.        , 1.5362494 ],
       [0.        , 1.53668189],
       [0.        , 1.53884053],
       [0.        , 1.53986323],
       [0.        , 1.54010737],
       [0.        , 1.54116642],
       [0.        , 1.54125416],
       [0.        , 1.5429436 ],
       [0.        , 1.54309201],
       [0.        , 1.54409087],
       [0.        , 1.55230021],
       [0.        , 3.82113934]]), array([[7.51849747, 7.83017302],
       [7.11130571, 8.18693161],
       [6.49749422, 7.77479362],
       [6.25824499, 7.06807661],
       [5.5745573 , 5.87091398],
       [5.28735828, 8.27310371],
       [5.06408834, 5.08756828],
       [4.52241039, 4.7492075 ],
       [4.35698748, 4.52561712],
       [4.1775341 , 4.23057985],
       [4.14234734, 4.30617666],
       [3.91534805, 3.98149538],
       [3.88338947, 4.04739904],
       [3.84724569, 4.8321414 ],
       [3.83829188, 8.12753773],
       [3.77888489, 4.43254805]]), array([[8.82402802, 9.37098503],
       [8.70413589, 8.9519825 ],
       [7.11699581, 7.13023138],
       [4.55432177, 4.71167469]])]
Gradient is  tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(3000,), dtype=float32)
Vectors stored.
6O17 ph vector generated, counter: 336
2024-03-06 18:11:50.145116: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:50.188880: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:51.254221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O31 ph vector generated, counter: 337
2024-03-06 18:11:54.737076: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:54.779829: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:11:55.844461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O33 ph vector generated, counter: 338
2024-03-06 18:11:59.846457: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:11:59.889875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:01.204360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O34 ph vector generated, counter: 339
2024-03-06 18:12:05.106697: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:05.150079: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:06.107575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O36 ph vector generated, counter: 340
2024-03-06 18:12:10.037542: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:10.081104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:10.996172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O46 ph vector generated, counter: 341
2024-03-06 18:12:14.592692: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:14.635204: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:15.582287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O4O ph vector generated, counter: 342
2024-03-06 18:12:19.400580: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:19.443797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:20.640416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O4Y ph vector generated, counter: 343
2024-03-06 18:12:24.150919: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:24.193883: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:25.205111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O4Z ph vector generated, counter: 344
2024-03-06 18:12:28.763273: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:28.807143: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:29.906362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O51 ph vector generated, counter: 345
2024-03-06 18:12:33.668857: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:33.711466: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:34.902316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O53 ph vector generated, counter: 346
2024-03-06 18:12:38.818565: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:38.862101: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:39.910824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O5I ph vector generated, counter: 347
2024-03-06 18:12:43.388425: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:43.439449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:44.392182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Failed: Cuda error /gpfs/gibbs/pi/gerstein/as4272/KnotFun/ripser-plusplus/./ripserplusplus/ripser++.cu:3285 'out of memory'
6O5O ph vector generated, counter: 348
2024-03-06 18:12:47.935640: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-03-06 18:12:47.978464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-06 18:12:49.089001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
